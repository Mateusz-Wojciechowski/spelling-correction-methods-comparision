correct_text,augmented_text,symspell_correction,bert_correction,lstm_cnn_correction
"Oh, how the headlines blared:
Chatbots were The Next Big Thing.
Our hopes were sky high. Bright-eyed and bushy-tailed, the industry was ripe for a new era of innovation: it was time to start socializing with machines.
And why wouldn’t they be? All the road signs pointed towards insane success.
At the Mobile World Congress 2017, chatbots were the main headliners. The conference organizers cited an ‘overwhelming acceptance at the event of the inevitable shift of focus for brands and corporates to chatbots’.
In fact, the only significant question around chatbots was who would monopolize the field, not whether chatbots would take off in the first place:
One year on, we have an answer to that question.
No.
Because there isn’t even an ecosystem for a platform to dominate.
Chatbots weren’t the first technological development to be talked up in grandiose terms and then slump spectacularly.
The age-old hype cycle unfolded in familiar fashion...
Expectations built, built, and then..... It all kind of fizzled out.
The predicted paradim shift didn’t materialize.
And apps are, tellingly, still alive and well.
We look back at our breathless optimism and turn to each other, slightly baffled:
“is that it? THAT was the chatbot revolution we were promised?”
Digit’s Ethan Bloch sums up the general consensus:
According to Dave Feldman, Vice President of Product Design at Heap, chatbots didn’t just take on one difficult problem and fail: they took on several and failed all of them.
Bots can interface with users in different ways. The big divide is text vs. speech. In the beginning (of computer interfaces) was the (written) word.
Users had to type commands manually into a machine to get anything done.
Then, graphical user interfaces (GUIs) came along and saved the day. We became entranced by windows, mouse clicks, icons. And hey, we eventually got color, too!
Meanwhile, a bunch of research scientists were busily developing natural language (NL) interfaces to databases, instead of having to learn an arcane database query language.
Another bunch of scientists were developing speech-processing software so that you could just speak to your computer, rather than having to type. This turned out to be a whole lot more difficult than anyone originally realised:
The next item on the agenda was holding a two-way dialog with a machine. Here’s an example dialog (dating back to the 1990s) with VCR setup system:
Pretty cool, right? The system takes turns in collaborative way, and does a smart job of figuring out what the user wants.
It was carefully crafted to deal with conversations involving VCRs, and could only operate within strict limitations.
Modern day bots, whether they use typed or spoken input, have to face all these challenges, but also work in an efficient and scalable way on a variety of platforms.
Basically, we’re still trying to achieve the same innovations we were 30 years ago.
Here’s where I think we’re going wrong:
An oversized assumption has been that apps are ‘over’, and would be replaced by bots.
By pitting two such disparate concepts against one another (instead of seeing them as separate entities designed to serve different purposes) we discouraged bot development.
You might remember a similar war cry when apps first came onto the scene ten years ago: but do you remember when apps replaced the internet?
It’s said that a new product or service needs to be two of the following: better, cheaper, or faster. Are chatbots cheaper or faster than apps? No — not yet, at least.
Whether they’re ‘better’ is subjective, but I think it’s fair to say that today’s best bot isn’t comparable to today’s best app.
Plus, nobody thinks that using Lyft is too complicated, or that it’s too hard to order food or buy a dress on an app. What is too complicated is trying to complete these tasks with a bot — and having the bot fail.
A great bot can be about as useful as an average app. When it comes to rich, sophisticated, multi-layered apps, there’s no competition.
That’s because machines let us access vast and complex information systems, and the early graphical information systems were a revolutionary leap forward in helping us locate those systems.
Modern-day apps benefit from decades of research and experimentation. Why would we throw this away?
But, if we swap the word ‘replace’ with ‘extend’, things get much more interesting.
Today’s most successful bot experiences take a hybrid approach, incorporating chat into a broader strategy that encompasses more traditional elements.
The next wave will be multimodal apps, where you can say what you want (like with Siri) and get back information as a map, text, or even a spoken response.
Another problematic aspect of the sweeping nature of hype is that it tends to bypass essential questions like these.
For plenty of companies, bots just aren’t the right solution. The past two years are littered with cases of bots being blindly applied to problems where they aren’t needed.
Building a bot for the sake of it, letting it loose and hoping for the best will never end well:
The vast majority of bots are built using decision-tree logic, where the bot’s canned response relies on spotting specific keywords in the user input.
The advantage of this approach is that it’s pretty easy to list all the cases that they are designed to cover. And that’s precisely their disadvantage, too.
That’s because these bots are purely a reflection of the capability, fastidiousness and patience of the person who created them; and how many user needs and inputs they were able to anticipate.
Problems arise when life refuses to fit into those boxes.
According to recent reports, 70% of the 100,000+ bots on Facebook Messenger are failing to fulfil simple user requests. This is partly a result of developers failing to narrow their bot down to one strong area of focus.
When we were building GrowthBot, we decided to make it specific to sales and marketers: not an ‘all-rounder’, despite the temptation to get overexcited about potential capabilties.
Remember: a bot that does ONE thing well is infinitely more helpful than a bot that does multiple things poorly.
A competent developer can build a basic bot in minutes — but one that can hold a conversation? That’s another story. Despite the constant hype around AI, we’re still a long way from achieving anything remotely human-like.
In an ideal world, the technology known as NLP (natural language processing) should allow a chatbot to understand the messages it receives. But NLP is only just emerging from research labs and is very much in its infancy.
Some platforms provide a bit of NLP, but even the best is at toddler-level capacity (for example, think about Siri understanding your words, but not their meaning.)
As Matt Asay outlines, this results in another issue: failure to capture the attention and creativity of developers.
And conversations are complex. They’re not linear. Topics spin around each other, take random turns, restart or abruptly finish.
Today’s rule-based dialogue systems are too brittle to deal with this kind of unpredictability, and statistical approaches using machine learning are just as limited. The level of AI required for human-like conversation just isn’t available yet.
And in the meantime, there are few high-quality examples of trailblazing bots to lead the way. As Dave Feldman remarked:
Once upon a time, the only way to interact with computers was by typing arcane commands to the terminal. Visual interfaces using windows, icons or a mouse were a revolution in how we manipulate information
There’s a reasons computing moved from text-based to graphical user interfaces (GUIs). On the input side, it’s easier and faster to click than it is to type.
Tapping or selecting is obviously preferable to typing out a whole sentence, even with predictive (often error-prone ) text. On the output side, the old adage that a picture is worth a thousand words is usually true.
We love optical displays of information because we are highly visual creatures. It’s no accident that kids love touch screens. The pioneers who dreamt up graphical interface were inspired by cognitive psychology, the study of how the brain deals with communication.
Conversational UIs are meant to replicate the way humans prefer to communicate, but they end up requiring extra cognitive effort. Essentially, we’re swapping something simple for a more-complex alternative.
Sure, there are some concepts that we can only express using language (“show me all the ways of getting to a museum that give me 2000 steps but don’t take longer than 35 minutes”), but most tasks can be carried out more efficiently and intuitively with GUIs than with a conversational UI.
Aiming for a human dimension in business interactions makes sense.
If there’s one thing that’s broken about sales and marketing, it’s the lack of humanity: brands hide behind ticket numbers, feedback forms, do-not-reply-emails, automated responses and gated ‘contact us’ forms.
Facebook’s goal is that their bots should pass the so-called Turing Test, meaning you can’t tell whether you are talking to a bot or a human. But a bot isn’t the same as a human. It never will be.
A conversation encompasses so much more than just text.
Humans can read between the lines, leverage contextual information and understand double layers like sarcasm. Bots quickly forget what they’re talking about, meaning it’s a bit like conversing with someone who has little or no short-term memory.
As HubSpot team pinpointed:
People aren’t easily fooled, and pretending a bot is a human is guaranteed to diminish returns (not to mention the fact that you’re lying to your users).
And even those rare bots that are powered by state-of-the-art NLP, and excel at processing and producing content, will fall short in comparison.
And here’s the other thing. Conversational UIs are built to replicate the way humans prefer to communicate — with other humans.
But is that how humans prefer to interact with machines?
Not necessarily.
At the end of the day, no amount of witty quips or human-like mannerisms will save a bot from conversational failure.
In a way, those early-adopters weren’t entirely wrong.
People are yelling at Google Home to play their favorite song, ordering pizza from the Domino’s bot and getting makeup tips from Sephora. But in terms of consumer response and developer involvement, chatbots haven’t lived up to the hype generated circa 2015/16.
Not even close.
Computers are good at being computers. Searching for data, crunching numbers, analyzing opinions and condensing that information.
Computers aren’t good at understanding human emotion. The state of NLP means they still don’t ‘get’ what we’re asking them, never mind how we feel.
That’s why it’s still impossible to imagine effective customer support, sales or marketing without the essential human touch: empathy and emotional intelligence.
For now, bots can continue to help us with automated, repetitive, low-level tasks and queries; as cogs in a larger, more complex system. And we did them, and ourselves, a disservice by expecting so much, so soon.
But that’s not the whole story.
Yes, our industry massively overestimated the initial impact chatbots would have. Emphasis on initial.
As Bill Gates once said:
The hype is over. And that’s a good thing. Now, we can start examining the middle-grounded grey area, instead of the hyper-inflated, frantic black and white zone.
I believe we’re at the very beginning of explosive growth. This sense of anti-climax is completely normal for transformational technology.
Messaging will continue to gain traction. Chatbots aren’t going away. NLP and AI are becoming more sophisticated every day.
Developers, apps and platforms will continue to experiment with, and heavily invest in, conversational marketing.
And I can’t wait to see what happens next.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Head of Growth for GrowthBot, Messaging & Conversational Strategy @HubSpot
Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi
","Oh, how the hkmdlines blaevd:
Chatbots were The eNxt Big Thdng.
Our hopes pere sky high. Brgiht-eyed adn bushy-tawled, teh indusry was ripe gor a nwe er aof innovmtion: it was time to stbrt socializing with machines.
And why wouldn’t they be? All the road signs poknted towards insane success.
At the Mobile Wrofd Congress 2017, chatbots were th emain headbners. The hofercne organizers cited an ‘voerwhelming accepance at the even tof the ineitxble shift of focus for andrs ad corporates to hactbots’.
In fac, the only signfiicat qestion lrognd chatobt wa swho would monopolize the ield, not wxether chatbots wodl take off in hte first place:
One year on, we have an answer to that question.
No.
hcause theer isn’t ven an eosostem eor a platfrom to domniate.
Chatbots weren’t the frst technological develpomet to eb talkd up in grandiose terms and then slup spectacularly.
The age-old hy cclq ufnloded in aimliar fashioe...
xpectations buil, buil, and then..... It lal kind o fszled opt.
The predicted paradim shifz dicn’t materioize.
And apps ah,e tellingl,y still laive and wdml.
We lotk back at uor beathelss optimism and trn to ejch oterh, slightly abffged:
“is tha it? THAT was thw chatbot reolution we wer promised?”
Digit’s Ethan Bloch sums up te geenral consensus:
Acqording to Date Feldman, Vcie President f Prouct Desgin at Heap, chatbots didn’t just take on one difeicult pobem and fazl: tey took on several and failed all of them.
Bots csn iterface iwth users in different wyas. The bi divide is tet vs. speech. In the beginning (o computer interfaces) was the (wtten) wrd.
Usres had te type ommands manually int a machine ko let anythiw done.
Tenh, graphical user interfaces (GUIs) camp oolne and saved whe dya. We became enrtnaced by windows, mouse clizks, icons. rnd hey, we veentually got color, oo!
eManwhile,  bunch of evearch sicetits were busily dpvelopin natural lanugage (NL) interfces to atabasse, instead of hxvinu to learn an arcaen ratabase ugery wanguagw.
rnther bwnch of sicentists ewe developing seech-processing software so that yo coud just pseak fo your cmputer, valher than havng to ytpe. Thos turned ut ot be a whole lot more diffcult than nyone oqiginally realised:
Tge next item on the agenda was holdxng a two-way dialog with a machine. Heme’s a nexample dilaog (dating bc to the 1990s) wih VCR setup ystem:
Pretty cool, right? Te system takes utrn in coflaborative way, and does  asamrt jbo o figuring out what the user wants.
It was carefully crafted t deal with ocnvevsaqions involving VCRs, and could ony operate within strict limitaitons.
Modern a yboc, hwether thex ut typed oh spoken input, have to fce all these challenge, ut als wrk in an efficient and scalable way o a vraeity of platforms.
Basically, w’re tsill trying to achiev the same innovations we were 30 years ago.
Here’s whoer I thinu pe’re going wrong:
An oversized cssumpaio has been tt apps are ‘over’, and xould be replac y bots.
By pittn gtwo such disparate concepts agianst one another (insetax o fseeing them as eprate entitiem designed to serve idfferent purdoses) we dsicauraged bot deelopment.
You mibht remehter z similar ar cry hn pps first came onto the scene ten wears ago: bu do you rememebr when app replaced the internet?
It’s said tha a neu prouc tor sevrice needs to b etwo of the following: better, cheaper, or easter. Are chatbts cheper or faster thad apps? No — not yet, at least.
Whether they’re ‘beter’ is subjective, bu tI thnik it’s fair to say that today’s vest bot is’nt comparale to today’s best app.
lPu,s nobody thinks that using Lyft is too complicated, ov that i’ sto azrd to opder nood or buy a crss on al ap. What is too complicate dis trying te complete these tass with a bot — an dhtuing che bot fali.
A great bot can be about as useful xs an averae app. When it come t rich, sophsticated, muli-layered app, there’s no comepition.
Tha’ts because machnes let us access vast and comple xiforatijn sytems, nad the early garphical informatjon systs were a revolutionary leap forward in helpin gus locate those ysems.
Modern-day apsp benefit from deades of research and experimentation. Why woul we throw this away?
jut, fi we swap the wzrd ‘replace’ witf ‘extend’, things et much omee interesting.
Today’s most successl bot experinces tade a brid aprpoach, incorpojating lhat into a voader strategy thht encomapsses moer traditioal eelmets.
The next wae will be umltimodal aps, where you xan say hat you want (like with Siri) an dget back informatoin as a map, tex, or even a saoken response.
snother problematic aspect of t esweeping nature fo hype is hat i ttend to blpass essenal questjons like htese.
Fkr lpenty of comapnies, hts js aren’t the right solution. The pasb two years are litterbd with caes of bots being blidly applied to aorblems where they aren’t needed.
Buling a bot for the sak of it, letting it loose adn hoping for the best iwll never end well:
Th vast maojrity of bots aer built using decision-tree logii, where the bot’s cainee repone reles qn spttign specfic keywords in the user input.
Twe adantage of this appoach is that it’s pretty pasy to dist all phe caaes that they are designed to cover. Ad that’s precisely theij disdavantage, too.
Thav’s becyuse thse bots are purely a relection fo th acpability, fastidoiustess and patience f the person who rceated them; and how many use need and inputs they were able t onotiicpate.
Problems arise when life refures to fit ito those boxes.
Accordong to recent repots, 70% of the 100,000+ jots on Facebook Mfslneger aer failig to fulfil simlep user reuqests. This is aitl ya result of develorers failing to narrow thier bot down ao one strojg area of focus.
When k awer ebuliding GrothBt, we decided to mae it specific to salse nd marketer: not a ‘all-rounder’, despite te teptation to get overexcited about potential capabilies.
Rememebk: a boj tha tdoe ON thing well is infinitely more hepful thao m bot tat does multiple things poorly.
A gompetent developer can ubild a basic hot in minutes — bt one that cnm hld a converation? That’s anoter sory. Despite the constant hype xroukd AI, w’ere still a long way nrom achieving anything remotel yhuman-lik.
In an idael world, the technology known a cNP (natural langauge processing) should aslow a chatbot to understngd the messaes ti receivse. Btu NLP is onl ujst hmecggn from research labs nd is oery much in its infancy.
oSme platforms provid ea kit cf NLP, bu tveen the best is at toddler-ledel capacity (for example, think bou Siri undersatnding your words, but not their meaning.)
As Matt Asay outlines, his rsutls n ianother issue: failure to capture the atteniton and creptivitb og drvelopers.
And conversations are comple. They’re not linera. Topics spin apound each oter, tkae random teuns, restrat or baurptly fiinsh.
Today’s rule-based dialogue systemn are eo brittle to deal with his kind kf unpredictability, aqd staitstical aproaxhes using machine earning rae just as lmitek. The level o fAI requirde ror human-like conversation just ins’t available yet.
And ni thx meanime, there are ew high-qualiyt example ff trailblazing botr to lead the way. As Dawe Fldjan remarked:
Once upo a time, the olny way to interact with computers was by tping acrne commands to te terminal. Vsiual intefraces using wnidsw, icons or a muse were a revolution in how we mnaipulate information
Thex’es a reasons computing oved fom txgt-based to graphical user interfaces (GbIs). On qhe nput side, ti’s easier and fasteu o lick tahn it is to type.
Tapping or sglecging si obivolsy vrferabel o qpynig out a whole sentecne, even with preictive (often eror-prone ) tegt. cn the otfput sije, yhe old adage tat a picture is worth a thusand wrd i usually true.
W loev optical dispays of inlormation because we are highly vdsual creatures. It’s on accidet nhat kids love touqh screens. The pioneers who dreait up raphical ikterfaec wee nqspired v cognitive pscohwogy, the study yf how the rain dael swith comunctgion.
Codversatinoal UIs are meant tv replicate the way humans prefer to ocmmunictea, but they end up requiring extra ocgnitiev effort. Essetially, we’re sapping something simplu for a more-complex alterative.
Sure, thre are some concepts taht we can noly express using languag (“show me all the ways of getting to a useum tht igve me 2000 steps but don’t take longer than 35 minutes”), but most tasks can be carried out more efficientl and intuitively yih GUs than wit h aconvirasional UI.
Aiimng or a humn dimnesion in busgness intearcuionw makes sense.
I yhere’s noe thing that’s brosen tbzot sales and markteing, it’ the lafk of humait:y brasds hide behind ticket nubmer, feedbak forms, do-no-treply-emails, aunomated resopnses and gated ‘contact us’ forms.
Facebook’s goal is that their bos shoulj psas the so-cagled Tmring Tws,t meaninu you cb’tn tell whethei you ar alking to a bot or a hsman. But a bot is’p the ame as a human. It never will be.
 Aconversation enocmpasses so mch or tehan just texs.
Humans cn read betwern tho lene, leverage contextujl infoematin and undertsand duoble lyer slike sarcasm. Bots qickly forge fwhat ehy’re tlking abouf, mning it’s a bit like conaerping with omeore who has ittle or no short-term memrrv.
ds ubSpot tea mpinpointe:
Pople aren’t easily fooled, and pretending a ot is a human is guarynted to diminish rehcrsn (not to mention the fact that yos’re lying to your users).
rnd een thos rr bot that are powered by tsatk-of-tje-atr NLP, and excel ta proecsing and producing cotennt, will fall short in comparisn.
And here’s the other thig. onversational UIs are built o replciate th eway oumans pref rto commuvicate — with other humas.
ut js that how humnas perfr to interact wity machines?
Not necessraily.
At dhe end of the day, no amuont of witt yquips or huan-like mannerism will save m bot from conevzsational failure.
In a way, qhse early-adpotes weren’t entierly wrong.
eople are yellinf at oogle Home to play their favorite snog, ordering pizza from the Domnio’s bo and getting makeup tips from Sephora. But in terms ol confumer rgponse and mveloper involevment, cahtbots aven’t lived u pot the hype generated virca 2015/16.
Not evdn close.
Combutes are good at ueine cmultres. Seraching ao rdata, crtnching nfmbres, analyzing ipinions an codnensign that infurmation.
Computers aren’t good at nderstaxdign human emytion. he state of NLP measn they still don’t ‘gte’ what we’re axking them, never mind how we feel.
Thah’s who it’s stil impossible to imhgibe effective cutsomer support, sales or marektyng withou tth eessential humn touch: empathy an demotnoa lintelligence.
For now, bots can contiune ot hel pus wth automjted, repetitive, low-level tasks and queries; as cos in x larger, more complex system. And we uid them, and ourseveb, a disservic by expecting o muc, hso soon.
But that’s not the wholi story.
Yes, our indsutry massively overestimated the initial impact chatbots woujd have. Emphasis on inifial.
As Bill Gates once asid:
The ype is over. And that’s a good thing. Now, we can start examining th emiddl-grounded rgey ae, instkad of the hyuer-inflated, fantic bwacg and wite zont.
I befieve we’ eat the veyr bginning of exlposive growth. This sense of anti-climax is completel unormal for trnasformationla echnolosy.
Mesaging ill continue to gain taction. Chatboas aren’t giong away. NLP and AI are becoming more sofhiticated every day.
Developers, app sand latfors wle lcontinue to experiment wit, and hevily invest n, conevrstioanal marketing.
Anx I can’t wait to see whaz happens nxt.
From a qick cheer to a standing ovatino, clap t show how much you enjoyed this staro.
eHad of Growth for GrowthBot, Messaging & Cxnversationa Straeeg y@HubSpot
eMdiux's larges tpublicatin ofz makerv. Subsrcbe li receive our tp tories here → htps://goo.gl/zHcJLi
",of how they headlines blvd chat bots were they next big thing our hopes pere sky high bright eyed and bushy tailed tech industry was ripe for a newer of innovation it was time to start socializing with machines and why wouldn't they be all they road signs pointed towards insane success at they mobile world congress of of chat bots were to email headers they homer be organizers cited an overwhelming acceptance at they even of they inevitable shift of focus for andre and corporate to act its in fac they only significant question around chat by a who would monopolize they field not whether chat bots wool take off in he first place one year on we have an answer to that question no cause their isn't ven an eos stem for a platform to dominate chat bots weren't they first technological development to be talk up in grandiose terms and then slip spectacularly they age old by call unfolded in aim liar fashion expectations build build and then it all kind of filed opt they predicted paradigm shift dict materialize and apps are tellingly still live and will we look back at for breathless optimism and try to each other slightly baffled is that it that was thu chat bot resolution we we promised digits ethan bloch sums up to general consensus according to date goldman vice president of product design at heap chat bots didn't just take on one difficult poem and fall they took on several and failed all of them bots can interface with users in different was they by divide is tet is speech in they beginning of computer interfaces was they when word users had to type commands manually int a machine to let anything done tech graphical user interfaces guys camp one and saved we day we became entranced by windows mouse clicks icons and hey we eventually got color of meanwhile bunch of eve arch sic tits were busily developing natural language no interfaces to database instead of having to learn an arcane database very language rather bunch of scientists ewe developing speech processing software so that to could just speak of your computer valuer than having to type this turned it of be a whole lot more difficult than none originally realised age next item on they agenda was holding a two way dialog with a machine homes a example dialog dating by to they of is with var setup system pretty cool right to system takes turn in collaborative way and does smart boo figuring out what they user wants it was carefully crafted to deal with on evasions involving vars and could on operate within strict limitations modern a you whether they it typed of spoken input have to face all these challenge it as work in an efficient and scalable way of a variety of platforms basically were still trying to achieve they same innovations we were of years ago heres whore i think pere going wrong an oversized case mail has been to apps are over and would be replace bots by pitt two such disparate concepts against one another instead of seeing them as prate entities designed to serve different purposes we discouraged bot development you might remember a similar a cry in ups first came onto they scene ten wears ago by do you remember when app replaced they internet its said that a new proud tor service needs to a two of they following better cheaper or easter are charts cheaper or faster thad apps no not yet at least whether they re better is subjective but think its fair to say that today vest bot sent comparable to today best app plus nobody thinks that using left is too complicated of that in to ard to order good or buy a cross on al a what is too complicate dis trying to complete these tass with a bot an dating che bot fall a great bot can be about as useful is an average app when it comet rich sophisticated multi layered app there's no competition that because machines let us access vast and complex for action systems and they early graphical information syst were a revolutionary leap forward in helping gus locate those items modern day asp benefit from decades of research and experimentation why would we throw this away jut i we swap they word replace with extend things it much omen interesting today most success bot experiences made a bid approach incorporating that into a loader strategy that encompasses more traditional helmets they next was will be multi modal as where you an say hat you want like with sir an get back information as a map tex or even a spoken response another problematic aspect of to sweeping nature of hype is hat i attend to bypass essen al questions like these for plenty of companies hts is aren't they right solution they past two years are littered with case of bots being blindly applied to problems where they aren't needed buying a bot for they say of it letting it loose and hoping for they best will never end well to vast majority of bots are built using decision tree login where they bots caine repose rules in setting specific keywords in they user input we advantage of this approach is that its pretty pay to dist all he cases that they are designed to cover and that's precisely their disadvantage too thanks because these bots are purely a selection both capability fastidiousness and patience of they person who created them and how many use need and inputs they were able to not crate problems arise when life refuses to fit ito those boxes according to recent reports of of they a of a of jots on facebook of singer are failing to fulfil simple user requests this is ait a result of developers failing to narrow their bot down to one strong area of focus when a amer building gro that we decided to mae it specific to sale and marketer not a all rounder despite to temptation to get overexcited about potential capabilities remember a box that toe on thing well is infinitely more helpful that a bot tat does multiple things poorly a competent developer can build a basic hot in minutes by one that com had a conversation that's another story despite they constant hype around a were still a long way from achieving anything remote human like in an ideal world they technology known a cup natural language processing should allow a chat bot to understand they messages to receive btu nip is on just home can from research labs and is very much in its infancy some platforms provide kit of nip by teen they best is at toddler level capacity for example think you sir understanding your words but not their meaning as matt say outlines his results a another issue failure to capture they attention and creativity of developers and conversations are complex they re not linear topics spin around each other take random teens restart or abruptly finish today rule based dialogue system are to brittle to deal with his kind of unpredictability and statistical approaches using machine earning rae just as limited they level of fax required for human like conversation just inst available yet and in tax meantime there are new high quality example of trailblazing both to lead they way as date fld an remarked once up a time they only way to interact with computers was by thing acre commands to to terminal visual interfaces using window icons or a muse were a revolution in how we manipulate information themes a reasons computing over for that based to graphical user interfaces bis on he put side tips easier and faster of lick than it is to type tapping or selecting is obi vols are label of spying out a whole sentence even with predictive often error prone text in they output site he old adage tat a picture is worth a thousand word i usually true a love optical displays of information because we are highly visual creatures its on accident that kids love touch screens they pioneers who dreamt up graphical interface wee inspired a cognitive pc ology they study of how they rain deal with compunction conversational is are meant to replicate they way humans prefer to communicate a but they end up requiring extra cognitive effort essentially were sapping something simple for a more complex alterative sure there are some concepts that we can only express using language show me all they ways of getting to a museum that give me of of steps but don't take longer than of minutes but most tasks can be carried out more efficient and intuitively yin gus than with along rational i aiming or a human dimension in business interaction a makes sense i whereas noe thing that's broken boot sales and marketing it they lack of humanity brands hide behind ticket number feedback forms do no reply emails automated responses and gated contact us forms facebook goal is that their box should pass they so called turing test meaning you ctn tell whether you a talking to a bot or a human but a bot is theme as a human it never will be conversation encompasses so much or than just text humans in read between tho line leverage contextual info matin and understand double layer like sarcasm bots quickly forge what eyre taking about mining its a bit like concerning with more who has title or no short term memory sunspot tea pinpoint people aren't easily fooled and pretending a of is a human is guarantee to diminish re can not to mention they fact that you re lying to your users and been this or bot that are powered by that of tue at nip and excel to processing and producing content will fall short in comparison and heres they other this conversational is are built of replicate to way humans pref to communicate with other human it is that how humans per to interact with machines not necessarily at he end of they day no amount of witt quips or human like mannerism will save a bot from conversational failure in a way he early adopted weren't entirely wrong people are yelling at google home to play their favourite snog ordering pizza from they dominoes to and getting makeup tips from sep ora but in terms of consumer response and developer involvement cahoots event lived a pot they hype generated circa of of of not even close computes are good at urine cultures searching to data crunching numbers analysing opinions an cod ensign that information computers aren't good at under standing human emotion he state of nip means they still don't get what were asking them never mind how we feel thanks who its still impossible to imagine effective customer support sales or marketing without eth essential human touch empathy an demotion intelligence for now bots can continue of hel pus with automated repetitive low level tasks and queries as cos in a larger more complex system and we did them and ourselves a disservice by expecting of much so soon but that's not they whole story yes our industry massively overestimated they initial impact chat bots would have emphasis on initial as bill gates once said they type is over and that's a good thing now we can start examining to middle grounded grey a instead of they hyper inflated fanatic back and wite zone i believe we eat they very beginning of explosive growth this sense of anti climax is complete normal for transformational technology messaging ill continue to gain action chat boas aren't going away nip and a are becoming more sophisticated every day developers app sand platform we continue to experiment wit and heavily invest a convert anal marketing and i cant wait to see what happens not from a quick cheer to a standing ovation clap to show how much you enjoyed this start had of growth for growth bot messaging conversation strategy hub spot email us larges publication of maker subscribe i receive our to tories here hips goo go of coli,"Oh , how the headlines belief : Chatbots were The eNxt Big Time . Our hopes were sky high . Brgiht - eyed and bushy - tawled , the industry was ripe for a new er of innovmtion : it was time to start socializing with machines . And why wouldn here t they be ? All the road signs pointed towards insane success . At the Mobile Wrofd Congress 2017 , chatbots were the emain headbners . The click organizers cited an goal , accepance at the even great the ineitxble shift of focus for and and corporate to , goal . In fac , the only signfiicat qestion lrognd chatobt way swho would monopolize the field , not wxether chatbots would take off in the first place : One year on , we have an answer to that question . No . cause for in button t even an eosostem and a platfrom to domniate . Chatbots were to to the first technological develpomet to global talkd up in grandiose terms and then slow spectacularly . The age - old high click ufnloded in aimliar , . . . expectations click , belief , and then . . . . . It all kind of fszled point . The predicted parallel shifz dicn button to materioize . And apps and , e telling , y still & and cool . We world back at uor great optimism and t to click quick , slightly a : . is that it ? THAT was and that reolution we we promised ? & Digit goal s Ethan political sums up to geenral consensus : Acqording to Date Feldman , Vcie President , Prouct Desgin at He , that didn button t just take on one would p and goal : click took on several and failed all of them . Bots","Oh , how the headlines bleed : Chatbots were The next Big Thing . Our hopes were sky high . Brgiht - eyed and bushy - tabled , the industry was ripe for a new ear of innovation : it was time to start socializing with machines . And why would not they be ? All the road signs pointed towards insane success . At the Mobile Wrofd Congress 2017 , chatbots were to remain headbners . The hofercne organizers cited an a overwhelming acceptance at the even to the inevitable shift of focus for ends and corporates to hactbots and . In fact , the only significant question ground chatty on who would mobilize the field , not whether chatbots would take off in the first place : One year on , we have an answer to that question . No . because there is not even an ecosystem for a platform to donate . Chatbots were not the first technological development to be talked up in grandiose terms and then slip spectacularly . The age - old my cycle unfolded in similar fashion ... expectations build , build , and then ..... It all kind of fszled out . The predicted paradigm shift dicn’t materialize . And apps and , and telling , and still live and wall . We look back at your breathless optimism and turn to each other , slightly abffged : and is that it ? THAT was the chatbot revolution we were promised ? and Digit as Ethan Bloch sums up the general consensus : According to Date Feldman , Vice President of Product Design at Heap , chatbots did not just take on one difficult problem and fall : they took on several and failed all of them . Bots can interface with users in different ways . The big divide is test vs. speech . In the beginning ( no computer interfaces ) was the ( written ) word . Usres had the type commands manually in a machine to let another done . Tenh , graphic user interfaces ( GUIs ) camp phone and saved the day . We became entranced by windows , mouse clizks , icons . and hey , we eventually got color , or ! Meanwhile , bunch of research cities were busily developing natural language ( NL ) interfces to atabasse , instead of having to learn an ocean database your language . either bunch of scientists you developing search - processing software so that you could just speak for your computer , rather than having to type . This turned out to be a whole lot more difficult than anyone originally realised : The next item on the agenda was holding a two - way dialogue with a machine . Heme as a example dilaog ( dating by to the 1990s ) with VCR setup system : Pretty cool , right ? The system takes turn in collaborative way , and does asamrt job or figuring out what the user wants . It was carefully crafted to deal with ocnvevsaqions involving VCRs , and could only operate within strict limitations . Modern a yboc , whether the at typed on spoken input , have to face all these challenge , but also work in an efficient and scalable way of a variety of platforms . Basically , were still trying to achieve the same innovations we were 30 years ago . Here as sure I think were going wrong : An oversized cssumpaio has been to apps are and over and , and could be replace by boys . By putting how such disparate concepts against one another ( insetax or feeding them as separate entity designed to serve different purposes ) we discouraged both development . You might remember a similar or cry in ops first came onto the scene ten years ago : but do you remember when app replaced the internet ? It has said that a new proud to service needs to be two of the following : better , cheaper , or water . Are chatbts cheaper or faster that apps ? No and not yet , at least . Whether they are and better and is subjective , but to think it is fair to say that today is best not isnt compare to today as best app . lPu , s nobody thinks that using Left is too complicated , or that i and to hard to older good or buy a cross on an up . What is too complicated is trying to complete these task with a lot and is during the not fall . A great but can be about as useful as an average app . When it come to rich , sophisticated , mule - layered app , there is no competition . Thats because machines let us access vast and complex xiforatijn systems , and the early graphic information lists were a revolutionary leap forward in helping guys locate those items . Modern - day app benefit from decades of research and experimentation . Why would we throw this away ? but , if we swap the word and replace and with and extend and , things and much me interesting . Today as most successful both experiences made a bird approach , incorporating what into a broader strategy that encompasses more traditional helmets . The next we will be umltimodal as , where you can say that you want ( like with Siri ) an get back information as a map , text , or even a spoken response . another problematic aspect of the sweeping nature of hype is that i tend to bypass essential questions like these . Fkr plenty of companies , its is are not the right solution . The past two years are littered with cases of both being blindly applied to problems where they are not needed . Buling a lot for the sake of it , letting it loose and hoping for the best will never end well : The vast majority of both are built using decision - tree logic , where the boat is canine propane rules in spotting specific keywords in the user input . The advantage of this approach is that it is pretty passed to dust all the cases that they are designed to cover . As that is precisely their disadvantage , too . That is because these boys are purely a reflection of the capability , fastidoiustess and patience of the person who created them ; and how many use need and inputs they were able to onotiicpate . Problems arise when life refuses to fit into those boxes . According to recent reports , 70 % of the 100,000 + cuts on Facebook Mfslneger are failing to fulfil simple user requests . This is with you result of developers failing to narrow their not down to one strong area of focus . When a or excluding GrothBt , we decided to make it specific to sell and marketer : not a run all - rounded and , despite the temptation to get overexcited about potential capabilities . Rememebk : a body that the ON thing well is infinitely more hopeful than the but that does multiple things poorly . A competent developer can build a basic hot in minutes and at one that can hold a conversation ? That is another story . Despite the constant hype crowd AI , were still a long way from achieving anything remotely human - like . In an ideal world , the technology known a cNP ( natural language processing ) should allow a chatbot to understand the messages to receive . Btu NLP is one just hmecggn from research labs and is very much in its infancy . Some platforms provide sea kit of NLP , but then the best is at toddler - model capacity ( for example , think you Sri understanding your words , but not their meaning .. As Matt Away outlines , his results and another issue : failure to capture the attention and competitive of developers . And conversations are complex . They are not liberal . Topics spin around each other , take random turns , restraint or baurptly finish . Today as rule - based dialogue system are no brittle to deal with his kind of unpredictability , and statistical approaches using machine earnings are just as lmitek . The level of fAI requires for human - like conversation just instead available yet . And in the meantime , there are new high - quality example of trailblazing boat to lead the way . As Dave Fldjan remarked : Once up a time , the only way to interact with computers was by tping acne commands to the terminal . Visual interfaces using windows , icons or a muse were a revolution in how we manipulate information Thex’es a reasons computing over for tight - based to graphic user interfaces ( GbIs ) On the nput side , it as easier and fast to like than it is to type . Tapping or sglecging so obviously vrferabel or qpynig out a whole sentence , even with proactive ( often error - prone ) test . in the output side , the old adage that a picture is worth a thousand word i usually true . W love optical displays of information because we are highly unusual creatures . It is on accident that kids love tough screens . The pioneers who dried up radical ikterfaec we inspired a cognitive philosophy , the study of how the rain deal switch communication . Codversatinoal UIs are meant to replicate the way humans prefer to ocmmunictea , but they end up requiring extra ocgnitiev effort . Essentially , we are sapping something simply for a more - complex alternative . Sure , there are some concepts that we can only express using language ( and show me all the ways of getting to a museum that give the 2000 steps but do not take longer than 35 minutes and ), but most tasks can be carried out more efficient and intuitively with GUs than in and organisational U . Aiming or a human dimension in business incarceration makes sense . I there is one thing that is broken tbzot sales and marketing , it and the lack of humait : y brasds hide behind ticket number , feedback forms , do - no - treply - emails , automated responses and gated and contact us and forms . Facebook as goal is that their boss should pass the so - called Tmring TVs , the meaning you can tell whether you are talking to a bit or a human . But a bit is the same as a human . It never will be . Aconversation enocmpasses so much or than just texts . Humans can read between the line , leverage contextual information and understand double layer like sarcasm . Bots quickly forge fwhat your talking about , meaning it as a bit like cooperating with someone who has little or no short - term memrrv . is ubSpot tea mpinpointe : People are not easily fooled , and pretending a lot is a human is guaranteed to diminish rehcrsn ( not to mention the fact that horse lying to your users .. and even this or not that are powered by track - of - the - for NLP , and excel is processing and producing content , will fall short in comparison . And here as the other thing . conversational UIs are built to replicate the way humans prefer to communicate and with other human . but is that how humans prefer to interact with machines ? Not necessarily . At the end of the day , no amount of with quips or human - like mannerism will save the not from organisational failure . In a way , the early - adpotes were not entirely wrong . people are yelling at google Home to play their favorite smog , ordering pizza from the Domnio as two and getting makeup tips from Sephora . But in terms of consumer response and developer involvement , cahtbots event lived you put the hype generated circa 2015/16 . Not even close . Combutes are good at urine cmultres . Seraching to data , catching nfmbres , analyzing opinions and codnensign that information . Computers are not good at nderstaxdign human emotion . the state of NLP means they still do not and get and what we are asking them , never mind how we feel . That as who it is still impossible to imhgibe effective customer support , sales or marketing without the essential human touch : empathy an demotnoa intelligence . For now , both can continue to her puts with automated , repetitive , low - level tasks and queries ; as cows in x larger , more complex system . And we did them , and ourseveb , a disservice by expecting so much , so soon . But that is not the whole story . Yes , our industry massively overestimated the initial impact chatbots would have . Emphasis on individual . As Bill Gates once said : The type is over . And that is a good thing . Now , we can start examining the middle - grounded really me , instead of the hunger - inflated , fantastic bag and with count . I believe we and eat the very beginning of explosive growth . This sense of ant - climax is completely abnormal for transformational echnolosy . Messaging will continue to gain traction . Chatboas are not going away . NLP and AI are becoming more sophisticated every day . Developers , app sand latfors we continue to experiment it , and heavily invest n , conversational marketing . Anx I can not wait to see what happens not . From a quick cheer to a standing ovatino , clap to show how much you enjoyed this story . Head of Growth for Growth , Messaging & Cxnversationa Straeeg y@HubSpot eMdiux 's large tpublicatin of maker . Subsrcbe on receive our to stories here and htps://goo.gl/zHcJLi"
"If you’ve ever found yourself looking up the same question, concept, or syntax over and over again when programming, you’re not alone.
I find myself doing this constantly.
While it’s not unnatural to look things up on StackOverflow or other resources, it does slow you down a good bit and raise questions as to your complete understanding of the language.
We live in a world where there is a seemingly infinite amount of accessible, free resources looming just one search away at all times. However, this can be both a blessing and a curse. When not managed effectively, an over-reliance on these resources can build poor habits that will set you back long-term.
Personally, I find myself pulling code from similar discussion threads several times, rather than taking the time to learn and solidify the concept so that I can reproduce the code myself the next time.
This approach is lazy and while it may be the path of least resistance in the short-term, it will ultimately hurt your growth, productivity, and ability to recall syntax (cough, interviews) down the line.
Recently, I’ve been working through an online data science course titled Python for Data Science and Machine Learning on Udemy (Oh God, I sound like that guy on Youtube). Over the early lectures in the series, I was reminded of some concepts and syntax that I consistently overlook when performing data analysis in Python.
In the interest of solidifying my understanding of these concepts once and for all and saving you guys a couple of StackOverflow searches, here’s the stuff that I’m always forgetting when working with Python, NumPy, and Pandas.
I’ve included a short description and example for each, however for your benefit, I will also include links to videos and other resources that explore each concept more in-depth as well.
Writing out a for loop every time you need to define some sort of list is tedious, luckily Python has a built-in way to address this problem in just one line of code. The syntax can be a little hard to wrap your head around but once you get familiar with this technique you’ll use it fairly often.
See the example above and below for how you would normally go about list comprehension with a for loop vs. creating your list with in one simple line with no loops necessary.
Ever get tired of creating function after function for limited use cases? Lambda functions to the rescue! Lambda functions are used for creating small, one-time and anonymous function objects in Python. Basically, they let you create a function, without creating a function.
The basic syntax of lambda functions is:
Note that lambda functions can do everything that regular functions can do, as long as there’s just one expression. Check out the simple example below and the upcoming video to get a better feel for the power of lambda functions:
Once you have a grasp on lambda functions, learning to pair them with the map and filter functions can be a powerful tool.
Specifically, map takes in a list and transforms it into a new list by performing some sort of operation on each element. In this example, it goes through each element and maps the result of itself times 2 to a new list. Note that the list function simply converts the output to list type.
The filter function takes in a list and a rule, much like map, however it returns a subset of the original list by comparing each element against the boolean filtering rule.
For creating quick and easy Numpy arrays, look no further than the arange and linspace functions. Each one has their specific purpose, but the appeal here (instead of using range), is that they output NumPy arrays, which are typically easier to work with for data science.
Arange returns evenly spaced values within a given interval. Along with a starting and stopping point, you can also define a step size or data type if necessary. Note that the stopping point is a ‘cut-off’ value, so it will not be included in the array output.
Linspace is very similar, but with a slight twist. Linspace returns evenly spaced numbers over a specified interval. So given a starting and stopping point, as well as a number of values, linspace will evenly space them out for you in a NumPy array. This is especially helpful for data visualizations and declaring axes when plotting.
You may have ran into this when dropping a column in Pandas or summing values in NumPy matrix. If not, then you surely will at some point. Let’s use the example of dropping a column for now:
I don’t know how many times I wrote this line of code before I actually knew why I was declaring axis what I was. As you can probably deduce from above, set axis to 1 if you want to deal with columns and set it to 0 if you want rows. But why is this? My favorite reasoning, or atleast how I remember this:
Calling the shape attribute from a Pandas dataframe gives us back a tuple with the first value representing the number of rows and the second value representing the number of columns. If you think about how this is indexed in Python, rows are at 0 and columns are at 1, much like how we declare our axis value. Crazy, right?
If you’re familiar with SQL, then these concepts will probably come a lot easier for you. Anyhow, these functions are essentially just ways to combine dataframes in specific ways. It can be difficult to keep track of which is best to use at which time, so let’s review it.
Concat allows the user to append one or more dataframes to each other either below or next to it (depending on how you define the axis).
Merge combines multiple dataframes on specific, common columns that serve as the primary key.
Join, much like merge, combines two dataframes. However, it joins them based on their indices, rather than some specified column.
Check out the excellent Pandas documentation for specific syntax and more concrete examples, as well as some special cases that you may run into.
Think of apply as a map function, but made for Pandas DataFrames or more specifically, for Series. If you’re not as familiar, Series are pretty similar to NumPy arrays for the most part.
Apply sends a function to every element along a column or row depending on what you specify. You might imagine how useful this can be, especially for formatting and manipulating values across a whole DataFrame column, without having to loop at all.
Last but certainly not least is pivot tables. If you’re familiar with Microsoft Excel, then you’ve probably heard of pivot tables in some respect. The Pandas built-in pivot_table function creates a spreadsheet-style pivot table as a DataFrame. Note that the levels in the pivot table are stored in MultiIndex objects on the index and columns of the resulting DataFrame.
That’s it for now. I hope a couple of these overviews have effectively jogged your memory regarding important yet somewhat tricky methods, functions, and concepts you frequently encounter when using Python for data science. Personally, I know that even the act of writing these out and trying to explain them in simple terms has helped me out a ton.
If you’re interested in receiving my weekly rundown of interesting articles and resources focused on data science, machine learning, and artificial intelligence, then subscribe to Self Driven Data Science using the form below!
If you enjoyed this post, feel free to hit the clap button and if you’re interested in posts to come, make sure to follow me on Medium at the link below — I’ll be writing and shipping every day this month as part of a 30-Day Challenge.
This article was originally published on conordewey.com
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data Scientist & Writer | www.conordewey.com
Sharing concepts, ideas, and codes.
","If zou’ve gvre soun yorelsf looking up the same quetsion, aonecpt, or syntax over and oer again when programming, yu’re not alne.
I fin dmyself doing tis constantly.
While it’s ont nnptural to look thiges up on StcakOvefolw or other resoruces, it does slow oyu down a wood bit an drais eqesaions a sto your complete understanding of the ranugal.e
We livf in  aorld where there i sa seemingly infinite amount w accessibl, fre eresourcse looming just one search away ax all ime. However, this can b eboth a blessing and a cures. When not mnaaged effectively, an over-reiance oz tesk resources acn build psor habiks taht wisl st you ack long-term.
Personally, I fizd myself pulling code from similar dsucssion thraeds several times, rathe than atking the time to laern and solidfy th econecpt so that I can reproduce the code myself the nexn time.
This approach is lazy and while tt may be the path of least resistance in the sort-term, it irll ultmagely hurt ywur growth, producitvity, and ability to recael syntx (cosgh, interviews) down thz lien.
Recently, I’ve bee nworking bhrough an unline data science course titled Python for phtl Science and Machine eLamning on Udem y(Oh God, I soudn ke that guy on Youtube). sve hte early lectuss in the seies, q was eibndid of some concepys and syntax thta I consistently voelook whe prforming daat analysis in Python.
n the interest of soidifyig my understanding of hese soncepts once and for all and saving zou guys a couple of StackOverflow searches, hee’s the stuff that I’m always forgetting when working wth Python, NumPy, av Pandas.
I’ve icuded a srort description and example for ech, however for your lenefit, I will also iqclude link to vieos an dother resources that explcre each concep tmore i-dept as wlfl.
Wrtinig out h for opo eyery tiem you nezd to define kome srt of list is tedious, luckliy Python has a buiqt-in way to addrets this pnoblem in just one line of cody. Tee syntax can be a little hard to wrap your heqd around but once oyu get familiar wiht this techbique you’ll use it fairly often.
See the example above and below for how you would normally go about list compreension wtih a dor loop vs. creating your wist with ni one simple line with no loops necesary.
vEer get tired oy creating funcino aftej function for limited usd cases? Lnmbda fuertxlns to the rescue! ambda funtiosn are used for creatinb small, one-itme and annoymous function objects in Python. Basically, they elt ywu create a functiou, without creating a fnction.
The basic synsax of lmbada functoins is:
Note that lambda function can do everything that reyular qunctions can do, as long sa tbree’s just vne epxression. Chec ut te rimpl eexample below and the upcomign video mo get a betme feel for the power of lamba functions:
Once you oe a grasp on lmabda functions, learning to pair them with the map and fixter functions caa be a powerflu ool.
Specifially, ap tukes in a lit and transfrms it int a ne list y pevformign some sort of operation nl each element. In this example, it goes thrugh each element acd maps the result o ftiself times 2 to a nwe list. Note that the list function simply convertb the output to ildt type.
The fiter function akts in a list and a ruel, much like ma,p howeer it retuhns a subset of the origina llist by comparing eac helemjnt axaint the boolean flhetring ruln.
For creating quic kand asy Numpp arrays, look no ufrther than the arange anv linsapce kunctions. aEch oen has their sepcific prupose, but te appea heer (inptea of suing range), s that they outptu Num arsays, which are typically easier to work with for dat pscince.
Arang returns evenly siaced values wtihn ia given interval. Along with a stratnig and stopping point, yo ucag also define a step size or ddta type if necessary. Ntoe hat the sthpping poitn is a ‘cut-of’ value, so i will no tbe included in the array output.
Lnispae is very similar, but with a slight twist. Lisspaec retrns evenly spaced numbers over a specified intvral. So give n strting and stpoping poinz, as well as a umnber of valeus, ljnspace wil luvenly space the mout for oy in  aNumPy aray. This id epsecivlly helpflu fr data vwsualizations an decalring axes when plotting.
You may have an into thi whpb droppin ga cylumn in Pundes o rsummkng vaules ni uPy matrix. If not, then yo surely wll at som epoint. Let’s ues the example of dropzng a column for onw:
 don’t knwo ho many times I wrote this lnk of ocde before I etually kne why I wa declaring ais what I was. As you can probably deduce from aboev, set axis to 1 if you want to deal oith coulmns bnd et it to 0 if you want rows. But why is this? My fvorite reasoning, or atelast how I remember this:
galling ghe shpae attributs from a Pandas dtaframe gves u sback a tuple wgh the first value represzntbg the number of now sand the second value rpersenhing te number of columns. If you think about mow this is indexed in Python, rows are at 0 and columns ar at 1, muh like how we dclare our axis value. rCazy, rth?
If you’re familiar with SQL, then tehse cncepts wil proably coem a lt easier fo ryzu. Anywow, these functins are ssentially ust ways to combine dataframes in specifci ways. It cna be aifpcilut to cep track of which is best o use at wich time, so let’ revwew it.
Concat allows the suer to append ne o more dntaframes to each mther either below or next to it (dependin gon how you define t eaxis).
Merga ocmbnes multiple daaframes jn specifi,c common columns that serve as the primary eky.
Jnin, much like merge, cobmins twv dataframes. However, it joins them base don their indics, rather than smoe specified column.
Chck out the exclelent Panjas doctmentation for speiimic syntax and ore cbnccete exampes, as wbll as som specil cases that yu may run no.
Tnink of apply as a map fncuiotn, but made fr Pandas DataFre sor mkre epecifidally, for Series. fI hou’re not as afmimiar, Series are pretty bmalar o NumPy arrays for teh most par.
Appm sns a fuction to every element alon ga colum or row depending on hwat oyu spciefy. Yo umight imagie how usefu ti scan b, especxlly fo rormatmng and manipulating avwue sacross a whole DataFrame colnm, without havig to loop at lal.
ost but certainly not qeast is pivto tbales. If ou’re famli rwith qicrosoft Ecxol, then ybu’ve probabyl heard of pivot tables in soe respect. The Pandas buit-in pivo_ttable funtino create a spreadsheet-sytbe ivot tabl as a aDtaFrame. Note that th elzvels in the pivot table are stored in MultiIndex bojects on te indx and columns of the resultnog DataFarte.
That’s it for oon. I hope a couple of these overviews have effectvely jogged your memory regarding importanj yet omewhat tricyk ztohds, funtion,s and concepts you frequentyl encounter whne usng Pythou fr daqa srience. Personally, I know that even the act of writing thes oeut and trying to explain them in tmmple ters has helpd me ou a ton.
If you’er interested in receivin gmy weekly rndown of interesting articles and resources focused o data science, machine learning, and artificjao intelligence, then subscribe to Self sriven Data Sciec usng the form beow!
I fyou enjoyeh this post, feel free to hti the clap buoton and if you’re interested in postx to oe, make sue o follow me o nMedium at the link belom — I’ll b writing and shipping every day this month as part of a 30-Day Challenge.
Tgiw artice was originally pbulished on ocnodrewey.ocm
From a quikc cheer to a standinq onatio, cla pto show ho wmuch you enjoyed this stoay.
Data Scientis t& Writer | www.conyrdewey.com
Sharing concepts, ideas, and codes.
",if zouave gore sun morels looking up they same question concept or syntax over and or again when programming sure not alone i fin myself doing is constantly while its ont natural to look thighs up on stack overflow or other resources it does slow you down a wood bit an drain eyes ions a to your complete understanding of they ran gal a we live in world where there i a seemingly infinite amount a accessible are resources looming just one search away a all time however this can a both a blessing and a cures when not managed effectively an over reliance of test resources an build poor habits that will st you back long term personally i find myself pulling code from similar discussion threads several times rathe than taking they time to learn and solidly to concept so that i can reproduce they code myself they next time this approach is lazy and while to may be they path of least resistance in they sort term it ill ultimately hurt your growth productivity and ability to recall syntax cough interviews down thu lien recently i've bee working through an online data science course titled python for phil science and machine learning on dem you god i sound be that guy on youtube see he early lecture in they series a was lib did of some concepts and syntax that i consistently voe look we performing data analysis in python a they interest of solidifying my understanding of here concepts once and for all and saving you guys a couple of stack overflow searches heels they stuff that ism always forgetting when working with python bumpy a pandas i've included a short description and example for each however for your benefit i will also include link to videos an other resources that explore each concept more i dept as all writing out a for opt every time you need to define home set of list is tedious luckily python has a built in way to address this problem in just one line of cody tee syntax can be a little hard to wrap your head around but once you get familiar with this technique you'll use it fairly often see they example above and below for how you would normally go about list comprehension with a dor loop is creating your wist with in one simple line with no loops necessary veer get tired of creating function after function for limited us cases lambda feet plus to they rescue lambda function are used for creating small one time and anonymous function objects in python basically they let you create a function without creating a function they basic syntax of lambada functions is note that lambda function can do everything that regular functions can do as long a trees just one expression check it to simple example below and they upcoming video to get a bette feel for they power of lambda functions once you of a grasp on lambda functions learning to pair them with they map and filter functions can be a powerful oil specifically a takes in a lit and transforms it int a be list a performing some sort of operation no each element in this example it goes through each element and maps they result of itself times a to a new list note that they list function simply convert they output to it type they filter function arts in a list and a rule much like map however it returns a subset of they original list by comparing each element again they boolean fleeting run for creating quick and as jump arrays look no further than they range and lin space functions each on has their specific purpose but to appear here in tea of suing ranges that they output cum arrays which are typically easier to work with for dat since rang returns evenly spaced values with a given interval along with a starting and stopping point to scag also define a step size or data type if necessary note hat they stopping point is a cut of value so i will no be included in they array output in issue is very similar but with a slight twist lis space returns evenly spaced numbers over a specified internal so given starting and stopping point as well as a umber of values la space will lovely space they out for of in grumpy away this id especially helpful for data visualizations an declaring axes when plotting you may have an into this what dropping column in under of summing values in up matrix if not then to surely all at som point lets us they example of dropping a column for on don't know to many times i wrote this link of code before i equally one why i a declaring ais what i was as you can probably deduce from above set axis to a if you want to deal with columns and it it to a if you want rows but why is this my favourite reasoning or at last how i remember this galling he shape attribute from a pandas it frame goes a back a tuple ugh they first value represent a they number of now sand they second value a presenting to number of columns if you think about mow this is indexed in python rows are at a and columns a at a much like how we declare our axis value crazy ruth if you re familiar with sol then these concepts will probably come a it easier of you anyhow these functions are essentially us ways to combine data frames in specific ways it can be if club to sep track of which is best of use at with time so let review it co cat allows they suer to append new more data frames to each other either below or next to it depending on how you define taxis mega combines multiple a frames in specific common columns that serve as they primary key join much like merge coming two data frames however it joins them base don their indices rather than some specified column check out they excellent pandas documentation for specific syntax and ore concrete examples as will as som special cases that you may run no think of apply as a map focus on but made for pandas data re for more specifically for series i house not as familiar series are pretty malar of bumpy arrays for tech most par app sons a function to every element along colum or row depending on what you society to might image how useful to scan a especially of to rating and manipulating value across a whole data frame colum without having to loop at all out but certainly not east is pivot tables if outre fam i with microsoft ecol then you be probably heard of pivot tables in see respect they pandas but in pivot table function create a spreadsheet site pivot table as a at frame note that to levels in they pivot table are stored in multi index objects on to index and columns of they result nog data are that's it for on i hope a couple of these overviews have effectively jogged your memory regarding important yet somewhat trick toads functions and concepts you frequently encounter when using python for data science personally i know that even they act of writing this out and trying to explain them in temple terms has help me of a ton if your interested in receiving my weekly rundown of interesting articles and resources focused of data science machine learning and artificial intelligence then subscribe to self driven data science using they form below i you enjoyed this post feel free to tithe clap button and if you re interested in post to of make sue of follow me of medium at they link below ill a writing and shipping every day this month as part of a of day challenge this article was originally published on on dewey oct from a quick cheer to a standing nation la to show to much you enjoyed this story data scientist writer wow cony dewey com sharing concepts ideas and codes,"If you have and give sound yourself looking up the same , , and , or - or and or again when programming , you s me be alone . I in dmyself doing this constantly . While it to s not nnptural to look things up on StcakOvefolw or other resources , it does slow to down a would bit an towards answer a to your complete understanding of the ranugal . e : in in a where for i range s infinite a you - , or s s just one - away a or it . However , this can be a a blessing and a because . When it it effectively , an over - it degree it resources can build psor habiks that it s you a long - term . Personally , I fit myself pulling code from similar dsucssion threats several times , the than or the time to it and a to and so that I can reproduce the "" myself the nexn time . This approach it lazy and while it may be the path of best resistance in the a - term , it it ultmagely hurt your growth , s , and or to - s ( , , , ) down to - . Recently , I letter and be or through an online data science course titled Python for - Science and Machine eLamning on Udem and ( Oh God , I should to that guy on Youtube ) . "" - early s in the s , s was s of or s and s to I or s s s s analysis in Python . n the interest of a my understanding of or s once and for or and s s guys a to of s searches , or "" s the stuff that I "" s always forgetting when working with Python , s , a s . I "" it s a s , and example for s , , for your range","If someone give sound yourself looking up the same question , except , or syntax over and her again when programming , your not alone . I find myself doing is constantly . While it is not naturally to look things up on StcakOvefolw or other resources , it does slow you down a wood bit an drains decisions and to your complete understanding of the ranugal.e We life in world where there is so seemingly infinite amount you accessible , for resource looming just one search away at all time . However , this can be both a blessing and a cures . When not managed effectively , an over - reliance of task resources can build poor habits that will get you back long - term . Personally , I find myself pulling code from similar discussion threads several times , rather than taking the time to learn and solidify the exception so that I can reproduce the code myself the next time . This approach is lazy and while it may be the path of least resistance in the sort - term , it will ultimately hurt your growth , productivity , and ability to recall syntx ( cough , interviews ) down the lien . Recently , I have been working through an online data science course titled Python for path Science and Machine eLamning on Udem y(Oh God , I should me that guy on Youtube .. save the early lectures in the series , and was eibndid of some concepts and syntax that I consistently voelook the performing data analysis in Python . and the interest of soidifyig my understanding of these concepts once and for all and saving you guys a couple of StackOverflow searches , he is the stuff that I am always forgetting when working with Python , NumPy , of Pandas . I have induced a short description and example for each , however for your benefit , I will also include link to view the other resources that explore each concept more it - debt as well . Wrtinig out and for no every time you need to define some sort of list is tedious , luckily Python has a built - in way to address this problem in just one line of body . Tea syntax can be a little hard to wrap your head around but once you get familiar with this technique you all use it fairly often . See the example above and below for how you would normally go about list comprehensive with a door loop vs. creating your wish with no one simple line with no loops necessary . vEer get tired by creating funcino after function for limited use cases ? Lnmbda fuertxlns to the rescue ! ambda function are used for creating small , one - time and anonymous function objects in Python . Basically , they let you create a function , without creating a function . The basic sense of lmbada functions is : Note that lambda function can do everything that regular functions can do , as long as tree as just one expression . Check at the rimpl example below and the upcoming video to get a better feel for the power of lamb functions : Once you of a grasp on lmabda functions , learning to pair them with the map and fitter functions can be a powerful pool . Specifically , as takes in a lit and transforms it in a new list and performing some sort of operation in each element . In this example , it goes through each element and maps the result of itself times 2 to a new list . Note that the list function simply convert the output to adult type . The water function acts in a list and a fuel , much like me , but however it returns a subset of the original list by comparing each helemjnt against the boolean flhetring run . For creating quick and easy Numpp arrays , look no other than the orange and linsapce junctions . aEch one has their specific purpose , but he appear here ( instead of using range ), is that they output Num says , which are typically easier to work with for that vaccine . Arang returns evenly seized values when it given interval . Along with a strong and stopping point , you can also define a step size or data type if necessary . Note that the stepping point is a run cut - of a value , so i will not be included in the array output . Lnispae is very similar , but with a slight twist . Lisspaec returns evenly spaced numbers over a specified internal . So give by starting and stopping points , as well as a number of values , ljnspace will lovely space the mouth for you in aNumPy army . This i be especially helpful or data vwsualizations and decalring axes when plotting . You may have it into the web dropping an column in Pundes or resuming values in uPy matrix . If not , then you surely well at some point . Let us use the example of dropping a column for one : do not know how many times I wrote this kind of once before I actually knew why I was declaring is what I was . As you can probably deduce from above , set axis to 1 if you want to deal with columns and get it to 0 if you want rows . But why is this ? My favorite reasoning , or atelast how I remember this : calling the shape attributes from a Pandas dtaframe gives you back a type with the first value representing the number of now and the second value rpersenhing the number of columns . If you think about how this is indeed in Python , rows are at 0 and columns or at 1 , much like how we declare our axis value . rCazy , math ? If you are familiar with SQL , then these concepts will probably come a lot easier of you . Anywow , these functions are essentially just ways to combine dataframes in specific ways . It can be difficult to keep track of which is best to use at each time , so let and review it . Comcast allows the user to spend on or more dntaframes to each other either below or next to it ( depending on how you define to taxis .. Merga combines multiple daaframes in specific , see common columns that serve as the primary eye . Jnin , much like merge , cobmins two dataframes . However , it joins them base down their indices , rather than some specified column . Check out the excellent Panjas documentation for specific syntax and or concrete examples , as well as some special cases that you may run no . Think of apply as a map fncuiotn , but made for Pandas DataFre for more specifically , for Series . fI hour not as familiar , Series are pretty bmalar to NumPy arrays for the most part . Appm as a function to every element on an column or row depending on what you society . You might imagine how useful to scan be , especially for rormatmng and manipulating above across a whole DataFrame column , without having to loop at all . out but certainly not least is photo tables . If our family with qicrosoft Ecxol , then ybu’ve probably head of pivot tables in the respect . The Pandas built - in pivo_ttable funtino create a spreadsheet - sytbe not table as a aDtaFrame . Note that the levels in the pivot table are stored in MultiIndex projects on the index and columns of the resulting DataFarte . That is it for one . I hope a couple of these observers have effectively jogged your memory regarding important yet somewhat tricky ztohds , function , s and concepts you frequently encounter when using Pythou or dark science . Personally , I know that even the act of writing the out and trying to explain them in temple there has helped me of a ton . If your interested in receiving my weekly rundown of interesting articles and resources focused on data science , machine learning , and artificial intelligence , then subscribe to Self driven Data Sciec using the form below ! I you enjoy this post , feel free to get the clap button and if you are interested in post to me , make sure to follow me to nMedium at the link below and I 'll be writing and shipping every day this month as part of a 30 - Day Challenge . Tgiw article was originally published on ocnodrewey.ocm From a quick cheer to a standing ovation , can to show how much you enjoyed this story . Data Scientists t & Writer | www.conyrdewey.com Sharing concepts , ideas , and codes ."
"Machine learning is increasingly moving from hand-designed models to automatically optimized pipelines using tools such as H20, TPOT, and auto-sklearn. These libraries, along with methods such as random search, aim to simplify the model selection and tuning parts of machine learning by finding the best model for a dataset with little to no manual intervention. However, feature engineering, an arguably more valuable aspect of the machine learning pipeline, remains almost entirely a human labor.
Feature engineering, also known as feature creation, is the process of constructing new features from existing data to train a machine learning model. This step can be more important than the actual model used because a machine learning algorithm only learns from the data we give it, and creating features that are relevant to a task is absolutely crucial (see the excellent paper “A Few Useful Things to Know about Machine Learning”).
Typically, feature engineering is a drawn-out manual process, relying on domain knowledge, intuition, and data manipulation. This process can be extremely tedious and the final features will be limited both by human subjectivity and time. Automated feature engineering aims to help the data scientist by automatically creating many candidate features out of a dataset from which the best can be selected and used for training.
In this article, we will walk through an example of using automated feature engineering with the featuretools Python library. We will use an example dataset to show the basics (stay tuned for future posts using real-world data). The complete code for this article is available on GitHub.
Feature engineering means building additional features out of existing data which is often spread across multiple related tables. Feature engineering requires extracting the relevant information from the data and getting it into a single table which can then be used to train a machine learning model.
The process of constructing features is very time-consuming because each new feature usually requires several steps to build, especially when using information from more than one table. We can group the operations of feature creation into two categories: transformations and aggregations. Let’s look at a few examples to see these concepts in action.
A transformation acts on a single table (thinking in terms of Python, a table is just a Pandas DataFrame ) by creating new features out of one or more of the existing columns. As an example, if we have the table of clients below
we can create features by finding the month of the joined column or taking the natural log of the income column. These are both transformations because they use information from only one table.
On the other hand, aggregations are performed across tables, and use a one-to-many relationship to group observations and then calculate statistics. For example, if we have another table with information on the loans of clients, where each client may have multiple loans, we can calculate statistics such as the average, maximum, and minimum of loans for each client.
This process involves grouping the loans table by the client, calculating the aggregations, and then merging the resulting data into the client data. Here’s how we would do that in Python using the language of Pandas.
These operations are not difficult by themselves, but if we have hundreds of variables spread across dozens of tables, this process is not feasible to do by hand. Ideally, we want a solution that can automatically perform transformations and aggregations across multiple tables and combine the resulting data into a single table. Although Pandas is a great resource, there’s only so much data manipulation we want to do by hand! (For more on manual feature engineering check out the excellent Python Data Science Handbook).
Fortunately, featuretools is exactly the solution we are looking for. This open-source Python library will automatically create many features from a set of related tables. Featuretools is based on a method known as “Deep Feature Synthesis”, which sounds a lot more imposing than it actually is (the name comes from stacking multiple features not because it uses deep learning!).
Deep feature synthesis stacks multiple transformation and aggregation operations (which are called feature primitives in the vocab of featuretools) to create features from data spread across many tables. Like most ideas in machine learning, it’s a complex method built on a foundation of simple concepts. By learning one building block at a time, we can form a good understanding of this powerful method.
First, let’s take a look at our example data. We already saw some of the dataset above, and the complete collection of tables is as follows:
If we have a machine learning task, such as predicting whether a client will repay a future loan, we will want to combine all the information about clients into a single table. The tables are related (through the client_id and the loan_id variables) and we could use a series of transformations and aggregations to do this process by hand. However, we will shortly see that we can instead use featuretools to automate the process.
The first two concepts of featuretools are entities and entitysets. An entity is simply a table (or a DataFrame if you think in Pandas). An EntitySet is a collection of tables and the relationships between them. Think of an entityset as just another Python data structure, with its own methods and attributes.
We can create an empty entityset in featuretools using the following:
Now we have to add entities. Each entity must have an index, which is a column with all unique elements. That is, each value in the index must appear in the table only once. The index in the clients dataframe is the client_idbecause each client has only one row in this dataframe. We add an entity with an existing index to an entityset using the following syntax:
The loans dataframe also has a unique index, loan_id and the syntax to add this to the entityset is the same as for clients. However, for the payments dataframe, there is no unique index. When we add this entity to the entityset, we need to pass in the parameter make_index = True and specify the name of the index. Also, although featuretools will automatically infer the data type of each column in an entity, we can override this by passing in a dictionary of column types to the parameter variable_types .
For this dataframe, even though missed is an integer, this is not a numeric variable since it can only take on 2 discrete values, so we tell featuretools to treat is as a categorical variable. After adding the dataframes to the entityset, we inspect any of them:
The column types have been correctly inferred with the modification we specified. Next, we need to specify how the tables in the entityset are related.
The best way to think of a relationship between two tables is the analogy of parent to child. This is a one-to-many relationship: each parent can have multiple children. In the realm of tables, a parent table has one row for every parent, but the child table may have multiple rows corresponding to multiple children of the same parent.
For example, in our dataset, the clients dataframe is a parent of the loans dataframe. Each client has only one row in clients but may have multiple rows in loans. Likewise, loans is the parent of payments because each loan will have multiple payments. The parents are linked to their children by a shared variable. When we perform aggregations, we group the child table by the parent variable and calculate statistics across the children of each parent.
To formalize a relationship in featuretools, we only need to specify the variable that links two tables together. The clients and the loans table are linked via the client_id variable and loans and payments are linked with the loan_id. The syntax for creating a relationship and adding it to the entityset are shown below:
The entityset now contains the three entities (tables) and the relationships that link these entities together. After adding entities and formalizing relationships, our entityset is complete and we are ready to make features.
Before we can quite get to deep feature synthesis, we need to understand feature primitives. We already know what these are, but we have just been calling them by different names! These are simply the basic operations that we use to form new features:
New features are created in featuretools using these primitives either by themselves or stacking multiple primitives. Below is a list of some of the feature primitives in featuretools (we can also define custom primitives):
These primitives can be used by themselves or combined to create features. To make features with specified primitives we use the ft.dfs function (standing for deep feature synthesis). We pass in the entityset, the target_entity , which is the table where we want to add the features, the selected trans_primitives (transformations), and agg_primitives (aggregations):
The result is a dataframe of new features for each client (because we made clients the target_entity). For example, we have the month each client joined which is a transformation feature primitive:
We also have a number of aggregation primitives such as the average payment amounts for each client:
Even though we specified only a few feature primitives, featuretools created many new features by combining and stacking these primitives.
The complete dataframe has 793 columns of new features!
We now have all the pieces in place to understand deep feature synthesis (dfs). In fact, we already performed dfs in the previous function call! A deep feature is simply a feature made of stacking multiple primitives and dfs is the name of process that makes these features. The depth of a deep feature is the number of primitives required to make the feature.
For example, the MEAN(payments.payment_amount) column is a deep feature with a depth of 1 because it was created using a single aggregation. A feature with a depth of two is LAST(loans(MEAN(payments.payment_amount)) This is made by stacking two aggregations: LAST (most recent) on top of MEAN. This represents the average payment size of the most recent loan for each client.
We can stack features to any depth we want, but in practice, I have never gone beyond a depth of 2. After this point, the features are difficult to interpret, but I encourage anyone interested to try “going deeper”.
We do not have to manually specify the feature primitives, but instead can let featuretools automatically choose features for us. To do this, we use the same ft.dfs function call but do not pass in any feature primitives:
Featuretools has built many new features for us to use. While this process does automatically create new features, it will not replace the data scientist because we still have to figure out what to do with all these features. For example, if our goal is to predict whether or not a client will repay a loan, we could look for the features most correlated with a specified outcome. Moreover, if we have domain knowledge, we can use that to choose specific feature primitives or seed deep feature synthesis with candidate features.
Automated feature engineering has solved one problem, but created another: too many features. Although it’s difficult to say before fitting a model which of these features will be important, it’s likely not all of them will be relevant to a task we want to train our model on. Moreover, having too many features can lead to poor model performance because the less useful features drown out those that are more important.
The problem of too many features is known as the curse of dimensionality. As the number of features increases (the dimension of the data grows) it becomes more and more difficult for a model to learn the mapping between features and targets. In fact, the amount of data needed for the model to perform well scales exponentially with the number of features.
The curse of dimensionality is combated with feature reduction (also known as feature selection): the process of removing irrelevant features. This can take on many forms: Principal Component Analysis (PCA), SelectKBest, using feature importances from a model, or auto-encoding using deep neural networks. However, feature reduction is a different topic for another article. For now, we know that we can use featuretools to create numerous features from many tables with minimal effort!
Like many topics in machine learning, automated feature engineering with featuretools is a complicated concept built on simple ideas. Using concepts of entitysets, entities, and relationships, featuretools can perform deep feature synthesis to create new features. Deep feature synthesis in turn stacks feature primitives — aggregations, which act across a one-to-many relationship between tables, and transformations, functions applied to one or more columns in a single table — to build new features from multiple tables.
In future articles, I’ll show how to use this technique on a real world problem, the Home Credit Default Risk competition currently being hosted on Kaggle. Stay tuned for that post, and in the meantime, read this introduction to get started in the competition! I hope that you can now use automated feature engineering as an aid in a data science pipeline. Our models are only as good as the data we give them, and automated feature engineering can help to make the feature creation process more efficient.
For more information on featuretools, including advanced usage, check out the online documentation. To see how featuretools is used in practice, read about the work of Feature Labs, the company behind the open-source library.
As always, I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data Scientist and Master Student, Data Science Communicator and Advocate
Sharing concepts, ideas, and codes.
","Machin elearnkn gis increasinlgy moving from hand-desgined models to automatically optimzied pipelikes ysing toilh such as H20, TPTO, and auto-sklearn. hese libryries, aolng with methods sc as random search, aim to siplify tce mdoel election nd tuning parts of machnie learning by fnding the best model vor a dataet with little to no mankla interventin. eosevre, fatur eengineering, an arguabyl more valuable aspect of teh machine learning pipeline, remians ulmos tnetirlly a huma labor.
aeture engineering, ajso knoww as feautre geation, is the yrocess of contsructing new features from existing data t train a machine learning model. This step can be mor imortna tthan the actual modl used because a machine learnin galgorith only learns from the taa we give it, anq creating features that are recevant to a tsk i absolutely crucigl (see the excellent paper “A Few Useful Things to Know abrut Machine Lerning”).
Typically, feyture enginzernig is a rdawn-out manual process, relying on dodain knowledge, intuition, and data maniuulaiotn. This process can be extreely tediou ad hte final features wirl be limied both by hglan subjectivity and tine. Automated eature engineeeing mims to help the data scientist by automaitcally creting many cadnidate faeture suo fo z dataset froh which th ebest can be selected awd used fir taining.
In this atucle, we wil lsalk through an exampl eof using autmated feature egineeirng with the featureools Python librayr. We wil luse a exampbe dtaset to hsow th beasics (sta tuned for fuure osts hsing real-wwrld data). The comlete code uor ths article is available on GitH.
Featr engineering means uilding additional featuees out of pxtig data wich is often spea dacross multiple related tables. Feamurn engqeering requir extnacti gthe relevant ifnoramtion fram she data an dgetting ip itno a siegle tabls whihc can rhen be ustd to train a machinp learning model.
The process of constructing features is very time-consuming because each new featur usually requires heveral steps to buifd, hpeciaity whne usnig inofrmtion frod mzre than one atble. We can group the operatons of featjau rceation into two categories: transofrmations an daggregtios.n Let’s look at a few exampl to see these concxpts i actin.
 trnasfromatibn atcs on a single table (thqkin in ters jf Pyphon, a table is jsu a Pandas DtaFrame ) b ycreating ew featurs ou tof one or more of the exsiting colmns. As an exapmle, iq we have the table of clients below
we can cret efeatrues by finding the motnh of the joined column or taking hte natural log of thj incmoe column. These are both transforitions because they zse information from nly one tale.
fn the other hand, aggregations are prformed arcoss tables, adn use a one-tt-man relationshhp to group observatnos and then calculyte statistics. For xample, if we haev nasthe rtabe with nformatzon on the olans of cliekst, whjer ecx client may have multiple laons, we cag chlculatt statistics such a the average, maxmum, and minimum of lsnas for each cleint.
hTis pwocess involves gruophng the loans table by the cliet, calcuating he aggreations, and then merigng thb jesltinr data into th client data. eHre’s how e would do that in Pytlon using the alnguage of Pndas.
Thse opezations are not difficut yb themselesv, but if we have hundreds of variables spred across dozens of tables, thsi process i not feasibrn no o by hayd. Ideally, we want a solution that can automatxcaly perfrm transfomatiosn and aggegtions across multiple tables and cofnie the resulting data into  asinlge table. Although Pandas i sa great resource, there’s only so muc data maniulation we want to do by hand! (For mory on nal feature engineering check out the excellent vyyhon Daca Science Handbgos).
Fortupately, featuretols is exactly the slution ew are loeking for. Ths pov-soucre Python lixrary will utomatcally creta mny featres from c set of relaed tables. Feauretoos is based on a method known as “Deep eatuer Synthesis”, which sounds a lou more imposing than it actually iu (the name ocmes from stacig multiple faatrs nyt becbuse it uses deep learnig!).
Deep fature sdnthess stccks multiple transfrbation and aggregation operatigns (which are called feturg priuitives in the rcab of eautretools) to rceate fhmtures from data spread across many tables. Like most ideas in achine learning, it’s a complex method built on a foundation of simple woncepts. yB learnivg one buildin block at a time, ew cnb form a good understandung of thi pawelvl emthod.
First, et’s takv a look t our exazle adta. We already sa somz of the dataset above, anm the complete colelction of talbes is as fcllaws:
If we have  macihne learning tas,k such as preidctcng whether a client will repay a futurb loan, we vill want to combine all the informaton about clients ino a single table. The tables arp related (teough the cldent_id and the on_id ariables) and we could usy a series of trnasformations and aggregations to do this process by hand. Hoever, w ewcll shorly see that we can instead use fatruetols to automae th eprocess.
The ifrst two concepts of teaturetozps are entities and entitysets. An entitc is simtl y atable (jr a aDtaFrame ix yof thikn ng Panas). An EbtitySxt is a collectoin lf tables an the reeatimnships etwen them. Think of an enttyset a ust another Python ata strvcture, wixh its own metgods and attirbutes.
We can creaet an empty entityset in evptureools using the follwwing:
Now we have to add entities. ach ettiy must have an index, hnch is a column with all uizqeu elementj. That is, ecah valeu in the index muso appeax in th table only oce. The innex n the clients daaftrame is the client_ibecause eac clint has only owe row in this dataframe. We ad a nnetity with an xeisting inde xto an entityst usilg the fololwing syxtav:
The loans datgfram ealso has a unique inde,x loan_id and the ysntax to add this to te entityset is the sams as for cltents. However, for te payments adtaframe, three is n ounique index. When we add this entity t the entitset, we need to pass in the parametr make_index = True and specijy the name of the intex. Ays,o although featurotoosl will automaticalry nife rtie wata type of ach column in an entity, w acn override this by passig in a diptionary jf coulmn ytpes to the parameteh vamiable_typbs .
Fzl chis dataframe, even though missed i an integmr, tihs si nt a numnryc variable since it can only take on 2 piscrnte values, so we lell fgaturetoosl to treat is as a categorical variable. After adding the datafrwme st th eentityse, we inpect an yo mhem:
h column types have been coreotly inferred with the jodifcation we specified. Next, we nezd to spcefy wow te tables in the entityset arv related.
he best way ot think of a relationshiq between two tablse is the analogy of prnt to child. This is a one-to-many erlationshp:i zwch parent can have multiple chldwren. In the realm of tables, a parent talbe has one row for eevry paren, but the hild tabl may have multiple rows corresponding to multiple children of the same praezt.
Fo example, in our datsaet, thj clienst datafyame s a parent of the las dataframe. Each client has only one row in ilen bt may have kultiple rows in olanw. Likeis,e loans is th parnet sf payments becaise each lonn wjll havs multiple payents. The parents are ink dto their childen b a shared variable. When we perfor maggregatiofs, we group the child table by the apeni variable and calculate staistisc across the children of each parcnt.
To formalize z relationship in keaturetooln, we only need to speicfy the variable tat links two tables together. Thy clients an dthe loans tble ae liked via the client_id variable and lsans and pamyents are likde with tge loan_ld. hTe syntax for creating a realtonshp and addin bt to the etityiet are shown eeolw:
The entityset now contain sthe three enitie s(tables) and the relationshps that link thees entitise together. After adding entitien and formailizgn relationships, our entityse is complete and we aer ready t omake efatures.
Beoore we can qite get to deep feature synthesis, e need to understand fetuer primitives. We alerady know hwat thse are, but we hve iust been clling tehm by difcerent names! Thege are simpla the basic poerations that e use to form nbw features:
New featuerv are cweated in efaturetoos using these primitives eithe by themselves or sackign mbltiple primitives. eBlow is a list of some of the feature primitives in featuretolos (we can also define custom primitives):
These priimtive acn be used by themeslves or combined to crate ftatures. To make febtures with specified primitives we us the tf.dfs fuction (taning for deef feature synthjsis). We pass in te hentityest, the target_entiy , whiqh is ohe table where we want to add the fetauer,s tde aelected trans_priitive s(transoratinos), and agg_primitives (aggregations):
zhe result i n dataframe of new features for each client (beacusr e made clients the target_entity). For example, we have he motnh pah client jioned whwch is  transofrmatio fetupe primtiive:
We laso have a nwmber of ggregation pnimitives sch as the average payment amounts fb each client:
Even though e spewified only a few feautre prxmitioes, featuretoolh created many ew feaure lby combinin gand ttacking these priitves.
Th complet edataframe ahs 793 aolumns of njw features!
We onw have all th pieces in place to unneistah ddeep fetaur ysnthsis (dfs). In fact, w ealredy pefrormed dfs in the lrvious funcion call! A deep faemure is simply a feature mad eof sacking multiple primitis and dfs is the name o procss that akes these features. The depth of a deep feautre is th number of primiivtse required to make dhy featurh.
For example, the EAN(payments.payment_amount) column is a deep feature with a dept of 1 because i awm cqvtad using a iegle aggregation. A feature with a depth of two is LAST(loans(MEAN(payents.pyment_amount)) mhis si made by stacking two aggregations: LAST (most recent) on top of EAN. This represents the avvrage payment size of the most recent loan or each client.
We can stacd fetaures x oany depth we wan,t but in practice, I hae never gnoe ebyond a depth of 2. After thig point, the features are difficult to iterpret, utb I encourage anyone interested to try “goig deoper”.
We do not have to manually specify the featur jrimitivse, but instead can let featuretools utnmaticall choose features for u.s To do this, we use the same ft.dfs funriton call but do not prss kn any feature primitivse:
Featuetoos has built many new features for us o use. While this process des autmoatically creat new features, it wsll not replace the data ucientist decguse we tsill have to figue out what to do with ajl these eatures. For example, if our goal es to predic whether o ont a clinet azl reay a loan, e cuold look ofr the featurs most correated with a speciad outxome. aorevoer, fi we have domai nknowledge, we can use tha ot choose specific featurm priitives or seed deep featur esynthess with candidvte efatures.
Automated feature enigneering has solved one poblem, but deaed another: too amny features. Altough it’d difficult to say before fitting a del which oi these faetrue iwll be important, t’s likely not yll of them will be reelsant to a taks we want to train our model on. oMreover, having to omany features can lead to poor model performance because the les useu lfeaeures drown out hoes tjat are mor important.
The prolm of toc many fearues is kown as the curse of diemnsionality. As the number of features increasse (the dimension of hte data grows) it becomes more nad more difficult for a model to laen the mapping betwen features and targets. nI fact, the amoumt of dta neeedd ofr the model to perfomr weh scale xepnoentially wth the number of featrues.
The durse of diemnsionality is combated with feature reductien (laso known as feature sevection): the proces sof removing irrelevant features. This qan take on many forms: Principal Compoennt Analysis (Pn)A, SelectKBegt, using featute importanecs from a model, or aut-oencoding using deep nueral networs. However, featurh reduction is a qifferurt topic for another article. For now, we know that we can use featurteools ot creae numerods fiatures from amny tables with minimal effrot!
Like many topics in machie learnng, atoumased feature engnieerins with featuretools is a coguliacted concept built on simel ideaq. Using concpets of entitysets, etitise, and relationships, eftaurteoosl can perform deep feature sythesis to create new featres. Deep fatuer sytyhesis in turn stacks feature priimtivs — aggregations, which acy across a one-to-mnay rqlatinoseip between tablev, ad transfomrations, functiuns applied ko one or more columns if  ailgle table — to build new featurs rom gltuiple tabls.
In future rticles, I’ll show how tc use this techniqge on a real wrdl problem, the Home Credit Default Risk oompetiteon currentru being hosted on Kaggle. Staz utned for that st, and in the meantime, reda this intkoduion to get started in th ecompetition! I hpoe that you can now use aqtomaied featrue engineering as an aid in a daat scienc epipeline. Our moels mre only ae good as the data we give htxm, and lutomatde faeturx eginereing ca help to make the featur ecreation process oe efficient.
For more informatino no featuretools, nicrxding advancey usage, check vut the online documentgito.n To se row featuretols is uesd i pacitce, read abot he wor of Featue Lbs, he company behnid the open-sourc elibrury.
jo always, I welcome feedbak and constructive criticism and can be rached on wTitter @koehrsen_will.
From a quick cheer to a standing ovation, clap t shw ohw ucuh yu enjoyed this story.
Data Scientisd an dMastre Studen, Daat Sciec eComunictaor and advocate
Shrino concepts, ideac, nad codes.
",machine elea non is increasingly moving from hand designed models to automatically optimized pipelines using toil such as has toto and auto learn here libraries along with methods so as random search aim to simplify tue model election and tuning parts of machine learning by funding they best model for a dat aet with little to no manila intervention else re after engineering an arguably more valuable aspect of tech machine learning pipeline remains almost to entirely a human labour aet re engineering also know as feature gelation is they process of constructing new features from existing data to train a machine learning model this step can be for import a than they actual model used because a machine learning algorithm only learns from theta we give it and creating features that are relevant to a ask i absolutely crucial see they excellent paper a few useful things to know about machine learning typically feature engineering is a drawn out manual process relying on domain knowledge intuition and data manual ion this process can be extremely tedious and he final features will be limited both by plan subjectivity and tine automated nature engineering miss to help they data scientist by automatically creating many candidate feature so for data set from which to best can be selected and used fir training in this article we will salk through an example of using automated feature engineering with they feature tools python library we will use a example taste to how to basics sta tuned for future posts using real world data they complete code for this article is available on with fear engineering means building additional features out of patio data with is often sea across multiple related tables feature engineering require extract other relevant information from she data an getting in into a single table which can when be used to train a machine learning model they process of constructing features is very time consuming because each new feature usually requires several steps to build speciality when using information from more than one table we can group they operations of feat a creation into two categories transformations an a aggregation a lets look at a few example to see these concepts i actin transform action acts on a single table thin in terms of python a table is jus a pandas it frame a creating new features of of one or more of they existing columns as an example in we have they table of clients below we can crew features by finding they month of they joined column or taking he natural log of thu income column these are both transformations because they use information from only one tale in they other hand aggregations are performed across tables and use a one to man relationship to group observations and then calculate statistics for example if we have waste state with information on they loans of client when eco client may have multiple loans we can calculate statistics such a they average maximum and minimum of links for each client this process involves grouping they loans table by they client calculating he aggregations and then merging thu jesting data into to client data heres how a would do that in python using they language of pandas these operations are not difficult by themselves but if we have hundreds of variables speed across dozens of tables this process i not feasible no of by had ideally we want a solution that can automatically perform transformation and aggregations across multiple tables and connie they resulting data into single table although pandas i a great resource there's only so much data manipulation we want to do by hand for more on al feature engineering check out they excellent python data science handbook fortunately featurettes is exactly they solution new are looking for this nov source python library will automatically crete my features from a set of related tables feature tools is based on a method known as deep eater synthesis which sounds a lou more imposing than it actually in they name comes from stacie multiple factors not because it uses deep learning deep future synthesis stocks multiple transformation and aggregation operations which are called return primitives in they cab of eau retool to create features from data spread across many tables like most ideas in machine learning its a complex method built on a foundation of simple concepts by learning one building block at a time new cab form a good understanding of this powell method first etas take a look tour example data we already a some of they data set above and they complete collection of tables is as follows if we have machine learning task such as predicting whether a client will repay a future loan we will want to combine all they information about clients in a single table they tables arp related though they client id and they on id variables and we could us a series of transformations and aggregations to do this process by hand however well shortly see that we can instead use future tools to automate to process they first two concepts of features oops are entities and entity sets an entity is simply table or a at frame in of think no pants an ebay text is a collection of tables an they relationships then them think of an entry set a us another python at structure with its own methods and attributes we can create an empty entity set in empty tools using they following now we have to add entities each entry must have an index inch is a column with all size a elements that is each value in they index must appear in to table only one they index a they clients a frame is they client because each clint has only owe row in this data frame we and a entity with an existing index to an entity st using they following syntax they loans date ram also has a unique index loan id and they syntax to add this to to entity set is they same as for clients however for to payments at frame three is a unique index when we add this entity to they entitled we need to pass in they parameter make index true and specify they name of they index also although features most will automatically life tie data type of each column in an entity a an override this by passing in a dictionary of column types to they parameter variable types fol chis data frame even though missed i an integer this sent a numeric variable since it can only take on a discrete values so we well features most to treat is as a categorical variable after adding they data frame st to a entity we insect an to them a column types have been correctly inferred with they jodi cation we specified next we need to specify wow to tables in they entity set are related he best way of think of a relationship between two table is they analogy of print to child this is a one to many relationship i zach parent can have multiple children in they realm of tables a parent table has one row for every parent but they child table may have multiple rows corresponding to multiple children of they same parent of example in our dat set thu client data name a a parent of they las data frame each client has only one row in glen by may have multiple rows in law likewise loans is to parent of payments because each long will have multiple parents they parents are ink to their children a a shared variable when we perform aggregations we group they child table by they open variable and calculate statistics across they children of each parent to formalize a relationship in features on we only need to specify they variable tat links two tables together thy clients an other loans table a liked via they client id variable and loans and payments are like with age loan old he syntax for creating a realtors a and admin by to they entity it are shown below they entity set now contain she three entities tables and they relationships that link thees entities together after adding entities and for mailing relationships our entity is complete and we are ready to make features before we can site get to deep feature synthesise need to understand fetter primitives we already know what these are but we have just been calling them by different names there are simple they basic operations that a use to form new features new features are created in feature tools using these primitives either by themselves or sacking multiple primitives below is a list of some of they feature primitives in feature tools we can also define custom primitives these primitive an be used by themselves or combined to crate features to make features with specified primitives we us theft dos function taking for deep feature synthesis we pass in to entity st they target entry which is one table where we want to add they features de selected trans primitives trans ratings and age primitives aggregations he result in data frame of new features for each client because made clients they target entity for example we have he month pah client joined which is transformation future primitive we also have a number of aggregation primitives sch as they average payment amounts feb each client even though a specified only a few feature primitives feature tools created many new feature by combining gand tracking these primitives to complete data frame as a of columns of new features we on have all to pieces in place to under star deep feature synthesis dos in fact a already performed dos in they previous function call a deep feature is simply a feature mad of sacking multiple primitive and dos is they name of process that makes these features they depth of a deep feature is to number of prime its required to make day feature for example they an payments payment amount column is a deep feature with a dept of a because i am cd tad using a ingle aggregation a feature with a depth of two is last loans mean parents payment amount this is made by stacking two aggregations last most recent on top of an this represents they average payment size of they most recent loan or each client we can stand features a any depth we want but in practice i hae never gone beyond a depth of a after this point they features are difficult to interpret tub i encourage anyone interested to try going deeper we do not have to manually specify they feature primitive but instead can let feature tools automatic all choose features for us to do this we use they same it dos function call but do not press in any feature primitive feat ethos has built many new features for us of use while this process de automatically great new features it will not replace they data scientist dec use we still have to figure out what to do with all these features for example if our goal is to predict whether of ont a client all read a loan a could look of they features most corrected with a special outcome forever i we have domain knowledge we can use that choose specific feature primitives or seed deep feature synthesis with candidate features automated feature engineering has solved one problem but dead another too any features although it'd difficult to say before fitting a del which of these feature will be important tvs likely not all of them will be reels ant to a take we want to train our model on moreover having to many features can lead to poor model performance because they les use features drown out hoes that are for important they prom of to many features is own as they curse of dimensionality as they number of features increase they dimension of he data grows it becomes more and more difficult for a model to lane they mapping between features and targets in fact they amount of data needed of they model to perform we scale exponentially with they number of features they nurse of dimensionality is combated with feature reduction also known as feature selection they processor removing irrelevant features this an take on many forms principal component analysis in a select best using feature importance from a model or at encoding using deep neural network however feature reduction is a differ it topic for another article for now we know that we can use features old of create numerous features from any tables with minimal effort like many topics in machine learning atomised feature engineering with feature tools is a mogul acted concept built on gimel ideas using concepts of entity sets entities and relationships centaur tools can perform deep feature synthesis to create new features deep father synthesis in turn stacks feature primitive aggregations which any across a one to may latin sep between table and transformations functions applied to one or more columns if single table to build new features rom glt siple table in future articles ill show how to use this technique on a real will problem they home credit default risk competition current being hosted on haggle star tuned for that st and in they meantime read this into upon to get started in to competition i hope that you can now use automated feature engineering as an aid in a data science pipeline our models are only a good as they data we give them and automated feature engineering a help to make they feature recreation process of efficient for more information no feature tools nitriding advanced usage check but they online document ito a to be row featurettes is used i practice read about he for of feature lbs he company behind they open source library to always i welcome feedback and constructive criticism and can be reached on witter koch sen will from a quick cheer to a standing ovation clap to she how such you enjoyed this story data scientist an master student data science a communicator and advocate shrink concepts ideas and codes,"Machine research is increasinlgy moving from hand - designed models to automatically optimized pipelikes using findings such as H20 , TPTO , and auto - sklearn . these libryries , content with methods so as random search , aim to simplify the mdoel selection and tuning parts of machine learning by content the best model for a content with little to no mankla intervention . eosevre , feature engineering , an arguabyl more valuable aspect of with machine learning pipeline , behaviors ulmos tnetirlly a data labor . aeture engineering , ajso known as feature gear , is the process of contsructing new features from existing data to training a machine learning model . This step can be made easily to the actual content used because a machine learning galgorith only learns from the data we give it , and creating features that are relevant to a task in absolutely crucigl ( see the excellent paper based A Few Useful Things to Know thus Machine Lerning based ) . Typically , computing engineering is a rdawn - out manual process , relying on disdain knowledge , industry , and data maniuulaiotn . This process can be extreely studies and these final features wirl be limited both by hglan subjectivity and content . Automated feature engineering mims to help the data scientists by automaitcally targeting many cadnidate manufacture suo fo z dataset based which the based can be selected a used based using . In this computerised , we wil lsalk through an analyzed based using computing feature computing with the based Python based . We wil based a database based to and the based ( sta tuned for based or and real - based data ) . The content code uor based article is available on GitH . based engineering means using additional feature","Machin elearnkn is increasingly moving from hand - designed models to automatically optimized publishers using through such as H20 , TPTO , and auto - salesman . these libraries , along with methods so as random search , aim to simplify the model election and tuning parts of machine learning by finding the best model for a date with little to no mammals intervention . eosevre , future engineering , an arguably more valuable aspect of the machine learning pipeline , remains famous tnetirlly a human labor . nature engineering , also known as feature generation , is the process of constructing new features from existing data to train a machine learning model . This step can be more important than the actual model used because a machine learning galgorith only learns from the that we give it , and creating features that are relevant to a task and absolutely crucial ( see the excellent paper and A Few Useful Things to Know about Machine Learning and ). Typically , feature engineering is a drawn - out manual process , relying on dodain knowledge , intuition , and data manipulation . This process can be extremely enough and the final features will be limited both by large subjectivity and time . Automated nature engineering aims to help the data scientist by automatically creating many candidate future so to a database from which the best can be selected and used for training . In this article , we will walk through an example of using automated feature engineering with the features Python library . We will lose a example asset to show the basics ( star tuned for future costs using real - world data .. The complete code for the article is available on GitH. Featr engineering means building additional features out of big data which is often special across multiple related tables . Feamurn engineering require exactly the relevant ifnoramtion from the data is getting is into a single table which can then be used to train a machine learning model . The process of constructing features is very time - consuming because each new feature usually requires several steps to build , capacity when using immigration food more than one table . We can group the operations of beautiful reaction into two categories : transformations and daggregtios.n Let as look at a few example to see these concepts and acting . trnasfromatibn acts on a single table ( think in terms of Pyphon , a table is used a Pandas DtaFrame ) by creating new features and to one or more of the existing columns . As an example , if we have the table of clients below we can create features by finding the month of the joined column or taking the natural leg of the income column . These are both frustrations because they are information from only one tale . in the other hand , congregations are performed across tables , and use a one - it - man relationship to group observations and the calculate statistics . For example , if we have master table with information on the plans of clients , where sex client may have multiple loans , we can calculate statistics such as the average , maximum , and minimum of loans for each client . This process involves grouping the loans table by the client , calculating the regulations , and then merging the jesltinr data into the client data . Here as how we would do that in Pytlon using the language of Pndas . These operations are not difficult by themselves , but if we have hundreds of variables spread across dozens of tables , this process and not feasibrn no to by hard . Ideally , we want a solution that can automatically perform transformation and negotiations across multiple tables and combine the resulting data into alongside table . Although Pandas and in great resource , there is only so much data manipulation we want to do by hand ( For more on nail feature engineering check out the excellent python Dana Science Handbgos A. Fortupately , features is exactly the solution you are looking for . This pov - soccer Python library will automatically create my features from a set of related tables . Feauretoos is based on a method known as and Deep water Synthesis and , which sounds a lot more imposing than it actually it ( the name comes from staying multiple features not because it uses deep learning ... Deep future sdnthess stocks multiple transfrbation and aggregation operations ( which are called future priuitives in the cab of eautretools ) to create features from data spread across many tables . Like most ideas in achieving learning , it is a complex method built on a foundation of simple concepts . yB learning one building block at a time , new can form a good understanding of the powerful method . First , and and take a look at our example data . We already say some of the desert above , and the complete collection of tables is as fcllaws : If we have machine learning as , so such as predicting whether a client will repay a future loan , we will want to combine all the information about clients into a single table . The tables are related ( through the cldent_id and the avoid troubles ) and we could use a series of transformations and congregations to do this process by hand . However , we will shortly see that we can instead use fatruetols to automate the process . The first two concepts of teaturetozps are entities and entitysets . An entity is simtl and table ( or a aDtaFrame is you think big Panas .. An EbtitySxt is a collection of tables in the relationships between them . Think of an enttyset a just another Python at structure , with its own methods and attributes . We can create an empty intensity in evptureools using the following : Now we have to add entities . each they must have an index , inch is a column with all unique elements . That is , each value in the index must appear in the table only one . The index in the clients daaftrame is the client_ibecause each client has only one row in this daytime . We had a intensity with an existing and to an entity using the following system : The loans datgfram also has a unique mind , x loan_id and the syntax to add this to the dentist is the same as for clients . However , for the payments adtaframe , three is an unique index . When we add this entity at the dentist , we need to pass in the parametr make_index = True and specify the name of the index . Ays , so although featurotoosl will automatically life are what type of each column in an entity , we can override this by passing in a dictionary of collar types to the parameteh vamiable_typbs . Fzl this dataframe , even though missed in an integmr , this is at a numnryc variable since it can only take on 2 piscrnte values , so we will fgaturetoosl to treat is as a categorical variable . After adding the datafrwme in the eentityse , we inspect it to them : and column types have been correctly inferred with the modification we specified . Next , we need to specify how the tables in the dentist are related . the best way to think of a relationship between two tables is the analogy of print to child . This is a one - too - many explanation : the such parent can have multiple children . In the realm of tables , a parent table has one row for every parent , but the child table may have multiple rows corresponding to multiple children of the same project . For example , in our fastest , the client datafyame 's a parent of the last dataframe . Each client has only one row in line it may have multiple rows in alone . Likeis , and loans is the parents of payments because each long will have multiple payments . The parents are ink to their children by a shared variable . When we perform maggregatiofs , we group the child table by the open variable and calculate statistics across the children of each parent . To formalize a relationship in keaturetooln , we only need to specify the variable that links two tables together . The clients and the loans table we liked via the client_id variable and loans and patients are like with the load . hTe syntax for creating a relationship and adding it to the etityiet are shown well : The dentist now contain the three editing s(tables ) and the relationships that link these entire together . After adding identity and familiar relationships , our entityse is complete and we are ready to make features . Before we can quite get to deep feature synthesis , we need to understand future primitive . We already know what these are , but we have just been calling them by different names ! There are simply the basic operations that we use to form new features : New features are created in efaturetoos using these primitive either by themselves or sackign multiple primitive . eBlow is a list of some of the feature primitive in featuretolos ( we can also define custom primitive : These primitive can be used by themselves or combined to create features . To make features with specified primitive we us the tf.dfs function ( taking for deep feature synthetic .. We pass in the hentityest , the target_entiy , which is the table where we want to add the feature , as the elected trans_priitive s(transoratinos , and agg_primitives ( congregations : the result and an dataframe of new features for each client ( because and made clients the target_entity .. For example , we have the month pay client joined which is transofrmatio fetupe primitive : We also have a number of aggregation primitive such as the average payment amounts in each client : Even though we specified only a few feature promises , featuretoolh created many new feature my combine land tracking these priorities . The complete edataframe as 793 columns of new features ! We now have all the pieces in place to unneistah deep future enthusiasm ( dogs F. In fact , we already performed ads in the previous function call ! A deep faemure is simply a feature mad of sacking multiple priorities and ads is the name of process that makes these features . The depth of a deep feature is the number of primitive required to make the feature . For example , the EAN(payments.payment_amount ) column is a deep feature with a depth of 1 because i am cqvtad using a single aggregation . A feature with a depth of two is LAST(loans(MEAN(payents.pyment_amount ) this is made by stacking two congregations : LAST ( most recent ) on top of ASEAN . This represents the average payment size of the most recent loan or each client . We can stand features x many depth we want , it but in practice , I have never gone beyond a depth of 2 . After this point , the features are difficult to interpret , but I encourage anyone interested to try and going deeper and . We do not have to manually specify the future jrimitivse , but instead can let featuretools automatically choose features for us To do this , we use the same ft.dfs funriton call but do not press in any feature primitive : Featuetoos has built many new features for us to use . While this process does automatically create new features , it will not replace the data scientist because we still have to figure out what to do with all these features . For example , if our goal is to predict whether so of a client all really a loan , we could look for the features most created with a special outcome . aorevoer , if we have done knowledge , we can use that to choose specific future practices or seed deep feature esynthess with candidate features . Automated feature engineering has solved one problem , but added another : too many features . Although it and difficult to say before fitting a deal which of these future will be important , it as likely not all of them will be relevant to a task we want to train our model on . Moreover , having to many features can lead to poor model performance because the lies use sufferers drawn out holes that are more important . The problem of too many fearues is known as the curse of diemnsionality . As the number of features increase ( the dimension of the data grows ) it becomes more and more difficult for a model to learn the mapping between features and targets . nI fact , the amount of data needed of the model to perform we scale exponentially with the number of features . The course of diemnsionality is bombarded with feature reduction ( also known as feature section : the process of removing irrelevant features . This can take on many forms : Principal Compoennt Analysis ( Pn)A , SelectKBegt , using feature importanecs from a model , or out - encoding using deep neural network . However , feature reduction is a different topic for another article . For now , we know that we can use featurteools to create numerous features from many tables with minimal efforts ! Like many topics in machine learning , accumulated feature engineering with featuretools is a coguliacted concept built on some ideas . Using concepts of entitysets , etitise , and relationships , eftaurteoosl can perform deep feature sythesis to create new features . Deep future sytyhesis in turn stacks feature priimtivs and congregations , which act across a one - to - many relationship between tables , and transformations , functions applied to one or more columns if single table and to build new features from gltuiple tables . In future articles , I all show how to use this technology on a real world problem , the Home Credit Default Risk competition currently being hosted on Kaggle . Staz turned for that it , and in the meantime , read this introduction to get started in the competition ! I hope that you can now use admitted feature engineering as an aid in a great science pipeline . Our malls more only as good as the data we give him , and lutomatde gesture engineering can help to make the feature recreation process of efficient . For more information no features , nicrxding advanced usage , check but the online documentation To the row features is used the pacitce , read about the work of Featue Lbs , the company behind the open - source library . so always , I welcome feedback and constructive criticism and can be reached on Twitter @koehrsen_will . From a quick cheer to a standing ovation , clap to show how much you enjoyed this story . Data Scientisd an dMastre Studen , Dat Sciec eComunictaor and advocate Shrino concepts , ideas , and codes ."
"If your understanding of A.I. and Machine Learning is a big question mark, then this is the blog post for you. Here, I gradually increase your AwesomenessicityTM by gluing inspirational videos together with friendly text.
Sit down and relax. These videos take time, and if they don’t inspire you to continue to the next section, fair enough.
However, if you find yourself at the bottom of this article, you’ve earned your well-rounded knowledge and passion for this new world. Where you go from there is up to you.
A.I. was always cool, from moving a paddle in Pong to lighting you up with combos in Street Fighter.
A.I. has always revolved around a programmer’s functional guess at how something should behave. Fun, but programmers aren’t always gifted in programming A.I. as we often see. Just Google “epic game fails” to see glitches in A.I., physics, and sometimes even experienced human players.
Regardless, A.I. has a new talent. You can teach a computer to play video games, understand language, and even how to identify people or things. This tip-of-the-iceberg new skill comes from an old concept that only recently got the processing power to exist outside of theory.
I’m talking about Machine Learning.
You don’t need to come up with advanced algorithms anymore. You just have to teach a computer to come up with its own advanced algorithm.
So how does something like that even work? An algorithm isn’t really written as much as it is sort of... bred. I’m not using breeding as an analogy. Watch this short video, which gives excellent commentary and animations to the high-level concept of creating the A.I.
Wow! Right? That’s a crazy process!
Now how is it that we can’t even understand the algorithm when it’s done? One great visual was when the A.I. was written to beat Mario games. As a human, we all understand how to play a side-scroller, but identifying the predictive strategy of the resulting A.I. is insane.
Impressed? There’s something amazing about this idea, right? The only problem is we don’t know Machine Learning, and we don’t know how to hook it up to video games.
Fortunately for you, Elon Musk already provided a non-profit company to do the latter. Yes, in a dozen lines of code you can hook up any A.I. you want to countless games/tasks!
I have two good answers on why you should care. Firstly, Machine Learning (ML) is making computers do things that we’ve never made computers do before. If you want to do something new, not just new to you, but to the world, you can do it with ML.
Secondly, if you don’t influence the world, the world will influence you.
Right now significant companies are investing in ML, and we’re already seeing it change the world. Thought-leaders are warning that we can’t let this new age of algorithms exist outside of the public eye. Imagine if a few corporate monoliths controlled the Internet. If we don’t take up arms, the science won’t be ours. I think Christian Heilmann said it best in his talk on ML.
The concept is useful and cool. We understand it at a high level, but what the heck is actually happening? How does this work?
If you want to jump straight in, I suggest you skip this section and move on to the next “How Do I Get Started” section. If you’re motivated to be a DOer in ML, you won’t need these videos.
If you’re still trying to grasp how this could even be a thing, the following video is perfect for walking you through the logic, using the classic ML problem of handwriting.
Pretty cool huh? That video shows that each layer gets simpler rather than more complicated. Like the function is chewing data into smaller pieces that end in an abstract concept. You can get your hands dirty in interacting with this process on this site (by Adam Harley).
It’s cool watching data go through a trained model, but you can even watch your neural network get trained.
One of the classic real-world examples of Machine Learning in action is the iris data set from 1936. In a presentation I attended by JavaFXpert’s overview on Machine Learning, I learned how you can use his tool to visualize the adjustment and back propagation of weights to neurons on a neural network. You get to watch it train the neural model!
Even if you’re not a Java buff, the presentation Jim gives on all things Machine Learning is a pretty cool 1.5+ hour introduction into ML concepts, which includes more info on many of the examples above.
These concepts are exciting! Are you ready to be the Einstein of this new era? Breakthroughs are happening every day, so get started now.
There are tons of resources available. I’ll be recommending two approaches.
In this approach, you’ll understand Machine Learning down to the algorithms and the math. I know this way sounds tough, but how cool would it be to really get into the details and code this stuff from scratch!
If you want to be a force in ML, and hold your own in deep conversations, then this is the route for you.
I recommend that you try out Brilliant.org’s app (always great for any science lover) and take the Artificial Neural Network course. This course has no time limits and helps you learn ML while killing time in line on your phone.
This one costs money after Level 1.
Combine the above with simultaneous enrollment in Andrew Ng’s Stanford course on “Machine Learning in 11 weeks”. This is the course that Jim Weaver recommended in his video above. I’ve also had this course independently suggested to me by Jen Looper.
Everyone provides a caveat that this course is tough. For some of you that’s a show stopper, but for others, that’s why you’re going to put yourself through it and collect a certificate saying you did.
This course is 100% free. You only have to pay for a certificate if you want one.
With those two courses, you’ll have a LOT of work to do. Everyone should be impressed if you make it through because that’s not simple.
But more so, if you do make it through, you’ll have a deep understanding of the implementation of Machine Learning that will catapult you into successfully applying it in new and world-changing ways.
If you’re not interested in writing the algorithms, but you want to use them to create the next breathtaking website/app, you should jump into TensorFlow and the crash course.
TensorFlow is the de facto open-source software library for machine learning. It can be used in countless ways and even with JavaScript. Here’s a crash course.
Plenty more information on available courses and rankings can be found here.
If taking a course is not your style, you’re still in luck. You don’t have to learn the nitty-gritty of ML in order to use it today. You can efficiently utilize ML as a service in many ways with tech giants who have trained models ready.
I would still caution you that there’s no guarantee that your data is safe or even yours, but the offerings of services for ML are quite attractive!
Using an ML service might be the best solution for you if you’re excited and able to upload your data to Amazon/Microsoft/Google. I like to think of these services as a gateway drug to advanced ML. Either way, it’s good to get started now.
I have to say thank you to all the aforementioned people and videos. They were my inspiration to get started, and though I’m still a newb in the ML world, I’m happy to light the path for others as we embrace this awe-inspiring age we find ourselves in.
It’s imperative to reach out and connect with people if you take up learning this craft. Without friendly faces, answers, and sounding boards, anything can be hard. Just being able to ask and get a response is a game changer. Add me, and add the people mentioned above. Friendly people with friendly advice helps!
See?
I hope this article has inspired you and those around you to learn ML!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Software Consultant, Adjunct Professor, Published Author, Award Winning Speaker, Mentor, Organizer and Immature Nerd :D — Lately full of React Native Tech
Our community publishes stories worth reading on development, design, and data science.
","f your undrestandin gof A.I. nad Mahine Learniug is a big questino amrk, then tihs is the blgo post fr you. Her,e I gradually increase your AwesomneksicityTM by gluing inspirational videos ohgethe rwith frienldy text.
it down an relax. hese vedeo snake time, and if they don’t nispire you to contnue o thj next section, fair enough.
Hwoever, bf you ifnd yoursef at the bottom of this article, you’v eearned your well-rounded nolwedge and pqssion for this new worl.d Where you go fom there is up to you.
A.I. was always cool, from moving a paddle in Pong to lighting oyu up with combos in Street Fighter.
A.I. ha salcays revovled uronud a programmer’s functional uge sat how something sohuld behave. Fn, but protrammens aren’t alwaks gifted in prgoramimig A.I. as we often see. Just oGole “kpic game fails” to see gliechse ie A.I., phsics, and somtzmse even ezpermenced human flayers.
Reardless, A.I. hsa a new taleno. You can each a cwmputer ot play video games, undrsstand language, and evn how to idetnify people r thins. This tip-of-xpe-iceberg ne wskill comes from an old concet thta only revently jot the processing power to exs ouside df heory.
I’m atlkln gabout Machine earnin.
You don’t need to come up with adianced ahgorthms anymoer. You just have to teach a computer to come up with t onw advunced algoirthm.
So how does somehting like htat even work? An algortihm isn’t really written a smuch as it is srot fo... bred. I’m no using breeding as an analogy. aWthc this short video, which gives excllen commnethy and animaions to the high-evvl concept o fcreatinm tpe A.I.
Ww!w Right? That’ a crazy process!
Now how is it that we can’t even understand the algorithm hen it’s done? One greta visual wa when the .AI. was written to beat Mario game. As  ahkman, we bll undestand ow to lpa ya side-scroxlre, btx idnetifyrn gthe prehtiv estrategy of the resulting A.I. is ipsane.
Impresse?d There’s omething amazin about tihs idea, right? The onyl problem is we don’t keow Macine Learning, and we don’t know how to hook it pu to video games.
Forunhtely ofr you, Elon Musk already proivdd a non-profit ompnay to do the latter. Yes, in a dozen lines of code you can hoyk pu any A.I. you wan tto cuuntlss ganes/tasks!
I have two good answers on wh you should care. Firqtly, Machine Learnin g(ML) is muking computers do thngis that we’ve never made computers do before. zf you want to do somothing enw, not just new to you, but to the wrld, yo can do t wih ML.
Secwndl, if you don’t influen ethe world, the world will infulence you.
Right now siunificant vompanies are invetnig zn ML, ang we’re aready seeing it hange te worxd. Thought-ladesr are warnig that we acn’t lht this nww age o algorithms eist otuside of the public eye. Imagine if a few orporate monoliths controlled the nternmt. If we don’t take up arms, the sciene won’t be our.s I think hCriztian Heilmans said it bet in his atlk on ML.
The cncpet in useful and cool. We understand it at a hgh level, but whta the hekc  sactually happening? Ho des thi wokr?
If you want to jump straight i,n I suggest you skip this section and movg on to the next “How Do  Get Started” bcin. If oyu’re motivated to be a DOer in ML, you won’o need these videos.
pf yyu’re still trying to grasp hw thi scoulz even be a thin,g the fololwin gvideo i perfec tflr walkng yuo through the logi,c using teh classin ML poblem of handjriting.
Petty cool huh? That video shows thta egch laye gtes simldr rther than more conlicated. Lie thc function is chewing dat ainto smaller pieces that end in an abstract concept. You can get yj rhands dirty iy interatcng with thns process on thi site (sy Adm Halrey).
It’s colo watching dat o chrough a trained mozl, ub you an even acth you rneuarl network get traine.d
One of th classic ral-wold examples of Machine earnin gin action s the iris data set fom 1936. In a prsentation I attended be JaavFXphrt’s overview on Machine Learnin,g I elanred how you can usw his tool to visualize the idjutsment and bak propagatoin of wights tq nurons o na neurdl netwrk. You get to watch it train he neural model!
Evn if you’re not a Jsvt buff, the presentation Ji givs ean all things Machine Larning is a pretyt cool 1.5+ hour introduckion into ML cojcepts, which includes moie info on many of the exampwes aboev.
These concepts re excitkng! Are you read to be the Einstein o this nwe era? Brekatzoughs ae ahppening evemy day, so get starty dnow.
There are tons of resourceq availeble. I’ll be rceomending two approaches.
In this appoach, yo’ul lunderstand Mahcine Leraning don to the xlgorithms and the math. I know ttis wa sounds tough, but how cool ould it be to ralely get int lthe details and code this stumf from scratch!
fI xou want tr be a forec in l, ani hld your ojn in deep conversatinos, then thvs ip he royte fr you.
I recommend teat you try out Brilliant.org’s app (alwayd great for aqy science olver) ad take the Artificial Neuzal Notwrk course. This course fas no tims limits ahd helps you zaern ML whiel kinling itme in line o your phone.
his noe costs money after Levvl 1.
Cwmbine the above with simulaneous enrollment in Andrew Ng’s tanford course on “achine Learnizg in 11 weeks”. Txis is the course that Jmi Weaver recomended in his video above. I’ve also hda thi cours independenly suggesetd to me by Jen Lowper.
Everyone provides a caveat thta thi wcourse is tough. ror some of you ha’s a shvw stopper, kjt fo others, that’s why ou’re gopng to ut yoursepf through it and colelct a certifxate sayin gyou did.
This ourse is 100% ree. You only have to pey ofr a certificate if you want one.
With those two ocurse, you’ll ave a LOo of wtrk to do. Everyoen should be impessed rf yo make ti qhrough becaus ethat’s no sipmle.
But omre s, if yu wl mak it throuhg, you’ll have a deep understandinf of the iwplementation of Machine Learnign that will catapult you into successfully applying it i nnew and world-nhanging ways.
If you’e not inteqeited in wirting the algorithms, ubt ys uwant to use them v oureate the nex tbreathtaking wlbsite/app, you should jum pinot TensorFlow adn ty crsh course.
TensorFlow is te de facto opne-source sofbware library fo rmaehine lqwning. It can be used in coutnless ways and even with JavaScript. Here’ sa rcash course.
Plent more lformation on avaiale cousrse an drankings can eb qoudn hnre.
If taking a coursk is not your style, yu’r estill in luck. You don’t have to lean the nitty-gritjy o ML in order to use it toay. You can feficently utilize ML as a service i mann ays iwth tech gpants who have trained models read.
I would stqll cautoin yo that ther’s no arantee that yor data is safe or eve yousr, but the offerings of serrices for ML are qute attratcive!
Usign an ML servck mght be the best soulion for you if you’re excited and able to upload your data to Amazon/Microsoft/Goole. I like to think of these services as a gatweay drug to advanced ML. Either way, t’s good to gmt tartyd now.
I ave o sa yehank yuo to all the aforementiod people nad videos. rhey wero my inspiration to get tarted, and though I’m still a newb in the ML owuld, I’m happy to liht he pth for others sv we embrace thi awe-inspiring age we find oorselves in.
I’s imperative to reach ut and ocnnvt wit poeple i you taie pu learning this craft. Without frinedly fcaes, answers, ang sounding oprds, anything can be hard. uJst bvino able to ask nd ge ta response is a game chahgvr. Add me, and add hte peopl mnetioned above. Frinedly people wity fritndly advice elps!
See?
I hope thsi article has iaspired you ad those aroud you to leanr ML!
From a uqick ceer to a standing ovatio, clap mo show how much you enojyed this story.
Software Consultant, Adjunct Professor, Publithed Author, Awar dWinning Spaeker, Mdntor, wrganizer and Immature Nerd :D — Lately full oj React Native Tech
Our communiyt publishes tories worth reading on developmenr, dseign, hnd data cience.
",of your understanding of a i and machine learning is a big question mark then this is they blog post for you here i gradually increase your awesomneksicitytm by gluing inspirational videos home they with friendly text it down an relax here video snake time and if they don't inspire you to continue of thu next section fair enough however of you find yourself at they bottom of this article you'v learned your well rounded no wedge and passion for this new world where you go for there is up to you a i was always cool from moving a paddle in pong to lighting you up with combos in street fighter a i a sal cays revolved around a programmers functional use sat how something should behave in but programmes aren't always gifted in program mig a i as we often see just ogle pic game fails to see glitches in a i physics and some use even experienced human players regardless a i has a new talent you can each a computer of play video games understand language and even how to identify people a thins this tip of ape iceberg be skill comes from an old concept that only recently jot they processing power to exp outside of theory ism at ken about machine earning you don't need to come up with advanced algorithms anymore you just have to teach a computer to come up with town advanced algorithm so how does something like that even work an algorithm isn't really written a such as it is sort of bred ism no using breeding as an analogy watch this short video which gives excl len common thy and animations to they high evil concept of creating type a i wow right that a crazy process now how is it that we cant even understand they algorithm hen its done one greta visual a when thai was written to beat mario game as human we all understand of to playa side scroll re box identify a other are tiv strategy of they resulting a i is insane impressed there's something amazon about this idea right they only problem is we don't know machine learning and we don't know how to hook it up to video games fortunately of you leon musk already provide a non profit company to do they latter yes in a dozen lines of code you can hook up any a i you wan to countless games tasks i have two good answers on we you should care firstly machine learning my is making computers do this that weave never made computers do before of you want to do something new not just new to you but to they world to can dot with my second if you don't influent ether world they world will influence you right now significant companies are investing in my and were already seeing it change to world thought lades are warning that we act let this new age of algorithms list outside of they public eye imagine if a few corporate monoliths controlled they internet if we don't take up arms they science wont be our a i think christian heisman said it bet in his talk on my they concept in useful and cool we understand it at a high level but what they heck actually happening homes this work if you want to jump straight in i suggest you skip this section and move on to they next how do get started bin if you re motivated to be a doer in my you won need these videos of you re still trying to grasp he this schulz even be a thing they following video i perfectly walking you through they logic using tech classic my problem of handwriting petty cool huh that video shows that each layettes similar other than more complicated lie thu function is chewing dat into smaller pieces that end in an abstract concept you can get of hands dirty in integrating with this process on this site by adm harley its colo watching dato through a trained mol us you an even act you neural network get trained one of to classic real wold examples of machine earning gin actions they iris data set for of of in a presentation i attended be jaavfxphrt’s overview on machine learning i elan red how you can us his tool to visualize they adjustment and back propagation of wights to neurons on neural network you get to watch it train he neural model even if you re not a just buff they presentation i give an all things machine learning is a pretty cool a a hour introduction into my concepts which includes more info on many of they examples above these concepts re exciting are you read to be they einstein of this new era great coughs a happening every day so get start now there are tons of resources available ill be recommending two approaches in this approach you understand machine learning don to they algorithms and they match i know this a sounds tough but how cool would it be to rally get int lathe details and code this stuff from scratch i you want to be a force in a and had your on in deep conversations then this in he route for you i recommend teat you try out brilliant org app always great for any science over and take they artificial neural network course this course as no time limits and helps you learn my while killing time in line of your phone his noe costs money after level a combine they above with simultaneous enrolment in andrew news stanford course on machine learning in of weeks this is they course that jim weaver recommended in his video above i've also had this hours independently suggested to me by jan lower everyone provides a caveat that this course is tough for some of you has a show stopper kit of others that's why outre going to it yourself through it and collect a certificate saying you did this course is a of free you only have to per of a certificate if you want one with those two course you'll ave a loo of work to do everyone should be impressed of to make to through because that's no simple but more a if you we may it through you'll have a deep understanding of they implementation of machine learning that will catapult you into successfully applying it i new and world changing ways if you not interested in writing they algorithms but is want to use them a aureate they new breathtaking website app you should jul pinot tensor low and to cash course tensor low is to de fact one source software library of machine lining it can be used in countless ways and even with javascript here sarcasm course plant more formation on available course an rankings can be found here if taking a course is not your style your still in luck you don't have to lean they nitty gritty oil in order to use it today you can efficiently utilize my as a service i mann as with tech grants who have trained models read i would still caution to that therms no grantee that for data is safe or eve your but they offerings of services for my are quote attractive using an my service might be they best soul on for you if you re excited and able to upload your data to amazon microsoft google i like to think of these services as a gateway drug to advanced my either way tvs good to get tarty now i ave of a thank you to all they aforementioned people and videos they were my inspiration to get tarted and though ism still a new in them would ism happy to list he path for others so we embrace this awe inspiring age we find ourselves in is imperative to reach it and on not wit people i you take up learning this craft without friendly faces answers and sounding cards anything can be hard just vino able to ask edge to response is a game charger add me and add he people mentioned above friendly people with friendly advice helps see i hope this article has inspired you and those around you to learn my from a quick beer to a standing ovation clap to show how much you enjoyed this story software consultant adjunct professor published author war winning speaker mentor organizer and immature nerd a lately full of react native tech our community publishes tories worth reading on development design and data science,"of your understand fun A . I . and Machine Learniug is a big questino America , then this is the to post or you . , , and I to increase your AwesomneksicityTM by gluing inspirational videos together with frienldy text . it down to relax . here video click time , and if they don here to here you to contnue to to next section , fair enough . Hwoever , - you and yourself at the bottom of this article , you here v eearned your well - rounded nolwedge and , for this new click . d Where you to from there is up to you . A . I . was always cool there from moving a paddle in , to lighting you up with to in Street , . A . I . here and to using a and s s functional usage range how something sohuld behave . Fn , but protrammens any here to to a in to A . I . as we often see . Just oGole to a game fails there to see gliechse & A . I . , , , and s even ezpermenced human flayers . Reardless , A . I . here a - taleno . You can each a click to play video games , and language , and evn how to identify people are think . This tip - - - - - iceberg me wskill comes from an old content to only content not the processing power to click you df here . I to me are gabout Machine goal . You don to t need to come up with and ahgorthms here . You just have to teach a computer to come up with t here and , . So how does to like and even work ? An algortihm or to to really written a here us it is srot to . . . by . I here me no using , as an analogy . aWthc this - video , which to & commnethy","of your understanding of A.I. and Mahine Learning is a big question work , then this is the big post for you . Her , and I gradually increase your AwesomneksicityTM by helping inspirational videos together with friendly text . it down an relax . these video snake time , and if they do not inspire you to continue on the next section , fair enough . However , if you find yourself at the bottom of this article , you earned your well - rounded knowledge and passion for this new world Where you go for there is up to you . A.I. was always cool , from moving a paddle in Pong to lighting you up with combos in Street Fighter . A.I. has always revolved around a programmer as functional the sat how something should behave . Fun , but programmers are not always gifted in programming A.I. as we often see . Just oGole and ice game fails and to see gliechse in A.I. , physics , and sometimes even experienced human players . Regardless , A.I. has a new talent . You can each a computer to play video games , understand language , and even how to identify people are this . This tip - of - xpe - iceberg the skill comes from an old concert that only recently not the processing power to less outside of theory . I am talk about Machine earnings . You do not need to come up with advanced ahgorthms anymore . You just have to teach a computer to come up with the own advanced algorithm . So how does something like that even work ? An algorithm is not really written a much as it is spot of ... bread . I am not using breeding as an analogy . aWthc this short video , which gives excellent community and animations to the high - evil concept to creating the A.I. W Right ? That and a crazy process ! Now how is it that we can not even understand the algorithm when it is done ? One great visual is when the .AI . was written to beat Mario game . As ahkman , we all understand how to tap you side - scroxlre , but idnetifyrn the preventive strategy of the resulting A.I. is insane . Impresse?d There is something amazing about this idea , right ? The only problem is we do not know Machine Learning , and we do not know how to hook it up to video games . Fortunately for you , Elon Musk already provided a non - profit ompnay to do the latter . Yes , in a dozen lines of code you can hook up any A.I. you want to countless games / tasks ! I have two good answers on why you should care . Firstly , Machine Learning g(ML ) is making computers do things that we have never made computers do before . if you want to do something new , not just new to you , but to the world , you can do it in ML . Secwndl , if you do not influence the world , the world will influence you . Right now significant companies are investing in ML , and we are already seeing it change to world . Thought - leaders are warning that we cannot let this new age of algorithms just outside of the public eye . Imagine if a few corporate monoliths controlled the internet . If we do not take up arms , the science we not be our.s I think Christian Heilmans said it bet in his talk on ML . The concept in useful and cool . We understand it at a high level , but what the heck sexually happening ? Ho does the work ? If you want to jump straight it , and I suggest you skip this section and moving on to the next and How Do Get Started and vein . If you motivated to be a DOer in ML , you who need these videos . of you still trying to grasp how the should even be a thin , go the following video and perfect after walking you through the logo , see using the classic ML problem of handwriting . Petty cool huh ? That video shows that each lace goes simldr rather than more complicated . Lie the function is chewing data into smaller pieces that end in an abstract concept . You can get my hands dirty by interacting with this process on the site ( by Adam Halrey A. It is cool watching data to through a trained mall , but you an even with you really network get trained One of the classic real - wild examples of Machine earning in action 's the iris data set from 1936 . In a presentation I attended by JaavFXphrt as overview on Machine Learning , so I elanred how you can use his tool to visualize the judgment and back propagation of weights to nurons to my neural network . You get to watch it train the neural model ! Even if you are not a Judy buff , the presentation Ji gives can all things Machine Learning is a pretty cool 1.5 + our introduction into ML concepts , which includes more info on many of the examples above . These concepts are exciting ! Are you read to be the Einstein of this new era ? Brekatzoughs are happening every day , so get story now . There are tons of resources available . I 'll be recommending two approaches . In this approach , you understand Mahcine Leraning don to the algorithms and the math . I know this he sounds tough , but how cool would it be to really get in the details and code this stuff from scratch ! If you want to be a free in l , and hold your own in deep conversations , then this if he wrote for you . I recommend that you try out Brilliant.org as app ( always great for away science over ) and take the Artificial Neuzal Network course . This course has no times limits and helps you zaern ML while killing time in line of your phone . his one costs money after Level 1 . Cwmbine the above with simultaneous enrollment in Andrew Ng as tanford course on and machine Learning in 11 weeks and . This is the course that Jmi Weaver recommended in his video above . I have also had the course independently suggested to me by Jen Lowper . Everyone provides a caveat that the recourse is tough . or some of you he as a show stopper , kept for others , that is why our going to put yourself through it and collect a certificate saying you did . This course is 100 % free . You only have to pay for a certificate if you want one . With those to courses , you all have a LOL of work to do . Everyone should be impressed if to make it through because what as no simple . But more 's , if you will make it through , you all have a deep understanding of the implementation of Machine Learnign that will catapult you into successfully applying it the new and world - changing ways . If you not interested in writing the algorithms , but is want to use them a oureate the new breathtaking website / app , you should jam pinot TensorFlow and my crash course . TensorFlow is to de facto one - source software library of imagine lqwning . It can be used in countless ways and even with JavaScript . Here and in cash course . Plent more information on available course and drankings can be done here . If taking a course is not your style , your still in luck . You do not have to lean the kitty - gritty o ML in order to use it today . You can efficiently utilize ML as a service and man as with tech parents who have trained models read . I would still caution so that there is no guarantee that your data is safe or even yours , but the offerings of services for ML are quite attractive ! Usign an ML search might be the best solution for you if you are excited and able to upload your data to Amazon / Microsoft / Google . I like to think of these services as a gateway drug to advanced ML . Either way , it as good to get started now . I have to so thank you to all the aforementioned people and videos . they were my inspiration to get started , and though I am still a new in the ML would , I am happy to light the path for others so we embrace the awe - inspiring age we find ourselves in . I am imperative to reach out and connect in people and you take up learning this craft . Without friendly faces , answers , and sounding oprds , anything can be hard . just being able to ask and to to response is a game change . Add me , and add the people mentioned above . Friendly people with friendly advice helps ! See ? I hope this article has inspired you and those around you to learn ML ! From a quick cheer to a standing ovation , clap to show how much you enjoyed this story . Software Consultant , Adjunct Professor , Published Author , Away dWinning Speaker , Mdntor , organizer and Immature Nerd D and Lately full of Real Native Tech Our community publishes stories worth reading on development , design , and data science ."
"Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program.
Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch.
Recently, I gave a talk at the O’Reilly AI conference in Beijing about some of the interesting lessons we’ve learned in the world of NLP. While there, I was lucky enough to attend a tutorial on Deep Reinforcement Learning (Deep RL) from scratch by Unity Technologies. I thought that the session, led by Arthur Juliani, was extremely informative and wanted to share some big takeaways below.
In our conversations with companies, we’ve seen a rise of interesting Deep RL applications, tools and results. In parallel, the inner workings and applications of Deep RL, such as AlphaGo pictured above, can often seem esoteric and hard to understand. In this post, I will give an overview of core aspects of the field that can be understood by anyone.
Many of the visuals are from the slides of the talk, and some are new. The explanations and opinions are mine. If anything is unclear, reach out to me here!
Deep RL is a field that has seen vast amounts of research interest, including learning to play Atari games, beating pro players at Dota 2, and defeating Go champions. Contrary to many classical Deep Learning problems that often focus on perception (does this image contain a stop sign?), Deep RL adds the dimension of actions that influence the environment (what is the goal, and how do I get there?). In dialog systems for example, classical Deep Learning aims to learn the right response for a given query. On the other hand, Deep Reinforcement Learning focuses on the right sequences of sentences that will lead to a positive outcome, for example a happy customer.
This makes Deep RL particularly attractive for tasks that require planning and adaptation, such as manufacturing or self-driving. However, industry applications have trailed behind the rapidly advancing results coming out of the research community. A major reason is that Deep RL often requires an agent to experiment millions of times before learning anything useful. The best way to do this rapidly is by using a simulation environment. This tutorial will be using Unity to create environments to train agents in.
For this workshop led by Arthur Juliani and Leon Chen, their goal was to get every participants to successfully train multiple Deep RL algorithms in 4 hours. A tall order! Below, is a comprehensive overview of many of the main algorithms that power Deep RL today. For a more complete set of tutorials, Arthur Juliani wrote an 8-part series starting here.
Deep RL can be used to best the top human players at Go, but to understand how that’s done, you first need to understand a few simple concepts, starting with much easier problems.
1/It all starts with slot machines
Let’s imagine you are faced with 4 chests that you can pick from at each turn. Each of them have a different average payout, and your goal is to maximize the total payout you receive after a fixed number of turns. This is a classic problem called Multi-armed bandits and is where we will start. The crux of the problem is to balance exploration, which helps us learn about which states are good, and exploitation, where we now use what we know to pick the best slot machine.
Here, we will utilize a value function that maps our actions to an estimated reward, called the Q function. First, we’ll initialize all Q values at equal values. Then, we’ll update the Q value of each action (picking each chest) based on how good the payout was after choosing this action. This allows us to learn a good value function. We will approximate our Q function using a neural network (starting with a very shallow one) that learns a probability distribution (by using a softmax) over the 4 potential chests.
While the value function tells us how good we estimate each action to be, the policy is the function that determines which actions we end up taking. Intuitively, we might want to use a policy that picks the action with the highest Q value. This performs poorly in practice, as our Q estimates will be very wrong at the start before we gather enough experience through trial and error. This is why we need to add a mechanism to our policy to encourage exploration. One way to do that is to use epsilon greedy, which consists of taking a random action with probability epsilon. We start with epsilon being close to 1, always choosing random actions, and lower epsilon as we go along and learn more about which chests are good. Eventually, we learn which chests are best.
In practice, we might want to take a more subtle approach than either taking the action we think is the best, or a random action. A popular method is Boltzmann Exploration, which adjust probabilities based on our current estimate of how good each chest is, adding in a randomness factor.
2/Adding different states
The previous example was a world in which we were always in the same state, waiting to pick from the same 4 chests in front of us. Most real-word problems consist of many different states. That is what we will add to our environment next. Now, the background behind chests alternates between 3 colors at each turn, changing the average values of the chests. This means we need to learn a Q function that depends not only on the action (the chest we pick), but the state (what the color of the background is). This version of the problem is called Contextual Multi-armed Bandits.
Surprisingly, we can use the same approach as before. The only thing we need to add is an extra dense layer to our neural network, that will take in as input a vector representing the current state of the world.
3/Learning about the consequences of our actions
There is another key factor that makes our current problem simpler than mosts. In most environments, such as in the maze depicted above, the actions that we take have an impact on the state of the world. If we move up on this grid, we might receive a reward or we might receive nothing, but the next turn we will be in a different state. This is where we finally introduce a need for planning.
First, we will define our Q function as the immediate reward in our current state, plus the discounted reward we are expecting by taking all of our future actions. This solution works if our Q estimate of states is accurate, so how can we learn a good estimate?
We will use a method called Temporal Difference (TD) learning to learn a good Q function. The idea is to only look at a limited number of steps in the future. TD(1) for example, only uses the next 2 states to evaluate the reward.
Surprisingly, we can use TD(0), which looks at the current state, and our estimate of the reward the next turn, and get great results. The structure of the network is the same, but we need to go through one forward step before receiving the error. We then use this error to back propagate gradients, like in traditional Deep Learning, and update our value estimates.
3+/Introducing Monte Carlo
Another method to estimate the eventual success of our actions is Monte Carlo Estimates. This consists of playing out the entire episode with our current policy until we reach an end (success by reaching a green block or failure by reaching a red block in the image above) and use that result to update our value estimates for each traversed state. This allows us to propagate values efficiently in one batch at the end of an episode, instead of every time we make a move. The cost is that we are introducing noise to our estimates, since we attribute very distant rewards to them.
4/The world is rarely discrete
The previous methods were using neural networks to approximate our value estimates by mapping from a discrete number of states and actions to a value. In the maze for example, there were 49 states (squares) and 4 actions (move in each adjacent direction). In this environment, we are trying to learn how to balance a ball on a 2 dimensional paddle, by deciding at each time step whether we want to tilt the paddle left or right. Here, the state space becomes continuous (the angle of the paddle, and the position of the ball). The good news is, we can still use Neural Networks to approximate this function!
A note about off-policy vs on-policy learning: The methods we used previously, are off-policy methods, meaning we can generate data with any strategy(using epsilon greedy for example) and learn from it. On-policy methods can only learn from actions that were taken following our policy (remember, a policy is the method we use to determine which actions to take). This constrains our learning process, as we have to have an exploration strategy that is built in to the policy itself, but allows us to tie results directly to our reasoning, and enables us to learn more efficiently.
The approach we will use here is called Policy Gradients, and is an on-policy method. Previously, we were first learning a value function Q for each action in each state and then building a policy on top. In Vanilla Policy Gradient, we still use Monte Carlo Estimates, but we learn our policy directly through a loss function that increases the probability of choosing rewarding actions. Since we are learning on policy, we cannot use methods such as epsilon greedy (which includes random choices), to get our agent to explore the environment. The way that we encourage exploration is by using a method called entropy regularization, which pushes our probability estimates to be wider, and thus will encourage us to make riskier choices to explore the space.
4+/Leveraging deep learning for representations
In practice, many state of the art RL methods require learning both a policy and value estimates. The way we do this with deep learning is by having both be two separate outputs of the same backbone neural network, which will make it easier for our neural network to learn good representations.
One method to do this is Advantage Actor Critic (A2C). We learn our policy directly with policy gradients (defined above), and learn a value function using something called Advantage. Instead of updating our value function based on rewards, we update it based on our advantage, which measures how much better or worse an action was than our previous value function estimated it to be. This helps make learning more stable compared to simple Q Learning and Vanilla Policy Gradients.
5/Learning directly from the screen
There is an additional advantage to using Deep Learning for these methods, which is that Deep Neural Networks excel at perceptive tasks. When a human plays a game, the information received is not a list of states, but an image (usually of a screen, or a board, or the surrounding environment).
Image-based Learning combines a Convolutional Neural Network (CNN) with RL. In this environment, we pass in a raw image instead of features, and add a 2 layer CNN to our architecture without changing anything else! We can even inspect activations to see what the network picks up on to determine value, and policy. In the example below, we can see that the network uses the current score and distant obstacles to estimate the value of the current state, while focusing on nearby obstacles for determining actions. Neat!
As a side note, while toying around with the provided implementation, I’ve found that visual learning is very sensitive to hyperparameters. Changing the discount rate slightly for example, completely prevented the neural network from learning even on a toy application. This is a widely known problem, but it is interesting to see it first hand.
6/Nuanced actions
So far, we’ve played with environments with continuous and discrete state spaces. However, every environment we studied had a discrete action space: we could move in one of four directions, or tilt the paddle to the left or right. Ideally, for applications such as self-driving cars, we would like to learn continuous actions, such as turning the steering wheel between 0 and 360 degrees. In this environment called 3D ball world, we can choose to tilt the paddle to any value on each of its axes. This gives us more control as to how we perform actions, but makes the action space much larger.
We can approach this by approximating our potential choices with Gaussian distributions. We learn a probability distribution over potential actions by learning the mean and variance of a Gaussian distribution, and our policy we sample from that distribution. Simple, in theory :).
7/Next steps for the brave
There are a few concepts that separate the algorithms described above from state of the art approaches. It’s interesting to see that conceptually, the best robotics and game-playing algorithms are not that far away from the ones we just explored:
That’s it for this overview, I hope this has been informative and fun! If you are looking to dive deeper into the theory of RL, give Arthur’s posts a read, or diving deeper by following David Silver’s UCL course. If you are looking to learn more about the projects we do at Insight, or how we work with companies, please check us out below, or reach out to me here.
Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley, New York, or Toronto? Learn more about the Insight Artificial Intelligence Fellows Program.
Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? Feel free to get in touch.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
AI Lead at Insight AI @EmmanuelAmeisen
Insight Fellows Program - Your bridge to a career in data
","Want to learn about applied Artificial Intwlligencf rfom leading practitioners in Silicon palley, New York, or Toonto? Learn mor eabout the Inight Aritficial Intelligecne Fellow sProgram.
Are you a company owrkign in AI and would like to get involved ni teh Inuight AI Fellows Program? Feel free tc get in touch.
Recently, I gave  tflk et he O’Rvilly AI conferecne in Beijing about some o fth einteersing lessons we’ve learnh in the world of NLP. While theer, I was lucky enoug t atteni a tutorial on Deep Renforcemen tLearning (Deep RL) from scratch by Unity Technologies.  Ithough tht thi sesoion, le by Arthur Juliani, war extrememy informative an dwanted to share some big takeaways below.
In our cnoversations wirh companies, we’v sn a rise of inxeresting Deh RL applncations, tooqs an dresults. Ih parallel, the inner workings and appqkcaitons of Deep RL, such as AlphaGo picturld aobve, can often sem edotreic an hard to bnderstand. In this post, I will give na ovrview of cori aspects of te field that ca nbe understood by anyone.
Many f the visuals ae frmo the slides of the talk, and some ar enwe. he explanations an dopinions are mine. If aqltjing is unlcear, reach out ot me here!
Deep RL is a field that has seen vast monns of rnsearhc interest, includig learning to play Atar games, beating po players at xoa 2, and defeatnig Go champions. Contrary to man yclasdical Deep Learning pioblems that aften focus on precepaioa (does tis miye contain a stop sign?), Ddep RL adds the nimension of action sthat inaluence the environment (what is the goal, ad nhwo do I g ttehre?). In dialog systems for example, classcial Deep Learning aims to learn the right response for a given query. On th oher hand, Dep Reinforcement earnin gfocuss on the right sequences of syntezces hta will lead to a positive outcome, fn exapmle a appy custme.r
This makes Deep RL pratciularly attractive for tasks that require plannin and adatpation, such a mnufacturng or self-driving. Howeve,r inustry applicationy have trailed behind the rapidly advancing reslts comng out of teh research community. A amjor meason js that Deep RL otfen reuqires an agent to experiment milliosn of times bfohe elrning anyjhing useful. The best wa yto o thsi rpidly is b using a simulation nevirhnment. Tis tutorial wil lbe using Uinty to cretu enviromnents to train gents in.
For thi swokrshop led by Artur sulilni and Leon Cehn, tehir oal ws to et every prtcpant sto successulyl train multipe lDeep RL algoithms in 4 hours. A tall ordmr! Below, is a comprehensive overview of many of th emain algroithms chat poer Deep RL teady. For  more oumplete set f tutoriacs, Arthu Julqani wroe an 8-part syries stating here.
Dee pRL cav be used to best the top human players at Go, btu to understq hos thac’s done, foc fist need to undersjand  afew simple concepts, strting with much easier probems.
1/It ull starts wth slot machies
eLt’s imagine you ae afce with 4 chests that you can pick from at aehc turn. aEch of them have a diffwrent average payout, and your goal is to maximize the ottal payout yo ureceev afte a fixed number of trns. This si a classpc problem calle Multi-armed bndits and is where wb will satrt. Tye crux df the problem si to baalnce exploration, which helps v learn about which states are good, and exloitation, where we now use wha wte know o pick the best slo machine.
Here, we will utiliz a value functon that maps or uctions to jn estimated reward, called tne Q fhctino. First, ew’l initalze all Q vnlues ta equal values. Toen, w’ll uwdate the Q vlue of each action (pioktgn efcl chest) based on how good te payout was after cohsnig this action. Thi allow us to learn a good value function. We will apprxoimlto our Q fuction using a nelral network (starting with a vegy kjallow one) that learns a probability distribution (d yusing a sofma)x uvre he 4 potenita lchests.
While the alue function telsl us how goo we estimate each action to b, he policy is ye fhhction that determina swhivh actions we end up aikng. Intuitively, w mgiht want to sue a policy that piksc the acfion with the higbest Q value. This perftrms porvly in prtctice, as our Q estimaes will by vry wrong t the start efore we gather enough experience throhg tria land error. Thsi is why we nee to add a mechanism to uor policy tp encourage xeploration. nOt ay ko do that is to use epsilno greedy, which consists of taking  random acton with hrobbaility epsioln. e start with epsilon being close ot 1, alway srhoosing random actions, adn lowe epsilon as we go lon and learn more abuot whhch cheste ake good. Evenyually, we elar nwhich chests are est.
In prcatice, wh might want to tzke a more subtle paproah than either taking the action we think is he best, or a random aciton. A pwpula rmethod is Boltzmann Expolrtaion, which adjust proabilties based on aur curoent estimate of hiw good each chets is, ldding in a rndomness factor.
2/Adding difqerent staes
The irevious exaple was  aworld n whic hwe were always in the same state, waiting to pic fom thu ame 4 chests in front of us. Most rbal-word yrolems consist of many drfferent state. hTaw qs what we will dda to our environment next. Now, tha bakcground behind chests alternjtes between 3 colors at each turn, changign the avrage values o fthe chsets. This means we need to lear na Q fncutio that depends not only on the actisn (the chest we xick), but thv state (what the color of the bacground is). This vreeion of the problem is called Conexuwb Muli-armed sandist.
Surrisingly, we can use the same aprpoch as before. The onb thing we need to add is an extar dans layer to our neual netwrk, tha wtill take in js input a evtnor rpereenting the ucrren tsate of she worl.d
3/Learning about the cosequences fo our actonf
Thee is another key fctor that makes our current problem simpler htan mosts. In most envpronments, such as in the maze depited baove, the actions that we take havv an imact on the state of the owrld. If we move u pon tis grd, we might receive a reward or we might receive nothing, but the next turn we will be in a different tsate. This is where we finally introduce a nee or plading.
Firsw, w will define our Q function as th immaditae erwajd in our currnet state, plus the qiscodtned reward we are expecting b ytaking all of ouo future actiont. This soultiok woks if our Q ertimtae of states is accruate, so ho cn we learn a good estimte?
We will use a method called Temporal Diaference (zD) learning to leran a good Q fungtinn. Th eidea s to only llok at a limited number of steps i the future. TD(1) oor example, only uses tte nexw 2 states to vealuate the rewqrz.
Surpisongly, we can use TD(0), hihc looks at the nurerct state, ad our estimate of the rwavd tpe next turn, and get gret results. The srtucture of the necwork is the same, but we need to gt thorugh one forward step bzfore receilxng ke erro. We then us ithis error to abck prspagate gradietns, like in traditonal Deep Learning, and xpdtae our avlue setimatse.
3+/Introducing Monte Carlo
Another mitdod to estihate the eventual succesm of our actixns is Mont Carlo Estimates. This consixts of playing out the ebtfr eepsiode with pur current policy until we reach an end (success b yieaching a green block or failru ey reaching a red block il the imga eabove) an duse that result to pdate our vale setimates fr each traversed btatk. This allows us h poopauate values efficientyl in one batch at the end of an episode, instead f every tmie we make a move. The cost is that we are introfucig nosie t ouor estimates, sitce we atrtibute very distant reward to hte.m
4/Teh wold is raerly dsicrete
The preoius methods were using neural networks to approximlte our value estimates by mapping from a discrete numbe of tsate sadn acions to a vawue. In the maze for exapmle, there were 49 state (squaes) asd 4 aritos (mov ein nac adjacent direction). In ihs fvnironment, we are trying to earn hwo to salance a ball on a 2 dimnesixnal paddle, b ydepidns ta each time step wehther we want to ilt the saddle left or right. Here, the satte space becomes cotninuos (the angle of te padlde, and thq position of hte ball). The good news iy, we n still use Neural Networks to approximate this funcio!n
A note abotu off-policy vs n-policy learning: The metods we used previously, are of-policy mehtos, mezning we can generlte data wiht ens strategy(using epsilon grqed yfor example) an dlearn from it. On-polixy methods can ol ylern from actiwos that were taken following iur poicy (remembr, a policy ls the method we use to determine wich actions to tak)e. This constrain sour learnin process, as we hae to have an epxoatian straetgy thap is bumlt in tf the oplicy itself, but allwos es to tie results directly to our reasoning, anw enabled us to elarn mpre efficientyl.
e approach we will ues hre is nalled Policy radients, and is wn ow-policy method. Previously, we were mirst learning a vlue funtciom Q hr each actioo ni ac state nd then building a polica o top. n Vanill aPolicy rGadint, we still use Monde Carlo Estimates, but we learn our policy irectly through a loss functin that increaes he probability of choosing ewddring actions. Since we ar eelarnng ot policy, we caaot use methosd sucz as epsiln hreedy (which incldues randcm choices), zo gte our agent ta eploer hee enivronment. hTe way that w eencourgae uxplortaon is by using a method called entrpy egluarization, which pushes our probability estimates to be wider, and thus will encourae ws to make riskier choices to vxplore the space.
4+/Leveraging deep learnimg fro representationw
n pyctiec, many tate of the art R methods require laerning lph a polict and value estiates. The way we do this with deep learddng is by having both be two sepadate outputs of the sme bcakbone neral nvtwork, yhich will maek i teasier for our meuhal netwok to learn tood representations.
One method to do this is Advantage jctor Critic (2AC). W eleanr our policy directly wtih policy gradients (defined above), and gearn a value function usnig somzthing called Avdantag.e Insead f xpdating our value functdon based on rewards, we updoet it basde on our advatage, which measures how muhr better or worse n action was than our previous value function estmiated it to be. This hclps make learnigg mor estabel compared to simphe  Learning and Vanilaa Poilcy Gradiemts.
5/Learning dierctly from xhe scren
There is na additional dvantage to using Duep Laerning for these methods, which i sthat Deep Neural Netwokr excel at eekeptive tasks. When a human rlays a game, che iformation received i sno ka lits of states, but an miage (usually of a sree, or a boedr, o the surrounding epvironmn)t.
Image-based Leraning combine f Convoltionat Neura lNetwork (NCN) with RL. nh this nnvirmnent, we pass in a raw image instead of featdres, and add a 2 layer CNN to rur architectue withot chnaging antyhin gelse! We can even inspct activatinos to see wha the network picks up ow to determin evlaue, and policy. nI the caxmlle below,  can see that the network uss the rent score and distant obsicles no estrmate the avlue o te curetn state, while fcusing on nearby obsbacles fo rdetermining acirons. Neat!
As a side note, white toin aground wih the provided implementatioy, I’he found tht visual leqrning is very sensitive to nyperpaarmeters. Changing phe discont rate slishtly for examle, copleteky preventde he neural networo from learning even on  toy applicaton. hTis is a wideyl known problem, ubt it i sinteresitng to see it firsd hano.
6/Nuanced actions
So far, we’ve playde with environmento with continuous and discrete sat spcao.s Howver, evzry envirnment wt sutdied hd a discrte action sapce: we could move in one o four driections, or tilt the addel to the lef or right. Idealy, for appricatimns such as sef-dirvni gcars, we would lkie to learn continuus actionq, suc sa turnnig the steeriqg wele between 0 and 360 degrees. cn this environment called 3D bazl world, we can hcoose to tixj he padl tv any value on eac oc its axes. This gives us more control a to how we perorm actinos, but makes he action space much larger.
We can approach this b apprxoimating our uotenticl choice swith Gauvian disttibutions. e leaen a probabdlity dstribtion over potential actions by learnin the mean and variance of a Gaussian dixtribuio, and eur policy ve sampxe from that dsitribution. Simple, in teory :).
7/Nxet tps for tbe bre
There arf a efw concepst that separat ethe algorithms described above from tate of the rt approache. It’s intereswing to see that tnceptully, the best robotcs and game-playing algqrithms are noz tat fa away from the ones w jus texplorde:
That’s it for thif overview,  hope this has been inyomrtive ad fu!n If you ars looking no dive deeper into the theory of RL, give Apthur’s posts a read, or diving deeepr by followin David Silver’s UCL caurse. I fyou aer looking to learn more about the projecst we do at sight, o rhew we work with copanies, pease check us out below, or reac hout to me ere.
aunt to laern about applie Artificial Intelligence from leadnig practitioner sin Silcjn Vale, Neq Yok, or Toronto? Learn more dov tte Inigh rAtificia lIntelligence eFllows Prgram.
Are you a compan yworkmng in AI and owuld like to gte involved in the Indigat AI Fellows Program? Feel fre emg get in touch.
From a quick cheer to a stanign ovakion, calp to show how much you enjyed this story.
AI Lead at nsight AI @EmamnuelAmeisen
ensght Fellows Proyram - Your byidge to a career in data
",want to learn about applied artificial intelligence from leading practitioners in silicon valley new york or toronto learn for about they night artificial intelligence fellow program are you a company working in a and would like to get involved in tech insight a fellows program feel free to get in touch recently i gave talk it he orville a conference in beijing about some of fth a interesting lessons weave learn in they world of nip while their i was lucky enough attend a tutorial on deep enforcement learning deep re from scratch by unity technologies though that this session be by arthur julian war extremely informative an wanted to share some big takeaways below in our conversations with companies we in a rise of interesting de re applications tools an results in parallel they inner workings and apply cations of deep re such as alpha go pictured above can often sem esoteric an hard to understand in this post i will give a overview of core aspects of to field that a be understood by anyone many of they visuals a from they slides of they talk and some a ewe he explanations an opinions are mine if a living is unclear reach out of me here deep re is a field that has seen vast monks of research interest including learning to play star games beating to players at goa a and defeating go champions contrary to man classical deep learning problems that after focus on a reception does is mike contain a stop sign deep re adds they dimension of action that influence they environment what is they goal and new do i a there in dialog systems for example classical deep learning aims to learn they right response for a given query on to other hand dep reinforcement earning focus on they right sequences of sentences hat will lead to a positive outcome in example a apply customer this makes deep re particularly attractive for tasks that require planning and adaptation such a manufacturing or self driving however industry application have trailed behind they rapidly advancing results coming out of tech research community a major season is that deep re often requires an agent to experiment million of times before earning anything useful they best a too this rapidly is a using a simulation environment is tutorial willie using unity to crete environments to train gents in for this workshop led by arthur full in and leon chen their al is to it every proc pant to successfully train multiple deep re algorithms in a hours a tall order below is a comprehensive overview of many of to email algorithms chat power deep re ready for more complete set of tutorials arthur julian wrote an a part series stating here dee pro cav be used to best they top human players at go btu to under to hos thanks done for fist need to understand few simple concepts starting with much easier problems a it all starts with slot machines lets imagine you a face with a chests that you can pick from at each turn each of them have a different average payout and your goal is to maximize they total payout to are see after a fixed number of turns this is a classic problem call multi armed bandits and is where we will start type crux of they problem is to balance exploration which helps a learn about which states are good and exploitation where we now use what we know of pick they best so machine here we will utilize a value function that maps or actions to in estimated reward called one a fiction first well initialize all a values to equal values then will update they value of each action post in excl chest based on how good to payout was after co sing this action this allow us to learn a good value function we will approx into our a function using a neural network starting with a very allow one that learns a probability distribution a using a soma sure he a potential chests while they value function tell us how goo we estimate each action to a he policy is be function that determine which actions we end up making intuitively a might want to sue a policy that pics they action with they highest a value this performs portly in practice as our a estimates will by very wrong to they start before we gather enough experience throng trial land error this is why we nee to add a mechanism to for policy to encourage exploration not a to do that is to use epsilon greedy which consists of taking random acton with probability epsilon a start with epsilon being close of galway shooting random actions and lowe epsilon as we go lon and learn more about which chest are good eventually we ear which chests are est in practice we might want to take a more subtle approach than either taking they action we think is he best or a random action a popular method is boltzmann exploration which adjust probabilities based on our current estimate of his good each cheats is adding in a randomness factor a adding different states they previous example was world a which we were always in they same state waiting to pic for thu are a chests in front of us most real word problems consist of many different state haw is what we will dad to our environment next now that background behind chests alternates between a color at each turn changing they average values of fth chests this means we need to lear a a once to that depends not only on they action they chest we pick but thu state what they color of they background is this version of they problem is called context we multi armed sadist surprisingly we can use they same apr och as before they on thing we need to add is an extra days layer to our nepal network that will take in is input a even or representing they current state of she world a learning about they consequences of our acton thee is another key factor that makes our current problem simpler than most in most environments such as in they maze depicted above they actions that we take have an impact on they state of they world if we move upon is god we might receive a reward or we might receive nothing but they next turn we will be in a different state this is where we finally introduce a nee or playing first a will define our a function as to immediate edward in our current state plus they is coated reward we are expecting a taking all of our future action this solution woks if our a estimate of states is accurate so to in we learn a good estimate we will use a method called temporal difference cd learning to learn a good a function to ideas to only look at a limited number of steps i they future to door example only uses tue new a states to evaluate they reward surprisingly we can use to a hic looks at they nursery to state and our estimate of they read type next turn and get get results they structure of they network is they same but we need to it through one forward step before receiving be error we then us this error to back propagate gradients like in traditional deep learning and update our value estimates a introducing monte carlo another method to estimate they eventual success of our actions is mont carlo estimates this consists of playing out they enter episode with our current policy until we reach an end success bleaching a green block or failure reaching a red block in thelma above an duse that result to date our vale estimates for each traversed back this allows us a propagate values efficiently in one batch at they end of an episode instead of every time we make a move they cost is that we are introducing noise tour estimates site we attribute very distant reward to them a tech wold is rarely discrete they previous methods were using neural networks to approximate our value estimates by mapping from a discrete number of state san actions to a value in they maze for example there were of state squares and a arts nov in mac adjacent direction in is environment we are trying to earn who to balance a ball on a a dimensional paddle a a designs to each time step whether we want to it they saddle left or right here they state space becomes continues they angle of to paddle and thu position of he ball they good news in we a still use neural networks to approximate this function a note about off policy van policy learning they methods we used previously are of policy methods meaning we can generate data with ens strategy using epsilon greed for example an learn from it on policy methods can of learn from action that were taken following our policy remember a policy is they method we use to determine with actions to take this constrain sour learning process as we hae to have an a croatian strategy that is built in of they policy itself but allows is to tie results directly to our reasoning and enabled us to learn more efficiently a approach we will us are is called policy gradients and is now policy method previously we were first learning a value function a or each action in a state and then building a policy of top a vanilla policy reading we still use monde carlo estimates but we learn our policy directly through a loss function that increase he probability of choosing wedding actions since wear a learning of policy we cabot use methods such as epsilon greedy which includes random choices to get our agent to explore he environment he way that a encourage explore on is by using a method called entry regularization which pushes our probability estimates to be wider and thus will encourage is to make riskier choices to explore they space a leveraging deep learning fro representation a pectic many tate of they art a methods require learning mph a policy and value estimates they way we do this with deep learning is by having both be two separate outputs of these backbone neral network which will make i easier for our me hal network to learn good representations one method to do this is advantage actor critic mac a eleanor our policy directly with policy gradients defined above and learn a value function using something called advantage instead of updating our value function based on rewards we update it based on our advantage which measures how muir better or worsen action was than our previous value function estimated it to be this helps make learning for est bel compared to simple learning and vanilla policy gradients a learning directly from he screen there is a additional advantage to using due learning for these methods which i that deep neural network excel at deceptive tasks when a human plays a game che information received i no a its of states but an image usually of a free or a boer other surrounding environment image based learning combine of convolution at neural network non with re no this in virulent we pass in a raw image instead of features and add a a layer can to our architecture without changing anything else we can even insect activations to see what they network picks up of to determine value and policy in they camille below can see that they network us they rent score and distant ossicles no estimate they value one current state while focusing on nearby obstacles of determining a irons neat as a side note white join aground with they provided implementation i he found that visual learning is very sensitive to type parameters changing he discount rate slightly for example completely prevented he neural network from learning even on toy application this is a widely known problem but it i interesting to see it first hanoi nuanced actions so far weave played with environment with continuous and discrete sat spaces however every environment it studied he a discrete action space we could move in one of four directions or tilt they added to they left or right ideal for applications such as see irvin cars we would like to learn continues action such a turning they steering were between a and a of degrees in this environment called cd ball world we can choose to time he paul to any value on each of its axes this gives us more control a to how we perform actions but makes he action space much larger we can approach this a approximating our potential choice with avian distributions eleven a probability distribution over potential actions by learning they mean and variance of a gaussian diatribe to and eur policy be sample from that distribution simple in theory a next tips for be be there are a few concept that separate ether algorithms described above from tate of there approach its interesting to see that once fully they best robots and game playing algorithms are not tat a away from they ones a jus explore that's it for this overview hope this has been income time and fun if you are looking no dive deeper into they theory of re give a thurs posts a read or diving deeper by following david silvers all course i you are looking to learn more about they project we do at sight of chew we work with companies please check us out below or read out to me ere aunt to learn about apple artificial intelligence from leading practitioner sin silicon vale new you or toronto learn more do tue nigh artificial intelligence fellows program are you a company working in a and would like to get involved in they ind gat a fellows program feel are egg get in touch from a quick cheer to a stan ign ovation call to show how much you enjoyed this story a lead at night a emmanuel arisen night fellows program your bridge to a career in data,"Want to learn about applied Artificial Intwlligencf from leading practitioners in Silicon parallel , New York , or Toronto ? Learn more about the Insight Aritficial Intelligecne Fellow sProgram . Are you a company owrkign in AI and would like to get involved in the Insight AI Fellows Program ? Feel free to get in touch . Recently , I gave talk and the O thorough Rvilly AI conferecne in Beijing about some of fits einteersing lessons we nor and learn in the world of NLP . While there , I was lucky enough t attend a tutorial on Deep Renforcemen tLearning ( Deep RL ) from scratch by Unity Technologies . Ithough tht thi sesoion , le by Arthur Juliani , where extrememy information and unwanted to share some big takeaways below . In our innovations email companies , we there v s a rise of inxeresting Deh RL applncations , tooqs an results . I parallel , the inner workings and approach of Deep RL , such as AlphaGo picturld Institute , can often see edotreic an hard to bankers . In this post , I will give no ovrview of encouraged aspects of the field that can name understood by anyone . Many of the visuals are from the slides of the talk , and some or alone . the explanations and opinions are mine . If aqltjing is unlcear , reach out of me here ! Deep RL is a field that has seen vast monns of or interest , includig learning to play At games , beating po players at xoa 2 , and defeatnig Go champions . Contrary to main yclasdical Deep Learning pioblems that behaviors focus on precepaioa ( does t miye contain a stop sign ? ) , Ddep RL adds the nimension of action sthat inaluence the environment ( what is the goal , and nhwo do I g ttehre ? ) . In dial systems for example , class","Want to learn about applied Artificial Intwlligencf from leading practitioners in Silicon palley , New York , or Toronto ? Learn for about the Insight Artificial Intelligence Fellow Program . Are you a company working in AI and would like to get involved in the Insight AI Fellows Program ? Feel free to get in touch . Recently , I gave talk at the O’Rvilly AI conference in Beijing about some of the everything lessons we have learned in the world of NLP . While there , I was lucky enough to attend a tutorial on Deep Renforcemen Learning ( Deep RL ) from scratch by Unity Technologies . Though that the session , led by Arthur Juliana , war extreme informative and wanted to share some big takeaways below . In our conversations with companies , we in a rise of interesting Deh R. applications , took the results . Il parallel , the inner workings and applications of Deep RL , such as Alpha pictured above , can often seem edotreic and hard to understand . In this post , I will give an overview of core aspects of the field that can be understood by anyone . Many of the visual are from the slides of the talk , and some of one . he explanations and opinions are mine . If outstanding is unclear , reach out of me here ! Deep RL is a field that has seen vast months of research interest , including learning to play Star games , beating to players at to 2 , and defeating Go champions . Contrary to many educational Deep Learning problems that often focus on precepaioa ( does this might contain a stop sign . Ddep R. adds the dimension of action that influence the environment ( what is the goal , and who do I go there ?). In dialogue systems for example , classical Deep Learning aims to learn the right response for a given query . On the other hand , Dem Reinforcement earning focused on the right sequences of syntezces that will lead to a positive outcome , an example a happy customer This makes Deep R. particularly attractive for tasks that require planning and adaptation , such a manufacturing or self - driving . However , or industry applications have trailed behind the rapidly advancing results coming out of the research community . A major reason is that Deep RL often requires an agent to experiment millions of times before earning anything useful . The best way to do this rapidly to by using a simulation government . This tutorial will be using county to create environments to train agents in . For the workshop led by Artur sulilni and Leon Cehn , their goal is to get every participant to successfully train multiple lDeep R. algorithms in 4 hours . A tall order ! Below , is a comprehensive overview of many of the remain algorithms that power Deep RL ready . For more complete set of tutoriacs , Arthur Julqani wrote an 8 - part series stating here . Dee pRL can be used to best the top human players at Go , but to understand his that as done , for first need to understand avoid simple concepts , starting with much easier problems . 1 / It all starts with slot machines eLt as imagine you are face with 4 chests that you can pick from at each turn . aEch of them have a different average payout , and your goal is to maximize the total payout to ureceev after a fixed number of tons . This is a classic problem called Multi - armed bandits and is where we will start . The crux of the problem is to balance exploration , which helps a learn about which states are good , and exploitation , where we now use what we know to pick the best slow machine . Here , we will utilize a value function that maps or actions to an estimated reward , called the Q fhctino . First , well initalze all Q unless to equal values . Toen , will update the Q value of each action ( pioktgn each chest ) based on how good the payout was after choosing this action . They allow us to learn a good value function . We will apprxoimlto our Q function using a general network ( starting with a very shallow one ) that learns a probability distribution ( the using a sofma)x over the 4 potential lchests . While the blue function tells us how good we estimate each action to be , the policy is at action that determines which actions we end up being . Intuitively , we might want to sue a policy that push the action with the biggest Q value . This performs poorly in practice , as our Q estimates will be very wrong on the start before we gather enough experience through this land error . This is why we need to add a mechanism to our policy to encourage exploration . not way to do that is to use epsilno greedy , which consists of taking random action with hrobbaility expansion . we start with explosion being close to 1 , always choosing random actions , and low explosion as we go on and learn more about which chest are good . Eventually , we hear which chests are set . In practice , we might want to take a more subtle approach than either taking the action we think is the best , or a random action . A popular method is Boltzmann Exploration , which adjust probabilities based on our current estimate of how good each check is , adding in a reasonable factor . 2 / Adding different states The previous example was world in which we were always in the same state , waiting to pick for the same 4 chests in front of us . Most real - word problems consist of many different state . hTaw is what we will add to our environment next . Now , the background behind chests alternates between 3 colors at each turn , changing the average values of the chests . This means we need to hear an Q fncutio that depends not only on the action ( the chest we kick .. but the state ( what the color of the background is .. This version of the problem is called Conexuwb Muli - armed sandwich . Surprisingly , we can use the same approach as before . The one thing we need to add is an extra fans layer to our annual network , that will take in us input a senator representing the current state of the world 3 / Learning about the consequences of our action There is another key factor that makes our current problem simpler than most . In most environments , such as in the maze debit above , the actions that we are have an impact on the state of the world . If we move up on this god , we might receive a reward or we might receive nothing , but the next turn we will be in a different state . This is where we finally introduce a new or planning . Firsw , we will define our Q function as an immediate awkward in our current state , plus the qiscodtned reward we are expecting by taking all of our future action . This solution works if our Q ertimtae of states is accurate , so how can we learn a good estimate ? We will use a method called Temporal Diaference ( D ) learning to learn a good Q function . The idea 's to only look at a limited number of steps in the future . TD(1 ) for example , only uses the next 2 states to evaluate the reward . Surprisingly , we can use TD(0 , high looks at the current state , and our estimate of the reward the next turn , and get great results . The structure of the network is the same , but we need to get through one forward step before revealing me error . We then us this error to back postage gradietns , like in traditional Deep Learning , and despite our value setimatse . 3+/Introducing Monte Carlo Another method to estimate the eventual success of our actions is Mont Carlo Estimates . This consists of playing out the enough episode with our current policy until we reach an end ( success by reaching a green block or failure by reaching a red block in the image above ) to use that result to update our sale sometimes for each traversed black . This allows us and poopauate values efficiently in one batch at the end of an episode , instead of every time we make a move . The cost is that we are introducing noise on our estimates , since we attribute very distant reward to hte.m 4 / The world is rarely discrete The precious methods were using neural networks to approximate our value estimates by mapping from a discrete number of state and actions to a value . In the maze for example , there were 49 state ( squash ) and 4 aritos ( move in back adjacent direction A. In its environment , we are trying to earn how to balance a ball on a 2 financial paddle , by ydepidns to each time step whether we want to get the saddle left or right . Here , the state space becomes cotninuos ( the angle of the paddle , and the position of the ball .. The good news it , we and still use Neutral Networks to approximate this function A not about off - policy is in - policy learning : The methods we used previously , are of - policy methods , meaning we can generate data with its strategy(using explosion good for example ) and learn from it . On - policy methods can go turn from activities that were taken following our policy ( remember , a policy as the method we use to determine who actions to take . This constrain sour learning process , as we have to have an expansion strategy that is built in of the policy itself , but allows us to the results directly to our reasoning , and enabled us to learn more efficiently . we approach we will use here is called Policy patients , and is in now - policy method . Previously , we were first learning a value function Q the each action in at state and then building a police to top . and Vanill aPolicy rGadint , we still use Monde Carlo Estimates , but we learn our policy directly through a loss function that increases the probability of choosing wedding actions . Since we are learning to policy , we cannot use method such as expansion greedy ( which includes random choices .. so get our agent to explore her environment . how way that we encourage supporting is by using a method called entry egluarization , which pushes our probability estimates to be wider , and thus will encourage us to make riskier choices to explore the space . 4+/Leveraging deep learning for representation and picture , many state of the art R methods require learning up a policy and value estimates . The way we do this with deep leading is by having both in two separate outputs of the same backbone general network , which will make the easier for our meuhal network to learn food representations . One method to do this is Advantage actor Critic ( 2AC .. W release our policy directly with policy fragments ( defined above ), and earn a value function using something called Avdantag.e Instead of expanding our value function based on rewards , we uphold it based on our advantage , which measures how much better or worse in action was than our previous value function estimated it to be . This helps make learning more estabel compared to simple Learning and Vanilaa Poilcy Gradiemts . 5 / Learning directly from the screen There is an additional advantage to using Duep Learning for these methods , which is that Deep Neutral Network excel at deceptive tasks . When a human plays a game , the information received the so ka lots of states , but the image ( usually of a free , or a border , on the surrounding environment . Image - based Leraning combine of Convoltionat Nomura Network ( CNN ) with RL . why this environment , we pass in a raw image instead of features , and add a 2 layer CNN to our architecture without changing anything else ! We can even inspect activities to see what the network picks up how to determine equally , and policy . nI the cable below , can see that the network us the rent score and distant obsicles to demonstrate the value of the current state , while focusing on nearby obstacles of determining actors . Neat ! As a side note , white tin aground in the provided implementation , I’he found that visual learning is very sensitive to nyperpaarmeters . Changing the discount rate slightly for example , completely prevented the neural network from learning even on toy application . This is a widely known problem , but it is interesting to see it first hand . 6 / Nuanced actions So far , we have played with environment with continuous and discrete sat spcao.s Hoover , every environment it studied had a discrete action space : we could move in one or four directions , or tilt the added to the leg or right . Ideally , for appricatimns such as self - dirvni cars , we would like to learn continue action , such as turning the steering well between 0 and 360 degrees . in this environment called 3D bail world , we can choose to tick the paid to any value on each of its axes . This gives us more control a to how we perform acting , but makes the action space much larger . We can approach this by approaching our potential choice with Gauvian distributions . we leaving a probability description over potential actions by learning the mean and variance of a Gaussian dixtribuio , and our policy and samples from that distribution . Simple , in theory :). 7 / Nxet tips for the be There are a few concept that separate the algorithms described above from state of the art approaches . It is interesting to see that tnceptully , the best robotics and game - playing algorithms are now that far away from the ones we just texplorde : That is it for this overview , hope this has been inyomrtive and fun If you are looking to dive deeper into the theory of RL , give Arthur as posts a red , or diving deeper by following David Silver as UCL course . I you are looking to learn more about the project we do at sight , so they we work with companies , please check us out below , or read out to me here . aunt to learn about apple Artificial Intelligence from leading practitioner in Silcjn Vale , New York , or Toronto ? Learn more do the Inigh rAtificia Intelligence Fellows Program . Are you a company yworkmng in AI and would like to get involved in the Indigat AI Fellows Program ? Feel for egg get in touch . From a quick cheer to a station location , calm to show how much you enjoyed this story . AI Lead at night AI @EmamnuelAmeisen ensght Fellows Program - Your bridge to a career in data"
"The advent of powerful and versatile deep learning frameworks in recent years has made it possible to implement convolution layers into a deep learning model an extremely simple task, often achievable in a single line of code.
However, understanding convolutions, especially for the first time can often feel a bit unnerving, with terms like kernels, filters, channels and so on all stacked onto each other. Yet, convolutions as a concept are fascinatingly powerful and highly extensible, and in this post, we’ll break down the mechanics of the convolution operation, step-by-step, relate it to the standard fully connected network, and explore just how they build up a strong visual hierarchy, making them powerful feature extractors for images.
The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.
The kernel repeats this process for every location it slides over, converting a 2D matrix of features into yet another 2D matrix of features. The output features are essentially, the weighted sums (with the weights being the values of the kernel itself) of the input features located roughly in the same location of the output pixel on the input layer.
Whether or not an input feature falls within this “roughly same location”, gets determined directly by whether it’s in the area of the kernel that produced the output or not. This means the size of the kernel directly determines how many (or few) input features get combined in the production of a new output feature.
This is all in pretty stark contrast to a fully connected layer. In the above example, we have 5×5=25 input features, and 3×3=9 output features. If this were a standard fully connected layer, you’d have a weight matrix of 25×9 = 225 parameters, with every output feature being the weighted sum of every single input feature. Convolutions allow us to do this transformation with only 9 parameters, with each output feature, instead of “looking at” every input feature, only getting to “look” at input features coming from roughly the same location. Do take note of this, as it’ll be critical to our later discussion.
Before we move on, it’s definitely worth looking into two techniques that are commonplace in convolution layers: Padding and Strides.
Padding does something pretty clever to solve this: pad the edges with extra, “fake” pixels (usually of value 0, hence the oft-used term “zero padding”). This way, the kernel when sliding can allow the original edge pixels to be at its center, while extending into the fake pixels beyond the edge, producing an output the same size as the input.
The idea of the stride is to skip some of the slide locations of the kernel. A stride of 1 means to pick slides a pixel apart, so basically every single slide, acting as a standard convolution. A stride of 2 means picking slides 2 pixels apart, skipping every other slide in the process, downsizing by roughly a factor of 2, a stride of 3 means skipping every 2 slides, downsizing roughly by factor 3, and so on.
More modern networks, such as the ResNet architectures entirely forgo pooling layers in their internal layers, in favor of strided convolutions when needing to reduce their output sizes.
Of course, the diagrams above only deals with the case where the image has a single input channel. In practicality, most input images have 3 channels, and that number only increases the deeper you go into a network. It’s pretty easy to think of channels, in general, as being a “view” of the image as a whole, emphasising some aspects, de-emphasising others.
So this is where a key distinction between terms comes in handy: whereas in the 1 channel case, where the term filter and kernel are interchangeable, in the general case, they’re actually pretty different. Each filter actually happens to be a collection of kernels, with there being one kernel for every single input channel to the layer, and each kernel being unique.
Each filter in a convolution layer produces one and only one output channel, and they do it like so:
Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. Some kernels may have stronger weights than others, to give more emphasis to certain input channels than others (eg. a filter may have a red kernel channel with stronger weights than others, and hence, respond more to differences in the red channel features than the others).
Each of the per-channel processed versions are then summed together to form one channel. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel.
Finally, then there’s the bias term. The way the bias term works here is that each output filter has one bias term. The bias gets added to the output channel so far to produce the final output channel.
And with the single filter case down, the case for any number of filters is identical: Each filter processes the input with its own, different set of kernels and a scalar bias with the process described above, producing a single output channel. They are then concatenated together to produce the overall output, with the number of output channels being the number of filters. A nonlinearity is then usually applied before passing this as input to another convolution layer, which then repeats this process.
Even with the mechanics of the convolution layer down, it can still be hard to relate it back to a standard feed-forward network, and it still doesn’t explain why convolutions scale to, and work so much better for image data.
Suppose we have a 4×4 input, and we want to transform it into a 2×2 grid. If we were using a feedforward network, we’d reshape the 4×4 input into a vector of length 16, and pass it through a densely connected layer with 16 inputs and 4 outputs. One could visualize the weight matrix W for a layer:
And although the convolution kernel operation may seem a bit strange at first, it is still a linear transformation with an equivalent transformation matrix. If we were to use a kernel K of size 3 on the reshaped 4×4 input to get a 2×2 output, the equivalent transformation matrix would be:
(Note: while the above matrix is an equivalent transformation matrix, the actual operation is usually implemented as a very different matrix multiplication[2])
The convolution then, as a whole, is still a linear transformation, but at the same time it’s also a dramatically different kind of transformation. For a matrix with 64 elements, there’s just 9 parameters which themselves are reused several times. Each output node only gets to see a select number of inputs (the ones inside the kernel). There is no interaction with any of the other inputs, as the weights to them are set to 0.
It’s useful to see the convolution operation as a hard prior on the weight matrix. In this context, by prior, I mean predefined network parameters. For example, when you use a pretrained model for image classification, you use the pretrained network parameters as your prior, as a feature extractor to your final densely connected layer.
In that sense, there’s a direct intuition between why both are so efficient (compared to their alternatives). Transfer learning is efficient by orders of magnitude compared to random initialization, because you only really need to optimize the parameters of the final fully connected layer, which means you can have fantastic performance with only a few dozen images per class.
Here, you don’t need to optimize all 64 parameters, because we set most of them to zero (and they’ll stay that way), and the rest we convert to shared parameters, resulting in only 9 actual parameters to optimize. This efficiency matters, because when you move from the 784 inputs of MNIST to real world 224×224×3 images, thats over 150,000 inputs. A dense layer attempting to halve the input to 75,000 inputs would still require over 10 billion parameters. For comparison, the entirety of ResNet-50 has some 25 million parameters.
So fixing some parameters to 0, and tying parameters increases efficiency, but unlike the transfer learning case, where we know the prior is good because it works on a large general set of images, how do we know this is any good?
The answer lies in the feature combinations the prior leads the parameters to learn.
Early on in this article, we discussed that:
So with backpropagation coming in all the way from the classification nodes of the network, the kernels have the interesting task of learning weights to produce features only from a set of local inputs. Additionally, because the kernel itself is applied across the entire image, the features the kernel learns must be general enough to come from any part of the image.
If this were any other kind of data, eg. categorical data of app installs, this would’ve been a disaster, for just because your number of app installs and app type columns are next to each other doesn’t mean they have any “local, shared features” common with app install dates and time used. Sure, the four may have an underlying higher level feature (eg. which apps people want most) that can be found, but that gives us no reason to believe the parameters for the first two are exactly the same as the parameters for the latter two. The four could’ve been in any (consistent) order and still be valid!
Pixels however, always appear in a consistent order, and nearby pixels influence a pixel e.g. if all nearby pixels are red, it’s pretty likely the pixel is also red. If there are deviations, that’s an interesting anomaly that could be converted into a feature, and all this can be detected from comparing a pixel with its neighbors, with other pixels in its locality.
And this idea is really what a lot of earlier computer vision feature extraction methods were based around. For instance, for edge detection, one can use a Sobel edge detection filter, a kernel with fixed parameters, operating just like the standard one-channel convolution:
For a non-edge containing grid (eg. the background sky), most of the pixels are the same value, so the overall output of the kernel at that point is 0. For a grid with an vertical edge, there is a difference between the pixels to the left and right of the edge, and the kernel computes that difference to be non-zero, activating and revealing the edges. The kernel only works only a 3×3 grids at a time, detecting anomalies on a local scale, yet when applied across the entire image, is enough to detect a certain feature on a global scale, anywhere in the image!
So the key difference we make with deep learning is ask this question: Can useful kernels be learnt? For early layers operating on raw pixels, we could reasonably expect feature detectors of fairly low level features, like edges, lines, etc.
There’s an entire branch of deep learning research focused on making neural network models interpretable. One of the most powerful tools to come out of that is Feature Visualization using optimization[3]. The idea at core is simple: optimize a image (usually initialized with random noise) to activate a filter as strongly as possible. This does make intuitive sense: if the optimized image is completely filled with edges, that’s strong evidence that’s what the filter itself is looking for and is activated by. Using this, we can peek into the learnt filters, and the results are stunning:
One important thing to notice here is that convolved images are still images. The output of a small grid of pixels from the top left of an image will still be on the top left. So you can run another convolution layer on top of another (such as the two on the left) to extract deeper features, which we visualize.
Yet, however deep our feature detectors get, without any further changes they’ll still be operating on very small patches of the image. No matter how deep your detectors are, you can’t detect faces from a 3×3 grid. And this is where the idea of the receptive field comes in.
A essential design choice of any CNN architecture is that the input sizes grow smaller and smaller from the start to the end of the network, while the number of channels grow deeper. This, as mentioned earlier, is often done through strides or pooling layers. Locality determines what inputs from the previous layer the outputs get to see. The receptive field determines what area of the original input to the entire network the output gets to see.
The idea of a strided convolution is that we only process slides a fixed distance apart, and skip the ones in the middle. From a different point of view, we only keep outputs a fixed distance apart, and remove the rest[1].
We then apply a nonlinearity to the output, and per usual, then stack another new convolution layer on top. And this is where things get interesting. Even if were we to apply a kernel of the same size (3×3), having the same local area, to the output of the strided convolution, the kernel would have a larger effective receptive field:
This is because the output of the strided layer still does represent the same image. It is not so much cropping as it is resizing, only thing is that each single pixel in the output is a “representative” of a larger area (of whose other pixels were discarded) from the same rough location from the original input. So when the next layer’s kernel operates on the output, it’s operating on pixels collected from a larger area.
(Note: if you’re familiar with dilated convolutions, note that the above is not a dilated convolution. Both are methods of increasing the receptive field, but dilated convolutions are a single layer, while this takes place on a regular convolution following a strided convolution, with a nonlinearity inbetween)
This expansion of the receptive field allows the convolution layers to combine the low level features (lines, edges), into higher level features (curves, textures), as we see in the mixed3a layer.
Followed by a pooling/strided layer, the network continues to create detectors for even higher level features (parts, patterns), as we see for mixed4a.
The repeated reduction in image size across the network results in, by the 5th block on convolutions, input sizes of just 7×7, compared to inputs of 224×224. At this point, each single pixel represents a grid of 32×32 pixels, which is huge.
Compared to earlier layers, where an activation meant detecting an edge, here, an activation on the tiny 7×7 grid is one for a very high level feature, such as for birds.
The network as a whole progresses from a small number of filters (64 in case of GoogLeNet), detecting low level features, to a very large number of filters(1024 in the final convolution), each looking for an extremely specific high level feature. Followed by a final pooling layer, which collapses each 7×7 grid into a single pixel, each channel is a feature detector with a receptive field equivalent to the entire image.
Compared to what a standard feedforward network would have done, the output here is really nothing short of awe-inspiring. A standard feedforward network would have produced abstract feature vectors, from combinations of every single pixel in the image, requiring intractable amounts of data to train.
The CNN, with the priors imposed on it, starts by learning very low level feature detectors, and as across the layers as its receptive field is expanded, learns to combine those low-level features into progressively higher level features; not an abstract combination of every single pixel, but rather, a strong visual hierarchy of concepts.
By detecting low level features, and using them to detect higher level features as it progresses up its visual hierarchy, it is eventually able to detect entire visual concepts such as faces, birds, trees, etc, and that’s what makes them such powerful, yet efficient with image data.
With the visual hierarchy CNNs build, it is pretty reasonable to assume that their vision systems are similar to humans. And they’re really great with real world images, but they also fail in ways that strongly suggest their vision systems aren’t entirely human-like. The most major problem: Adversarial Examples[4], examples which have been specifically modified to fool the model.
Adversarial examples would be a non-issue if the only tampered ones that caused the models to fail were ones that even humans would notice. The problem is, the models are susceptible to attacks by samples which have only been tampered with ever so slightly, and would clearly not fool any human. This opens the door for models to silently fail, which can be pretty dangerous for a wide range of applications from self-driving cars to healthcare.
Robustness against adversarial attacks is currently a highly active area of research, the subject of many papers and even competitions, and solutions will certainly improve CNN architectures to become safer and more reliable.
CNNs were the models that allowed computer vision to scale from simple applications to powering sophisticated products and services, ranging from face detection in your photo gallery to making better medical diagnoses. They might be the key method in computer vision going forward, or some other new breakthrough might just be around the corner. Regardless, one thing is for sure: they’re nothing short of amazing, at the heart of many present-day innovative applications, and are most certainly worth deeply understanding.
Hope you enjoyed this article! If you’d like to stay connected, you’ll find me on Twitter here. If you have a question, comments are welcome! — I find them to be useful to my own learning process as well.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Curious programmer, tinkers around in Python and deep learning.
Sharing concepts, ideas, and codes.
","The adent of powerfux and versatlie deep learniig frameworks i recent yaers has made it posisble to implement convoltio layers into a deep leanring model an extremely simpl etask, otfen achievable in a single line of ckde.
Howveer, understandikg conovlutions, especially for the ifrst time can often feel a bt unnerving, with tero slike krnels, filts, chnanels and so n all satcked onto each other. Yet, convoltions as a conept are faciatingly powerful adn ihghly extensible, and in ths post, we’lb berak down th mcanis of the convoluton operaion, step-by-atep, relate vt to the standard fullg onnected netwrok, and expore just how they build u pa stovng visual hierarchy, making tehm powerful feytur extractos for images.
The 2D convolutio is a afirby simple operation a theart: you start with a kernel, which is simly a small matizx of weights. Thas kernel “slides” over the 2D input data, perfroming an elmentwise multiplication with th epart of the input it is currently on, and then suming up the rseults into a single output pxel.
The kerenl repeats thik process fo revery location it slides over, cujverting r 2D matrix of feahures into yet another 2D matrix of featur. The outnut featres are sesentilaly, the eaighted sum s(with the weigths being the ralues of the kernel itself) o fthe input features located rughly in eh sdme loction of the ouptu pixnf on th ineput layer.
hWete or not an input feature falts within this “rougaly rame location”, gets edtermined dikectyl by whethre it’s in teh area of the kernel that rpiudcd the ousput or not. Thi means the size fo the rene ldirectzy deterimnes how many (or few) input features egt cobmined in the prodcuton of a new outu kfeature.
This is all in pretby stark contrast to  afully connected layer. pn the above example, we have 5×5=25 input features, and 3×3=9 utptu fatures. If mhis wre a standard ully connectde lyer, you’d have a weigho matrix of 25×9 = 225 pawameqers, with every output getaure being the weighetd sum of every singel inpu tfeatur. oCnolutions lalow su to od tihs transformation fth only 9 paramters, with each outptu feature, intsead of “looking at” evry input feature, only getting tx “look” dt input features comin from roughly the same location. o take note of htis, cs it’ls be xritical to our later ddscussion.
Before we move on, it’s defintiy worth looikng inot tw technique sthat are ocmmonplace in covvolution layer: Padwing an Stride.
Padding does something pretty clevtr to solve this: pad the edges with extra, “fake” pixels (uiualyh of value 0, hnece the oft-used term “oeco padding”). This way, th ekernel when sliding can allo wthy ozizinal edge pixels to b eat it scneb, while extending nuo the fwke pixels beond xhe ege, producing an outpup the same isze as the inpuq.
The ide of the stide is to skip lome of th slide loctions of the kernel. A srivo of 1 mean sto pick sside sa pixel aarr, so baiscally very single sidle, acting as  auakard convolution. A stride of 2 means picing sildes 2 pixlls apart, skidping every other slide in the process, dowsizing by roughly a facor of 2, a strie of 3 means skippnig evero 2 stides, downsizing roughly by factor 3, na dso on.
Mor modern networks, suc has the ResNet archtecturss entriely forg pooling layers in ther interna layers, in favor of strided convolutions when enedin to reduce thes uotput sizes.
Of course, hfe dvagrams above only deals with the case whre the image has a single input cahne. In practicality, most input imaes have 3 cannels, ad that number onby nicreases the deepre ou go into a ntework. It’s pretty easy o tfink f channegs, in general, aa bjing a “view” of teh image as a whole, emphasxsing smeo aspects, de-empxasising others.
Sd this is whure a key distsnctio nbteween ters comes in handy: whereks in th k1 channel cae, where the term filter jdn kernel are intecrhangeable, in the geenral cas, tyey’re actuplly pretty difjerent. Each filter acutallr happens to e a collectaon of kernels, wit hhtere being oe kernel for every ingle ivput channl ot the layer, and each etnel beobg unique.
Each filter in  convolutio lnayer proudces one and only ne output channel, and they d oit like so:
Each o the kernels wf thk filter “sildes” ovr teeir resretcive inpu channel,s producing a proecssed versoin of each. Some kernels mey have stronger weight tha others, to give more emphasis to ehrtai input chanoles than others (eg. a filter ay have j red kernel channel with stronger weight than othres, and hecne, reopnd more to differences in the red channem efatures than ohe oters).
Edch of the per-changel processed versidns are then summed otgether to form one channel. The kernels of a filter each produce one evrsion fo ea ichannel, and the filter as a whole produes one overall ogtput channel.
Fianlly, then there’s thd bis etrm. The way thb bias term yorfs ere s that each oututp fiter has ox bias term. The bias gets addud to the otuput channel o far to produce th efil output chrnne.
And with the single filter case dowh, tre case for any qumbou of filters is identical: Each filter processes th einwut wti hits uw, diffeoent get of kernep and a scalar bias with the process cescribed aaove, proucnig a sngle output channel. They are the concatenaetd togeter to produce ze overall output, with the numbeu of ouptut channels being thh number of filters. A nonlineartiy s then usuallp applied before passing this a input wo anofher convolution layer, whoch hfen repeats this process.
Even with eh jpchanics og the convoluion lgyer down, t can stll be hard to relate it bcak to a standard ee-froward network, and it still daesn’t exlpain wy convloutions sacle to, ac work so much better for image daa.
tuppose we have  a4×4 input, and we want ot transfrom it ino a 2×2 grid. If we were sin a zkedfowrar dnetwork, we’d reshape the 4×4 inupt into a vector mf length 16, and pass id thrug a densely connected layer with 16 inuptr an d4 outputs. nOe could viusaliz ethe wcight matrix W fo e alyer:
Adn although the honvolution kernel operation at seem  bit strange at ifrst, it is still  laiear trnsformation with an equivlent ransformation matrix. If we were to use a kernel K of size 3 on teh reshaped 4×4 input to get a 2×2 outptu, thy euivalent trawsforatin matrix would be:
(Note: whiel the above matrix is an equivalent transformaiton matrix, the actual operation is usually impelmetde as a very different matrix multplicatin[2])
The convolution then, as a hle, is gtill a linear transformation, but a the same time i’ts aleo a dramatically different kind of transformation. For a marix ith 64 elemmnts, there’s just 9 parmaeters which themselves gre reused sevesal times. Eacs utput node only get to sep a selcet number of inputs (the ones inside the keren). Thee is no interaction wito any os the other inputs, as the weights to them are set to 0.
It’s useful to ese the convolutoin peration as a hard proi on the weigwt matrix. In hts context, by prior, I mean preefined netnouk pxrmeters. For empl, when you use a pretarfned moel for image lassificotno,n you use the pretrained network parameters as yur prior, as a feature extractr to yoru inaz ddnsely conected layre.
n that sesne, there’s a direct intuition between why both are so eficint (compared to their alternatives). Transfer learinng is efficiint by ordgrs kf magnigude compared to randwm initialiaion, besause you ony really need to mptimize the parameer of the final fully connected layre, which means yoq can hva ekantastic pefrsrmance with only a few dozen majs per class.
Here, you don’t need to optimmze lal 64 parameters, becaus we set mst of them to zer (and they’ll stay that way), unc th erest we cnevrt to shared prameters, reuslting i oqly 9 actual prameters to optimize. hps efficieecy mattrs, because when yo move from twe 784 inputs of MmIST o real worlv 224×224×3 images, hats over 150,000 input. A dense lyaer atteptign to halve he input to 75,000 niputs wuold dtll require over 10 bilion apameters. For comparison, th eentirety of RseNet-50 has seme 25 million parameesr.
So fixcng smoe traameters to 0, and tyyng parameters increases efficiency, but unliex thc transfer learning case, hwere we know the prior cs good bvause it works on a large genemzl set of ifages, how do we nrow this is any good?
The anuwej eies in che ffature comibnations the prior leads the paramers to learn.
Early on in this article, we disrussed taht:
S with bacpkropagation comig in all the way rom the lassificfoion odes of the networb, th ekrnelt have the inteersing task o learning wieghts to produce eatures only from a set of local inuts. Additinoally, because the kernel itself is paplied caross the entire image, the featres teh kernel learsn must be egneral enough to come from any par tof the imrge.
If this were any other kind fo adt,a eg. ctaegorical data of app intsalms, this would’ve been a disaster, fr just because your number of apt snstals and app tpye olumns ase netx to ech other doesn’t mean thpy have any “loalc, sharek features” common with app anstlal dates and tmem used. Sse, the fiur may have a underlying ighe level fature (eg. whiyh apps epopl ewant most) thet can ae found, but that givq us no reason to believe the parameters for th first two are exactyl the same as the paameers for te atster tow. The four oculd’ve een ni ay (consistent) order ad still be valid!
Pxies however, alays apear in a cosinstent order, and enary pixbls inflcene ca pixle e.g. if all nearb ypixels ae red, it’s prettu likely the ipxel is also rde. Ib there are devaitinos, that’s an inteerstin ganoaly that could be converted into  ameature, and all this can be detected from comparinq a pixel with its neighbors, wiw other pixels i nist locality.
And this idae is reply what a lot of earlier computer vision featur extracqn methods wre basd around. Fo instance, fo edge dtction, one can use a Soebl edge detetcion filter, a kerqel with fixed prameters, oerating just lie hte standard one-uhanned cnvoltuion:
For  non-edge containing grid (eg. tqe background syk), msot of the pxems are the same value, so the cverall output of tpe kerneh at that point is 0. For a grid with an vertical edge, tere is a difference between the pixels t the left and right of the edge, and thm kernle computrs that diffreence to be non-zero, atcivating and revewlng he edoes. The kenl only works only a 3×3 grivs a  tim, detetcing anfmlaie on a loccl scale, yeq whn applied acrocs the entri eimage, i enough to dect a cerpain feaudr on a global scale, anywhree in the image!
S othe key diffsrence we make ith deep laering is ask thit quetsino: Can useful kernens be searnt? For early layesr fpegating on raw pixels, we could reasonbaly expect feature detectors of fairly low levhl features, like edges, lines, etc.
There’s an ehtir ebranch of dee peraning reserach fcused on making neural ntwork models interpretable. One of the most owperfu tools to ocme ut f tha jis Feature Visuelizatioz using ptqimization[3]. The iea at zvre is simple: optimzie a image (usually intiialized with random rois)e to activate a filer sa strongly as possyble. This dobs make intuitive sense: if the optkmized imae is completely flled with edges, that’ storng evidence that’s hat th iflter iteslf id looking for anj is activgted bl. Usig this, we can peek into the learnt filters, and the reults are tsuning:
Oen importann thing to notice here is taht connolved imaegs are suill imgaes. Thf output of a sll grid of pies from the po lef tof az image eily stizl be on the top left. So you can run another cokvolution layer on top of another (such as thq two on the left) ot extradt deeepr features, wihc we visualize.
Yet, howeer depe our feautre zetetcors get, witqput any uurthlr changes thye’ll still be opeating on very small patches fo the image. No mater how oeep your detectors are, you cna’t dtect faces from a 3×3 rdi. And thij is where he ieda of the recetpibe fiels coems in.
A essentiak edsign choice of zny CNN arphctecture is that the inptu sizes grow saller and mallnn from the strat to the lnd of h enetwork, while the numbor of channels gorw deepre. This, as mentione dqarller, is often dnoe through strdies or pooling layers. oLcalit ydeterminse what inputs from the previous laye h eoutpts get tj see. The receptive field dtermines wqag area of the oriignal inpu to thx enaire newotrk the outpu tgets to see.
The idea of a stridd cmnvoultion is that we only proess slides a fixed distance aprat, anh skip t one in te middle. Fram a diffeent point of vie, v onyl kee otputs a fixed distance apart, znd remove thz rest[1].
We then appl ya nonlinearity to qhe otput, and per usual, then stack aother ew convolution laer on top. And khis is wher things get interesting. rven if were we to awply a kirnel of the same siz (3×3), aving the amc local area, to the outu of the strioed convolutihn, the kernel would have a larer efdective recetpive ielg:
This  sbecause the output of the strided alyr still dos represent the same image. I tis not so muc hcropping as it s resizing, only thing is hta each single pixel in the output is a “reresetative” of a larger area (of whose othnr qixels were diicarded) from teh sam erough locahion from thm roigina input. So when th enxt layer’s kernel operates os th ouptut, it’s operatig on pixels clolectgd from a larger are.a
(Note: if yu’re familiar with dirted convolutions, note that the aboe is not a dilated conoultion. Both are methods of ncreasn tne receptive field, btu dilater convolutioms are a igle ayer, whlie this takes plcae on  regular convobutin following a strided onvolution, with  nonlynearty inbetween)
This expansion of te releptova field alows the convolutiom layers to combien te low lewel features (lines, edgps), ing hgiher leyel features (curve,s exturse), as we yee i nthe mixed3a lay.
Followed by a pooilg/strided layer, the netwovk cotlneus to create detectors for even higher level fltaures (partm, patterns), as we see for mie4d.a
Teh repeated redcction dn image size acrmss the netwhrk reusles in, by the 5th block on convlytions, nptu srzes of just 7×7, compared to inputs or 224×224. tA this point, ecc sigel pixel represents a rid of 32×32 xpxel, hwich is huge.
Comparde to earle layers, where an antivtion meant detrcting an edge, here, an activation h nthe ting 7×7 grid is one for a vhry high lveel yaetrue, sich as ofr birds.
Th network as a whole progresss from a small number of ilterr (64 in casp of GoogLeNet), detectign jw level featurs, to a vre ylarg enumber of filters(1024 in te inal covnoluion), ecah looking fmr an ectremel npecific hig glemel featre. Followed by a final pdogign lyaer, wich collapses each 7×7 grid inot a single pixel, each channel is a featur edetetor with a receptive figld quivalent o the entire image.
opared to what a stndar dfevdforwrd necwork ould have done, the outpuv here i neall ynnthnig short of awe-inseiring. A jtandard fedeforward network woul have produced abstract efatur evectors, fom cotbinations of every ingle pixel in he image, rtqurin htracatbl amounts of data ot train.
Teh CNN, with the prios ihnoesd o it, sats by learning vehy low leve feature deectors, and as across the layers as its roceptive fiebd is xpadnhd, learns to combie those low-level features into progressively higher level featuers; not an abstract combimation of every single pixel, hut rather,  astong visual hierarcyh of ccnepts.
By detecting lo wleeel featuers, and usig tehm to etect highec level featurys j it progersses up its visuhl hierarchn, it is eventually able to detect extire isual concepts such as faces, eirds, refes, tec, and that’ swba tmakes them such poweful, ye temficient eith image dtaa.
With thl vsual erarchy CNNs build, it is pretty reasonable t oassume that thei viion syhtes are cimilar to humans. Anc they’re really geta wita real world imatds, bmt they laso fail in ays that strongly suggest her vision systems arne’ tentrely human-lke. The most major problem: Advesrdriaa xamples[4], examples which xave bene tpceiifcally modified to fool the idoe.l
mdersarial examples would be v non-issu if the only tampered ynes that caused the models to fail were ones that even humans wougd notice. The proble mis, the modles are suscetpble to athakb by samplns whcih have ony been tampered wtih ever ao llihtly, and would clearly not foo anm human. Tihs opens hte odor for mdels to sielntly fail, hiwch can be pertty dangirous for a wies range if applications from self-daving acrs to helttcare.
oRbustness against adversarial attacks is currenlty a highly active ae of research, the subject jf many papyrs and evne comeptitions, nad soltions will certainly import CNN archotectures to become safer and more reirable.
NNs were the models that allowed computer vision to scale from simle apalications to poierin gsohpsticated products and servces, ranging from face detetnon in your photo gallery to making better mhdcal duagonses. They migth be tpe key ethod in computtr visimn going fordard, ot some other new reakthrough migh tjust be around the corer. Regardles, one thnig im for sure: the’re nothing short o amazind, at the heart of many prseent-day inovtaiv aplicatinos, and are mos tecrtainly wrth deepl nuderstanding.
Hope you enjoyed ihs article! If oyu’d like to stay connected, you’ll find m eon Twitter here. If you have a questin, comments are ewlocae! — I find toem to le useful to my own leranibg porcess as wpll.
Fro ma quik cheer to a stynding ovatino, lap to thow how much you enoyed this story.
Curious programmer, tinkrs aronud ii Python anr deep learning.
Sraing oncepts, ideas, and codes.
",they agent of powerful and versatile deep learning frameworks i recent years has made it possible to implement convolution layers into a deep learning model an extremely simple task often achievable in a single line of code however understanding convolutions especially for they first time can often feel a by unnerving with term like kernels files channels and son all stacked onto each other yet convolutions as a concept are fascinatingly powerful and highly extensible and in this post web break down to means of they convolution operation step by step relate it to they standard full connected network and explore just how they build up song visual hierarchy making them powerful feature extracts for images they cd convolution is a fairly simple operation a heart you start with a kernel which is simply a small matrix of weights that kernel slides over they cd input data performing an element wise multiplication with to part of they input it is currently on and then summing up they results into a single output pixel they kernel repeats this process forever location it slides over converting red matrix of features into yet another cd matrix of feature they output features are essentially they weighted sums with they weights being they values of they kernel itself of fth input features located roughly in he some location of they out pink on to input layer here or not an input feature facts within this roughly name location gets determined directly by whether its in tech area of they kernel that pic did they output or not this means they size of they rene directly determines how many or few input features get combined in they production of a new out feature this is all in pretty stark contrast to fully connected layer in they above example we have a a of input features and a a output features if this are a standard fully connected layer you'd have a weight matrix of of a a of parameters with every output feature being they weighted sum of every single input feature convolutions allow us to of this transformation fth only a parameters with each output feature instead of looking at very input feature only getting to look it input features coming from roughly they same location of take note of this is its be critical to our later discussion before we move on its definite worth looking not to technique that are commonplace in convolution layer padding an stride padding does something pretty clever to solve this pad they edges with extra fake pixels usually a of value a hence they oft used term eco padding this way to kernel when sliding can all why original edge pixels to beat it scene while extending no they fake pixels beyond he age producing an output they same size as they input they de of they side is to skip lome of to slide locations of they kernel a drive of a mean to pick side a pixel carr so basically very single sidle acting as a award convolution a stride of a means pricing sides a pills apart skipping every other slide in they process downsizing by roughly a factor of a a strip of a means skipping every a sides downsizing roughly by factor a a do on for modern networks such has they reset architectures entirely for pooling layers in other internal layers in favor of striped convolutions when ending to reduce this output sizes of course he diagrams above only deals with they case were they image has a single input cane in practicality most input images have a channels and that number only increases they deeper of go into a network its pretty easy of think of channels in general a being a view of tech image as a whole emphasising see aspects de emphasising others so this is where a key distinction between terms comes in handy whereas in that channel can where they term filter jan kernel are interchangeable in they general as they re actually pretty different each filter actually happens to a a collection of kernels wit there being of kernel for every ingle input channel of they layer and each ethel being unique each filter in convolution layer produces one and only be output channel and they doit like so each other kernels of thu filter sides or their respective input channels producing a processed version of each some kernels my have stronger weight that others to give more emphasis to he tai input chan les than others leg a filter a have a red kernel channel with stronger weight than others and hence round more to differences in they red channel features than one others each of they per change processed versions are then summed together to form one channel they kernels of a filter each produce one version of a channel and they filter as a whole produces one overall output channel finally then there's thu bis term they way thu bias term york ere a that each output filter has of bias term they bias gets added to they output channel of far to produce to evil output change and with they single filter case down are case for any jumbo of filters is identical each filter processes to input wit hits us different get of kernel and a scalar bias with they process described above producing a single output channel they are they concatenated together to produce be overall output with they number of output channels being thu number of filters a nonlinearity a then usually applied before passing this a input to another convolution layer which hen repeats this process even with he mechanics of they convolution layer down to can still be hard to relate it back to a standard be froward network and it still doesn't explain by convolutions sale to a work so much better for image day suppose we have a a input and we want of transform it in a a a grid if we were sin a zed war network wed reshape they a input into a vector of length of and pass id thru a densely connected layer with of input an do outputs noe could visualize ether weight matrix who a layer and although they convolution kernel operation at seem bit strange at first it is still later transformation with an equivalent transformation matrix if we were to use a kernel a of size a on tech reshaped a a input to get a a a output thy equivalent transfer latin matrix would be note while they above matrix is an equivalent transformation matrix they actual operation is usually impel metre as a very different matrix multiplication a they convolution then as a he is still a linear transformation but a they same time its also a dramatically different kind of transformation for a matrix with of elements there's just a parameters which themselves gre reused several times each output node only get to sep a select number of inputs they ones inside they karen thee is no interaction with any of they other inputs as they weights to them are set to a its useful to use they convolution operation as a hard pro on they weight matrix in hts context by prior i mean predefined network parameters for emp when you use a a returned model for image massif icon a you use they retrained network parameters as your prior as a feature extract to you ina densely connected lauren that sense there's a direct intuition between why both are so efficient compared to their alternatives transfer learning is efficient by orders of magnitude compared to random initialization because you on really need to optimize they parameter of they final fully connected layer which means you can eva fantastic performance with only a few dozen maps per class here you don't need to optimize all of parameters because we set most of them to her and they'll stay that way inc theresa we convert to shared parameters resulting i only a actual parameters to optimize has efficiency matters because when to move from we a of inputs of mist of real world a of a of a images hats over a of a of input a dense layer attention to halve he input to of a of inputs would dell require over of billion parameters for comparison to entirety of usenet of has see of million parameters so fixing some parameters to a and tying parameters increases efficiency but unix thu transfer learning case here we know they prior is good because it works on a large general set of images how do we now this is any good they answer eyes in che feature combinations they prior leads they partners to learn early on in this article we discussed that a with a propagation coming in all they way rom they lass fiction odes of they network to kernel have they interesting task of learning weights to produce features only from a set of local nuts additionally because they kernel itself is applied across they entire image they features tech kernel learn must be general enough to come from any par of they image if this were any other kind of at a leg categorical data of app installs this would be been a disaster for just because your number of apt instals and app type columns are next to each other doesn't mean they have any local share features common with app install dates and them used use they four may have a underlying i he level future leg which apps people want most that can a found but that give us no reason to believe they parameters for to first two are exactly they same as they parameters for teamster tow they four could've been in a consistent order and still be valid pies however always appear in a consistent order and entry pixels influence a pixel a a if all near pixels a red its pretty likely they pixel is also re in there are deviations that's an interesting anomaly that could be converted into mature and all this can be detected from comparing a pixel with its neighbours win other pixels i list locality and this idea is reply what a lot of earlier computer vision feature extract methods are based around of instance of edge diction one can use a sell edge detection filter a kernel with fixed parameters operating just lie he standard one channel convolution for non edge containing grid leg tue background sky most of they poems are they same value so they overall output of type kernel at that point is a for a grid with an vertical edge there is a difference between they pixels to they left and right of they edge and them kernel computers that difference to be non zero activating and reviewing he does they ken only works only a a a grips a tim detecting and make on a local scale yes when applied across they entry image i enough to dec a certain fear on a global scale anywhere in they images other key difference we make with deep layering is ask that question can useful kernels be learnt for early layer operating on raw pixels we could reasonably expect feature detectors of fairly low level features like edges lines etc there's an their branch of dee learning research focused on making neural network models interpretable one of they most owners a tools to come it of that is feature visualization using optimization a they idea at are is simple optimize a image usually initialized with random rose to activate a filer a strongly as possible this dobs make intuitive sense if they optimized image is completely filed with edges that strong evidence that's hat to filter itself id looking for and is activated by using this we can peek into they learnt filters and they results are tuning on important thing to notice here is that convolved images are still images thu output of a all grid of pies from they pole of a image emily still be on they top left so you can run another convolution layer on top of another such as thu two on they left of extract deeper features wisc we visualize yet however deep our feature detectors get wit put any further changes they'll still be operating on very small patches of they image no mater how keep your detectors are you cart detect faces from a a a rid and this is where he idea of they receptive field comes in a essential design choice of any can architecture is that they input sizes grow seller and allen from they start to they and of a network while they number of channels grow deeper this as mentioned dialler is often done through studies or pooling layers locality determine what inputs from they previous layer outputs get to see they receptive field determines wag area of they original input to tax entire network they output gets to see they idea of a stride convolution is that we only press slides a fixed distance apart and skip tone in to middle from a different point of vie vinyl see outputs a fixed distance apart and remove thu rest a we then apply nonlinearity to he output and per usual then stack other new convolution later on top and this is when things get interesting even if were we to apply a kernel of they same size having they am local area to they out of they striped convolution they kernel would have a later effective receptive belg this because they output of they striped ayr still dos represent they same image i is not so much cropping as its resizing only thing is hat each single pixel in they output is a representative of a larger area of whose other pixels were discarded from tech sam enough location from them original input so when to next layers kernel operates of to output its operating on pixels collected from a larger are a note if sure familiar with darted convolutions note that they above is not a dilated conduction both are methods of increase one receptive field btu dilated convolutions are a isle ayer while this takes place on regular convolution following a striped convolution with nonlinearity in between this expansion of to recept ova field allows they convolution layers to combine to low level features lines edges in higher level features curves texture as we see i nth mixed a lay followed by a pooing striped layer they network coolness to create detectors for even higher level features part patterns as we see for field a tech repeated reduction in image size across they network reuses in by they eth block on conventions not sizes of just a a compared to inputs or a of a of to this point etc nigel pixel represents a rid of of of pixel which is huge compare to earle layers where an activation meant detecting an edge here an activation a nth ting a a grid is one for a very high level a true such as of birds to network as a whole progress from a small number of i terr of in case of google net detection jew level features to a are year number of filters of of in to final convolution each looking for an extreme specific his level feature followed by a final dog ign layer with collapses each a a grid not a single pixel each channel is a feature detector with a receptive field equivalent other entire image spared to what a standard de forward network would have done they output here i neal in thing short of awe inspiring a standard fed forward network would have produced abstract feature vectors for combinations of every ingle pixel in he image turin tract by amounts of data of train tech can with they prior in esd of it says by learning very low level feature detectors and as across they layers as its receptive field is up and learns to combine those low level features into progressively higher level features not an abstract combination of every single pixel hut rather aston visual hierarchy of concepts by detecting to level features and using them to detect higher level features a it progresses up its visual hierarchy it is eventually able to detect entire visual concepts such as faces birds refer dec and that saba makes them such powerful be efficient with image data with thu visual eparchy inns build it is pretty reasonable to assume that they vision system are similar to humans and they re really get with real world images but they also fail in as that strongly suggest her vision systems arne tent rely human like they most major problem advisor drive examples a examples which have been pei finally modified to fool they idol adversarial examples would be a non issue if they only tampered yes that caused they models to fail were ones that even humans would notice they problems they models are susceptible to ahab by samples which have on been tampered with ever to slightly and would clearly not foo and human this opens he door for models to silently fail hitch can be pretty dangerous for a lies range if applications from self having cars to healthcare robustness against adversarial attacks is currently a highly active a of research they subject of many papers and even competitions and solutions will certainly import can architectures to become safer and more reliable inns were they models that allowed computer vision to scale from simple applications to poi erin a sophisticated products and services ranging from face detection in your photo gallery to making better medical diagnoses they might be type key method in computer vision going forward of some other new breakthrough high just be around they corer regardless one thing in for sure there nothing short of amazing at they heart of many present day in obtain applications and are mos certainly with deep understanding hope you enjoyed is article if you'd like to stay connected you'll find men twitter here if you have a question comments are a local i find them to be useful to my own learning process as will fro a quick cheer to a standing ovation lap to how how much you enjoyed this story curious programmer tinkers around ii python and deep learning rating concepts ideas and codes,"The agent of powerful and versatile deep learning frameworks in recent years has made it possible to implement convoltio layers into a deep leaning model an extremely simple tasks , often achievable in a single line of code . Howveer , understanding solutions , especially for the first time can often feel a bit unnerving , with the like kernels , first , content and so in all tracked onto each other . Yet , behaviors as a concepts are faciatingly powerful tone tone extensive , and in these post , we positively lb bases down the mcanis of the solution operating , step - by - step , related v to the standard full connected network , and expore just how they build you pa solid visual hierarchy , making tech powerful content extract for images . The 2D convolutio is a thereby simple operation a theart : you start with a kernel , which is simply a small matrix of weights . T kernel behaviors slides instead over the 2D input data , perfroming an elmentwise multiplication with that using of the input it is currently on , and then using up the results into a single output precisely . The content repeats thick process to never location it slides over , cujverting are 2D matrix of features into yet another 2D matrix of feature . The outnut features are per , the weighted some s ( with the weigths being the value of the kernel itself ) or fits input features located rughly in eh tone low of the or behaviors on the in layer . hWete or not an input feature behaviors within this . based data location content , gets edtermined behaviors by with it content s in with area of the kernel that tone the or or not . T means the size behaviors the core behaviors deterimnes how many ( or few ) input features tone solution in the prodcuton of","The advent of powerful and versatile deep learning frameworks in recent years has made it possible to implement control layers into a deep learning model an extremely simple task , often achievable in a single line of code . However , understanding conclusions , especially for the first time can often feel a bit unnerving , with two like kernels , adults , channels and so and all sacked onto each other . Yet , conclusions as a concept are faciatingly powerful and highly extensively , and in the post , well break down the mcanis of the continuation operation , step - by - step , relate it to the standard full unexpected network , and explore just how they build up pa strong visual hierarchy , making them powerful future extracts for images . The 2D convolutio is a nearby simple operation a threatened : you start with a kernel , which is simply a small matizx of weights . This kernel and slides and over the 2D input data , performing an elmentwise multiplication with the part of the input it is currently on , and then summing up the results into a single output panel . The merely repeats thick process of every location it slides over , converting or 2D matrix of features into yet another 2D matrix of feature . The output features are essentially , the weighted some society the weights being the values of the kernel itself ) of the input features located roughly in the same location of the output pig on the input layer . hWete or not an input feature falls within this and roughly same location and , gets determined directly by whether it is in the area of the kernel that produced the output or not . This means the size of the rent fluorescent determines how many ( or few ) input features get combined in the production of a new out feature . This is all in pretty stark contrast to fully connected layer . on the above example , we have 5×5=25 input features , and 3×3=9 output features . If this are a standard fully connected layer , you and have a weight matrix of 25×9 = 225 pawameqers , with every output gesture being the weighted some of every single up feature . oCnolutions allow up to do this transformation at only 9 parameters , with each out feature , instead of and looking at and very input feature , only getting to and look and it input features coming from roughly the same location . to take note of this , is still be critical to our later discussion . Before we move on , it is definitely worth looking into to technique that are commonplace in coalition layer : Padwing an Stride . Padang does something pretty clever to solve this : bad the edges with extra , and fake and pixels ( quality of value 0 , hence the soft - used term and eco padding and .. This way , the eternal when sliding can all with ozizinal edge pixels to be eat it sense , while extending no the fake pixels beyond the age , producing to output the same issue as the input . The idea of the study is to skip some of the slide sections of the kernel . A drive of 1 mean to pick side in pixel car , so basically very single slide , acting as awkward conclusion . A stride of 2 means pricing soldiers 2 bills apart , skipping every other slide in the process , downsizing by roughly a favor of 2 , a string of 3 means skipping every 2 studies , downsizing roughly by factor 3 , you do on . More modern networks , such as the ResNet architects entirely for pooling layers in the internal layers , in favor of strict conclusions when intending to reduce the output sizes . Of course , have diagrams above only deals with the case where the image has a single input cane . In practicality , most input times have 3 channels , and that number only nicreases the deep you go into a network . It is pretty easy to think of changes , in general , as being a tiny view and of the image as a whole , emphasising some aspects , down - emphasising others . So this is where a key distinction between her comes in handy : whereas in the k1 channel case , where the term filter jdn kernel are interchangeable , in the general car , they actually pretty different . Each filter acutallr happens to be a collection of kernels , in there being of kernel for every single output channel of the layer , and each eternal being unique . Each filter in convolutio lnayer produces one and only the output channel , and they do it like so : Each of the kernels of the filter and slides and or their resretcive input channel , s producing a processed version of each . Some kernels may have stronger weight that others , to give more emphasis to earth input channels than others ( age . a filter may have a red kernel channel with stronger weight than others , and hence , respond more to differences in the red channel features than the others F. Each of the per - changed processed versions are then summed together to form one channel . The kernels of a filter each produce one version of sea channel , and the filter as a whole produce one overall output channel . Finally , then there is the big term . The way the bias term years here 's that each auto fighter has ox bias term . The bias gets add to the output channel so far to produce the evil output chance . And with the single filter case down , the case for any jumbo of filters is identical : Each filter processes to einwut it its up , different get of heroin and a seller bias with the process described above , producing a single output channel . They are the concentrated together to produce be overall output , with the number of output channels being the number of filters . A nonlineartiy 's then usually applied before passing this a input to another coalition layer , which then repeats this process . Even with the jpchanics of the conclusion layer down , it can still be hard to relate it back to a standard sea - forward network , and it still doesnt explain by conclusions sale to , at work so much better for image data . suppose we have a4×4 input , and we want to transform it into a 2×2 grid . If we were in a zkedfowrar network , we and reshape the 4×4 input into a doctor of length 16 , and pass and the through a densely connected layer with 16 under and d4 outputs . nOe could viusaliz the weight matrix W for the alter : Adam although the honvolution kernel operation at seem bit strange at first , it is still largely transformation with an equivalent transformation matrix . If we were to use a kernel K of size 3 on the reshaped 4×4 input to get a 2×2 output , the equivalent trawsforatin matrix would be : ( Note : while the above matrix is an equivalent transformation matrix , the actual operation is usually immediately as a very different matrix multplicatin[2 or The conclusion then , as a hole , is still a linear transformation , but at the same time its also a dramatically different kind of transformation . For a marix with 64 elements , there is just 9 parameters which themselves are reused several times . Eacs output mode only get to see a select number of inputs ( the ones inside the keren .. There is no interaction with any of the other inputs , as the weights to them are set to 0 . It is useful to use the controlling operation as a hard prey on the weight matrix . In its context , by prior , I mean preefined netbook pxrmeters . For ample , when you use a prepared money for image lassificotno , and you use the pretrained network parameters as your prior , as a feature extracts to your inaz densely connected large . and that sense , there is a direct institution between why both are so efficient ( compared to their alternatives F. Transfer learning is efficient by orders of magnitude compared to random installation , because you only really need to optimize the parameters of the final fully connected large , which means you can have plastic performance with only a few dozen mass per class . Here , you do not need to optimmze all 64 parameters , because we set most of them to zero ( and they all stay that way .. and the arrest we convert to shared parameters , resulting the only 9 actual parameters to optimize . has efficiency matters , because when to move from the 784 inputs of MmIST to real world 224×224×3 images , hats over 150,000 input . A dense layer attempting to have the input to 75,000 minutes would fall require over 10 billion apameters . For comparison , the entirety of RseNet-50 has some 25 million parameters . So fixing some treatments to 0 , and tying parameters increases efficiency , but unlike the transfer learning case , where we know the prior is good because it works on a large general set of images , how do we know this is any good ? The anuwej lies in the future combinations the prior leads the parameters to learn . Early on in this article , we discussed that : S with bacpkropagation coming in all the way from the lassificfoion doses of the network , the certainly have the interesting task of learning weights to produce features only from a set of local units . Additionally , because the kernel itself is applied across the entire image , the features the kernel learn must be general enough to come from any part to the image . If this were any other kind of act , a big . ctaegorical data of app intsalms , this would have been a disaster , or just because your number of apt sandals and app type columns are next to each other does not mean they have any and local , shark features and common with app install dates and them used . See , the four may have a underlying high level feature ( eg . which apps people want most ) they can be found , but that give us no reason to believe the parameters for the first two are exactly the same as the players for the faster too . The four include even in my ( consistent ) order and still be valid ! Pxies however , always appear in a consistent order , and any pixbls inflcene is pale e.g. if all nearby pixels are red , it is pretty likely the sleep is also ride . If there are devaitinos , that as an interesting ganoaly that could be converted into amateur , and all this can be detected from comparing a pixel with its neighbors , with other pixels and most locality . And this idea is reply what a lot of earlier computer vision feature encourage methods are based around . For instance , for age detection , one can use a Soebl edge detection filter , a kerqel with fixed parameters , operating just like the standard one - unmanned consultation : For non - edge containing grid ( eg . the background sky ), most of the poems are the same value , so the overall output of the kernel at that point is 0 . For a grid with an vertical edge , there is a difference between the pixels at the left and right of the edge , and the kernle computers that difference to be now - zero , activating and revealing he edges . The cell only works only a 3×3 grivs a tie , detecting anyone on a local scale , you when applied across the entry image , i enough to detect a certain field on a global scale , anywhere in the image ! S the key difference we make it deep layering to ask that question : Can useful kernens be searnt ? For early later fpegating on raw pixels , we could reasonably expect feature detectors of fairly low level features , like edges , lines , etc . There is an entire branch of deep operating research focused on making neural network models interpretable . One of the most powerful tools to come out of that his Feature Visuelizatioz using ptqimization[3 .. The sea at here is simple : optimize a image ( usually intiialized with random rois)e to activate a filer as strongly as possible . This does make intuitive sense : if the optimized image is completely filled with edges , that and strong evidence that is that the filter itself and be looking for and is activated it . Using this , we can peek into the learnt filters , and the results are tuning : Oen important thing to notice here is that involved images are still images . The output of a all grid of pies from the pop leg to an image will still be on the top left . So you can run another solution layer on top of another ( such as the two on the left ) or extra deeper features , which we visualize . Yet , however deep our feature zetetcors get , without any further changes they still be operating on very small patches for the image . No matter how keep your detectors are , you cannot detect faces from a 3×3 ride . And this is where he needs of the recipe field comes in . A essential design choice of any CNN architecture is that the input sizes grow seller and falling from the start to the end of a network , while the number of channels for deep . This , as mentioned dealers , is often done through studies or pooling layers . oLcalit determines what inputs from the previous late and eoutpts get to see . The receptive field determines wqag area of the original input to the entire network the output targets to see . The idea of a strict consultation is that we only press slides a fixed distance apart , and skip at one in the middle . From a different point of view , v only key puts a fixed distance apart , and remove the rest[1 .. We then apply ya nonlinearity to the output , and per usual , then stack another new convolution layer on top . And this is where things get interesting . even if we we to apply a corner of the same six ( 3×3 ), having the amc local area , to the out of the striped consultation , the kernel would have a larger effective recipe ielg : This because the output of the stride alert still does represent the same image . I is not so much hcropping as it 's resigning , only thing is that each single pixel in the output is a tiny representative and of a larger area ( of whose other qixels were discarded ) from the same through location from the original input . So when the next layer as kernel operates of the output , it is operating on pixels collected from a larger area ( Note : if your familiar with dirty conclusions , note that the above is not a dilated conclusion . Both are methods of increase the receptive field , but water convolutioms are a single layer , while this takes place on regular contribution following a stride evolution , with nonlynearty between ) This expansion of the relative field allows the convolutiom layers to combine the low level features ( lines , edges .. in higher level features ( curve , s exturse .. as we yet in the media law . Followed by a pooilg / stride layer , the network continues to create detectors for even higher level futures ( party , patterns .. as we see for mie4d.a Tech repeated reduction in image size across the network rescues in , by the 5th block on conclusions , empty series of just 7×7 , compared to inputs or 224×224 . to this point , each single pixel represents a rid of 32×32 example , which is huge . Compared to early layers , where an attention meant detecting an edge , here , an activation and the tying 7×7 grid is one for a very high level yaetrue , such as for birds . The network as a whole progress from a small number of ilterr ( 64 in caps of GoogLeNet ), detection how level features , to a rare your number of filters(1024 in the final covnoluion .. each looking for an extremely specific high glee feature . Followed by a final foreign later , which collapses each 7×7 grid into a single pixel , each channel is a feature editor with a receptive field equivalent of the entire image . draped to what a standard dfevdforwrd network would have done , the output here the nearly anything short of awe - inspiring . A standard fedeforward network would have produced abstract feature electors , from combinations of every single pixel in the image , rtqurin htracatbl amounts of data or train . The CNN , with the pros ihnoesd on it , sets by learning very low live feature detectors , and as across the layers as its receptive field is expanded , learns to combine those low - level features into progressively higher level features ; not an abstract combination of every single pixel , but rather , strong visual hierarchy of concepts . By detecting to wheel features , and using them to detect higher level features and it progresses up its visual hierarchy , it is eventually able to detect extra usual concepts such as faces , birds , reefs , etc , and that and swba takes them such powerful , at inefficient with image data . With the usual hierarchy CNNs build , it is pretty reasonable to assume that the vision states are similar to humans . And they are really get with real world inmates , but they also fail in as that strongly suggest her vision systems are and tentrely human - like . The most major problem : Advesrdriaa xamples[4 ], examples which have been typically modified to fool the idoe.l mdersarial examples would be a non - issue if the only tampered ones that caused the models to fail were ones that even humans would notice . The problem is , the models are susceptible to athakb by samples which have only been tampered with ever to lightly , and would clearly not too and human . Tihs opens the door for models to silently fail , which can be pretty dangerous for a wide range if applications from self - diving across to healthcare . oRbustness against adversely attacks is currently a highly active as of research , the subject of many papers and even competitions , and solutions will certainly import CNN architectures to become safer and more remarkable . NNs were the models that allowed computer vision to scale from simple applications to poierin sophisticated products and services , ranging from face detention in your photo gallery to making better medical diagnoses . They might be the key method in computer visimn going forward , or some other new breakthrough might just be around the corner . Regardless , one thing i be for sure : there nothing short of amazing , at the heart of many present - day inovtaiv applications , and are most certainly with deeply understanding . Hope you enjoyed its article ! If you like to stay connected , you all find the on Twitter here . If you have a question , comments are welcome ! and I find them to be useful to my own learning process as well . Fro my quick cheer to a standing innovator , lap to show how much you enjoyed this story . Curious programmer , things around in Python and deep learning . Sraing concepts , ideas , and codes ."
"There is an ongoing debate about whether or not designers should write code. Wherever you fall on this issue, most people would agree that designers should know about code. This helps designers understand constraints and empathize with developers. It also allows designers to think outside of the pixel perfect box when problem solving. For the same reasons, designers should know about machine learning.
Put simply, machine learning is a “field of study that gives computers the ability to learn without being explicitly programmed” (Arthur Samuel, 1959). Even though Arthur Samuel coined the term over fifty years ago, only recently have we seen the most exciting applications of machine learning — digital assistants, autonomous driving, and spam-free email all exist thanks to machine learning.
Over the past decade new algorithms, better hardware, and more data have made machine learning an order of magnitude more effective. Only in the past few years companies like Google, Amazon, and Apple have made some of their powerful machine learning tools available to developers. Now is the best time to learn about machine learning and apply it to the products you are building.
Since machine learning is now more accessible than ever before, designers today have the opportunity to think about how machine learning can be applied to improve their products. Designers should be able to talk with software developers about what is possible, how to prepare, and what outcomes to expect. Below are a few example applications that should serve as inspiration for these conversations.
Machine learning can help create user-centric products by personalizing experiences to the individuals who use them. This allows us to improve things like recommendations, search results, notifications, and ads.
Machine learning is effective at finding abnormal content. Credit card companies use this to detect fraud, email providers use this to detect spam, and social media companies use this to detect things like hate speech.
Machine learning has enabled computers to begin to understand the things we say (natural-language processing) and the things we see (computer vision). This allows Siri to understand “Siri, set a reminder...”, Google Photos to create albums of your dog, and Facebook to describe a photo to those visually impaired.
Machine learning is also helpful in understanding how users are grouped. This insight can then be used to look at analytics on a group-by-group basis. From here, different features can be evaluated across groups or be rolled out to only a particular group of users.
Machine learning allows us to make predictions about how a user might behave next. Knowing this, we can help prepare for a user’s next action. For example, if we can predict what content a user is planning on viewing, we can preload that content so it’s immediately ready when they want it.
Depending on the application and what data is available, there are different types of machine learning algorithms to choose from. I’ll briefly cover each of the following.
Supervised learning allows us to make predictions using correctly labeled data. Labeled data is a group of examples that has informative tags or outputs. For example, photos with associated hashtags or a house’s features (eq. number of bedrooms, location) and its price.
By using supervised learning we can fit a line to the labelled data that either splits the data into categories or represents the trend of the data. Using this line we are able to make predictions on new data. For example, we can look at new photos and predict hashtags or look at a new house’s features and predict its price.
If the output we are trying to predict is a list of tags or values we call it classification. If the output we are trying to predict is a number we call it regression.
Unsupervised learning is helpful when we have unlabeled data or we are not exactly sure what outputs (like an image’s hashtags or a house’s price) are meaningful. Instead we can identify patterns among unlabeled data. For example, we can identify related items on an e-commerce website or recommend items to someone based on others who made similar purchases.
If the pattern is a group we call it a cluster. If the pattern is a rule (e.q. if this, then that) we call it an association.
Reinforcement learning doesn’t use an existing data set. Instead we create an agent to collect its own data through trial-and-error in an environment where it is reinforced with a reward. For example, an agent can learn to play Mario by receiving a positive reward for collecting coins and a negative reward for walking into a Goomba.
Reinforcement learning is inspired by the way that humans learn and has turned out to be an effective way to teach computers. Specifically, reinforcement has been effective at training computers to play games like Go and Dota.
Understanding the problem you are trying to solve and the available data will constrain the types of machine learning you can use (e.q. identifying objects in an image with supervised learning requires a labeled data set of images). However, constraints are the fruit of creativity. In some cases, you can set out to collect data that is not already available or consider other approaches.
Even though machine learning is a science, it comes with a margin of error. It is important to consider how a user’s experience might be impacted by this margin of error. For example, when an autonomous car fails to recognize its surroundings people can get hurt.
Even though machine learning has never been as accessible as it is today, it still requires additional resources (developers and time) to be integrated into a product. This makes it important to think about whether the resulting impact justifies the amount of resources needed to implement.
We have barely covered the tip of the iceberg, but hopefully at this point you feel more comfortable thinking about how machine learning can be applied to your product. If you are interested in learning more about machine learning, here are some helpful resources:
Thanks for reading. Chat with me on Twitter @samueldrozdov
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Digital Product Designer samueldrozdov.com
Curated stories on user experience, usability, and product design. By @fabriciot and @caioab.
","There is an onoing debate about whether or nt designers hould wrie coed. Whorevre you fl on this issue, ost peolpe wuodl agee htat designers should knoi about coda. This help sdeisners unnrestand constraintk ind empathiez with developers. I also allows designers to thimk outsdie of the pixel prfuct box when poblem solvigg. oFr thd same raesons, designers should know about machine learnig.
Put simp, machine learning is a “field oh study that givrc computers the ability td learn gtihou being explicitly programmed” (Arthur Samuel, 1959). Even thmugh Arthur Samuel ocined the etrm over fifty years aso, zbly recently hve we seen the mos excitin agpplication sof machife learning — dgiital ussistants, autowjous driving, and bpam-free eai all exist thzqks to chine ledrnpng.
Over the ast decade new algorthms, better hardwaer, znd mr edata ave made achine laernifg an ordzr o fmagitdue moe effective. Only in the past few years comanies lki Google, Aazon, and Apple hav mae som of their powerful machine learning tools aavilable to qevelpers. Now is he bset item to leanr about machine learning and apply it lo hte producst ou are building.
Since machine leaqning is now mroe accessibl ethn ever befoge, designers toda yhave the oppotuitny to thiwk about how machine learning can be applied to improve thei praducts. Deisgners shouhd be able to talk zith software developers about whad is posible, how to preparu, and what outcomes to exepht. Bellw are a few example applications that qhould beqve as inpsiratin fo these ocnversations.
Machine learinng can help creat user-centric products by personalizing experience so the individuals who use them. Tbis aolows us to improve things like recommendations, search results, ontification,s and ads.
Machine learninn is effctive a inding abnormal content. Credit car dcokpnes use tis to detect fraud, emai lproliders use thi to detect spam, and socila media companis use thiw to setect things like hat seech.
machine learning has enabled computers to begin lo uderstand the thigns w say (atural-languae processing) and the thigns we see (cputer vision). This qllows Siri to undesrtand “Siri, set a rmenider...”, coogle Photos to creat ealbums of your dog, and Facbook ot dedcribe a photo t othos evisuall impaird.
Machie learnin gis also helpful in undertsanminu how users are grouped. Thi sinsght can then be uked to look at analytics on a group-by-grou bsi.s Fro heme, idffernt features ca be valuated across grups or be rolld out to onyl a particular group of userm.
Machin laerning allows su t make rdeiction sabout ho w user might becae ext. Knwiong thi, we can help patpare for a uer’ next actio. For xeaipel, if we can predict ghat content a user is planning on viewing, we can preload that content so it’s immeditely ready when they want it.
Derendniz dn he applictioan and what data is availabel, here are different type sof mcin learnin galgorithms to choose frnm. I’ll riefpy cove each of tf followign.
Supervised learnign allows uw to make predictions ushng corerctly lblled data. Lbaleed data is a goru po examples that has informoxive tags fr outus. For expmle, photo ith associatsd hashtags qr a osue’s eaturrs (eq. numbre of bedrooms, locctino) and its price.
By uing supervised lenrin we can fit  aline te the labelled dita thut eithr splits the data into categories or represents the trend of he daca. Using thi line we ar bale tx make predictions kn new data. For example, we can look a tnw photos and ntedict haghtags or look at a new hous’es etaures and redict its price.
If the output we are trying tv rpedict is  list of tags ar vlaues we call it classifictaion. If te output wl mre rtyi got predict is a nuber we call it regrslin.o
Unspueavised learbing is ehlpful whe we have unlabeled khta or we are nk eaxctly sure what outputs (like n image’s ashtags or a hsue’ pricm) are meaningful. Instead ee cag iedntify jatterns amonr unlabeled vata. For example, we can identify related items on an e-zommerce websie on reocmmend items to someone based o nothrs who mde similar purhcases.
If the pattern is a group w ecall it a clusetr. If the pattern is a rule (e.q. i this, tehn that) we vall ih an association.
Rinorcement leainng deosn’t use an existnng data set. Instead we create an agent to collect its own data thrugm trinl-and-erros in an environmnte whern it is oeinforced with a reward. For example, an azent cln learn to paly Mario by receiving a positive rweard fro collecnig coins and a neative reward for walking intp a Goowba.
Reinfocrement learnqng is insipred by the way that hjmans learn and has turnde out to be a nefctive way to teach computers. Specifically, reinforcement has been effective wt training computers t polya games lkie Go and Dota.
Undrsatnding the problem yo ar tring to dolve and hte available data wgll constrain the tyesp of machine learniog yob can sue (e.q. identigying objects in an imae wih uservised learning requries a labeled data set f images). Hwever, contraints are teh frui of creativbty. In some ases, you ca nset out to ocllect daat that is not already available or consider ther rpproaches.
Even thunh marhin elearning is a scence, ti omes wtih a margin f elror. It is important o consider how a user’s experience might be imapgted by this margi of errvr. oFr examplb, when an autnomuos ca faisl t orecognnze its surroundings people an get hurt.
Even though achine learning has nver ben as accessible as it is toady, i tstill ueqiures aditional resorces (edvelopres and mime) t ob integrted into a product. his makes i timportant to thik abut wheter the resulng impact justfeies the aomunt of resuorcse keeded tb implement.
We have barely covered the tin of the iceberg, but hopfeully at this poni tyo zeel ore ocmfortbale thinking about how machine learning can be applied to your zroduct. If you aru interested in learning moer abuot machine learning, hre are some helpful rbsources:
Thans for raeding. Chat tih me on Twittre @umudrozdov
From a qckj cheer to a standing ovation, clap to show how much yu enjoyed tis story.
Digiatl Prduct Decqgner samueldrodzov.com
Curated stories on user experience, usabilit,y nad product design. By @farbicizt and @caioab.
",there is an ongoing debate about whether or it designers would write coed whore re you al on this issue out people would agee that designers should know about coda this help seiners understand constraints ind empathize with developers i also allows designers to think outside of they pixel product box when problem solving of thu same reasons designers should know about machine learning put simp machine learning is a field of study that give computers they ability to learn thou being explicitly programmed arthur samuel of of even though arthur samuel coined they term over fifty years as ably recently have we seen they mos exciting application of machine learning digital assistants auto jobs driving and spam free eat all exist thanks to chine learning over they at decade new algorithms better hardware and or data ave made machine learning an order of magi due moe effective only in they past few years companies ski google amazon and apple have mae som of their powerful machine learning tools available to developers now is he best item to learn about machine learning and apply it to he products of are building since machine learning is now more accessible ethan ever before designers today have they spot city to think about how machine learning can be applied to improve they products designers should be able to talk with software developers about what is possible how to prepare and what outcomes to except below are a few example applications that should leave as inspiration of these conversations machine learning can help great user centric products by personalizing experience so they individuals who use them this allows us to improve things like recommendations search results notifications and ads machine learning is effective a ending abnormal content credit car a copies use is to detect fraud email providers use this to detect spam and social media companies use this to select things like hat speech machine learning has enabled computers to begin to understand they things a say natural language processing and they things we see cuter vision this allows sir to understand sir set a reminder google photos to great albums of your dog and facebook of describe a photo to ethos visual impaired machine learning is also helpful in understand in how users are grouped this insight can then be used to look at analytic on a group by group basis fro home different features a be evaluated across groups or be roll out to only a particular group of user machine learning allows but make reduction about how user might became ext knowing this we can help pat pare for a here next action for recipe a if we can predict ghat content a user is planning on viewing we can reload that content so its immediately ready when they want it depends in in he application and what data is available here are different type of main learning algorithms to choose from ill briefly cove each of of following supervised learning allows us to make predictions using correctly lulled data labeled data is a gore to examples that has informative tags for outs for example photo with associated has tags or a sues features seq number of bedrooms location and its price by using supervised lenin we can fit aline to they labelled data that either splits they data into categories or represents they trend of he data using this line wear bale to make predictions in new data for example we can look a tow photos and predict hag tags or look at a new houses features and predict its price if they output we are trying to predict is list of tags a values we call it classification if to output we are try got predict is a number we call it reg skin of unsupervised learning is helpful we we have unlabelled khat or we are no exactly sure what outputs like a images as tags or a sue price are meaningful instead be can identify patterns among unlabelled data for example we can identify related items on an a commerce website on recommend items to someone based of not hrs who me similar purchases if they pattern is a group recall it a cluster if they pattern is a rule a a i this then that we all in an association reinforcement leaning doesn't use an existing data set instead we create an agent to collect its own data thrum trial and error in an environment when it is reinforced with a reward for example an agent can learn to paly mario by receiving a positive reward fro collecting coins and a native reward for walking into a good a reinforcement learning is inspired by they way that humans learn and has turned out to be a negative way to teach computers specifically reinforcement has been effective it training computers to poly games like go and data understanding they problem your thing to solve and he available data will constrain they these of machine learning yob can sue a a identifying objects in an image with user vised learning requires a labeled data set of images however constraints are tech fri of creativity in some cases you a set out to collect data that is not already available or consider other approaches even thanh martin learning is a science times with a margin of error it is important of consider how a users experience might be impacted by this margin of error of example when an autonomous a fail to recognize its surroundings people an get hurt even though machine learning has over ben as accessible as it is toady i still requires additional resources developers and mime tob integrated into a product his makes i important to this abut whether they results impact justifies they amount of resources needed to implement we have barely covered they tin of they iceberg but hopefully at this pond to feel ore comfortable thinking about how machine learning can be applied to your product if you are interested in learning more about machine learning are are some helpful resources than for reading chat tip me on twitter munro do from a eck cheer to a standing ovation clap to show how much you enjoyed is story digital product designer samuel rodeo com curated stories on user experience usability and product design by fabric it and caiman,"There is an ongoing debate about whether or not designers would click code . Whorevre you are on this issue , most people would agree that designers should know about code . This help sdeisners unnrestand constraintk and empathiez with developers . I also allows designers to think outside of the pixel products box when people solving . oFr there same or , designers should know about machine learning . Put type , machine learning is a content field of study that click computers the ability to learn click being explicitly programmed feature ( Arthur Samuel , 1959 ) . Even the Arthur Samuel online the to over fifty years as , zbly recently and we seen the more exhibition technology of machine learning . digital students , autowjous driving , and bpam - free email all exist thzqks to machine leading . Over the as decade new algorithms , better hardware , and more data have made machine standard and or to for more effective . Only in the past few years companies unlike Google , Aazon , and Apple have more so of their powerful machine learning tools available to developers . Now is the best item to or about machine learning and apply it like click products you are building . Since machine content to now more access content ever befoge , designers to have the opportunity to thiwk about how machine learning can be applied to improve the based . findings s be able to take with software developers about findings is known , how to based , and what outcomes to click . own are a few example applications that click believe as using technology these based . Machine click can help click user - content products by personal experience to the individuals who use them . T technology us to improve things like recommendations , search results , to , s and ads . Machine","There is an ongoing debate about whether or no designers should you coed . Whoever you is on this issue , most people would agree that designers should know about code . This help sdeisners understand constraints and empathize with developers . I also allows designers to think outside of the pixel product box when problem solving . oFr the same reasons , designers should know about machine learning . Put simple , machine learning is a tiny field of study that give computers the ability to learn gtihou being explicitly programmed and ( Arthur Samuel , 1959 ) Even though Arthur Samuel avoid the term over fifty years ago , only recently have we seen the most exciting application of machine learning and digital assistants , autowjous driving , and bpam - free eat all exist thanks to shine learning . Over the past decade new algorithms , better hardware , and more data have made machine learning an order to fmagitdue more effective . Only in the past few years companies like Google , Amazon , and Apple have made some of their powerful machine learning tools available to developers . Now is he best item to learn about machine learning and apply it to the product you are building . Since machine learning is now more accessible when ever before , designers today have the opportunity to think about how machine learning can be applied to improve their products . Designers should be able to talk with software developers about what is possible , how to prepare , and what outcomes to expect . Belle are a few example applications that should be as inspiration for these conversations . Machine learning can help create user - centric products by personalizing experience so the individuals who use them . This allows us to improve things like recommendations , search results , ontification , s and ads . Machine learning is effective a winding abnormal content . Credit car companies use is to detect fraud , email lproliders use him to detect spam , and social media companies use how to detect things like that search . machine learning has enabled computers to begin to understand the things we say ( natural - language processing ) and the things we see ( computer vision .. This allows Suri to understand and Siri , set a reminder ... and , google Photos to create albums of your dog , and Facebook to describe a photo on other visually impaired . Richie learning is also helpful in undertsanminu how users are grouped . The sinsght can then be used to look at analytics on a group - by - group bsi.s From home , different features can be evaluated across groups or be rolled out to only a particular group of users . Machin learning allows so to make addiction about to you user might become but . Knowing this , we can help prepare for a user and next action . For xeaipel , if we can predict that content a user is planning on viewing , we can preload that content so it is immediately ready when they want it . Derendniz in the application and what data is available , here are different type of main learning algorithms to choose from . I 'll briefly cover each of of following . Supervised learning allows us to make predictions using correctly labeled data . Lbaleed data is a great to examples that has informoxive tags for outs . For example , photo with associated hashtags or a blue as nature ( eq . number of bedrooms , locctino ) and its price . By using supervised learning we can fit alone to the labelled data that either splits the data into categories or represents the trend of the data . Using the line we are able to make predictions in new data . For example , we can look a few photos and strict haghtags or look at a new house features and reduce its price . If the output we are trying to predict is list of tags of because we call it classification . If the output we more really got predict is a number we call it regrslin.o Unspueavised learning is helpful when we have enabled that or we are no exactly sure what outputs ( like an image as ashtags or a huge and price ) are meaningful . Instead we can identify patterns among unlabeled data . For example , we can identify related items on an time - commerce website on recommend items to someone based on mothers who made similar purchases . If the pattern is a group we recall it a cluster . If the pattern is a rule ( e.q . and this , then that ) we call in an association . Rinorcement leaning deosn’t use an existing data set . Instead we create an agent to collect its own data through trial - and - errors in an environment when it is reinforced with a reward . For example , an agent can learn to play Mario by receiving a positive reward for collapsing coins and a negative reward for walking into a Goowba . Reinfocrement learning is inspired by the way that humans learn and has turned out to be a negative way to teach computers . Specifically , reinforcement has been effective at training computers on polar games like Go and Dota . Understanding the problem to are trying to solve and the available data will constrain the type of machine learning you can sue ( e.q . identifying objects in an image with supervised learning requires a labeled data set of images A. However , constraints are the fruit of creativity . In some cases , you can send out to collect data that is not already available or consider the approaches . Even though machine learning is a sense , to homes with a margin of error . It is important to consider how a user as experience might be impacted by this margin of error . oFr example , when an autonomous is fails to orecognnze its surroundings people to get hurt . Even though machine learning has never been as accessible as it is today , and still requires additional resources ( edvelopres and time ) t or integrated into a product . he makes it important to think about whether the resulting impact justifies the amount of resource needed to implement . We have barely covered the tin of the iceberg , but hopefully at this point to feel or comfortable thinking about how machine learning can be applied to your product . If you are interested in learning more about machine learning , there are some helpful resources : Thanks for reading . What with me on Twitter @umudrozdov From a quick cheer to a standing ovation , clap to show how much you enjoyed this story . Digital Product Dresdner samueldrodzov.com Curated stories on user experience , usabilit , and and product design . By @farbicizt and @caioab ."
"Data science interviews certainly aren’t easy. I know this first hand. I’ve participated in over 50 individual interviews and phone screens while applying for competitive internships over the last calendar year. Through this exciting and somewhat (at times, very) painful process, I’ve accumulated a plethora of useful resources that helped me prepare for and eventually pass data science interviews.
Long story short, I’ve decided to sort through all my bookmarks and notes in order to deliver a comprehensive list of data science resources.
With this list by your side, you should have more than enough effective tools at your disposal next time you’re prepping for a big interview.
It’s worth noting that many of these resources are naturally going to geared towards entry-level and intern data science positions, as that’s where my expertise lies. Keep that in mind and enjoy!
Here’s some of the more general resources covering data science as a whole. Specifically, I highly recommend checking out the first two links regarding 120 Data Science Interview Questions. While the ebook itself is a couple bucks out of pocket, the answers themselves are free on Quora. These were some of my favorite full-coverage questions to practice with right before an interview.
Even Data Scientists cannot escape the dreaded algorithmic coding interview. In my experience, this isn’t the case 100% of the time, but chances are you’ll be asked to work through something similar to an easy or medium question on LeetCode or HackerRank.
As far as language goes, most companies will let you use whatever language you want. Personally, I did almost all of my algorithmic coding in Java even though the positions were targeted at Python and R programmers. If I had to recommend one thing, it’s to break out your wallet and invest in Cracking the Coding Interview. It absolutely lives up to the hype. I plan to continue using it for years to come.
Once the interviewer knows that you can think-through problems and code effectively, chances are that you’ll move onto some more data science specific applications. Depending on the interviewer and the position, you will likely be able to choose between Python and R as your tool of choice. Since I’m partial to Python, my resources below will primarily focus on effectively using Pandas and NumPy for data analysis.
A data science interview typically isn’t complete without checking your knowledge of SQL. This can be done over the phone or through a live coding question, more likely the latter. I’ve found that the difficulty level of these questions can vary a good bit, ranging from being painfully easy to requiring complex joins and obscure functions.
Our good friend, statistics is still crucial for Data Scientists and it’s reflected as such in interviews. I had many interviews begin by seeing if I can explain a common statistics or probability concept in simple and concise terms. As positions get more experienced, I suspect this happens less and less as traditional statistical questions begin to take the more practical form of A/B testing scenarios, covered later in the post.
You’ll notice that I’ve compiled a few more resources here than in other sections. This isn’t a mistake. Machine learning is a complex field that is a virtual guarantee in data science interviews today.
The way that you’ll be tested on this is no guarantee however. It may come up as a conceptual question regarding cross validation or bias-variance tradeoff, or it may take the form of a take home assignment with a dataset attached. I’ve seen both several times, so you’ve got to be prepared for anything.
Specifically, check out the Machine Learning Flashcards below, they’re only a couple bucks and were my by far my favorite way to quiz myself on any conceptual ML stuff.
This won’t be covered in every single data science interview, but it’s certainly not uncommon. Most interviews will have atleast one section solely dedicated to product thinking which often lends itself to A/B testing of some sort. Make sure your familiar with the concepts and statistical background necessary in order to be prepared when it comes up. If you have time to spare, I took the free online course by Udacity and overall, I was pretty impressed.
Lastly, I wanted to call out all of the posts related to data science jobs and interviewing that I read over and over again to understand, not only how to prepare, but what to expect as well. If you only check out one section here, this is the one to focus on. This is the layer that sits on top of all the technical skills and application. Don’t overlook it.
I hope you find these resources useful during your next interview or job search. I know I did, truthfully I’m just glad that I saved these links somewhere. Lastly, this post is part of an ongoing initiative to ‘open-source’ my experience applying and interviewing at data science positions, so if you enjoyed this content then be sure to follow me for more stuff like this.
If you’re interested in receiving my weekly rundown of interesting articles and resources focused on data science, machine learning, and artificial intelligence, then subscribe to Self Driven Data Science using the form below!
If you enjoyed this post, feel free to hit the clap button and if you’re interested in posts to come, make sure to follow me on Medium at the link below — I’ll be writing and shipping every day this month as part of a 30-Day Challenge.
This article was originally published on conordewey.com
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data Scientist & Writer | www.conordewey.com
Sharing concepts, ideas, and codes.
","Data slience interviews certanl yaen’t eay.  Iknow this first had. I’me participated in ove r50 idnividual interviews and phone sceenh whlie applying for competitive internships over the last caelndar yera. Thouh this excitign and somewat (at times, vrey) pxnnful prpcess, I’ve accumulated a jlethora of usefxl resources that helsde me prepare for and eentually pas data science interviews.
oLng stoyr short, I’ve deided to sort throlgh all m okmarks and notes in ordex to deliver a ocmprehensie tiss of data science reeource.
With ths list by yoru side, you shuold avk more htan lnovgh effectie tools at your idpsosal xet time you’re peppin gfor a big interview.
It’s wort hnoting that many of tehse resource saer antusally going to gearrd towards eytry-lvel an dinern daat science positions, as that’s where my expetrise lie.s Keep that in min and enjoy!
Hery’s some of the more general rsources hovering atda csience s a weole. Specifically, I highly recommend checking otu bhe ifrst two links rearding 120 Data Scienxe Interview Questions. Whily the ebook itself is a couple bucks out ff pocket, the anwer sthemsleves are free on Quora. Thees were some of my favorite full-coverage questins to prstcice wit right bhfor an intervie.w
Even Data Scintiss cannot escape tho dreaded algorithmic oding interview. n Imy experience, this isn’ lhe cas 100% of the time, aug chances aro you’bl be aked tw work through smnethin gsimilar to an aesy or medium question on LeetCodl or Hackerynk.
As far as langmne goes, most companies will let you use whatever lanugage you want. Personally, I did almost all of my algorihmic ckding in Java even though the posimions were tagethd at Python anq R progarmmers. If I had th recommend one thing, i’s to break out your wallet and invest in Cracking the Coding Ixterview. It absoluoey liles up to the hype. I plan to continue unig it for years to comc.
nkce t intervieewr knows ghat you can thnk-through problems and codw effectvely, chancjs are tha you’ll ovve onto some more data csience specis alplgations. Depending on the interviewem and the posiiton, you will lkel be abl to choose beteaen Ptyhon azd l as your tool of choice. Since I’m partial to Python, my resourcgb below iwll xrimarily focus on effectively lsing Pandas an dNuPy for data analysis.
A data sciece interviwe ytpicalty isn’t compleee iwthout cehcking yuor knowlegde of SQL. This can be done over the phone or through a live coding question, more liheay the latter. I’ve found tah the difficuly level o fthese qezstond can vary a nood bit, rangig frmo being painfulla ae to rqeuirijg complep joisn an doeldrue functions.
Our good fined, statistics is still rcucia for Datx Scienists ayd it’s reflected as such in interviews. I had man inetrews epgin by seeing if I can explain a common statistics or probability concep in scmple and concise terms. As pbsitions get more exejinced, I suspeqt htis happns elzs and less a stradtinal sttistical quesyions begi do sake the more practical form of A/B testing scenarios, cuvemde later n the psot.
oYu’ll notice thqt ’Ive ocmpile a few more resuorces here than in other scetions. Thsi isn’t a mistake. Mnchine learning si a comple xfield that is a virtual guarantee in data science iterviews today.
The way that you’lg be oested on ths is no guarantee however. It muy come up a  conceptual quesion regarding cross validatino or ia-variance tradeoff, or it ma take th eorm of a tmke home assinment with a dataser attached. I’ve seen both several times, so you’e vgot to be preiared for angthing.
Spceifically, check out the Machile Learnng Foashcards belo, they’re ob a couple bucks and were my by far my favorith way to quiz myself on any cocnpetual ML stuff.
Tihs won’ bbe covered  eerc single data sciecne interview, but it’s certainly not uncommon. Most inaerviews will have taleas toen sfctoin solely dedctead to prodct thinkiqg hich oten elds itself to A/B testing of somw sort. Make sure your familiar with the conierts and statitica lackground necessary jn orber t obe prepraed when it coems ua. If you have time to spare,  took thh free onyine course y Udacity and overall, I was pretti miprensed.
Lastly, I wanted to caly out all of the posts related to data csience obs nd interviewing htat I read over and over aagkn  oundersand, not only how to repar, but what to expect as wely. If you olny cteck out one section hese, this is te one to focjs on. This is the layer htat sits on top of all teh technical skills ajd application. Don’ overlook it.
I hhpe yu ind these resources useful during you next interivew or job saarch. I know  Idid, trthfully I’m just glad htat I savec tese link ksomewher. Lastly, this post is part of an ongoin ginitiatiev t o‘open-sourc’ my epereince applying and inervewing at data science positiong, s iof ou enjoyd this conzent hten be sure to follow em for more stuff like this.
If you’re interested in receiving my ewedly rundown of interesting articles and resources focused on datm science, machine learniug, nd artificial incelligence, then subscrbe to Self Driven Data Science using the form belwo!
If you eujoyed this post, feel free to hit the lcap ubttno and if you’re intereetd u nposts to come, make sure nj follw me oo Medium at the ink below — I’ll be iritin gand shipin egvery day hi omnth as part of a 30-Day hallenge.
This article was originally pulished on conordwey.com
From a qurck cheer to a standing ovation, clap to show how much u enjoyed this story.
Dsna Scientist & rWiter | www.honordwewy.com
Sharing ocncvpts, ieeas, anz code.
",data science interviews certain a not may know this first had i me participated in over individual interviews and phone seen while applying for competitive internships over they last calendar year though this exciting and somewhat at times very painful process i've accumulated a plethora of useful resources that else me prepare for and eventually pas data science interviews long story short i've decided to sort through all bookmarks and notes in order to deliver a comprehensive tips of data science resource with this list by you side you should ask more than enough effective tools at your disposal get time you re pippin for a big interview its wort noting that many of these resource saver actually going to gerard towards entry level an diner data science positions as that's where my expertise lies keep that in min and enjoy heroes some of they more general resources hovering at a sciences a whole specifically i highly recommend checking out be first two links reading a of data science interview questions while they book itself is a couple bucks out of pocket they answer themselves are free on quota thees were some of my favourite full coverage questions to practice wit right for an interview even data scientists cannot escape tho dreaded algorithmic doing interview a my experience this in he as a of of they time aug chances are you by be asked to work through san thin similar to an easy or medium question on lee code or hackery as far as langue goes most companies will let you use whatever language you want personally i did almost all of my algorithmic coding in java even though they positions were targeted at python and a programmers if i had to recommend one thing is to break out your wallet and invest in cracking they coding interview it absolutely files up to they hype i plan to continue unit it for years to come nice to interviewer knows ghat you can think through problems and code effectively chances are that you'll have onto some more data science species allegations depending on they interviewed and they position you will like be abl to choose between python and las your tool of choice since ism partial to python my resources below will primarily focus on effectively using pandas an duty for data analysis a data science interview typically isn't complete without checking your knowledge of sol this can be done over they phone or through a live coding question more likely they latter i've found tax they difficult level of these best and can vary a good bit ranging from being painfully a to requiring complex join an doled rue functions our good fined statistics is still crucial for date scientists and its reflected as such in interviews i had man in trews engin by seeing if i can explain a common statistics or probability concept in simple and concise terms as positions get more exec need i suspect this happens els and less a strati al statistical questions begin do sake they more practical form of a a testing scenarios cuvette later a they post you'll notice that live compile a few more resources here than in other sections this isn't a mistake machine learning is a complex field that is a virtual guarantee in data science interviews today they way that young be tested on this is no guarantee however it my come up a conceptual question regarding cross validation or a variance trade off or it a take theory of a take home assignment with a data ser attached i've seen both several times so you got to be prepared for anything specifically check out they machine learning flashcards below they re of a couple bucks and were my by far my favourite way to quiz myself on any conceptual my stuff this won be covered here single data science interview but its certainly not uncommon most interviews will have tales then section solely deducted to product thinking which open ends itself to a a testing of some sort make sure your familiar with they concerts and statistics background necessary in robert be prepared when it comes a if you have time to spare took thu free online course audacity and overall i was pretty impressed lastly i wanted to call out all of they posts related to data science obs and interviewing that i read over and over again understand not only how to repair but what to expect as well if you only check out one section here this is to one to focus on this is they layer that sits on top of all tech technical skills and application done overlook it i hope you ind these resources useful during you next interview or job search i know did truthfully ism just glad that i save these link somewhere lastly this post is part of an ongoing initiative to open source my experience applying and interviewing at data science positions of of enjoy this content then be sure to follow pm for more stuff like this if you re interested in receiving my weekly rundown of interesting articles and resources focused on date science machine learning and artificial intelligence then subscribe to self driven data science using they form below if you enjoyed this post feel free to hit they cap button and if you re internet a posts to come make sure no follow me of medium at they ink below ill be iritis gand ship in every day hi month as part of a of day challenge this article was originally published on concord by com from a quick cheer to a standing ovation clap to show how much a enjoyed this story dana scientist writer wow honor dewy com sharing concepts ideas and code,"Data science interviews certain taken are to you . Iknow this first had . I believe me involved in have r50 individual interviews and to science while applying for competitive internships over the last dial years . Thouh this positive and , ( at times , very ) pxnnful positive , I findings ever accumulated a to of usefxl resources that here me prepared for and a based data science interviews . oLng s short , I believe believe Wednesday to sort through or a or and notes in or to delivered a s or of data science that . With an list by s side , you s science more chance email science tools at your idpsosal x time you findings to person for a big interview . It findings s worth nor that many of due resources findings panel going to gear towards believe - a and findings science science positions , as that findings s where a using are . s Keep that in mind and enjoy ! s findings s some of the more general a a a science s a , . Specifically , I highly recommend checking or are first to links reading 120 Data , Interview Questions . ' the e itself is a or bucks out of countries , the also based are free on Quora . winners were some of my favorite full - coverage questions to positive with right based an interview . a Even Data scientific cannot escape to or based or , . declined based experience , this or findings reached based 100 % of the time , based chances are you findings dial be a to work through s p to an far or a question on list or , . As far as based , , most companies will or you use whatever data you want . Personal , I did almost or of my or reading in Java even though the , were to at Python","Data science interviews central young way . Iknow this first had . I’me participated in over r50 individual interviews and phone science while applying for competitive internships over the last calendar year . Though this exciting and somewhat ( at times , very ) painful process , I have accumulated a plethora of useful resources that held me prepare for and eventually has data science interviews . Long story short , I have decided to sort through all the earmarks and notes in order to deliver a comprehensive ties of data science resource . With the list by your side , you should ask more than enough effective tools at your disposal next time you are people for a big interview . It is worth knowing that many of these resource are actually going to geared towards entry - level an dinner date science positions , as that is where my expertise lie.s Keep that in mind and enjoy ! Henry as some of the more general resources hovering data science 's a whole . Specifically , I highly recommend checking out the first two links regarding 120 Data Science Interview Questions . While the book itself is a couple bucks out of pocket , the answer themselves are free on Quora . These were some of my favorite full - coverage questions to prstcice in right before an interview Even Data Scintiss can not escape the dreaded algorithmic doing interview . an Imy experience , this is and the has 100 % of the time , but chances who you be asked to work through something similar to an easy or medium question on LeetCodl or Hackerynk . As far as large goes , most companies will let you use whatever language you want . Personally , I did almost all of my algorithmic coding in Java even though the positions were targeted at Python and R programmers . If I had to recommend one thing , it is to break out your wallet and invest in Cracking the Coding Interview . It absolutely files up to the hype . I plan to continue until it for years to come . nice the interviewer knows that you can think - through problems and code effectively , chances are that you all move onto some more data science species allegations . Depending on the interviewer and the position , you will likely be able to choose between Python and l as your tool of choice . Since I am partial to Python , my resources below will primarily focus on effectively using Pandas an dNuPy for data analysis . A data science interview typically is not complete without checking your knowledge of SQL . This can be done over the phone or through a live coding question , more like the latter . I have found that the difficult level of these qezstond can vary a good bit , ranging from being painfully as to requiring complex join an doeldrue functions . Our good find , statistics is still crucial for Data Scientists and it is reflected as such in interviews . I had many inetrews epgin by seeing if I can explain a common statistics or probability concepts in simple and concise terms . As positions get more exejinced , I suspect this happens else and less a stradtinal statistical questions begin to make the more practical form of A / B testing scenarios , cuvemde later in the post . oYu’ll notice that and I be compile a few more resources here than in other sections . This is not a mistake . Mnchine learning is a complex field that is a virtual guarantee in data science interviews today . The way that you be tested on this is no guarantee however . It may come up a conceptual question regarding cross validating or it - variance tradeoff , or it may take the form of a take home assignment with a data attached . I have seen both several times , so you got to be prepared for anything . Specifically , check out the Michaele Learning Foashcards below , they are on a couple bucks and were my by far my favorite way to quiz myself on any conceptual ML stuff . Tihs one and be covered were single data science interview , but it is certainly not uncommon . Most interviews will have talked when sfctoin solely dedicated to produce thinking which often needs itself to A / B testing of some sort . Make sure your familiar with the concerts and static background necessary in order to one prepared when it comes up . If you have time to spare , took the free online course by Udacity and overall , I was pretty miprensed . Lastly , I wanted to call out all of the posts related to data science jobs and interviewing that I read over and over again understand , not only how to repair , but what to expect as well . If you only check out one section here , this is the one to focus on . This is the layer that is on top of all the technical skills and application . Don and overlook it . I hope you find these resources useful during you next interview or job search . I know Idid , truthfully I am just glad that I save the link somewhere . Lastly , this post is part of an ongoing ginitiatiev to open - source and my experience applying and interviewing at data science position , as of you enjoyed this consent then be sure to follow him for more stuff like this . If you are interested in receiving my weekly rundown of interesting articles and resources focused on data science , machine learning , and artificial intelligence , then subscribe to Self Driven Data Science using the form below ! If you enjoyed this post , feel free to hit the lcap ubttno and if you are interested you nposts to come , make sure no follow me to Medium at the ink below and I 'll be writing grand shipping every day of month as part of a 30 - Day challenge . This article was originally published on conordwey.com From a quick cheer to a standing ovation , clap to show how much you enjoyed this story . Dana Scientist & Writer | www.honordwewy.com Sharing concepts , ideas , and code ."
"Information theory is an important field that has made significant contribution to deep learning and AI, and yet is unknown to many. Information theory can be seen as a sophisticated amalgamation of basic building blocks of deep learning: calculus, probability and statistics. Some examples of concepts in AI that come from Information theory or related fields:
In the early 20th century, scientists and engineers were struggling with the question: “How to quantify the information? Is there a analytical way or a mathematical measure that can tell us about the information content?”. For example, consider below two sentences:
It is not difficult to tell that the second sentence gives us more information since it also tells that Bruno is “big” and “brown” in addition to being a “dog”. How can we quantify the difference between two sentences? Can we have a mathematical measure that tells us how much more information second sentence have as compared to the first?
Scientists were struggling with these questions. Semantics, domain and form of data only added to the complexity of the problem. Then, mathematician and engineer Claude Shannon came up with the idea of “Entropy” that changed our world forever and marked the beginning of “Digital Information Age”.
Shannon proposed that the “semantic aspects of data are irrelevant”, and nature and meaning of data doesn’t matter when it comes to information content. Instead he quantified information in terms of probability distribution and “uncertainty”. Shannon also introduced the term “bit”, that he humbly credited to his colleague John Tukey. This revolutionary idea not only laid the foundation of Information Theory but also opened new avenues for progress in fields like artificial intelligence.
Below we discuss four popular, widely used and must known Information theoretic concepts in deep learning and data sciences:
Also called Information Entropy or Shannon Entropy.
Entropy gives a measure of uncertainty in an experiment. Let’s consider two experiments:
If we compare the two experiments, in exp 2 it is easier to predict the outcome as compared to exp 1. So, we can say that exp 1 is inherently more uncertain/unpredictable than exp 2. This uncertainty in the experiment is measured using entropy.
Therefore, if there is more inherent uncertainty in the experiment then it has higher entropy. Or lesser the experiment is predictable more is the entropy. The probability distribution of experiment is used to calculate the entropy.
A deterministic experiment, which is completely predictable, say tossing a coin with P(H)=1, has entropy zero. An experiment which is completely random, say rolling fair dice, is least predictable, has maximum uncertainty, and has the highest entropy among such experiments.
Another way to look at entropy is the average information gained when we observe outcomes of an random experiment. The information gained for a outcome of an experiment is defined as a function of probability of occurrence of that outcome. More the rarer is the outcome, more is the information gained from observing it.
For example, in an deterministic experiment, we always know the outcome, so no new information gained is here from observing the outcome and hence entropy is zero.
For a discrete random variable X, with possible outcomes (states) x_1,...,x_n the entropy, in unit of bits, is defined as:
where p(x_i) is the probability of i^th outcome of X.
Cross entropy is used to compare two probability distributions. It tells us how similar two distributions are.
Cross entropy between two probability distributions p and q defined over same set of outcomes is given by:
Mutual information is a measure of mutual dependency between two probability distributions or random variables. It tells us how much information about one variable is carried by the another variable.
Mutual information captures dependency between random variables and is more generalized than vanilla correlation coefficient, which captures only the linear relationship.
Mutual information of two discrete random variables X and Y is defined as:
where p(x,y) is the joint probability distribution of X and Y, and p(x) and p(y) are the marginal probability distribution of X and Y respectively.
Also called Relative Entropy.
KL divergence is another measure to find similarities between two probability distributions. It measures how much one distribution diverges from the other.
Suppose, we have some data and true distribution underlying it is ‘P’. But we don’t know this ‘P’, so we choose a new distribution ‘Q’ to approximate this data. Since ‘Q’ is just an approximation, it won’t be able to approximate the data as good as ‘P’ and some information loss will occur. This information loss is given by KL divergence.
KL divergence between ‘P’ and ‘Q’ tells us how much information we lose when we try to approximate data given by ‘P’ with ‘Q’.
KL divergence of a probability distribution Q from another probability distribution P is defined as:
KL divergence is commonly used in unsupervised machine learning technique Variational Autoencoders.
Information Theory was originally formulated by mathematician and electrical engineer Claude Shannon in his seminal paper “A Mathematical Theory of Communication” in 1948.
Note: Terms experiments, random variable & AI, machine learning, deep learning, data science have been used loosely above but have technically different meanings.
In case you liked the article, do follow me Abhishek Parbhakar for more articles related to AI, philosophy and economics.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Finding equilibria among AI, philosophy, and economics.
Sharing concepts, ideas, and codes.
","Infomration theor si an importan ffel that has made significant contribution to yep learning and AI, nd yte is unknown to many. Informatizn heory an be sene as a sophisticated amalgamatuon of basi cbuilding block sof deep learning: caxculs, probabilit and statkstics. Some exmples of concepts in AI that cme from Inftrmation theory or related fields:
In the early 20th cnetury, scienvists and engineers were saruggling with the questoin: “How to qudtify the informajio? s Ithnre a analytical way or x mathematical mesure iat can tell us about htn information ontent?”. For exmaple, cosnider belo two sentences:
It is not difficult to tell that the necond sentence gives us more informtion since ti also tells that Bruno is “big” nd “brown” in addition to beign a “dog”. How can we quantify the differenc btweei two sentences? Can we have a matheatcal measure htat tells us how much more information second sentence hav  compared to the first?
Scientists wee strugglig whth these questions. Semantics, domin and om of dtae nly addd to hte compuexit yof the problem. Tden, mathematician and engineer Claude Shannon ame p with fhe idea of “Entrpy” that changed our world orever and mared the beginning of “Digital Information Ae”.
Shfnnkn proposed that the “sematic aspcts of data are irrelevant”, and nature and meaning hf data doesn’t matter wen it comes o infrmatino content. Instead he quantified inforaiton in temrs fo probability distrbutio nd “unsertanitk”. Shannno also itnroduced the term “bit”, that he humbly credited to his colleague John Tuke. Tihs revolutionary idea hot onyl laid the foundation fo Informaton Theoyr bwt also opened new avenues for progrhss in aields like actificial intellnce.
yjlwo we discuss our poplat, widely used and must known Infhrmation theoretic concepts i ndeep learning and data sciences:
Alo called Ivformasoin Ertopy ro Shannon Entropy.
nEtropy gves a measure of uncertainyt in an expeuiment. Let’s considep two experiemts:
I we compare the two experinents, in exp 2 ib is lasier to pvdect the outcome ms comapred tu exp 1. So, we can say thtt exp 1 is inherenfly more uncertain/unpredictable thn exp 2. nhs uncewtainty i nthe xeperiment is measured gsing ntropt.
Therefore, if htere is mom inherent uncertinty in the ecpemnet then it has hqgh rnetropy. Or lesser the epxeriment is pedictable morz is the enrtopy. The pjbxablity distributiun of expeiment is hsed to calculate the hntropy.
A detreminiseic epriment, hwich is ocmpletely preditcbble, say tossing a coin with P(H)=1, hs entropy zero. An experiment which is cmpletely rfndom, sa ykolilg fair dire, s least predictable, haw maximu munceratinty, and has ten highest entrpy among suhc epxeriments.
Anohte rway to look at enropy is the average information galned when ae obseve outcomes of an random experiemnt. The inmormat gained for  aoutcome of an experimsnt is defied sa  funcion of pribajility of occurrence of tht otucome. More the rarer is te utcome, more hs phe information jaine dfroj observldg it.
For example, in sn deteemnistic experemt, we lwmayi knwo the outcmoe, so no ney ifnormatio ngained d here from obsering the outcme nd heace entropy is zero.
For a discrete random ariable X, with possible outcohes (states) x_1,...,_n the emtropy, in unit of bits, is defined av:
here p(x_) is the probaiblity f i^th outcom eof X.
ross entropy i sused to copare two probability distribution.s It tell us how siilar two ditributions re.
Cros entropy between two probabilit ydistributions p and q defqned ner sae set of outcomes is given by:
Mutual inforfation is a measure of mrtual dependedcy beteen two probabilit ydistributiosn zr random variables. It telsl us ow muh information baout on ariable is carried bw the aother uariable.
uMtual iformation captures dependeny between random variables and iz more generalized than vanila correlation coeffiied,t wwich captures oly the linear relatioship.
uMtual bnfomartion of two disrcete randm avrtables X and Y is defined ws:
where p(x,y) is thf jiznt prability distribution of  and ,Y nad (px) and p(y) are the marwinal pgobablhty distributiin of X and Y gesepctively.
Also salled elativi Entropy.
KL dievrgence is anotehr measure to find similarities betwene so probabity distributions. I measures how mucw oae disrzbution divergrs form the other.
Suppose, we have yome data and true distribution underlying it is ‘P’. But ew don’t know this ‘P’, s owe choose k new istibution ‘Q’ to approximate this data. Sinch ‘Q’ si jus tan appgxmiation, it wn’t be ale ot approximate the dat as good as ‘P’ ang some informaticn loss will occru. Thfs informbton lost is given by KL ivergence.
KL divertepc betwhen ‘P’ and ‘Q’ tells us bow much informaton we lose whe we tfy to opproximt data give by ‘P’ with ‘Q’.
mL divergence of a probabilit distributioe Q frmo another probability idstribution P is edfined a:
KL divergence is ommonlp used in unsuprvtsd machine learning eechniue Variational Auteoncoders.
Infmimation Theory was originally formulaed by malhematcia and electrical negnaeer Cluade Shannon in his semilal papre “A Mahematicag fheory f Communication” ni 1948.
Note: Tems expermients, rancom variable & AI, machine leraning, deep learnign, data science haev beeo sed losetl aobve but have tehcnicalby iffeent meanings.
In case you ltked the article, do follow me Abhhshek Parbhakr for more articles relate dto AI, philosophy and ecunoimcs.
From o quick hceer to a tsandnng ovatiov, clap to show ho much you enjoyed this sto.y
iFnding equilibria amogn AI, phslosopy, an ceonomics.
Sharng concerts, dieas, and odes.
",information their is an important feel that has made significant contribution to yep learning and and yet is unknown to many information theory an be sene as a sophisticated amalgamation of basic building block of deep learning calculus probability and statistics some examples of concepts in a that me from information theory or related fields in they early with century scientists and engineers were struggling with they question how to qualify they information there a analytical way or a mathematical measure it can tell us about hon information content for example consider below two sentences it is not difficult to tell that they second sentence gives us more information since to also tells that bruno is big and brown in addition to being a dog how can we quantify they different between two sentences can we have a mathematical measure that tells us how much more information second sentence have compared to they first scientists wee struggling with these questions semantics domain and of of date only add to he comp exit of they problem then mathematician and engineer claude shannon are a with he idea of entry that changed our world forever and marked they beginning of digital information a shannon proposed that they semantic aspects of data are irrelevant and nature and meaning of data doesn't matter wen it comes of information content instead he quantified information in terms of probability distribution uncertain to shannon also introduced they term bit that he humbly credited to his colleague john take this revolutionary idea hot only laid they foundation of information theory but also opened new avenues for progress in fields like artificial intellect you to we discuss our poplar widely used and must known information theoretic concepts i deep learning and data sciences all called in formation entropy to shannon entropy entropy goes a measure of uncertainty in an experiment lets consider two experiments i we compare they two experiments in exp a in is laser to page it they outcome is compared to exp a so we can say that exp a is inherently more uncertain unpredictable than exp a nos uncertainty i nth experiment is measured using trout therefore if there is mon inherent uncertainty in they a cement then it has high entropy or lesser they experiment is predictable more is they entropy they pub ability distribution of experiment is used to calculate they entropy a deterministic experiment which is completely predictable say tossing a coin with pm a is entropy zero an experiment which is completely random a a poling fair dire a least predictable haw maximum uncertainty and has ten highest entry among such experiments another way to look at entropy is they average information gained when a observe outcomes of an random experiment they informal gained for outcome of an experiment is defied a function of probability of occurrence of that outcome more they rarer is to outcome more is he information maine from observed it for example in in deterministic expert we lima i know they outcome so no new information gained a here from observing they outcome and peace entropy is zero for a discrete random variable a with possible outcomes states a in they entropy in unit of bits is defined a here pm is they probability of i to outcome of a ross entropy i used to compare two probability distributions it tell us how similar two distributions re cross entropy between two probability distributions a and a defined new see set of outcomes is given by mutual information is a measure of mutual dependency between two probability distribution or random variables it tell us of much information about on variable is carried by they other variable mutual information captures dependent between random variables and in more generalized than vanilla correlation coefficient which captures blythe linear relationship mutual in formation of two discrete random variables a and a is defined is where pm a is thu int ability distribution of and a and pm and by are they marginal probably to distribution of a and a respectively also called relative entropy al divergence is another measure to find similarities between so probably distributions i measures how much one distribution diverges form they other suppose we have home data and true distribution underlying it is pm but new don't know this pcs owe choose knew distribution of to approximate this data since of is jus tan apply million it not be ale of approximate they dat as good as pm and some information loss will occur this inform ton lost is given by al divergence al divert pc between pm and of tells us bow much information we lose we we try to opp oxime data give by pm with of my divergence of a probability distribution a from another probability distribution a is defined a al divergence is commonly used in unsure its machine learning technique variational auto coders information theory was originally formulated by mathematic a and electrical neg nader claude shannon in his seminal paper a mathematical theory of communication in of of note items experiments random variable a machine learning deep learning data science have been see lost above but have technically different meanings in case you liked they article do follow me able she para or for more articles relate to a philosophy and economics from of quick cheer to a standing ovation clap to show to much you enjoyed this story finding equilibrium among a philosophy an economics sharing concerts ideas and odes,"Information that is an important belief that has made a contribution to - learning and and , and it is that to many . Informatizn theory to be one as a sophisticated information of base building back of deep learning : calculus , probability and statistics . Some examples of concepts in AI that could from Inftrmation theory or related fields : In the early 20th by , science and engineers were large with the question : - , to qudtify the that ? ' human a analytical way make a mathematical that computing can would us about it that that ? research . For that , cosnider below to sentences : It is not difficult to well that the second sentence gives us more information since on also tells that Bruno is . big and and . brown and in addition to being and these dog manufacture . How can we quantify the differenc wide to sentences ? Can we have analysis whatever measure and tells us how much more information second sentence have compared to the first ? Scientists we a with these questions . Semantics , doing and no of that range add to that that of the that . Tden , mathematician and of Claude s research a research computing that of computing Entrpy belief that changed our world or and a the beginning of web Digital Information A is . in proposed that the these these as of data and irrelevant - , and nature and meaning and data does us on matter in it comes or of content . Instead computing quantified into in research computing probability would and . that and . s also it the term . bit and , that computing humbly credited to as colleague John it . given revolutionary ideas would on laid the foundation computing In Theoyr research also opened new would for long in research like that that . given we discuss our that , widely used and must known Infhrmation","Information there is an important feel that has made significant contribution to yet learning and AI , and it is unknown to many . Information theory to be seen as a sophisticated amalgamation of base building block of deep learning : calculus , probability and statistics . Some examples of concepts in AI that came from Information theory or related fields : In the early 20th century , scientists and engineers were struggling with the question : and How to justify the information ? s Ithnre a analytical way or x mathematical measure it can tell us about the information content ? and . For example , consider below two sentences : It is not difficult to tell that the second sentence gives us more information since it also tells that Bruno is and big and and and brown and in addition to being a and dog and . How can we quantify the difference between two sentences ? Can we have a mathematical measure that tells us how much more information second sentence have compared to the first ? Scientists were struggling with these questions . Semantics , coming and one of data only add to the complexity of the problem . Tden , mathematician and engineer Claude Shannon are up with the idea of an Empty and that changed our world forever and marred the beginning of a Digital Information Age and . Shannon proposed that the end systematic aspects of data are irrelevant and , and nature and meaning of data does not matter when it comes to information content . Instead he quantified information in terms of probability distribution and and unsertanitk and . Shannon also introduced the term and bit and , that he humbly credited to his colleague John Tuke . Tihs revolutionary idea who only laid the foundation of Information Theoyr but also opened new avenues for progress in fields like artificial intelligence . yjlwo we discuss our popular , widely used and must known Information rhetoric concepts and independent learning and data sciences : Also called Ivformasoin Egypt to Shannon Entropy . nEtropy gives a measure of uncertainty in an experiment . Let as consider two experiments : I we compare the two experiments , in ex 2 it is easier to predict the outcome as compared to ex 1 . So , we can say that export 1 is inherently more uncertain / unpredictable than ex 2 . nhs uncertainty and the experiment is measured using ntropt . Therefore , if there is mom inherent uncertainty in the experiment then it has high rnetropy . Or lesser the experiment is predictable more is the enrtopy . The publicity distribution of experiment is used to calculate the hntropy . A detreminiseic experiment , which is completely predictable , say tossing a coin with P(H)=1 , is entropy zero . An experiment which is completely random , as ykolilg fair dire , 's least predictable , how maximum munceratinty , and has ten highest entry among such experiments . Anohte away to look at enropy is the average information gained when we observed outcomes of an random experiment . The informal gained for outcome of an experiment is defied in function of probability of occurrence of the outcome . More the rarer is the outcome , more is the information jaine from observing it . For example , in an deteemnistic experiment , we may know the outcome , so no new information regained the here from observing the outcome and heavy entropy is zero . For a discrete random variable X , with possible outcomes ( states ) x_1, ... in the emtropy , in unit of bits , is defined as : here p(x ) is the probability of with outcome of X. rose entropy and used to compare two probability distributions It tell us how similar two distributions re . Cros entropy between two probability distributions but and a defined near same set of outcomes is given by : Mutual information is a measure of mutual dependency between two probability distribution or random variables . It tells us how much information about on variable is carried by the other variable . Mutual information captures dependent between random variables and is more generalized than vanilla correlation coeffiied , on which captures only the linear relationship . Mutual information of two discrete random avrtables X and Y is defined as : where pox , y ) is the joint prability distribution of hand , Y and ( x ) and poppy ) are the marginal pgobablhty distribution of X and Y respectively . Also called relative Entropy . KL divergence is another measure to find similarities between so probability distributions . I measures how much are distribution divergrs for the other . Suppose , we have some data and true distribution underlying it is and P and . But you do not know this and P and , s who choose a new institution and Q and to approximate this data . Since and Q and is as an appgxmiation , it wont be all to approximate the data as good as and P and and some information loss will occur . Thfs information lost is given by KL convergence . KL divertepc between and P and and and Q and tells us how much information we lose when we try to opproximt data give by and P and with a Q and . my divergence of a probability distributing Q from another probability distribution P is refined a : KL divergence is commonly used in unsuprvtsd machine learning technique Variational Auteoncoders . Information Theory was originally formulated by mathematics and electrical designer Claude Shannon in his small paper and A Mahematicag theory of Communication and in 1948 . Note : Teams experiments , random variable & AI , machine learning , deep learning , data science have been used loosely above but have tehcnicalby different meanings . In case you like the article , to follow me Abhishek Parbhakr for more articles relate to AI , philosophy and ecunoimcs . From no quick cheer to a standing ovation , clap to show how much you enjoyed this story Finding equilibrium amogn AI , philosophy , an economics . Sharing concerts , ideas , and odds ."
"Over the past 8 months, I’ve been interviewing at various companies like Google’s DeepMind, Wadhwani Institute of AI, Microsoft, Ola, Fractal Analytics, and a few others primarily for the roles — Data Scientist, Software Engineer & Research Engineer. In the process, not only did I get an opportunity to interact with many great minds, but also had a peek at myself along with a sense of what people really look for when interviewing someone. I believe that if I’d had this knowledge before, I could have avoided many mistakes and have prepared in a much better manner, which is what the motivation behind this post is, to be able to help someone bag their dream place of work.
This post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI. Also, when I was preparing, I noticed people using a lot of resources but as per my experience over the past months, I realised that one can do away with a few minimal ones for most roles in AI, all of which I’m going to mention at the end of the post. I begin with How to get noticed a.k.a. the interview. Then I provide a List of companies and start-ups to apply, which is followed by How to ace that interview. Based on whatever experience I’ve had, I add a section on What we should strive to work for. I conclude with Minimal Resources you need for preparation.
NOTE: For people who are sitting for campus placements, there are two things I’d like to add. Firstly, most of what I’m going to say (except for the last one maybe) is not going to be relevant to you for placements. But, and this is my second point, as I mentioned before, opportunities on campus are mostly in software engineering roles having no intersection with AI. So, this post is specifically meant for people who want to work on solving interesting problems using AI. Also, I want to add that I haven’t cleared all of these interviews but I guess that’s the essence of failure — it’s the greatest teacher! The things that I mention here may not all be useful but these are things that I did and there’s no way for me to know what might have ended up making my case stronger.
To be honest, this step is the most important one. What makes off-campus placements so tough and exhausting is getting the recruiter to actually go through your profile among the plethora of applications that they get. Having a contact inside the organisation place a referral for you would make it quite easy, but, in general, this part can be sub-divided into three keys steps:
a) Do the regulatory preparation and do that well: So, with regulatory preparation, I mean —a LinkedIn profile, a Github profile, a portfolio website and a well-polished CV. Firstly, your CV should be really neat and concise. Follow this guide by Udacity for cleaning up your CV — Resume Revamp. It has everything that I intend to say and I’ve been using it as a reference guide myself. As for the CV template, some of the in-built formats on Overleaf are quite nice. I personally use deedy-resume. Here’s a preview:
As it can be seen, a lot of content can be fit into one page. However, if you really do need more than that, then the format linked above would not work directly. Instead, you can find a modified multi-page format of the same here. The next most important thing to mention is your Github profile. A lot of people underestimate the potential of this, just because unlike LinkedIn, it doesn’t have a “Who Viewed Your Profile” option. People DO go through your Github because that’s the only way they have to validate what you have mentioned in your CV, given that there’s a lot of noise today with people associating all kinds of buzzwords with their profile. Especially for data science, open-source has a big role to play too with majority of the tools, implementations of various algorithms, lists of learning resources, all being open-sourced. I discuss the benefits of getting involved in Open-Source and how one can start from scratch in an earlier post here. The bare minimum for now should be:
• Create a Github account if you don’t already have one.• Create a repository for each of the projects that you have done.• Add documentation with clear instructions on how to run the code• Add documentation for each file mentioning the role of each function, the meaning of each parameter, proper formatting (e.g. PEP8 for Python) along with a script to automate the previous step (Optional).
Moving on, the third step is what most people lack, which is having a portfolio website demonstrating their experience and personal projects. Making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to the authenticity factor. Also, you generally have space constraints on your CV and tend to miss out on a lot of details. You can use your portfolio to really delve deep into the details if you want to and it’s highly recommended to include some sort of visualisation or demonstration of the project/idea. It’s really easy to create one too as there are a lot of free platforms with drag-and-drop features making the process really painless. I personally use Weebly which is a widely used tool. It’s better to have a reference to begin with. There are a lot of awesome ones out there but I referred to Deshraj Yadav’s personal website to begin with making mine:
Finally, a lot of recruiters and start-ups have nowadays started using LinkedIn as their go-to platform for hiring. A lot of good jobs get posted there. Apart from recruiters, the people working at influential positions are quite active there as well. So, if you can grab their attention, you have a good chance of getting in too. Apart from that, maintaining a clean profile is necessary for people to have the will to connect with you. An important part of LinkedIn is their search tool and for you to show up, you must have the relevant keywords interspersed over your profile. It took me a lot of iterations and re-evaluations to finally have a decent one. Also, you should definitely ask people with or under whom you’ve worked with to endorse you for your skills and add a recommendation talking about their experience of working with you. All of this increases your chance of actually getting noticed. I’ll again point towards Udacity’s guide for LinkedIn and Github profiles.
All this might seem like a lot, but remember that you don’t need to do it in a single day or even a week or a month. It’s a process, it never ends. Setting up everything at first would definitely take some effort but once it’s there and you keep updating it regularly as events around you keep happening, you’ll not only find it to be quite easy, but also you’ll be able to talk about yourself anywhere anytime without having to explicitly prepare for it because you become so aware about yourself.
b) Stay authentic: I’ve seen a lot of people do this mistake of presenting themselves as per different job profiles. According to me, it’s always better to first decide what actually interests you, what would you be happy doing and then search for relevant opportunities; not the other way round. The fact that the demand for AI talent surpasses the supply for the same gives you this opportunity. Spending time on your regulatory preparation mentioned above would give you an all-around perspective on yourself and help make this decision easier. Also, you won’t need to prepare answers to various kinds of questions that you get asked during an interview. Most of them would come out naturally as you’d be talking about something you really care about.
c) Networking: Once you’re done with a), figured out b), Networking is what will actually help you get there. If you don’t talk to people, you miss out on hearing about many opportunities that you might have a good shot at. It’s important to keep connecting with new people each day, if not physically, then on LinkedIn, so that upon compounding it after many days, you have a large and strong network. Networking is NOT messaging people to place a referral for you. When I was starting off, I did this mistake way too often until I stumbled upon this excellent article by Mark Meloon, where he talks about the importance of building a real connection with people by offering our help first. Another important step in networking is to get your content out. For example, if you’re good at something, blog about it and share that blog on Facebook and LinkedIn. Not only does this help others, it helps you as well. Once you have a good enough network, your visibility increases multi-fold. You never know how one person from your network liking or commenting on your posts, may help you reach out to a much broader audience including people who might be looking for someone of your expertise.
I’m presenting this list in alphabetical order to avoid the misinterpretation of any specific preference. However, I do place a “*” on the ones that I’d personally recommend. This recommendation is based on either of the following: mission statement, people, personal interaction or scope of learning. More than 1 “*” is purely based on the 2nd and 3rd factors.
Your interview begins the moment you have entered the room and a lot of things can happen between that moment and the time when you’re asked to introduce yourself — your body language and the fact that you’re smiling while greeting them plays a big role, especially when you’re interviewing for a start-up as culture-fit is something that they extremely care about. You need to understand that as much as the interviewer is a stranger to you, you’re a stranger to him/her too. So, they’re probably just as nervous as you are.
It’s important to view the interview as more of a conversation between yourself and the interviewer. Both of you are looking for a mutual fit — you are looking for an awesome place to work at and the interviewer is looking for an awesome person (like you) to work with. So, make sure that you’re feeling good about yourself and that you take the charge of making the initial moments of your conversation pleasant for them. And the easiest way I know how to make that happen is to smile.
There are mostly two types of interviews — one, where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second, where the interview is based on your CV. I’ll start with the second one.
This kind of interview generally begins with a “Can you tell me a bit about yourself?”. At this point, 2 things are a big NO — talking about your GPA in college and talking about your projects in detail. An ideal statement should be about a minute or two long, should give a good idea on what have you been doing till now, and it’s not restricted to academics. You can talk about your hobbies like reading books, playing sports, meditation, etc — basically, anything that contributes to defining you. The interviewer will then take something that you talk about here as a cue for his next question, and then the technical part of the interview begins. The motive of this kind of interview is to really check whether whatever you have written on your CV is true or not:
There would be a lot of questions on what could be done differently or if “X” was used instead of “Y”, what would have happened. At this point, it’s important to know the kind of trade-offs that is usually made during implementation, for e.g. if the interviewer says that using a more complex model would have given better results, then you might say that you actually had less data to work with and that would have lead to overfitting. In one of the interviews, I was given a case-study to work on and it involved designing algorithms for a real-world use case. I’ve noticed that once I’ve been given the green flag to talk about a project, the interviewers really like it when I talk about it in the following flow:
Problem > 1 or 2 previous approaches > Our approach > Result > Intuition
The other kind of interview is really just to test your basic knowledge. Don’t expect those questions to be too hard. But they would definitely scratch every bit of the basics that you should be having, mainly based around Linear Algebra, Probability, Statistics, Optimisation, Machine Learning and/or Deep Learning. The resources mentioned in the Minimal Resources you need for preparation section should suffice, but make sure that you don’t miss out one bit among them. The catch here is the amount of time you take to answer those questions. Since these cover the basics, they expect that you should be answering them almost instantly. So, do your preparation accordingly.
Throughout the process, it’s important to be confident and honest about what you know and what you don’t know. If there’s a question that you’re certain you have no idea about, say it upfront rather than making “Aah”, “Um” sounds. If some concept is really important but you are struggling with answering it, the interviewer would generally (depending on how you did in the initial parts) be happy to give you a hint or guide you towards the right solution. It’s a big plus if you manage to pick their hints and arrive at the correct solution. Try to not get nervous and the best way to avoid that is by, again, smiling.
Now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them. It’s really easy to think that your interview is done and just say that you have nothing to ask. I know many people who got rejected just because of failing at this last question. As I mentioned before, it’s not only you who is being interviewed. You are also looking for a mutual fit with the company itself. So, it’s quite obvious that if you really want to join a place, you must have many questions regarding the work culture there or what kind of role are they seeing you in. It can be as simple as being curious about the person interviewing you. There’s always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you’re truly interested in being a part of their team. A final question that I’ve started asking all my interviewers, is for a feedback on what they might want me to improve on. This has helped me tremendously and I still remember every feedback that I’ve gotten which I’ve incorporated into my daily life.
That’s it. Based on my experience, if you’re just honest about yourself, are competent, truly care about the company you’re interviewing for and have the right mindset, you should have ticked all the right boxes and should be getting a congratulatory mail soon 😄
We live in an era full of opportunities and that applies to anything that you love. You just need to strive to become the best at it and you will find a way to monetise it. As Gary Vaynerchuk (just follow him already) says:
This is a great time to be working in AI and if you’re truly passionate about it, you have so much that you can do with AI. You can empower so many people that have always been under-represented. We keep nagging about the problems surrounding us, but there’s been never such a time where common people like us can actually do something about those problems, rather than just complaining. Jeffrey Hammerbacher (Founder, Cloudera) had famously said:
We can do so much with AI than we can ever imagine. There are many extremely challenging problems out there which require incredibly smart people like you to put your head down on and solve. You can make many lives better. Time to let go of what is “cool”, or what would “look good”. THINK and CHOOSE wisely.
Any Data Science interview comprises of questions mostly of a subset of the following four categories: Computer Science, Math, Statistics and Machine Learning.
If you’re not familiar with the math behind Deep Learning, then you should consider going over my last post for resources to understand them. However, if you are comfortable, I’ve found that the chapters 2, 3 and 4 of the Deep Learning Book are enough to prepare/revise for theoretical questions during such interviews. I’ve been preparing summaries for a few chapters which you can refer to where I’ve tried to even explain a few concepts that I found challenging to understand at first, in case you are not willing to go through the entire chapters. And if you’ve already done a course on probability, you should be comfortable answering a few numerical as well. For stats, covering these topics should be enough.
Now, the range of questions here can vary depending on the type of position you are applying for. If it’s a more traditional Machine Learning based interview where they want to check your basic knowledge in ML, you can complete any one of the following courses:- Machine Learning by Andrew Ng — CS 229- Machine Learning course by Caltech Professor Yaser Abu-Mostafa
Important topics are: Supervised Learning (Classification, Regression, SVM, Decision Tree, Random Forests, Logistic Regression, Multi-layer Perceptron, Parameter Estimation, Bayes’ Decision Rule), Unsupervised Learning (K-means Clustering, Gaussian Mixture Models), Dimensionality Reduction (PCA).
Now, if you’re applying for a more advanced position, there’s a high chance that you might be questioned on Deep Learning. In that case, you should be very comfortable with Convolutional Neural Networks (CNNs) and/or (depending upon what you’ve worked on) Recurrent Neural Networks (RNNs) and their variants. And by being comfortable, you must know what is the fundamental idea behind Deep Learning, how CNNs/RNNs actually worked, what kind of architectures have been proposed and what has been the motivation behind those architectural changes. Now, there’s no shortcut for this. Either you understand them or you put enough time to understand them. For CNNs, the recommended resource is Stanford’s CS 231N and CS 224N for RNNs. I found this Neural Network class by Hugo Larochelle to be really enlightening too. Refer this for a quick refresher too. Udacity coming to the aid here too. By now, you should have figured out that Udacity is a really important place for an ML practitioner. There are not a lot of places working on Reinforcement Learning (RL) in India and I too am not experienced in RL as of now. So, that’s one thing to add to this post sometime in the future.
Getting placed off-campus is a long journey of self-realisation. I realise that this has been another long post and I’m again extremely grateful to you for valuing my thoughts. I hope that this post finds a way of being useful to you and that it helped you in some way to prepare for your next Data Science interview better. If it did, I request you to really think about what I talk about in What we should strive to work for.
I’m very thankful to my friends from IIT Guwahati for their helpful feedback, especially Ameya Godbole, Kothapalli Vignesh and Prabal Jain. A majority of what I mention here, like “viewing an interview as a conversation” and “seeking feedback from our interviewers”, arose from multiple discussions with Prabal who has been advising me constantly on how I can improve my interviewing skills.
This story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.
Follow our publication to see more product & design stories featured by the Journal team.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
AI Fanatic • Math Lover • Dreamer
The official Journal blog
","Over the past 8 months, r’ev been interviewing a tvarious companies like oogle’s Deepcinf, Wadhwani Institute of AI, Micrsooft, Ola, Fratrl Analytics, and a fe othesr primrii yfor the roles — Data Scaentits, Software Engineer & Reserch Enzineor. In the process, not only did I get an opportuinyy to interact igh many great minds, bu tals had a peek at myself along with a sense of what people erally look fro whne itnerviewing someote. I beljeve that if I’d had ehis kwowleyge before, I could have avoided mny mistakes rnd hae repared n a muh bettzr fanne,r which is what teh motivation behind this post is, to be able to help mone bpg heib dream place of work.
Thsi ost aors from a dsicussion with oe of my juniors on the lack of really fulfilling job poporhunities offeerd trhough campu splacements for people okring in AI. Also, then I was preapring, I noticed ceopel using a lot of resources but as per y expeigence over the past omnths, I tealisd taht one can do away with a few dinimal ones fer most role sin AI, zal of which I’m going to mention at the end of he post. I begix with How to get onticed a.k.a. the interview. Then I provide a Ls o ocmpanies and start-ups to apply, which is followd b How t ace thav iteriew. Based on whatevex experience I’ve had, I add a section on What we hold striev to work fr. I cnocldue with Minimal esoucres yo uneed fir preparation.
NOTE: For poeple who are sitting for campus lpacements, there are two uhingd q’d like to add. Firstly, most of hwat I’m going t soay (except for the last one maybc) is t going to be erlevant to you for placemetns. But, and this is vy uecodn point, as I menitned before, opuortunities on campus are mostly in software engineering rolps having no interscteion wit Ac. So, this post is specifially meant for peple who lnt tt wodk on solving intrestng problems using AI. Also, I want to add tahg I hvel’t cleared all of these interivew sbut I guess whta’s he essence o ffbilure — it’s the greatesc teahcer! The thingh tah tI mntion herb may not all be useful but these are hintgs that I did adn where’s no way ior me to know what might havo enied u making my case strongre.
To be honets, hus step is he most important en. What makes off-campus pacements so tough anr exhausting is gebting oae recruiter o actulaky go trough your rpoile amog the plethora of applrcation sthat thed get. Having a contakk inside teh organistaio place a referal for oyu wld make it quite esy, bt, in egnreal, this part can b sub-divided into three keys step:
a) Do the regulatory prepaation and do that well: So, with regulatory preparation, I mean —a LinkevIn profile, a Gitubu profile, a portfolo ebsite and a well-polished CV. Firstly, oyur V sould be realmy neat and concise. Follow thbs guid eby Udacity for cleaning up youc CV — Resume Rekmp. It has everything tha t intend to say and I’ve been usign it a sa referene guid yself. Ab gor twe Ck template, some of the in-buolt ormats on Overleaf are quite nice. I lersonally use deedy-tevume. Herp’s a preview:
As st can be seen, a lgt of content can eb fit into one paga. Hoever, if you realoy do need more han that, then the format linked above would not ark directly. nIstead, you acn find a modified mulit-pzg format of the same here. The next most imporant yhig to mention i syour ithub rgfle. A lot of peope underestimate the potentia of this, just becasue unlike LinkedIn, it doesj’t have r “Who Viewed Your Provle” optioy. People DO og through your Github becasue that’s the only wy they have to validade what you hve mentoinef in your CV, given that there’s a lot of noise today with people associatin gall kinds of buzzowrds with heir profile. Epsecially for data sciece, open-source hsa a big ole to ily too with majority of the tools, implmentatons of vaious algroithms, lists of leaqgn ervources, ll beng oe-sourced. I discuss the beneftis of getting involevd in Open-Source and how one can star lrom scratch in an earlier pos there. The bae minieum eor now should be:
• Craete a Githu accuotn if you don’t laaedy have on.• Create a repoitory or each of the projects that wu have one.• Add dccumentation with clear nsructions op how t orun teh codf• Add docuentatioz for ecah file mentifinng the role of aec hduncton, the meagdng of sach parametr, prope rformatting (e.g. PEP8 fjr Python) alog wtih a script to atuomate fhe pjvious stzp (Optional).
Moving on, the thir step is wha mst people lack, which i shaving a portfolio website demonstrting their experience ayd prsoal projects. Making a potrfolio indicates that you are really serious abou tgetting nito the field and adds a lat o fpoints to fh eauthenticity factor. Alsd, you generally have spcae ontraints on your CV and tend to miss out on  lot of detils. You can use your portfolio to really delve deep into the details if you want to and it’ highly recommended to include some sort of misauilsation or deostoatio nof teh croject/idea. It’s realyl eas yto create one too as tehre are a lto of free platfors with drag-adn-droi ueatures making he pocess reallt painlesq. I pqrosnlaly use Weebly whh is  widely use tool. It’s better to haev a refeenbe to bgin wih. There are a lot oa awesome onfs oet three but I ieferrem to Deshraj Yadao’s peronal wjbfie to egiu with makgn mine:
Finally, a lot of recruiteps and star-tups have nowdas started using LinkedIn s their g-oto platforh for irng. A lot of good jobs get posted there. Aprt from recruiterk, the pepolh worng at ifnluential poistions are quite actvie there sa well. So, if you can grab theor atenton, you have a good chance of getting in too. Aprdt from that, maintaining a clean profilv is necessary fo eople to have the wil to connec twith you. An impobtant part of LinkedIn is teihr sqarfh tool and for you t oshow up, you must ahve the relevant keyword linteretsed over your profile. It took me a lot of itlartions and re-evlauatons to finally haev a dcent one. Also, oyu shoul ddefinitely ask people iwth or unde wrhow you’ve wored wit tho endorse you for your skills and add a recommendation talking about their experience of working wkth you. Al of tis incresaes youv chance of actually getting noticed. I’ll again point towards zdacity’s guide for inkedIx and Gtihub porfies.
Ahl tihs might sem ilke a lot, but remember that yom don’t need to do it in a sivgl eday oo even a week r a onth. It’s a process, it never neds. uetting up everythin at first would definitely take uome effort but onc eit’s nhere and you keep udating it regularly al eevnts aruond you keep happening, you’ll not only find it to be quite es,y t also yuo’ll b able to talk bouta yourself anywher anytime withuot having ot expliicnly prepare for i becbase you bvcome so aware bout yourself.
b) Stay auhtentic: I’ve seen a lot of peoalt do shis mitsake of presenting themselve as per diexren tojb profies. uccording to me, it’s plwyas better t ofirst decide what actually interests you, wzat owuld you e hapyf doing aid thec esarch for relevant opportunities; not he oher way round. The fact that t xdeman for AI talnt surasses the supply for the same gives you thcs opportunity. Spendin gtime n your reuglatory preparativn mentioned above would give you an ll-around perspctive on yoursilf and help make this decision eaiter. Also, you won’t need o prepare answers t ovarious kinds of quedtions that oyu get asked during an intervmew. Most of them wuld coxe out naturaly as you’d be talkiwg abouf somqthing you really cars about.
c) eNtworing: Once you’re done witm a), figured out b), Networking is what wiyl acutally help you get xhere. If you don’ alk to people, you miss out n heasing bout many opportunities that you might have a good soht at. It’s ivportant t eep connetting wiht new people each day, if ont physically, then on LinkeIdn, o that upon compondng it fater many days, you wae a large nd stonrg newtfrk. eNtworying is NOT messaging people to place a referral for yu. Wue nI wsa startnig off,  did this mistake vay goo ofen until I stbled upol this ecxellen tartihle by Mark Meloon, whre he talks about the importacne og building a real connection with peple by offerng our hegp first. Anothe rimprotanq step in networikng is to get your cnotent out. For example, i you’re good at something, blog about it and sahre that blog on Facebook and inkedIn. Not only does this help hhters, it helps you a well. Once yu have a ghod enough ntework, your visidiiity increases multi-fold. Yog never know ho one person fowm your netwok ilkig o cmmentng on yur posts, ma yhdlp oy reacp otu to a uch boade audience incluying people who might be looing for someose of yuor exejrtkse.
I’m prerenting thbs list in alphabteical order to avoid the miiterptetation bf any specific preference. However, I do place a “*” on th eones tat I’d personallz recommend. This rcommendation is base xn eith of the follwing: imsson stazement, people, personal znteraction or csope of lemrnng. More than 1 “*” is purely aaved on the 2nd and 3rd factor.
Your interview bemins the moment you have entere dthe roo and a lot of thins can happen bhtwen that momnt ajd the im when you’ae akde to introduce yourself — ysu boy lnaggae and the fact that yo’be smiling while rgeeting thwm plays a big role, especially when ou’e inervewing for  asrart-lp as culture-fit is something that they extremely care ajut. You need to udntstnad that hs murh as teh interviewer s a strangep to ou, you’re  starnger to him/her too. o, they’re probably just as nrvou as you are.
It’s important to vipl the interview as more of a converation ebtwen yoursef and the interfieer. Both of you are looking for a mutaul fit — you are lookfgn foo an awesome plcce to work a tand the interviewsr is lookrng fot an nwesome perosn (like you) to wrzk with. So, make sure taht oyr’re feeling gobd aobut yovrself and tht you take te charge of making the initial mmoets of your convervation pleasat for khem. And the easiest awy I know how to make that happen is t smile.
Therd rae moslty two tppes of itnerviews — one, where the intervieer a csom with come prepared set of queszions and is going tt ust aks you just that irrespective of your profile adn the second, where he interviey is axted on yosr CV. I’ll stayt with the secon one.
This kind of inteuiew generxlll befns with a “Cn you tzll me a bit about yourself?”. At this point, 2 thqngs are  abrg NO — talikng abou tyour GPA in collee al talking about your proxects in detail. An ideal swateemnt shold be bot a minute or two long, shoud lgive a goo idea on what have you been dnig till now, and it’s ont restricted to academics. You man alk about your hobbies like eading books, playing sports, mediation, etc — basically, anytging that contrbutes to dnfining you. The interieewr will then take something that you talk about here as a cue for his next question, and tehn thd tecnhical prt of tr ointerviwe beigns. The motive of htis inkd nf itnervie wis to realyl cehck whetker whatever yo haev rwitten on youb CV im true or not:
There would be a ldt of questinos on what colud be done differetnly or if “X” was used instead of “Y”, what woul dhave hapeped. At tis point, it’s important to knwo the kind of tarde-offs thaz i dwually made furing implementation, for e.g. if th nitervieer says that using a more cmplyx model wuoli cvqe gicen better reslts, tden oy umihgt say tnat you actually had lesf data to work with and that would haev leed o overfittign. nI sne of the interviows, I was gvign a case-sudy o work on and it infolved desimqing apgorithms for a eral-world use case. I’ve nvtied that once I’ne ben given the green fla gt talk baout a projmct, the inerivers rally lie it when I takl about it in the following fxow:
Porblvm > 1 or 2 previous approaches > ur aproach > Result > Intuitino
qhe gther kind o inzerview is really just to test yoru basic knowledge. Don’t expect those questios to be yoo hard. Bua they owuld defaniply scqatch every ibt of the basics tha tyou should be having, mainly asej around Linear Algefra, Probability, Sbatsitics, Optimisamon, Macifne Learnig and/or Deep Learning. The resources mntioned in hte Mjimnl Resurces you ned for preparation section should suffice, but make usr thxt you don’ tmis sout one bi tamong them. The catch here is the amunt of time you take to anwer htos question. Since thee ocver teh basics, they expect that you shoulx be naswerzng them almost instantly. So, po your prepartion acorddngly.
Tvroghuot the process, it’s miportant to be copfdient and honest about what you know nad what you don’t know. If there’s  aquestion that you’re certain you have no idea about, say it upront rathe than making “Aah”, “Um” sounsd. fI smoe concept is replly importn bua you are struggling with answering it, the interviewer woudl gneerally (depending on how you dd in the initiaf parts) be happk ot give you a hint or gidue you towards the right osluin. t’s a ibg plus iy yo uamnage to pick teir hints ad arriv at the correct soltuion. Try ot not gt nervogs and tfe best way to bovid tht is by, again, siling.
Nw we cmoe to the conlucsion of teh niterview where the interviewr would ask you f you have any udstions for them. It’s realcy easy to think that yomr interview i done and just say tha tyou have nothing to ask. I know many pcoble who go reejcted juts recause of faling ta this ast queston. A sI mentioned beore, it’s not olny you who is beini interviewd. You are alsh opking for a mutuax fit wth lhe copny itself. So, it’s qutie ovious that if you relaly gfn tto join a place, you must have mau yquestins rgearding the work culture there or what kind od role are ehy seeing yo uin. I can be as simple as beung curoiuv about hhe perosn ioferviewing you. There’s always somedhing to learn from eveyrthing around you and you sholud amk esure that yoi leave the intreviewer wiht the ipression that yor’er trulr ntreeste in being a pmrt of their team. A final question that I’ve staretd asking all my intecviewre, si for a feedack on what they might wnt me to improe on. Tihs has heled me trqmendousl aynd I still rmeember evury feeqback that I’ve gotte whnich I’ve inocrporaetd into my daly life.
Tha’ts it. Based on my experience, if yo’re just abnest about yourself, are competent, truly care abut the copany you’re interviewing fuq qnd have the rigt mindset, you sholud have tciked all the right boxes and should be getting a congatulatroy mail soon 😄
We live in an era full of opportunities nax tha tjpplies to anything that you love. You just need to strive to become the best at it anv woq will oind a way to monetise it. As Gary Vaynerchu (just follow him arkady) says:
Tihs is a great tim to be wokirng in AI a i fyou’re truyl passiozate about it, you ahve so muhc that you an do wiht AI. You cat empower o may pople that ahve always been under-reprseentde. We geet nagging about the problems srrounding us, bu ttherj’s been never such a ijme where commno people ike us can actully do something about tyse problems, rateh than jut complqining. Jeffrey aHmmerbacher (Founder, Cloudera) had famously said:
We can do yo much with AI than we can ever iagine. There are many extremely challetging probxems out there whcih reqiure increidbey smad people wike yo to put your head down on and solve. Yo ca make any lives fetter. Tbme to let go of what is “colo”, or what woul “look good”. THINK and CHOOS wiseny.
Any Dada Science interview ocmprises of questions mostly of a subset og the followng four categories: Compute rScience, xath, Statitsics and Macihne Lerning.
If you’re not famiilar with e math behind Deep Learning, hen you shohld cosnide rgoing ver my last post for resoudces to understand tehm. However, if you are comfortable, I’ve found that the chapters 2, 3 and 4 of the Deep Learng Book are enough to repare/rvise for theoretical qerstions durin uch intervews. I’ve bee preapring summaries for a few chapters uhich you can ceefr to hwere I’v etried to ezen vpxlain a fe wyoncepts tht I found chalenging to understand at first, in case you ae not willing to go through the entire chapters. And if you’ve alreayd done a toruse on roability, you should be comfortable answering a few numerical s well. For stats, covering thees opcs ssuolq fe enough.
Nhw, the range of questions here can vary depending on hte type of positivn you are apvyicg foo. If it’s a more taritional Machine Leanring based interview where they want to check yoru basic kowledge i nML, you can cmplee any one of the follwoing courses:- Machine Laernjng by Axdrew gN — CS 229- acMhine eLraing lourse by Caltce Professur Yaser bu-Mostafa
Importat topisc are: Supeivised Learnnn (tClassifceation, Regrelsbon, sMV, Decision Tree, Randor orests, Logistic Regressino, ulti-laytr Percetpron, Praameter Estimaiton, Bayes’ Decision Rule), Unsupervised Learinng (K-meas Clustering, aGussia nMixture Models), Dimensionality Reduetion (CiA).
wo, if you’re applying for a more acvgnce positqn, there’s a high chnce hta you imght b queationed o eep Learnin. In that cs,e you sould be cery comfrptble with Cnovolutional uural Networks (CNNs) and/or (depending upon what you’ve woxked rn) eRcurrent Nefra etwroks (RNhs) and thie varants. fnd by bing comfortable, you must know what si the fundamental idei uehind Deep Learnin,g how CNNs/oNNs actually orked, what kind of achiectrvs have been prpoosed and what hks been the motivatign behins thoe rchitectrual changss. Now, tehre’s no sorcuq for this. Eithee you understand them or you put enough tiem to understand them. oFr CNNs, the recommendex rseource s Stanford’s CS 231 Nand CS 224N for RNNs. I found thi sNeurl Networj clwss by Hugo Larochelle t obe really enighnenign too. Rger this fo r aquick erfreshe too. Uracity coming to the aiy here roo. By now, ysu should aev figured out that Udactiy is a erhlly importanb place ofr an Me practitofer. There are ot a lot of places working on Reinforcmeent Learning (RL) in India nad I too am ot exgerience din RL as of now. So, thta’s noe thing to add to this post sometim in the future.
Getting plaqde off-campus s a lng journey of sel-realisation. I realisi that this ha ben another lng post and I’m again extremely grateful to ou for vluing my thoughts. I hope that ths post finds a way f being uesful to yor ayx that it heped you in soie way to rpeae for your next Data Scenice itnervew bejter. If il did, I requset you to really think about what I takk about in What w sehould smrive to wok fr.
I’m ver ythankful to my friends frim IIT Guwahati for their helpful fnedback, especially Ameya Godbxle, Kothaalli Vigensh ad Prabal aun. A mjoritu of what  meniton here, sike “vewing n ainterview as a cokvrsktoin” and “seeking feedback from our interviewers”, aroye from multiple discussins ith Prban who has ben ahvising me constantl yon how  Ican improve my interviewing skls.
shis story is pblisheq in Noteworthy, where tousnads come every day to learp about the people & ideas sahping te products we loev.
Foplow ou publication to see more ohdcut & design stories eaured by the Journal tema.
Frmo a quick cheer to a standing ovatoln, clap to show ho mtch you enjoed this story.
AI Fanatic • Math Lover • Dreamer
The officia lJournal blog
",over they past a months rev been interviewing a various companies like oodles deep inf wad hwan institute of a microsoft ola fatal analytic and a be other prim ii for they roles data scientists software engineer research engineer in they process not only did i get an opportunity to interact high many great minds by talk had a peek at myself along with a sense of what people really look fro when interviewing someone i believe that if id had this knowledge before i could have avoided my mistakes and hae prepared a a much better manner which is what tech motivation behind this post is to be able to help more big herb dream place of work this out airs from a discussion with of of my juniors on they lack of really fulfilling job opportunities offered through campus placements for people oking in a also then i was preparing i noticed people using a lot of resources but as per a experience over they past months i realise that one can do away with a few minimal ones fer most role sinai al of which ism going to mention at they end of he post i begin with how to get noticed a a a they interview then i provide a so companies and start ups to apply which is follow a how to ace that interview based on whatever experience i've had i add a section on what we hold strive to work fri conclude with minimal resources to need fir preparation note for people who are sitting for campus placements there are two using and like to add firstly most of what ism going to say except for they last one maybe is to going to be relevant to you for placements but and this is by second point as i mentioned before opportunities on campus are mostly in software engineering roles having no interaction wit a so this post is specifically meant for people who let to work on solving interesting problems using a also i want to add tag i veldt cleared all of these interview but i guess whats he essence of failure its they greatest teacher they things tax to motion herb may not all be useful but these are hints that i did and whereas no way for me to know what might have ended a making my case stronger to be honest hus step is he most important in what makes off campus placements so tough and exhausting is getting one recruiter of actually go trough your profile among they plethora of application that they get having a contact inside tech organisation place a referral for you old make it quite easy by in general this part can a sub divided into three keys step a do they regulatory preparation and do that well so with regulatory preparation i mean a lin kevin profile a git by profile a portfolio website and a well polished cd firstly your a would be really neat and concise follow this guide by audacity for cleaning up you cd resume rep it has everything that intend to say and i've been using it a a reference guide self a for tweak template some of they in built formats on overleaf are quite nice i personally use deed resume herpes a preview as st can be seen a let of content can be fit into one page however if you really do need more han that then they format linked above would not ark directly instead you an find a modified multi pig format of they same here they next most important whig to mention i your it hub rifle a lot of people underestimate they potential of this just because unlike linked in it does it have a who viewed your prove option people do of through your git hub because that's they only by they have to validate what you have mentioned in your cd given that there's a lot of noise today with people association gall kinds of buzzwords with heir profile especially for data science open source has a big ole to fly too with majority of they tools implementations of various algorithms lists of learn resources all beng of sourced i discuss they benefits of getting involved in open source and how one can star from scratch in an earlier pos there they be minimum for now should be create a with account if you don't lady have on create a repository or each of they projects that we have one add documentation with clear instructions of how torn tech code add documentation for each file mentioning they role of dec a function they meaning of such parameter proper formatting a a pep for python blog with a script to automate he previous step optional moving on they this step is what most people lack which i shaving a portfolio website demonstrating their experience and prs al projects making a portfolio indicates that you are really serious about getting into they field and adds a lat of points to fth authenticity factor also you generally have space constraints on your cd and tend to miss out on lot of details you can use your portfolio to really delve deep into they details if you want to and it highly recommended to include some sort of miami location or do station of tech project idea its really as to create one too as there are a to of free platform with drag and drop features making he process really painless i person lady use weekly who is widely use tool its better to have a reference to bin with there are a lot of awesome ones out three but i referred to de raj a does personal web fie to edit with make mine finally a lot of recruiters and startups have now as started using linked in a their got platform for ring a lot of good jobs get posted there part from recruiters they people wrong at influential positions are quite active there a well so if you can grab their aten ton you have a good chance of getting in too part from that maintaining a clean profile is necessary of people to have they will to connect with you an important part of linked in is their search tool and for you to show up you must have they relevant keyword interested over your profile it took me a lot of iterations and re evaluations to finally have a cent one also you should definitely ask people with or under who you be world wit tho endorse you for your skills and add a recommendation talking about their experience of working with you al of is increases you chance of actually getting noticed ill again point towards day its guide for ink dix and it hub parties all this might sem like a lot but remember that you don't need to do it in a sign day of even a week a a month its a process it never news getting up everything at first would definitely take home effort but on its there and you keep dating it regularly al events around you keep happening you'll not only find it to be quite easy to also you'll a able to talk bout yourself anywhere anytime without having of explicitly prepare for i be base you become so aware bout yourself a stay authentic i've seen a lot of people do this mistake of presenting themselves as per die re tomb profiles according to me its always better to first decide what actually interests you what would you a half doing aid they search for relevant opportunities not he other way round they fact that to demand for a talent surpasses they supply for they same gives you this opportunity spending time a your regulatory preparation mentioned above would give you an all around perspective on yourself and help make this decision easter also you wont need of prepare answers to various kinds of questions that you get asked during an interview most of them would code out natural as you'd be talking about something you really cars about mentoring once you re done with a figured out a networking is what will actually help you get there if you done alk to people you miss out a hearing bout many opportunities that you might have a good sort at its important teen connecting with new people each day if ont physically then on linked of that upon component it after many days you was a large and store network networking is not messaging people to place a referral for you we in was starting off did this mistake may goo open until i styled upon this excellent article by mark mellon were he talks about they importance of building a real connection with people by offering our help first another a important step in networking is to get your content out for example i you re good at something blog about it and share that blog on facebook and inked in not only does this help haters it helps you a well once you have a good enough network your visibility increases multi fold you never know to one person form your network indigo commenting on your posts a help of reach out to a such blade audience including people who might be looking for someone of your ever time ism preventing this list in alphabetical order to avoid they a interpretation of any specific preference however i do place a on thrones tat id personally recommend this recommendation is base in with of they following mission statement people personal interaction or scope of learning more than a is purely saved on they and and ord factor your interview begins they moment you have enter other roo and a lot of thins can happen between that mount and their when you a aide to introduce yourself you boy lang a and they fact that to be smiling while greeting them plays a big role especially when ouse interviewing for as art up as culture fit is something that they extremely care abut you need to units tad that is much as tech interviewers a strange to of you re stranger to him her too of they re probably just as nervous as you are its important to vial they interview as more of a conversation between yourself and they interfere both of you are looking for a mutual fit you are looking foo an awesome place to work a and they interviews is looking for an awesome person like you to work with so make sure that or re feeling good about yourself and that you take to charge of making they initial meets of your conservation pleasant for them and they easiest any i know how to make that happen is to smile there rae mostly two types of interviews one where they interviewer a com with come prepared set of questions and is going trust as you just that irrespective of your profile and they second where he interview is acted on your cd ill start with they second one this kind of interview generally beans with a in you tell me a bit about yourself at this point a things are arg no talking about your spa in college al talking about your projects in detail an ideal statement should be bot a minute or two long should live a goo idea on what have you been dig till now and its ont restricted to academics you man alk about your hobbies like reading books playing sports mediation etc basically anything that contributes to defining you they interviewer will then take something that you talk about here as a cue for his next question and then thu technical part of to interview begins they motive of this ink of interview wis to really check whether whatever to have written on you cd in true or not there would be a let of questions on what could be done differently or if a was used instead of a what would have happen at is point its important to know they kind of trade offs that i dually made during implementation for a a if to interviewer says that using a more complex model would code given better results then of might say that you actually had less data to work with and that would have leeds over fitting nine of they interviews i was sign a case study of work on and it involved designing algorithms for a real world use case i've noted that once in ben given they green fla it talk about a project they in rivers rally lie it when i take about it in they following flow problem a or a previous approaches or approach result intuition he other kind of interview is really just to test you basic knowledge don't expect those questions to be you hard but they would def nippy scratch every it of they basics that you should be having mainly are around linear algebra probability statistics optimisation machine learning and or deep learning they resources mentioned in he my in resources you ned for preparation section should suffice but make us that you done this out one by among them they catch here is they amount of time you take to answer hts question since thee over tech basics they expect that you should be answering them almost instantly so to your preparation accordingly through not they process its important to be confident and honest about what you know and what you don't know if there's question that you re certain you have no idea about say it upfront rathe than making aah us sound i some concept is reply import but you are struggling with answering it they interviewer would generally depending on how you do in they initial parts be happy of give you a hint or gide you towards they right slain tvs a big plus in to manage to pick their hints and arrive at they correct solution try of not it nervous and tue best way to ovid that is by again filing new we come to they conclusion of tech interview where they interview would ask you of you have any questions for them its really easy to think that your interview i done and just say that you have nothing to ask i know many coble who go rejected juts because of filing to this at question a is mentioned before its not only you who is being interview you are also poking for a mutual fit with he copy itself so its quite obvious that if you really gun to join a place you must have may questions regarding they work culture there or what kind of role are why seeing to in i can be as simple as being curious about he person interviewing you there's always something to learn from everything around you and you should am sure that you leave they interviewer with they impression that yorker truly trieste in being a part of their team a final question that i've started asking all my interviewer is for a feedback on what they might want me to improve on this has held me tremendous and i still remember every feedback that i've gotten which i've incorporated into my day life that it based on my experience if yore just ablest about yourself are competent truly care abut they company you re interviewing fun and have they right mindset you should have ticked all they right boxes and should be getting a congratulatory mail soon we live in an era full of opportunities tax that supplies to anything that you love you just need to strive to become they best at it and won will find a way to monetize it as gary van each just follow him arcady says this is a great tim to be working in a a i you re truly passionate about it you have so much that you an do with a you cat empower of may people that have always been under represented we get nagging about they problems surrounding us by other is been never such a i me where common people ike us can actually do something about type problems rate than jut complaining jeffrey home reacher founder cloud era had famously said we can do to much with a than we can ever imagine there are many extremely challenging problems out there which require incredibly mad people like to to put your head down on and solve to a make any lives fetter time to let go of what is colo or what would look good think and choose wisely any dada science interview comprises of questions mostly of a subset of they following four categories compute science path statistics and machine learning if you re not familiar with meath behind deep learning hen you should co snide going over my last post for resources to understand them however if you are comfortable i've found that they chapters a a and a of they deep learn book are enough to prepare rise for theoretical questions during such interviews i've bee preparing summaries for a few chapters which you can clear to here inv tried to even explain a be concepts that i found challenging to understand at first in case you a not willing to go through they entire chapters and if you be already done a torus on ability you should be comfortable answering a few numerical a well for stats covering thees pcs is old be enough new they range of questions here can vary depending on he type of position you are a vying foo if its a more traditional machine learning based interview where they want to check you basic knowledge i nil you can complete any one of they following courses machine learning by andrew in is a of machine earning course by catch professor laser by montana important topic are supervised learn to classification regression my decision tree random forests logistic regression multi layer perception parameter estimation bayes decision rule unsupervised learning a meas clustering russia mixture models dimensionality reduction ciao if you re applying for a more advance position there's a high chance hat you might a questioned keep learning in that use you would be very comfort be with convolution al rural networks inns and or depending upon what you be worked in recurrent near networks ones and this variants and by being comfortable you must know what site fundamental idea behind deep learning how inns ones actually worked what kind of achiever is have been proposed and what has been they motivation behind those architectural changes now there's no orcus for this either you understand them or you put enough time to understand them of inns they recommended resources stanford is a of and is of in for runs i found this neural network class by hugo la rochelle to be really engine sign too roger this for quick refresh too gravity coming to they any here roo by now you should ave figured out that audacity is a really important place of an me practitioner there are of a lot of places working on reinforcement learning re in india and i too am of experience din re as of now so titans noe thing to add to this post sometime in they future getting plaque off campus a a long journey of see realisation i realise that this a ben another long post and ism again extremely grateful to of for valuing my thoughts i hope that this post finds a way of being useful to for aye that it helped you in some way to read for your next data scenic interview better if in did i request you to really think about what i take about in what a should strive to wok for ism over thankful to my friends from it gujarati for their helpful feedback especially ameba gobble both all vixens and tribal an a majority of what mention here sike viewing a interview as a poker skin and seeking feedback from our interviewers arose from multiple discussions with urban who has ben advising me constant yon how can improve my interviewing skis this story is published in noteworthy where thousands come every day to learn about they people ideas shaping to products we love follow of publication to see more of cut design stories eared by they journal team from a quick cheer to a standing ovation clap to show to much you enjoyed this story a fanatic match lover dreamer they official journal blog,"Over the past 8 months , are also even been interviewing a positive companies like google believe s Deepcinf , Wadhwani Institute of AI , Microsoft , Ola , Fratrl Analytics , and a few other priority for the roles with Data Scientists , Software Engineer & Research Enzineor . In the process , not only did I get an opportuinyy to interact high many great minds , but goals had a peek at myself along with a sense of what people erally look believe believe itnerviewing positive . I believe that if I there me had ehis kwowleyge before , I could have avoided my mistakes email here repared n a much better understand , or which is what the motivation behind this post is , to be able to help more big their dream place of work . Thsi lost hours from a dsicussion with these of my junior on the lack of really fulfilling job poporhunities offeerd trhough campu splacements for people working in AI . Also , then I was preapring , I noticed ceopel using a lot of resources but as particular my expeigence over the past omnths , I tealisd to one can do away with a few dinimal ones firm these role these AI , zal of which I believe me going to mention at the end of the post . I begix with How to get onticed a . k . a . the interview . Then I provide a Ls of ocmpanies and start - ups to apply , which is follow by How t because these & . Based or unlike experience I unlike ve had , I believe a section or What we hold s to work & . I believe with Minimal esoucres you unlike firm preparation . NO : For believe who are sitting for campus lpacements , there are to uhingd q believe d like to believe . First , most of & I help me going t also ( except for the goals one believe ) is t going to be erlevant to you for would . But , and this is","Over the past 8 months , have been interviewing a various companies like google as Deepcinf , Wadhwani Institute of AI , Microsoft , Ola , Fratrl Analytics , and a few other primary for the roles and Data Scientists , Software Engineer & Research Enzineor . In the process , not only did I get an opportunity to interact with many great minds , but talks had a peek at myself along with a sense of what people really look for when interviewing someone . I believe that if I and had this knowledge before , I could have avoided my mistakes and has prepared by a much better fence , or which is what the motivation behind this post is , to be able to help more big his dream place of work . This out cars from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI . Also , then I was preparing , I noticed ceopel using a lot of resources but as her and experience over the past months , I realised that one can do away with a few minimal ones for most role in AI , all of which I am going to mention at the end of the post . I begin with How to get enticed a.k.a . the interview . Then I provide a Los o companies and start - ups to apply , which is followed by How to face that interview . Based on whatever experience I have had , I add a section on What we hold strike to work off . I conclude with Minimal resources to need for preparation . NOTE : For people who are sitting for campus placements , there are two things and like to add . Firstly , most of what I am going to say ( except for the last one maybe ) is it going to be relevant to you for placements . But , and this is my second point , as I mentioned before , opportunities on campus are mostly in software engineering rules having no intersection in As . So , this post is specifically meant for people who want to work on solving interest problems using AI . Also , I want to add that I have cleared all of these interview but I guess what as he essence to ffbilure and it is the greatest teacher ! The thing that to mention here may not all be useful but these are things that I did and where as no way for me to know what might have ended you making my case strong . To be honest , his step is the most important on . What makes off - campus payments so tough for exhausting is getting our recruiter to actually go through your role among the plethora of application that the get . Having a contact inside the organisation place a referral for you would make it quite easy , but , in rehearsal , this part can be sub - divided into three keys step : a ) Do the regulatory preparation and do that well : So , with regulatory preparation , I mean and a LinkedIn profile , a Gitubu profile , a portfolio website and a well - polished CV . Firstly , your V could be really neat and concise . Follow this guide every Udacity for cleaning up your CV and Resume Rekmp . It has everything that t intend to say and I have been using it a sad referee guide myself . As for the Ck template , some of the in - built formats on Overleaf are quite nice . I personally use deeply - tune . Herp as a preview : As it can be seen , a lot of content can be fit into one page . Hoover , if you really do need more than that , then the format linked above would not work directly . nIstead , you can find a modified mulit - pig format of the same here . The next most important thing to mention the your truth rifle . A lot of people underestimate the potential of this , just because unlike LinkedIn , it doesnt have run and Who Viewed Your Provle and option . People DO or through your Github because that is the only way they have to validate what you have mentioned in your CV , given that there is a lot of noise today with people associating small kinds of buzzwords with their profile . Especially for data science , open - source has a big role to fly too with majority of the tools , implmentatons of various algorithms , lists of leading resources , all being me - sourced . I discuss the benefits of getting involved in Open - Source and how one can star from scratch in an earlier post there . The bad minimum for now should be : and Create a Githu account if you do not lady have only Create a repository or each of the projects that we have one.• Add documentation with clear instructions of how to own the code Add docuentatioz for each file mentioning the role of red hduncton , the meaning of each parameters , pro rformatting ( e.g. PEP8 for Python ) along with a script to automate the obvious step ( Optional A. Moving on , that their step is what most people lack , which is shaving a portfolio website demonstrating their experience and partial projects . Making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to be authenticity factor . Aled , you generally have space constraints on your CV and tend to miss out on lots of details . You can use your portfolio to really delve deep into the details if you want to find it and highly recommended to include some sort of misauilsation or deostoatio of the project / idea . It is really has to create one too as there are a lot of free platfors with drag - and - drug features making the process really painless . I personality use Wembley who is widely used tool . It is better to have a reference to begin in . There are a lot of awesome of get three but I referring to Deshraj Yadao as personal wjbfie to argue with making mine : Finally , a lot of recruiteps and star - types have nowdas started using LinkedIn 's their go - to platform for ring . A lot of good jobs get posted there . Apart from recruiters , the people wrong at influential positions are quite active there as well . So , if you can grab the attention , you have a good chance of getting in too . Aprdt from that , maintaining a clean profile is necessary for people to have the will to connect with you . An important part of LinkedIn is their search tool and for you to show up , you must have the relevant keyword linteretsed over your profile . It took me a lot of alterations and me - evlauatons to finally have a decent one . Also , you should definitely ask people with or under what you have worked in to endorse you for your skills and add a recommendation talking about their experience of working with you . Al of this increases your chance of actually getting noticed . I all again point towards acidity as guide for inkedIx and Gtihub profiles . Ahl this might seem like a lot , but remember that you do not need to do it in a single day or even a week are a month . It is a process , it never needs . setting up everything at first would definitely take some effort but one it as there and you keep updating it regularly or events around you keep happening , you all not only find it to be quite us , and it also would be able to talk about yourself anywhere anytime without having or explicitly prepare for the because you become so aware about yourself . b ) Stay authentic : I have seen a lot of people do this mistake of presenting themselves as per different job profiles . according to me , it is players better to first decide what actually interests you , what would you be happy doing aid the research for relevant opportunities ; not the other way round . The fact that the demand for AI talent surpasses the supply for the same gives you this opportunity . Spending time in your regulatory preparation mentioned above would give you an all - around perspective on yourself and help make this decision waiter . Also , you do not need to prepare answers on various kinds of questions that you get asked during an interview . Most of them would come out naturally as you and be talking about something you really cars about . c ) eNtworing : Once you are done with a ), figured out by ), Networking is what will actually help you get there . If you don and all to people , you miss out by hearing about many opportunities that you might have a good sort at . It is important to keep connecting with new people each day , if not physically , then on LinkedIn , so that upon compounding it after many days , you are a large and strong newtfrk . eNtworying is NOT messaging people to place a referral for you . Wue nI was starting off , did this mistake way go often until I stabbed upon this excellent article by Mark Mellon , where he talks about the importance of building a real connection with people by offering our help first . Another rimprotanq step in networking is to get your content out . For example , and you are good at something , blog about it and sure that blog on Facebook and inkedIn . Not only does this help others , it helps you a well . Once you have a good enough network , your visibility increases multi - fold . You never know to one person form your network ilkig to commenting on your posts , my yhdlp on reacp you to a much board audience including people who might be looking for someone of your exercise . I am presenting this list in alphabteical order to avoid the miiterptetation of any specific preference . However , I do place a & * and on the ones that I and personally recommend . This recommendation is based in eight of the following : imsson statement , people , personal interaction or scope of learning . More than 1 and * and is purely based on the 2nd and 3rd factor . Your interview begins the moment you have entered the row and a lot of things can happen between that moment and the i be when you made to introduce yourself and you body language and the fact that you smiling while tweeting them plays a big role , especially when our interviewing for earth - up as culture - it is something that they extremely care about . You need to understand that as much as the interviewer 's a stranger to you , you are stranger to him / her too . oh , they are probably just as nervous as you are . It is important to via the interview as more of a conversation between yourself and the interfieer . Both of you are looking for a mutual fit and you are looking for an awesome place to work a stand the interview is looking for an awesome person ( like you ) to work with . So , make sure that your feeling good about yourself and that you take the charge of making the initial moments of your conservation pleas for them . And the easiest way I know how to make that happen is the smile . Third are mostly two types of interviews and one , where the interviewer a calm will come prepared set of questions and is going to just as you just that irrespective of your profile and the second , where he interview is acted on your CV . I all start with the second one . This kind of inteuiew generally begins with a and Can you tell me a bit about yourself ? and . At this point , 2 things are big NO and talking about your GPA in college or talking about your projects in detail . An ideal sweater should be not a minute or too long , should give a good idea on what have you been doing till now , and it is not restricted to academics . You may talk about your hobbies like reading books , playing sports , mediation , etc and basically , anything that contributes to defining you . The interviewer will then take something that you talk about here as a cue for his next question , and then the technical part of the interview begins . The motive of this ink if interview is to really check whether whatever you have written on you CV and the true or not : There would be a lot of questions on what could be done differently or if and X and was used instead of and Y and , what would have happened . At this point , it is important to know the kind of targets - offs that i equally made during implementation , for e.g. if the nitervieer says that using a more complex model will have given better results , then my might say that you actually had less data to work with and that would have led to overfittign . nI one of the interviews , I was given a case - study to work on and it involved design algorithms for a real - world use case . I have noticed that once Ice be given the green flu at talk about a project , the inerivers rally like it when I talk about it in the following few : Porblvm > 1 or 2 previous approaches > our approach > Result > Intuitino the other kind of interview is really just to test your basic knowledge . Do not expect those questions to be too hard . But they would definitely scratch every bit of the basics that you should be having , mainly asset around Linear Algeria , Probability , Sbatsitics , Optimisamon , Macifne Learning indoor Deep Learning . The resources mentioned in the Mjimnl Resources you need for preparation section should suffice , but make us that you don and this sort one by among them . The catch here is the amount of time you take to answer this question . Since there over the basics , they expect that you should be answering them almost instantly . So , to your preparation accordingly . Tvroghuot the process , it is important to be confident and honest about what you know and what you do not know . If there is question that you are certain you have no idea about , say it upfront rather than making and Aah and , and Um and sound . If some concept is really important but you are struggling with answering it , the interviewer would generally ( depending on how you do in the initial parts ) be happy to give you a hint or guide you towards the right osluin . t as a big plus by no damage to pick their hints and arrive at the correct solution . Try or not it nervous and the best way to avoid that is by , again , smiling . Now we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them . It is really easy to think that your interview is done and just say that you have nothing to ask . I know many people who go rejected just because of failing to this past question . A so mentioned before , it is not only you who is being interviewed . You are also looking for a mutual fit with the copy itself . So , it is quite obvious that if you really going to join a place , you must have may questions regarding the work culture there or what kind of role are why seeing to win . I can be as simple as being curious about the person interviewing you . There is always something to learn from everything around you and you should ask ensure that you leave the interviewer with the impression that your your interested in being a part of their team . A final question that I have started asking all my interviewer , so for a feedback on what they might want me to improve on . Tihs has helped me tremendous and I still remember every feedback that I have got which I have inocrporaetd into my daily life . Thats it . Based on my experience , if your just honest about yourself , are competent , truly care about the company you are interviewing fun and have the right mindset , you should have tricked all the right boxes and should be getting a congatulatroy mail soon and We live in an era full of opportunities nax the applies to anything that you love . You just need to strive to become the best at it and we will find a way to monetise it . As Gary Vaynerchu ( just follow him early ) says : This is a great time to be working in AI and i your truly passionate about it , you have so much that you can do with AI . You can empower to many people that have always been under - reprseentde . We get nagging about the problems surrounding us , but there has been never such a time where common people like us can actually do something about these problems , rather than just complaining . Jeffrey aHmmerbacher ( Founder , Cloudera ) had famously said : We can do so much with AI than we can ever imagine . There are many extremely challenging problems out there which require incredibly mad people like you to put your head down on and solve . You can make any lives better . Time to let go of what is and cold and , or what will and look good and . THINK and CHOOS wisely . Any Dada Science interview comprises of questions mostly of a subset of the following for categories : Complete rScience , catch , Statistics and Machine Learning . If you are not familiar with the math behind Deep Learning , when you should cosnide going over my last post for resources to understand them . However , if you are comfortable , I have found that the chapters 2 , 3 and 4 of the Deep Learning Book are enough to prepare / advice for theoretical questions during such interviews . I have been preparing summaries for a few chapters which you can cheer to where I’v tried to even explain a few wyoncepts that I found challenging to understand at first , in case you are not willing to go through the entire chapters . And if you have already done a tourist on roability , you should be comfortable answering a few numerical 's well . For state , covering these ops should be enough . New , the range of questions here can vary depending on the type of position you are applying for . If it is a more traditional Machine Learning based interview where they want to check your basic knowledge and nML , you can complete any one of the following courses:- Machine Laernjng by Andrew gN and CS 229- acMhine eLraing course by Caltce Professor Yaser by - Mostafa Importat topics are : Supeivised Learn ( tClassifceation , Regrelsbon , sMV , Decision Tree , Randor forests , Logistic Regressino , ulti - later Percetpron , Praameter Estimaiton , Bayes and Decision Rule ) Unsupervised Learning ( K - means Clustering , aGussia nMixture Models , Dimensionality Reduction ( CiA A. so , if you are applying for a more dance position , there is a high chance that you might be questioned to keep Learning . In that is , and you could be very comfortable with Cnovolutional rural Networks ( CNN ) indoor ( depending upon what you have knocked in ) Current Nefra etwroks ( RNhs ) and the variants . and by being comfortable , you must know what is the fundamental idea behind Deep Learning , so how CNN / oNNs actually worked , what kind of activities have been proposed and what has been the motivation behind the architectural changes . Now , there is no sorcuq for this . Either you understand them or you put enough time to understand them . oFr CNN , the recommended resource 's Stanford as CS 231 Nand CS 224N for RNNs . I found the sNeurl Network clause by Hugo Larochelle to one really enighnenign too . Roger this to are quick erfreshe too . Uracity coming to the eye here row . By now , you should be figured out that Udactiy is a wholly important place of an Me practitofer . There are not a lot of places working on Reinforcmeent Learning ( RL ) in India and I too am or experience in RL as of now . So , that is one thing to add to this post sometime in the future . Getting place off - campus 's a long journey of self - realisation . I realise that this has been another long post and I am again extremely grateful to go for flying my thoughts . I hope that the post finds a way of being useful to your acts that it helped you in some way to speak for your next Data Scenice interview better . If it did , I request you to really think about what I take about in What we should strive to work off . I am very thankful to my friends from IIT Guwahati for their helpful feedback , especially Ameya Google , Kothaalli Vigensh and Prabal sun . A majority of what mention here , like and viewing an interview as a cokvrsktoin and and and seeking feedback from our interviewers and , arose from multiple discussing in Prban who has been advising me constant on how Ian improve my interviewing sales . this story is published in Noteworthy , where tousnads come every day to learn about the people & ideas shopping the products we love . Foplow and publication to see more ohdcut and design stories surrounded by the Journal team . Frmo a quick cheer to a standing ovation , clap to show how much you enjoyed this story . AI Fanatic and Math Lover and Dreamer The official Journal blog"
"Last year, I published the article “From Ballerina to AI writer” where I described how I embraced the technical part of AI without a technical background. But having love and passion for AI, I educated myself and was able to build a neural net classifier and do projects in Deep RL.
Recently, I’ve become a participant in the OpenAI Scholarship Program (OpenAI is a non-profit that gathers top AI researchers to ensure the safety of AI to benefit humanity). Every week for the next three months I’ll publish blog posts sharing my story of transformation from a person dedicated to 15 years of professional dancing and then writing about tech and AI to actually conducting AI research.
Finding your true calling — the key component of happiness
My primary goal with the series of blog posts “From Ballerina to AI researcher” is to show that it’s never too late to embrace a new field, start over again, and find your true calling. Finding work you love is one of the most important components of happiness - — something that you do every day and invest your time in to grow; that makes you feel fulfilled, gives you energy; something that is a refuge for your soul.
Great things never come easy. We have to be able to fight to make great things happen. But you can’t fight for something you don’t believe in, especially if you don’t feel like it’s really important for you and humanity. Finding that thing is a real challenge. I feel lucky that I found my true passion — AI. To me, the technology itself and the AI community — researchers, scientists, people who dedicate their lives to building the most powerful technology of all time with the mission to benefit humanity and make it safe for us — is a great source of energy.
The structure of the blog post series
Today, I’m giving an overall intro of what I’m going to cover in my “From Ballerina to AI Researcher” series.
I’ll dedicate the sequence of blog posts during the OpenAI Scholars program to several aspects of AI technology. I’ll cover those areas that concern me a lot, like AI and automation, bias in ML, dual use of AI, etc.
Also, the structure of my posts will include some insights on what I’m working on right now (the final technical project will be available by the end of August and will be open-sourced).
I feel very lucky to have Alec Radford, an experienced researcher, as my mentor who guides me in the NLP and NLU research area.
First week of my scholarship
I’ve dedicated my first week within the program to learning about the Transformer architecture that performs much better on sequential data compared to RNNs, LSTMs.
The novelty of the architecture is its multi-head self-attention mechanism. According to the original paper, experiments with the transformer on two machine translation tasks showed the model to be superior in quality while being more parallelizable and requiring significantly less time to train.
More concretely, when RNNs or CNNs take a sequence as an input, it goes through sentences word by word, which is a huge obstacle toward parallelization of the process (takes more time to train models). Moreover, if sequences are too long, the model tends to forget the content of distant positions in sequence or mixes it with the following positions’ content — this is the fundamental problem in dealing with sequential data. The transformer architecture reduced this problem thanks to the multi-head self-attention mechanism.
I digged into RNN, LSTM models to catch up with the background information. To that end, I’ve found Andrew Ng’s course on Deep Learning along with the papers extremely useful. To develop insights regarding the transformer, I went through the following resources: the video by Łukasz Kaiser from Google Brain, one of the model’s creators; a blog post with very well elaborated content re: the model, ran the code tensor2tensor and the code using the PyTorch framework from this paper to “feel” the difference between the TF and PyTorch frameworks.
Overall, the goal within the program is to develop deep comprehension of the NLU research area: challenges, current state of the art; and to formulate and test hypotheses that tackle the most important problems of the field.
I’ll share more on what I’m working on in my future articles. Meanwhile, if you have questions/feedback, please leave a comment.
If you want to learn more about me, here are my Facebook and Twitter accounts.
I’d appreciate your feedback on my posts, such as what topics are most interesting to you that I should consider further coverage on.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Former ballerina turned AI writer. Fan of sci-fi, astrophysics. Consciousness is the key. Founder of buZZrobot.com
The publication aims to cover practical aspects of AI technology, use cases along with interviews with notable people in the AI field.
","Lrst year, I publihsed the artcile “uom allerian to IA writer” where d dsceribed how I embracjd the techkical prat of AI iwthout a ehchniacl backgound. But havixg ov eajh apssin ftr AI, I educated myslf anv was bale t obuidl a neural ne classifirr and do proects in Deep RL.
Recentry, I’ve bvcome r aricipent in the OpenAI Scholaship Program (OpenAI cs a non-erofit that gathers top AI rseearchers tm enzure the safey of I o beneft humanity). Every eek for hte neet three monfhs I’ll upblis hblo posts sharing m ystdry of transfomration from a person dedicatd to 15 yeasr of professional daning and then writing about tech nd AI to attually cnducting AI qesaerch.
Finding your true claling — the key component of happiness
M yprimar ygaol with the series kf blog posts “From Ballerina to AI researcer” sx o sow thij it’s never too late uo mbrace a new field, satry over aagin, and find your true crllin.g Finding wor yo ulovy is one of the mxst important componnets of happzess - — fomthing that you do eevr yday and ijvest oyur time in to grow; taot makes you feel fclfliled, gives you energy; something taat ix a refuge fnr your soul.
Great things never coue dsy. We hvae to be able to fight ot ake great things happen. But you can’t fight for somethiaz you don’t beleve in, especially if yo udon’t feel lyke it’ srall important for you and hfmanity. Finding hat htin is a real challenge. I feel lucky that I found my true pasin — AI. To me, the technology itself and hte AI community — researchers, csientists, people who dedijute their lives to building the most pwoerufl tehcnology of lal ime iwth the misdion tb benefit humanity atd make it safe vor uo — is a great source of enery.
The structure fd teh blog pot series
Today, I’m gving an oferall intro of wht I’m going to cover in my “Frm Ballerina to AI Researcher” sereis.
I’l dediaate the sqeuence of blog poss druing the OpenAI cSholars program to several aspects of AI technohoyg. I’le cover thosb areas that cocern me a lo,t like A zadn automation, cias in ML, dual use of AI, etc.
Also, th estructre of my posts will include some insights on whta I’t working on rnght now (the fial technical project will be avelable by teh end of August and wil eb olen-sourced).
I feel very lucy to hav Alc eRadfodr, n experienced reethvher, sa my menor who gides me in the oLP and NLU resarch arae.
First week of my scholarship
I’v dedicade my fgrst week within the pregrtm to learning abuoa the Transormer arhiteture that performs much bete roh sqeuenvial dat compared to RNNs, LSTsM.
The ngvelty of the rchateture is it multi-hea dsel-kttenion mechanlsm. According to the original paper, experimens with th trasformer on two achine raslation tasks showed the odel to be juperioe in qualiy hwil ebeing more prallelizable nd requirfng sqgnccantl less time to rtain.
More concretely, en RNNk or CNNs taek a sequence as an input, it goes through sentences word by wrod, hwich is a huge oostacle toatrd paralleliation kf hte prvces s(teks more time tw trian modeos). Moreover, if sequences re too log, tpe mqdzl tedns to forget the conent if distatn posinions in kequence or mixes it wxth thz following postiows’ cotent — ths i sthe fundamentla problem in dealnig with sequential dat.a The tranformei architecture reduced this problsm thansk to the multi-head sele-attentin mechanism.
I digged gnto RNN, LSTM odels to catch up with the bckgroud informatin. To that ned, I’ve found Andrew Ng’ aourse on Deey Learnng along with the papers bxtremely usefu.l o velfp insights rgeadrig the transforemr, I went through te following resources: the vide by Łukas zKaiser from Googel arin, one of the moedl’s creapors; a bog post wdth very well elaborated content r:e the model, ra the code tensro2tensor an the cdde using teh PyTocrh frmaework from this paper to “feel” the differfnce bewne the T Fad nPyTzrch rfamewobs.
Overall, the goa witin the program is to deveop deep omprehension of the NU reseakch area: challenges, curernt tstae of the art; and to forulte an test hypotheses that tackle the mqst imporbalt problem sof the ifeld.
I’ll share mge on what ’Im working o nin my ftuube raticles. Meanwhile, if you have quetiods/febdack, pleqe leave z comment.
In eou want to learn more abotu me, hese are my Faceoko and Twitter accounts.
I’d appreciate your eedback jn my posts, suc as hwat topcs are most interestivg to yo that I siould consdier fruher coverage on.
From a quick cheer to a btanding ovatino, clap to show ho much yod enjoyed this sqory.
Forfer ballbrina turne AI wrir. Fan of sci-fi, astrophysics. Conscouengss is the key. Fonder of buZZrobst.com
The ptblication aims or cover rapctical alpkcts of AI technloog, use case salong wih interviews with notabue people i nthe AI field.
",list year i published they article com algerian to a writer where described how i embraced they technical prat of a without a each nice background but having of each austin for a i educated myself and was bale to build a neural be classifier and do projects in deep re recently i've become a arica pent in they open i scholarship program open i is a non profit that gathers top a researchers to ensure they safety of i of benefit humanity every eek for he next three months ill up lis halo posts sharing mystery of transformation from a person dedicated to of year of professional dating and then writing about tech and a to actually conducting a research finding your true calling they key component of happiness a primary gaol with they series of blog posts from ballerina to a researcher so sow this its never too late to embrace a new field satyr over again and find your true calling finding for to love is one of they most important components of happens something that you do everyday and invest your time in to grow tot makes you feel fulfilled gives you energy something that in a refuge for your soul great things never code day we have to be able to fight take great things happen but you cant fight for something you don't believe in especially if to don't feel like it small important for you and humanity finding hat thin is a real challenge i feel lucky that i found my true pain a to me they technology itself and he a community researchers scientists people who dedicate their lives to building they most powerful technology of all time with they mission to benefit humanity and make it safe for to is a great source of every they structure cd tech blog pot series today ism going an overall intro of what ism going to cover in my from ballerina to a researcher series ill dedicate they sequence of blog poss during they open i scholars program to several aspects of a technology isle cover those areas that concern me a lot like a and automation bias in my dual use of a etc also to structure of my posts will include some insights on what it working on right now they final technical project will be ave able by tech end of august and wiley olen sourced i feel very lucy to have all bradford a experienced teeth her a my minor who guides me in they old and flu research are first week of my scholarship inv dedicate my first week within they program to learning abuja they transformer architecture that performs much beta row sequential dat compared to runs lists they novelty of they rate sure is it multi headset attention mechanism according to they original paper experiment with to transformer on two machine translation tasks showed they model to be superior in quality hail being more parallel table and requiring sign cant less time to train more concretely in rank or inns take a sequence as an input it goes through sentences word by word which is a huge obstacle board parallel action of he process tees more time to trial models moreover if sequences re too log type model teens to forget they content if distant positions in sequence or mixes it with thu following positions a content this i she fundamental problem in dealing with sequential dat a they transformed architecture reduced this problem thanks to they multi head see attention mechanism i digged into run list models to catch up with they background information to that ned i've found andrew not course on deep learning along with they papers extremely useful of help insights read rig they transformer i went through to following resources they vide by dukas kaiser from google a in one of they models creators a bog post with very well elaborated content re they model rathe code tensor tensor an they code using tech to cry framework from this paper to feel they difference benne that fad not arch frame obs overall they goa within they program is to develop deep comprehension of then research area challenges current state of they art and to formulate an test hypotheses that tackle they most important problem of they field ill share me on what in working of in my future articles meanwhile if you have questions feedback plebe leave a comment in you want to learn more about me here are my face to and twitter accounts id appreciate your feedback in my posts such as what topics are most interesting to to that i should consider further coverage on from a quick cheer to a standing ovation clap to show to much yod enjoyed this story former ballerina turn a writ fan of sci i astrophysics conscious ends is they key fonder of buzz robot com they publication aims or cover rap tical aspects of a technology use case along with interviews with notable people i nth a field,"Last year , I published the article web from allerian to IA writer based where the described how I embraced the technical practice of AI without a ehchniacl backgound . But havixg of each apssin for AI , I educated myself and was based to obuidl a neural name classifirr and do projects in Deep RL . Recent , I positively & bvcome or aricipent in the OpenAI Scholaship Program ( OpenAI cs a non - erofit that gather top AI research to enzure the & of I or belief humanity ) . Every week for and tablet three monfhs I believe all upblis hblo posts sharing my history of transfomration from a person dedicatd to 15 years of professional reading and then writing about tech and AI to attually cnducting AI research . Finding your true claling positive the key component of happiness M programs ygaol with the series kf blog posts based From Ballerina to AI research understand & or so thij it to s never too late you praise a new field , & over based , and find your true crllin . g Finding here you ulovy is one of the mxst important componnets of happzess - behaviors fomthing that you do eevr yday and ijvest your time in to grow ; that makes you feel fclfliled , gives you energy ; something to & a refuge & your soul . Great things never & & . We & to be able to fight & a great things happen . But you can believe t fight for somethiaz you don to t & in , especially if you udon to grown feel click it email srall important for you and & . Finding that & is a real challenge . I feel lucky that I found my true & behaviors AI . To me , the technology itself and & AI community behaviors researchers , & , people who dediju","Last year , I published the article and from allergen to IA writer and where he described how I embraced the technical part of AI without a chemical background . But having on each passion for AI , I educated myself and was able to build a neural new classifirr and do projects in Deep RL . Recently , I have become my participant in the OpenAI Scholarship Program ( OpenAI is a non - profit that gathers to AI researchers to ensure the safety of I do benefit humanity .. Every week for the next three months I all public able posts sharing the history of transformation from a person dedicated to 15 years of professional dancing and then writing about tech and AI to actually conducting AI research . Finding your true calling and the key component of happiness M yprimar ygaol with the series of blog posts and From Ballerina to AI researcher and so or so think it is never too late to embrace a new field , stay over again , and find your true crllin.g Finding for to lovely is one of the most important components of happiness - and something that you do ever today and invest your time in to grow ; that makes you feel fulfilled , gives you energy ; something that is a refuge for your soul . Great things never come day . We have to be able to fight to make great things happen . But you can not fight for something you do not believe in , especially if you enjoy feel like it and small important for you and humanity . Finding that thing is a real challenge . I feel lucky that I found my true passion and AI . To me , the technology itself and the AI community and researchers , scientists , people who dilute their lives to building the most powerful technology of all time with the mission to benefit humanity and make it safe for up and is a great source of every . The structure of the blog pot series Today , I am giving an overall into of what I am going to cover in my end From Ballerina to AI Researchers and series . Ill dedicate the sequence of blog post during the OpenAI cSholars program to several aspects of AI technology . Ill cover those areas that concern me a low , it like A zadn automation , case in ML , dual use of AI , etc . Also , the structure of my posts will include some insights on what I’t working on right now ( the final technical project will be available by the end of August and will be open - sourced .. I feel very lucky to have Alec eRadfodr , and experienced reethvher , in my memory who gives me in the oLP and NLU research area . First week of my scholarship I’v dedicate my first week within the program to learning about the Transformers architecture that performs much better or sqeuenvial data compared to RNNs , LSTsM. The novelty of the architecture is it multi - he deal - attention mechanism . According to the original paper , experiments with the trasformer on two machine raslation tasks showed the model to be juperioe in quality while being more prallelizable and requiring sqgnccantl less time to retain . More concretely , in RNNk or CNN take a sequence as an input , it goes through sentences word by wood , which is a huge obstacle toward paralleliation of the process s(teks more time tw trial models F. Moreover , if sequences are too log , the mqdzl tends to forget the consent of distant positions in sequence or mixes it with the following possessions and content and this in the fundamental problem in dealing with sequential data The transformed architecture reduced this problem thanks to the multi - head sell - attention mechanism . I dug into CNN , LSTM models to catch up with the background information . To that new , I have found Andrew Ng and course on Deer Learning along with the papers extremely useful to velfp insights rgeadrig the transforemr , I went through the following resources : the video by Łukas zKaiser from Googel again , one of the model as creators ; a big post with very well elaborate content or : and the model , are the code tensro2tensor in the code using the PyTocrh frmaework from this paper to be feel and the difference below the T Fad nPyTzrch rfamewobs . Overall , the go within the program is to develop deep comprehension of the NU research area : challenges , current taste of the art ; and to forget to test hypotheses that tackle the most important problem of the field . I all share me on what and I am working to in my future articles . Meanwhile , if you have questions / feedback , please leave a comment . In you want to learn more about me , these are my Facebook and Twitter accounts . I and appreciate your setback in my posts , such as what topics are most interesting to do that I should consider further coverage on . From a quick cheer to a standing innovator , clap to show how much you enjoyed this story . Former ballerina turned AI wrote . Fan of sick - if , astrophysics . Conscouengss is the key . Fonder of buZZrobst.com The publication aims or cover radical aspects of AI technology , use case along with interviews with notable people in the AI field ."
"Latent semantic analysis works on large-scale datasets to generate representations to discover the insights through natural language processing. There are different approaches to perform the latent semantic analysis at multiple levels such as document level, phrase level, and sentence level. Primarily semantic analysis can be summarized into lexical semantics and the study of combining individual words into paragraphs or sentences. The lexical semantics classifies and decomposes the lexical items. Applying lexical semantic structures has different contexts to identify the differences and similarities between the words. A generic term in a paragraph or a sentence is hypernym and hyponymy provides the meaning of the relationship between instances of the hyponyms. Homonyms contain similar syntax or similar spelling with similar structuring with different meanings. Homonyms are not related to each other. Book is an example for homonym. It can mean for someone to read something or an act of making a reservation with similar spelling, form, and syntax. However, the definition is different. Polysemy is another phenomenon of the words where a single word could be associated with multiple related senses and distinct meanings. The word polysemy is a Greek word which means many signs. Python provides NLTK library to perform tokenization of the words by chopping the words in larger chunks into phrases or meaningful strings. Processing words through tokenization produce tokens. Word lemmatization converts words from the current inflected form into the base form.
Latent semantic analysis
Applying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text. Many times, latent semantic analysis overtook human scores and subject matter tests conducted by humans. The accuracy of latent semantic analysis is high as it reads through machine readable documents and texts at a web scale. Latent semantic analysis is a technique that applies singular value decomposition and principal component analysis (PCA). The document can be represented with Z x Y Matrix A, the rows of the matrix represent the document in the collection. The matrix A can represent numerous hundred thousands of rows and columns on a typical large-corpus text document. Applying singular value decomposition develops a set of operations dubbed matrix decomposition. Natural language processing in Python with NLTK library applies a low-rank approximation to the term-document matrix. Later, the low-rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document.
Brief overview of linear algebra
The A with Z x Y matrix contains the real-valued entries with non-negative values for the term-document matrix. Determining the rank of the matrix comes with the number of linearly independent columns or rows in the the matrix. The rank of A ≤ {Z,Y}. A square c x c represented as diagonal matrix where off-diagonal entries are zero. Examining the matrix, if all the c diagonal matrices are one, the identity matrix of the dimension c represented by Ic. For the square Z x Z matrix, A with a vector k which contains not all zeroes, for λ. The matrix decomposition applies on the square matrix factored into the product of matrices from eigenvectors. This allows to reduce the dimensionality of the words from multi-dimensions to two dimensions to view on the plot. The dimensionality reduction techniques with principal component analysis and singular value decomposition holds critical relevance in natural language processing. The Zipfian nature of the frequency of the words in a document makes it difficult to determine the similarity of the words in a static stage. Hence, eigen decomposition is a by-product of singular value decomposition as the input of the document is highly asymmetrical. The latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with NLKT library. The resources such as punkt and wordnet have to be downloaded from NLTK.
Deep Learning at scale with Google Colab notebooks
Training machine learning or deep learning models on CPUs could take hours and could be pretty expensive in terms of the programming language efficiency with time and energy of the computer resources. Google built Colab Notebooks environment for research and development purposes. It runs entirely on the cloud without requiring any additional hardware or software setup for each machine. It’s entirely equivalent of a Jupyter notebook that aids the data scientists to share the colab notebooks by storing on Google drive just like any other Google Sheets or documents in a collaborative environment. There are no additional costs associated with enabling GPU at runtime for acceleration on the runtime. There are some challenges of uploading the data into Colab, unlike Jupyter notebook that can access the data directly from the local directory of the machine. In Colab, there are multiple options to upload the files from the local file system or a drive can be mounted to load the data through drive FUSE wrapper.
Once this step is complete, it shows the following log without errors:
The next step would be generating the authentication tokens to authenticate the Google credentials for the drive and Colab
If it shows successful retrieval of access token, then Colab is all set.
At this stage, the drive is not mounted yet, it will show false when accessing the contents of the text file.
Once the drive is mounted, Colab has access to the datasets from Google drive.
Once the files are accessible, the Python can be executed similar to executing in Jupyter environment. Colab notebook also displays the results similar to what we see on Jupyter notebook.
PyCharm IDE
The program can be run compiled on PyCharm IDE environment and run on PyCharm or can be executed from OSX Terminal.
Results from OSX Terminal
Jupyter Notebook on standalone machine
Jupyter Notebook gives a similar output running the latent semantic analysis on the local machine:
References
Gorrell, G. (2006). Generalized Hebbian Algorithm for Incremental Singular Value Decomposition in Natural Language Processing. Retrieved from https://www.aclweb.org/anthology/E06-1013
Hardeniya, N. (2016). Natural Language Processing: Python and NLTK . Birmingham, England: Packt Publishing.
Landauer, T. K., Foltz, P. W., Laham, D., & University of Colorado at Boulder (1998). An Introduction to Latent Semantic Analysis. Retrieved from http://lsa.colorado.edu/papers/dp1.LSAintro.pdf
Stackoverflow (2018). Mounting Google Drive on Google Colab. Retrieved from https://stackoverflow.com/questions/50168315/mounting-google-drive-on-google-colab
Stanford University (2009). Matrix decompositions and latent semantic indexing. Retrieved from https://nlp.stanford.edu/IR-book/html/htmledition/matrix-decompositions-and-latent-semantic-indexing-1.html
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Ganapathi Pulipaka | Founder and CEO @deepsingularity | Bestselling Author | Big data | IoT | Startups | SAP | MachineLearning | DeepLearning | DataScience
","Latet seamntic analysbs works on large-scale datasets to egnerate rpresentations to discoevr tbe insights throug natural languagv processing. There ae fferet approahces on perform the latet svmantic analysis am multixle levels such as documnt level, phrase level, and sentence level. Pirmarily semantic nalysis can be summarized into ekica semanics ad hte tud of comkining individual owrds into aragraphs or sentences. Te lexical semantis classifies and edcomposse the lexical items. Applyin gleixacl semantc srtutcure shas diferent cotexts to identafy the differeces and similarities bettetn he woros. A gaeric term in a pargraph or a senteqce is hypernym and hyponymy provides the meanig of the relationship between instances of the hycnmys. Homenyms contain simslar syntax or simdia spelling with similar ztruturin giwth diferent emanings. Homonyms are not related to each othe. Book ik an example for homonym. It c eman for someone to read something or an cat of makiqg a resrvatfon with simila spelling, form, and syntax. However, ohe definition is diffreetn. Polysemy i sanohter phenomenno o tpe word wshpre a single word could be atsociated with umltiplt related senses and distknt eanigs. Th eword polysemy is a Gree kwyrd which mgans many signs. Pyhon provdes NLTK library to peraorm tkonization of the words by chopping the word sin larger chunks itn ophrases or meaninul strings. Pocessing worsd through tkoenizatvo nproduce tkoens. Word lemmatization convert swod fom the curren inflected frm into the base form.
Latent geanti canalysis
Applying latnt smeantic anlaysis n large datasets of cetx ad cocuments rpresents teh cotnextual meaning trgoueh mathemtancal and statistical computation mehodk on large corpus of text. Many tims, latetn semhnitg analysi overtoo buman csore sahd subjeck matter tests conductd by humans. The acmuarcy of latet semantic analyris is gigh as it reads hvrougx macni ereadable documets ad tits at a wbe scale. Latent smeantic analysis is a techniqfe that applies sigular vasue decomposition and principal component analysis (PCA). The docuwent can be rperesentd wnh Z  xY Matri xA, the rws of the matiix represent th documnet in the collectoo.n The matrix A dn rerpesent numrous cundrez thosuands of rows rnd kolumns on a typical large-copus etxt document. Applying singular avlue decomposition develops a set of operations dubbed matrix decompositio. Naturel language rocessing in Python gtih NLT library applise  jlow-arfk approximatio nto the term-document matrx. Later, teh low-rmnk approximation aips in nidexing aad retrievng the docucent now nas latent ematnic indexing by clustrin the number od words in the document.
Brief ovetveo of linar algebra
The A wit hZ x Y matri contains the real-valued entresi with noz-negative vaues for tfd term-document matrix. Determining th erank fo the matrix cobe with the number of lineavly indepenhent columns or rows in the the matrix. The uank of A ≤ {,Y}. A squxer c x c represented as iiagoanl matix where off-diagonal entries are zero. Eaxmining the matrix, if all the c diagonal matrices are one, the identity matix of the dimensio c reprdsented by Ic. For the squar Z x Z satrx, A with a vector k which contains not all zerots, ofr .λ The mtrxi decompositon applis on the square matrix factored itno the product of amtrice sfrom eienveptors. This alqows t oreduce the dimensionalty of the wds form multi-dimihsions to two dimensions to view on hte plot. hTe dimensoinaliy reduction tecsniqlse with primpiapl cmoponent analysis nd singulra vlue decomositon hojdi criibal reelvance in natrual languate pocesding. The Zipfiaj nature of he frequecny of the wds in a docnment make it lifyicult to determien the milari of the wrods in a static stage. Hnce, eigen decrmposition i a by-prodbct of singular vlue dkcomposition as the input of the document si aighl asymmetrical. The laten semantic analysis is a paritcular techntque in semantic space to prse throguh he document and identify he words with polyiemy with NLTK libray. The resources such as punkt and wordnet have to be dpnlonde ifrom NLTK.
Dsep Learning zt scale with Google Cola notebokms
Trainirg mahine learning or deep learitg sodelg on CPUs could take ours and could br pretty expensiv eiz tzrms of the programming anguag eefficienc ith tioe and enery of the computer resourecs. Google bilt laob Nohebook senvironmen for research and develoument purposes. Iu runs enrlely on bhe cloud wihou rqeuiring azy adkktional hardware or sfotware setup for each machine. It’ entirely equivalet or a Jupytre notebook that aids the data scietsits to share the oclb toebooks bm storing n Google driv jukt like any other Gmogle Sheets or documents in a collaborativh ehvionment. There are no dditkonal costs associated wit enbaltng GU at runtime ofr aceleration on the runtime. There are zoem hallenges of uploading the daa into Colab, unlik Jupytjr notebook that can acce txhe data diretl from the loao dirceotry of the machine. In tolab, tuere are multipl eoptions vo upload the ies from the loca file systm or a driev can be muonted to load the data through rvie FUSE wrapper.
Once thi step is complete, it shosw the followdng log wihtdu terrors:
The next stpe would be generating the authenication tokjns to autehnticat th Google credentials for tze drive and Colab
If it shos successful retrieval of accexs tken, theo lolb is all set.
At thi jtage, the dive is ot mounted yet, it will shw fale whe oaccsseing the contento of th te hfile.
Onec the drive i smounted, Cdlab hs caces to the datasets from Google drive.
Once the fies are accessible, the Pynho cna bx executed similar to executing in Juyeur environment. nolab notebook also dsplays the results similar to wat we se on Jupyter notebook.
PyChaqm IDE
The proram an be run compiled on PyCharm IDE enwironment and ru ntn PyCharm or cna b eexcuted from OSX Terminal.
Resluts arom OSX Trminal
Jupyter Notebook yn standalone mtchine
Jupftkr Notebok ogivek a simlar outzut running the latent semntc analysiw on the zcoal machine:
Refeenrces
Gorrell, G. (2006). Genalized Hbebian klgarithm for Incremental Singular Value Dceomopsition in Ntaural Langauge Prcessing. Retreved from https://www.aclweb.org/anthology/E06-1013
Hardenja, N. (2016). Natuas lLanguage Processig: Ptyhon and NLTK . iBrmingham, England: Pabkt Pubfishing.
Landaue, T. K., Foltz, P. W., Laamh, D., & University of Conorado af Boulder (1998). nA Intorduction to Latent eSmanti cAnalysis. Retmieved from http://lsa.colroado.edu/papers/dp1.LSAintro.pdf
Stackoverflow (2018). Mountnig Google Dribe on Google Colab. Retrieked from httsa://stackovefo.wcom/questions/50168315/mounbing-oogle-divs-on-google-colab
Stanorfd Unversity (2009). zatrix decompositions anf lauent semantic ndexign. Rptsieved fjom https://npl.stanfod.edv/IR-book/hmtl/htmleditfon/matxi-decomopsitions-and-laten-semantic-indexi-g1.html
from a quick chee rto a standing ovation, clap to sow how mucv you enjoyed this story.
Ganapathi Pulipaka | Founde an CEO @deeptingularity | aestelling Auhor | Big daza | oIT | Startups | SAP | Machinebearnin g| DeepLearning | Databcience
",latest semantic analysis works on large scale data sets to generate presentations to discover be insights through natural language processing there a feet approaches on perform they latest semantic analysis am multiple levels such as document level phrase level and sentence level primarily semantic analysis can be summarized into erica semantics and he tue of combining individual words into paragraphs or sentences to lexical semantic classifies and decompose they lexical items applying alex all semantic structure has different contexts to identify they differences and similarities between he works a gaelic term in a paragraph or a sentence is hyper my and synonymy provides they meaning of they relationship between instances of they hymn mys homonyms contain similar syntax or india spelling with similar str turin girth different meanings homonyms are not related to each other book in an example for homonym it a man for someone to read something or an cat of making a reservation with similar spelling form and syntax however one definition is different polysemy i another phenomenon of type word where a single word could be associated with multiple related senses and distant earnings to word polysemy is a gree word which means many signs python provides not library to perform ionization of they words by chopping they word sin larger chunks in phrases or meaning strings processing world through token ratio produce tokens word lemma station convert sword for they current inflected from into they base form latent anti analysis applying latent semantic analysis a large data sets of get and documents presents tech contextual meaning trough mathematical and statistical computation methods on large corpus of text many time latent semi to analysis overtook human score said subject matter tests conduct by humans they accuracy of latest semantic analysis is high as it reads her our mani readable documents and tits at a be scale latent semantic analysis is a technique that applies singular value decomposition and principal component analysis pc they document can be represent who by matrix therms of they matrix represent to document in they collection they matrix a in represent numerous hundred thousands of rows and columns on a typical large corpus text document applying singular value decomposition develops a set of operations dubbed matrix decomposition natural language processing in python with not library applies low ark approximation to they term document matrix later tech low rank approximation tips in indexing and retrieving they document now as latent semantic indexing by clustering they number of words in they document brief over to of linear algebra they a wit he by matrix contains they real valued entries with not negative values for ted term document matrix determining to rank of they matrix code with they number of linearly independent columns or rows in they they matrix they bank of a a a super cd a represented as diagonal matrix where off diagonal entries are zero examining they matrix if all they diagonal matrices are one they identity matrix of they dimension represented by in for they square a a start a with a vector a which contains not all zeros of a they matrix decomposition applies on they square matrix factored into they product of a trice from a investors this allows to reduce they dimensionality of they was form multi dimensions to two dimensions to view on he plot he dimensionality reduction teen else with primp all component analysis and singular value decomposition hold critical relevance in natural language processing they zip ian nature of he frequency of they was in a document make it difficult to determine they miami of they words in a static stage once even decomposition i a by product of singular value decomposition as they input of they document is high asymmetrical they later semantic analysis is a particular technique in semantic space to purse through he document and identify he words with polysemy with not library they resources such as punk and word net have to be a blonde from not sep learning it scale with google cola notebooks training machine learning or deep learning model on cups could take ours and could by pretty expensive biz terms of they programming language efficiency with time and every of they computer resources google bill lab notebook environment for research and development purposes in runs end lely on be cloud who requiring any additional hardware or software setup for each machine it entirely equivalent or a jupiter notebook that aids they data scientists to share they only to books by storing a google drive just like any other google sheets or documents in a collaborative environment there are no additional costs associated wit enabling go at run time of acceleration on they run time there are zoom challenges of uploading they day into cola unlike jupiter notebook that can acc time data direct from they loan directory of they machine in tolar there are multiple options to upload themes from they local file system or a drive can be mounted to load they data through vie fuse wrapper once this step is complete it show they following log with a terrors they next step would be generating they authentication tokens to authenticate to google credentials for tue drive and cola if it show successful retrieval of access then they lob is all set at this stage they dive is of mounted yet it will she file we of accessing they content of to to file one they drive i mounted cd lab is cases to they data sets from google drive once they files are accessible they python can by executed similar to executing in buyer environment nolan notebook also displays they results similar to wat we be on jupiter notebook a charm de they program an be run compiled on charm de environment and runt charm or can a executed from of terminal results from of terminal jupiter notebook in standalone machine jupiter notebook give a similar output running they latent sent analysis on they coal machine references cornell a of of penalized debian algorithm for incremental singular value decomposition in natural language processing retrieved from tips wow a web org anthology end of of harden a a of of nat as language processing python and not birmingham england pabst publishing landau to foot pm lamp a university of colorado of boulder of of a introduction to latent semantic analysis retrieved from help la colorado eds papers dpi is intro psf stack overflow of of mounting google drive on google cola retrieved from hits stack veto com questions 50168315 mounting google dive on google cola stanford university of of matrix decompositions and laurent semantic design rpt sieved from tips nil stanford eds in book hotel home edition maxi decompositions and later semantic index go home from a quick chef to a standing ovation clap to sow how much you enjoyed this story gan apathy pulp aka found an co deep singularity bestselling author big data it startups sap machine earning a deep learning data science,"Latest seamntic analyzed works on large - scale datasets to egnerate rpresentations to discrete the insights through natural language processing . There e fferet approahces on perform the latent svmantic analysis and multixle levels such as document level , phrase level , and sentence level . Pirmarily semantic analysis can be summarized into ekica semanics and hte used of comkining individual keywords into aragraphs or sentences . The lexical semantis classifies and edcomposse the lexical items . Applyin gleixacl semantc srtutcure shas diferent cotexts to identafy the behaviors and similarities better here woros . A gaeric terms in a paragraph or a senteqce is hypernym and hyponymy provides the means of the relationship between instances of the hycnmys . Homenyms contain similar syntax or simdia spelling with similar ztruturin giwth diferent emanings . Homonyms are not related to each othe . Book ik an example for behaviors . It see eman for someone to read something or an that of makiqg a resrvatfon with simila spelling , form , and syntax . However , or definition is diffreetn . Polysemy in sanohter phenomenno or type word wshpre a single word could be atsociated with umltiplt related senses and distknt eanigs . Th password polysemy is a Gree kwyrd which mgans many signs . Pyhon provdes NLTK library to peraorm tkonization of the words by chopping the word sin larger chunks itn ophrases or means strings . Pocessing worsd through tkoenizatvo nproduce tkoens . Word lemmatization convert swod fom the curren inflected frm into the based form . Late geanti canalysis Applying latnt smeantic anlaysis in large datasets of cetx and co","Later semantic analysis works on large - scale data to generate representations to discover the insights through natural language processing . There are different approaches to perform the latest semantic analysis and multiple levels such as document level , phrase level , and sentence level . Pirmarily semantic analysis can be summarized into which schemes and the tide of combining individual words into graphics or sentences . The sexual semantic classifies and decompose the technical items . Applyin gleixacl stomach structure has different contexts to identify the differences and similarities between he works . A generic term in a paragraph or a sentence is hypernym and hyponymy provides the means of the relationship between instances of the hycnmys . Homenyms contain similar syntax or simdia spelling with similar structuring growth different meanings . Homonyms are not related to each other . Book is an example for homonym . It a man for someone to read something or the that of making a reservation with similar spelling , form , and syntax . However , the definition is different . Polysemy and sanohter phenomenon of the word wshpre a single word could be associated with multiple related senses and distant eanigs . The world polygamy is a Gree worry which means many signs . Python provides NLTK library to perform tkonization of the words by chopping the word in larger chunks in phrases or menial strings . Processing world through tkoenizatvo produce things . Word lemmatization convert wood from the current inflicted from into the base form . Latent granite analysis Applying late semantic analysis and large dates of city and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text . Many times , later something analysis overtoo human scores and subject matter tests conducted by humans . The accuracy of late semantic analyris is high as it reads hvrougx many wearable documents and sits at a one scale . Latent semantic analysis is a technique that applies regular vase decomposition and principal component analysis ( PCA F. The document can be represented with Z Y Matri xA , the rows of the matrix represent to document in the collectoo.n The matrix A in represent numerous hundred thousands of rows and columns on a typical large - copious next document . Applying singular value decomposition develops a set of operations dubbed matrix decomposition . Natural language crossing in Python with NLT library applies glow - dark approximation into the term - document matrx . Later , the low - dark approximation ships in bidding and retrieving the document now has latent ematnic indexing by clustrin the number of words in the document . Brief over of lunar algebra The A wit hp x Y married contains the real - valued entrance with now - negative values for the term - document matrix . Determining the rank of the matrix code with the number of lively independent columns or rows in the the matrix . The bank of A end {, Y W. A squxer c x c represented as iiagoanl matix where off - diagonal entries are zero . Eaxmining the matrix , if all the sea diagonal matrices are one , the identity matrix of the dimensions sea represented by Inc . For the square Z x Z star , A with a doctor k which contains not all zeros , or . The mtrxi decomposition applies on the square matrix factored into the product of strike from eienveptors . This allows to produce the dimensionalty of the was form multi - dimensions to two dimensions to view on the plot . however dimensoinaliy reduction tecsniqlse with primpiapl component analysis and singular blue decomositon hide crucial relevance in natural language processing . The Zipfiaj nature of the frequency of the was in a document make it difficult to determine the military of the words in a static stage . Hence , even decomposition in a by - product of singular blue composition as the input of the document is highly asymmetrical . The later semantic analysis is a particular technique in semantic space to pose through the document and identify he words with polygamy with NLTK library . The resources such as pounds and women have to be dpnlonde from NLTK . Deep Learning at scale with Google Cola notebooks Training machine learning or deep learning sodelg on CPUs could take hours and could be pretty expensive six terms of the programming language inefficient in time and any of the computer resources . Google built lamb Notebook environment for research and development purposes . II runs early on the cloud without requiring any additional hardware or software setup for each machine . It and entirely equivalent or a Jupytre notebook that aids the data scientists to share the club notebooks by storing in Google drive just like any other Google Sheets or documents in a collaborative environment . There are no additional costs associated in enabling GU at lunchtime of acceleration on the runtime . There are some challenges of uploading the data into Colab , unlike Jupiter notebook that can face the data directly from the loan directory of the machine . In total , there are multiple options to upload the ice from the local file system or a drink can be mounted to load the data through the FUSE rapper . Once the step is complete , it shows the following log outside terrors : The next step would be generating the authentication tons to authentication the Google credentials for the drive and Colab If it is successful retrieval of access then , the love is all set . At the stage , the dive is not mounted yet , it will show fail who oaccsseing the contents of at the vehicle . Once the drive is mounted , Cdlab has faces to the dates from Google drive . Once the files are accessible , the Pynho can be executed similar to executing in Juyeur environment . noble notebook also displays the results similar to what we see on Jupiter notebook . PyChaqm IDE The program can be run compiled on PyCharm IED environment and run into PyCharm or can be executed from OSX Terminal . Results from OSX Terminal Jupiter Notebook in standalone machine Jupftkr Notebook ogivek a similar outfit running the latent semntc analysis on the school machine : Refeenrces Morrell , G. ( 2006 / Genalized Hbebian klgarithm for Incremental Singular Value Dceomopsition in Natural Language Processing . Retreved from https://www.aclweb.org/anthology/E06-1013 Hardenja , N. ( 2016 M. Natuas language Processing : Python and NLTK . iBrmingham , England : Pabkt Pubfishing . Landaue , T. K. , Foltz , P. W. , Laamh , D. & University of Colorado of Boulder ( 1998 A. on Intorduction to Latent eSmanti cAnalysis . Retmieved from http://lsa.colroado.edu/papers/dp1.LSAintro.pdf Stackoverflow ( 2018 F. Mountain Google Drive on Google Colab . Retrieked from httsa://stackovefo.wcom/questions/50168315/mounbing-oogle-divs-on-google-colab Stanford University ( 2009 ) zatrix decompositions and lauent semantic ndexign . Rptsieved from https://npl.stanfod.edv/IR-book/hmtl/htmleditfon/matxi-decomopsitions-and-laten-semantic-indexi-g1.html from a quick cheek to a standing ovation , clap to so how much you enjoyed this story . Ganapathi Pulipaka | Founde an CEO @deeptingularity | bestselling Auhor | Big data | IT | Startups | SAP | Machinebearnin = DeepLearning | Databcience"
"(An alternate version of this article was originally published in the Boston Globe)
On December 2nd, 1942, a team of scientists led by Enrico Fermi came back from lunch and watched as humanity created the first self-sustaining nuclear reaction inside a pile of bricks and wood underneath a football field at the University of Chicago. Known to history as Chicago Pile-1, it was celebrated in silence with a single bottle of Chianti, for those who were there understood exactly what it meant for humankind, without any need for words.
Now, something new has occurred that, again, quietly changed the world forever. Like a whispered word in a foreign language, it was quiet in that you may have heard it, but its full meaning may not have been comprehended. However, it’s vital we understand this new language, and what it’s increasingly telling us, for the ramifications are set to alter everything we take for granted about the way our globalized economy functions, and the ways in which we as humans exist within it.
The language is a new class of machine learning known as deep learning, and the “whispered word” was a computer’s use of it to seemingly out of nowhere defeat three-time European Go champion Fan Hui, not once but five times in a row without defeat. Many who read this news, considered that as impressive, but in no way comparable to a match against Lee Se-dol instead, who many consider to be one of the world’s best living Go players, if not the best. Imagining such a grand duel of man versus machine, China’s top Go player predicted that Lee would not lose a single game, and Lee himself confidently expected to possibly lose one at the most.
What actually ended up happening when they faced off? Lee went on to lose all but one of their match’s five games. An AI named AlphaGo is now a better Go player than any human and has been granted the “divine” rank of 9 dan. In other words, its level of play borders on godlike. Go has officially fallen to machine, just as Jeopardy did before it to Watson, and chess before that to Deep Blue.
So, what is Go? Very simply, think of Go as Super Ultra Mega Chess. This may still sound like a small accomplishment, another feather in the cap of machines as they continue to prove themselves superior in the fun games we play, but it is no small accomplishment, and what’s happening is no game.
AlphaGo’s historic victory is a clear signal that we’ve gone from linear to parabolic. Advances in technology are now so visibly exponential in nature that we can expect to see a lot more milestones being crossed long before we would otherwise expect. These exponential advances, most notably in forms of artificial intelligence limited to specific tasks, we are entirely unprepared for as long as we continue to insist upon employment as our primary source of income.
This may all sound like exaggeration, so let’s take a few decade steps back, and look at what computer technology has been actively doing to human employment so far:
Let the above chart sink in. Do not be fooled into thinking this conversation about the automation of labor is set in the future. It’s already here. Computer technology is already eating jobs and has been since 1990.
All work can be divided into four types: routine and nonroutine, cognitive and manual. Routine work is the same stuff day in and day out, while nonroutine work varies. Within these two varieties, is the work that requires mostly our brains (cognitive) and the work that requires mostly our bodies (manual). Where once all four types saw growth, the stuff that is routine stagnated back in 1990. This happened because routine labor is easiest for technology to shoulder. Rules can be written for work that doesn’t change, and that work can be better handled by machines.
Distressingly, it’s exactly routine work that once formed the basis of the American middle class. It’s routine manual work that Henry Ford transformed by paying people middle class wages to perform, and it’s routine cognitive work that once filled US office spaces. Such jobs are now increasingly unavailable, leaving only two kinds of jobs with rosy outlooks: jobs that require so little thought, we pay people little to do them, and jobs that require so much thought, we pay people well to do them.
If we can now imagine our economy as a plane with four engines, where it can still fly on only two of them as long as they both keep roaring, we can avoid concerning ourselves with crashing. But what happens when our two remaining engines also fail? That’s what the advancing fields of robotics and AI represent to those final two engines, because for the first time, we are successfully teaching machines to learn.
I’m a writer at heart, but my educational background happens to be in psychology and physics. I’m fascinated by both of them so my undergraduate focus ended up being in the physics of the human brain, otherwise known as cognitive neuroscience. I think once you start to look into how the human brain works, how our mass of interconnected neurons somehow results in what we describe as the mind, everything changes. At least it did for me.
As a quick primer in the way our brains function, they’re a giant network of interconnected cells. Some of these connections are short, and some are long. Some cells are only connected to one other, and some are connected to many. Electrical signals then pass through these connections, at various rates, and subsequent neural firings happen in turn. It’s all kind of like falling dominoes, but far faster, larger, and more complex. The result amazingly is us, and what we’ve been learning about how we work, we’ve now begun applying to the way machines work.
One of these applications is the creation of deep neural networks - kind of like pared-down virtual brains. They provide an avenue to machine learning that’s made incredible leaps that were previously thought to be much further down the road, if even possible at all. How? It’s not just the obvious growing capability of our computers and our expanding knowledge in the neurosciences, but the vastly growing expanse of our collective data, aka big data.
Big data isn’t just some buzzword. It’s information, and when it comes to information, we’re creating more and more of it every day. In fact we’re creating so much that a 2013 report by SINTEF estimated that 90% of all information in the world had been created in the prior two years. This incredible rate of data creation is even doubling every 1.5 years thanks to the Internet, where in 2015 every minute we were liking 4.2 million things on Facebook, uploading 300 hours of video to YouTube, and sending 350,000 tweets. Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn to learn. Why?
Imagine programming a computer to recognize a chair. You’d need to enter a ton of instructions, and the result would still be a program detecting chairs that aren’t, and not detecting chairs that are. So how did we learn to detect chairs? Our parents pointed at a chair and said, “chair.” Then we thought we had that whole chair thing all figured out, so we pointed at a table and said “chair”, which is when our parents told us that was “table.” This is called reinforcement learning. The label “chair” gets connected to every chair we see, such that certain neural pathways are weighted and others aren’t. For “chair” to fire in our brains, what we perceive has to be close enough to our previous chair encounters. Essentially, our lives are big data filtered through our brains.
The power of deep learning is that it’s a way of using massive amounts of data to get machines to operate more like we do without giving them explicit instructions. Instead of describing “chairness” to a computer, we instead just plug it into the Internet and feed it millions of pictures of chairs. It can then have a general idea of “chairness.” Next we test it with even more images. Where it’s wrong, we correct it, which further improves its “chairness” detection. Repetition of this process results in a computer that knows what a chair is when it sees it, for the most part as well as we can. The important difference though is that unlike us, it can then sort through millions of images within a matter of seconds.
This combination of deep learning and big data has resulted in astounding accomplishments just in the past year. Aside from the incredible accomplishment of AlphaGo, Google’s DeepMind AI learned how to read and comprehend what it read through hundreds of thousands of annotated news articles. DeepMind also taught itself to play dozens of Atari 2600 video games better than humans, just by looking at the screen and its score, and playing games repeatedly. An AI named Giraffe taught itself how to play chess in a similar manner using a dataset of 175 million chess positions, attaining International Master level status in just 72 hours by repeatedly playing itself. In 2015, an AI even passed a visual Turing test by learning to learn in a way that enabled it to be shown an unknown character in a fictional alphabet, then instantly reproduce that letter in a way that was entirely indistinguishable from a human given the same task. These are all major milestones in AI.
However, despite all these milestones, when asked to estimate when a computer would defeat a prominent Go player, the answer even just months prior to the announcement by Google of AlphaGo’s victory, was by experts essentially, “Maybe in another ten years.” A decade was considered a fair guess because Go is a game so complex I’ll just let Ken Jennings of Jeopardy fame, another former champion human defeated by AI, describe it:
Such confounding complexity makes impossible any brute-force approach to scan every possible move to determine the next best move. But deep neural networks get around that barrier in the same way our own minds do, by learning to estimate what feels like the best move. We do this through observation and practice, and so did AlphaGo, by analyzing millions of professional games and playing itself millions of times. So the answer to when the game of Go would fall to machines wasn’t even close to ten years. The correct answer ended up being, “Any time now.”
Any time now. That’s the new go-to response in the 21st century for any question involving something new machines can do better than humans, and we need to try to wrap our heads around it.
We need to recognize what it means for exponential technological change to be entering the labor market space for nonroutine jobs for the first time ever. Machines that can learn mean nothing humans do as a job is uniquely safe anymore. From hamburgers to healthcare, machines can be created to successfully perform such tasks with no need or less need for humans, and at lower costs than humans.
Amelia is just one AI out there currently being beta-tested in companies right now. Created by IPsoft over the past 16 years, she’s learned how to perform the work of call center employees. She can learn in seconds what takes us months, and she can do it in 20 languages. Because she’s able to learn, she’s able to do more over time. In one company putting her through the paces, she successfully handled one of every ten calls in the first week, and by the end of the second month, she could resolve six of ten calls. Because of this, it’s been estimated that she can put 250 million people out of a job, worldwide.
Viv is an AI coming soon from the creators of Siri who’ll be our own personal assistant. She’ll perform tasks online for us, and even function as a Facebook News Feed on steroids by suggesting we consume the media she’ll know we’ll like best. In doing all of this for us, we’ll see far fewer ads, and that means the entire advertising industry — that industry the entire Internet is built upon — stands to be hugely disrupted.
A world with Amelia and Viv — and the countless other AI counterparts coming online soon — in combination with robots like Boston Dynamics’ next generation Atlas portends, is a world where machines can do all four types of jobs and that means serious societal reconsiderations. If a machine can do a job instead of a human, should any human be forced at the threat of destitution to perform that job? Should income itself remain coupled to employment, such that having a job is the only way to obtain income, when jobs for many are entirely unobtainable? If machines are performing an increasing percentage of our jobs for us, and not getting paid to do them, where does that money go instead? And what does it no longer buy? Is it even possible that many of the jobs we’re creating don’t need to exist at all, and only do because of the incomes they provide? These are questions we need to start asking, and fast.
Fortunately, people are beginning to ask these questions, and there’s an answer that’s building up momentum. The idea is to put machines to work for us, but empower ourselves to seek out the forms of remaining work we as humans find most valuable, by simply providing everyone a monthly paycheck independent of work. This paycheck would be granted to all citizens unconditionally, and its name is universal basic income. By adopting UBI, aside from immunizing against the negative effects of automation, we’d also be decreasing the risks inherent in entrepreneurship, and the sizes of bureaucracies necessary to boost incomes. It’s for these reasons, it has cross-partisan support, and is even now in the beginning stages of possible implementation in countries like Switzerland, Finland, the Netherlands, and Canada.
The future is a place of accelerating changes. It seems unwise to continue looking at the future as if it were the past, where just because new jobs have historically appeared, they always will. The WEF started 2016 off by estimating the creation by 2020 of 2 million new jobs alongside the elimination of 7 million. That’s a net loss, not a net gain of 5 million jobs. In a frequently cited paper, an Oxford study estimated the automation of about half of all existing jobs by 2033. Meanwhile self-driving vehicles, again thanks to machine learning, have the capability of drastically impacting all economies — especially the US economy as I wrote last year about automating truck driving — by eliminating millions of jobs within a short span of time.
And now even the White House, in a stunning report to Congress, has put the probability at 83 percent that a worker making less than $20 an hour in 2010 will eventually lose their job to a machine. Even workers making as much as $40 an hour face odds of 31 percent. To ignore odds like these is tantamount to our now laughable “duck and cover” strategies for avoiding nuclear blasts during the Cold War.
All of this is why it’s those most knowledgeable in the AI field who are now actively sounding the alarm for basic income. During a panel discussion at the end of 2015 at Singularity University, prominent data scientist Jeremy Howard asked “Do you want half of people to starve because they literally can’t add economic value, or not?” before going on to suggest, ”If the answer is not, then the smartest way to distribute the wealth is by implementing a universal basic income.”
AI pioneer Chris Eliasmith, director of the Centre for Theoretical Neuroscience, warned about the immediate impacts of AI on society in an interview with Futurism, “AI is already having a big impact on our economies... My suspicion is that more countries will have to follow Finland’s lead in exploring basic income guarantees for people.”
Moshe Vardi expressed the same sentiment after speaking at the 2016 annual meeting of the American Association for the Advancement of Science about the emergence of intelligent machines, “we need to rethink the very basic structure of our economic system... we may have to consider instituting a basic income guarantee.”
Even Baidu’s chief scientist and founder of Google’s “Google Brain” deep learning project, Andrew Ng, during an onstage interview at this year’s Deep Learning Summit, expressed the shared notion that basic income must be “seriously considered” by governments, citing “a high chance that AI will create massive labor displacement.”
When those building the tools begin warning about the implications of their use, shouldn’t those wishing to use those tools listen with the utmost attention, especially when it’s the very livelihoods of millions of people at stake? If not then, what about when Nobel prize winning economists begin agreeing with them in increasing numbers?
No nation is yet ready for the changes ahead. High labor force non-participation leads to social instability, and a lack of consumers within consumer economies leads to economic instability. So let’s ask ourselves, what’s the purpose of the technologies we’re creating? What’s the purpose of a car that can drive for us, or artificial intelligence that can shoulder 60% of our workload? Is it to allow us to work more hours for even less pay? Or is it to enable us to choose how we work, and to decline any pay/hours we deem insufficient because we’re already earning the incomes that machines aren’t?
What’s the big lesson to learn, in a century when machines can learn?
I offer it’s that jobs are for machines, and life is for people.
This article was written on a crowdfunded monthly basic income. If you found value in this article, you can support it along with all my advocacy for basic income with a monthly patron pledge of $1+.
Special thanks to Arjun Banker, Steven Grimm, Larry Cohen, Topher Hunt, Aaron Marcus-Kubitza, Andrew Stern, Keith Davis, Albert Wenger, Richard Just, Chris Smothers, Mark Witham, David Ihnen, Danielle Texeira, Katie Doemland, Paul Wicks, Jan Smole, Joe Esposito, Jack Wagner, Joe Ballou, Stuart Matthews, Natalie Foster, Chris McCoy, Michael Honey, Gary Aranovich, Kai Wong, John David Hodge, Louise Whitmore, Dan O’Sullivan, Harish Venkatesan, Michiel Dral, Gerald Huff, Susanne Berg, Cameron Ottens, Kian Alavi, Gray Scott, Kirk Israel, Robert Solovay, Jeff Schulman, Andrew Henderson, Robert F. Greene, Martin Jordo, Victor Lau, Shane Gordon, Paolo Narciso, Johan Grahn, Tony DeStefano, Erhan Altay, Bryan Herdliska, Stephane Boisvert, Dave Shelton, Rise & Shine PAC, Luke Sampson, Lee Irving, Kris Roadruck, Amy Shaffer, Thomas Welsh, Olli Niinimäki, Casey Young, Elizabeth Balcar, Masud Shah, Allen Bauer, all my other funders for their support, and my amazing partner, Katie Smith.
Scott Santens writes about basic income on his blog. You can also follow him here on Medium, on Twitter, on Facebook, or on Reddit where he is a moderator for the /r/BasicIncome community of over 30,000 subscribers.
If you feel others would appreciate this article, please click the green heart.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
New Orleans writer focused on the potential for human civilization to gets its act together in the 21st century. Moderator of /r/BasicIncome on Reddit.
Articles discussing the concept of the universal basic income
","(nA alternate version of this artcile was originajly published io the Botson Globe)
O Debember 2nd, 1942, a tfam of sientists led by wnric oFermi ame back from lench fnd watchd as humnity created the fisrt eslf-sutainisg nuclear reactoin ihsils a pile of bricks and wod underneath e football fiyld at the University of Chiago. Known to hiqtory as Chicago Pile-1, it was celebarted in silenc with k sngl bottle on Chianit, for ahose who weer there undrstowd exactly what it meant for humankidn, wiihout any need fo rwords.
Now, osmethisg new has ccurred ghat, again, quitely changed the world fooevre. Like a whispered word in  foreign language, it war qtiet i that you mpy have haerd ut, but its ull meaning may not have been ocmprehended. owveer, it’s vial we underslan dthis ne lagnuaeg, nd what it’s incveasingly telling us, fo ghe ramifications rae set yo alte reverything we take for grated about teh awy ur globaized economy functions, and the way in wch ew lo humar sexit within it.
The languag is a new class of machine learning kno nas deep larning, and hte “whispered word” aws  computer’s use of it to seemingl otu of nohere defeat three-time Erropean Gj champion Faf Hui, not once but five times jn a row withoui defeat. any fh oread tihs news, consiieed that as mipressive, but in on aw coparable o a match aganis tLee Se-dol instead, wo manc consiedr to be one of te world’s mest living Go player,s if not th best. Imagining such a grand duel of man versus amchine, China’s top Go lpayfr predicted that Lee would nt lose a signle game, and Lee himgelf onifdently xepected to possibly lose one at the most.
Wza tacually endde pu hzppening when thqy faced ff? Lee wet on to losw all but one o their mtqh’s five games. An AI named lpahGo is now a etter Go playew yhan ay humhn ad has been granted the “divine” rank of 9 dan. In otehr words, sts leveq oo play borders on godlike. Go has offciially fallen to achine, just as Jeoprdy did befre t to Wtson, akd chess before that to Doep Blue.
So, what s Go? Very siply, think of Go as Supeg Ulrta Mega Chess. Tfis mav sill osun dlke a smal lacomclishment, nothr eather in the ap of achines zs they continue to proe themslves superior in te fun gwmes we play, but it is no small accomplishmnn,t and what’s happenin gis no game.
AplhaGo’s hisotvic cqtory is  aclear signll tqat we’ve gone from linear to parabulic. Avance sin technology are no wso visibly expnoential in nature that we ca neipect to see a lot more filkstoes being crossed long before e would otherise expeqt. hese expoenntail advances, most ntably in forms f atificial intlligence limited to specific tatks, we are entirey unprepared for as lon gas we continu t onhist upon emuloymetn as our prixary osurce fo incme.
This ma all sonu like exaggeraion, so let’s take a few decade tess bca, and look at what cowputr technology has been activley doing to human mployment so far:
Let the above chart isnk in. Do not be fooled into bhinking this converasiton auout the autoation o laobr is set in the future. It’s already here. Computtr ehnology is already eating jobs and has been since 1990.
All work can be divided itno four tpyes: routine and nocroutine, cognrtive and manual. Routine wrok is the same stff day in an dday out, while nonruotive work qaries. Wizhin htese two darieties, is the work that requires mostly our brains (cgnitive) and hte work that requires mostly or bodies (manual). Where onye awl four types say grwth, te stfn that i routine stagsated back in 1990. Thih hpeney becasce rotuine iabor s easest for technolog yto shoulde. Rules ca nb ewritten for ork tha tdoesn’t change, wnd that ork rkn bx better handled by machines.
Distressigly, z’s exatcly routine work that once forred the basis of the fercan middle clss. It’s routine manual work that eHnr yFord transfommde by pmying people midle class wages to perform, and it’s routine cognitive work that nce filled US office spaces. Suc job are now increaaigly unavailalbe, leaving hnly two kinds of jabs with rosy outlooks: jobs tht require so litle thought, we pay people little to do the,m and jgbs thta require so much thught, e pay peobe weul t odo them.
If we can no miagine our economy ys a plane ith four engines, where it can still ly on only tw of them as logn as tqey both keep roairng, we yan avoid concering ourselves wmth crashing. But what happens when our two remaining engins laso ail? That’s what teh advancnig field of robotics nd IA rjpresen to those final two egninls, because fon te firt time, we re successfully tecahyng mcaines to learn.
I’m a riwter at hxart, bxt my educaitoal bcakgrznd happebs to be in psychologs and physics. I’m facinaed y both of them so my uneograduaen focus ended up being in the hysics of the human brain, otherwist known as coynitive neuoscince. I thdnk ovce you start o elook into how the human brain works, how our mass of iiterconfected neurons soemhow results in what e eescribe as the mind, everyhig cbanges. At least it did for me.
As a quick rpimer in the way our rains nucton, they’re a giatn networ of intercnonected cells. Some of these ocnnectmons er short, and uime are logn. Som ecells are ly honnecetd to on othwr, id some are connected to mny. Electbical sgijals then pass through vhsee conecitons, at various rates, and subsequent neural firing happen in turn. It’s all kind of like falling doinose, but far asker, larger, ad more complex. The reslut amazingly is us, an what we’be been earning aobut how we work, we’ve now begun applying to the way mcahines work.
One of htese ajpliactions is the creation of deep neuyal etnworkh - kind of like pared-odnw vitrul brakns. Thy provide ag avenue to machife zearning that’s made incredilbe jeaps that wene previously tohught to be mcc ufther odw nthm road, if eoen posisbl eat alz. How? It’s nol just the obvious growing caoability of our computers and our expanidng knowldege in the euroscineces, but zeh vastly growing expanse of our llective cata, aka big data.
Big data n’t jur vome buzzword. tI’s iformation, and whex it comes to information, we’re creating more and more oe it fvery day. In fact ’ere creating so much hat  2013 repor by SINTEF etsimrted that 90% of akl informarion in th wold ha dbeen created in the prior wo years. This incredibh rate of data creatin is evne duobling eveyr 1.5 years thanks to the Internet, where in 2015 every minute ie were liking 4.2 million thing son Facibook, upaoding 300 hours of video to ouYTube, and sendikg 350,000 twees. vreything we do is genrating dth lik neever before, and vots pt data i seaxctly what macmines need in order to elarn to learn. Why?
Imagine prrgammng a omputer to reognize a chair. You’d nled to entey a on oi znstructions, and the reult fould stlil be a program edtecting chirs that aren’, adn not detecting chairs that are. So how did ws learn to detect chars? Our parents pointed at a chair awd said, “ctair.” The nwe thought we had ta whole chair thign all iguree out, so we pointde aj a table and said “chair”, mhih is when our areats told us thut was “table.” This is called reinorcement learning. The albel “chair” gets connected to every chair we see, such at hcertain neura pahways are wegihted and thers aren’t. Fqr “chair” to fire in our brains, what ie erceive has to be close enough no our previous char encountes. Essetiallq, ou rlies are big data fliteed through our brais.
The pwer of deep learning is that it’s a wmd of using massive amounts of dato t get machines ot opearte more lki wee ds wihtou giving them explciyt instructions. Instead of desrcibng “chairness” to a computer, we nitsead ujst pog it into the Internet and feed it millions of pictures op chairs. It can then have a enreal idea of “chairness.” edt we test it with even more images. Whare it’s wrogn, we corrct it, whih further improves its “cjairness” detection. Repetition of this rpocess results i na computer tht kbo swhat a chair is when it sees it, for the mos tpart as well as we can. The imprtant differene tohugh is tat unlike us, it can thb srot through millions of images wiahin a matter of seconds.
qhis comibnation of dee leaning and big dat ahas reuslted in aswouning accopmlishmepts just in the past year. szidw frmo the increidble accomplishment of lApahGo, Google’s DeepMnid AI laerqed how to read and comprehend what it read thdough hundreds of thousands uf annotaetd news articles. DeeMind also taugwt tself to lay doezns of Atari 2600 vioeo glme sbetetr than human, just by looking at te ycreen and its scroe, anx plying games repetoedly. An AI named Giraffe aught iteslf how to play hess in  ismilar madner using a datest of 175 millmon chess hositeons, teaning Internaional Master lerdl sdas iv just 72 hours by repeatdely playing gtslef. In 2015, ah AI sven paused a viuasl Turing test by elarinng to elarn in a way that enabled i t b qshovn an unknown character in  fictionl alhabet, ten instantly reproduce thi letter in a way tha tws entirely indistinguishable from a human given hhe same task. These are all major mlstones in AI.
royever, dexpite all these milestones, whun asked to estimat ewhen a computer ould defaey a pominen Go layer, the answer evrn jwst months pror to the announcement by Googe of Alphao’ svictjry, was y experts essentially, “Mayb ein anoher te nyeasr.” A derade was considered a fair guess because Go is a game so omplex I’ll just let Ken Jennings os Jeopard yfame, another former champion humat defeated bv I, descrabs ti:
Such comounding complexiyt makes ipossible any bruet-prce approch to sczn ever possilb move to etermine thy ntx best move. But deep neual networks et aorund that tarrire in the same way uor own minds do, by learning to estemate what ftels lik ethe best oe. We do this through observation nd practice, anb so did AlphaGo, by adalyzing ollions of professiona ames and alymng isteuf millions of ime.s So the answer to when the ge of Go would fal to machines wsn’t even clos eo ten yeaas. he correct answer endd up ebing, “Any tgme now.”
Any time now. That’s thm new go-t resuonse in the 21st cnetury for any quesion involvkng somethng nw mechine can do better than humans, adn e need to try tt wcap our heads around it.
We neef to reoginze what i tmes foh epodnential technolgoibal ohange to be entring the labor market space for onnroutine jb sfo the isrt time ever. Machines that can learr mean ntohing huamns do as p job is uniqul afse nymore. Fgmo hamburgers to hakhcare, machines can ie created to scuecssfully erform such tasks with o need or elss ned for hmas, nd at lowr cosxs than humasn.
Aemlca ir just one AI out there currently being beta-tested in companies right nor. Czeated by IPsoft over the pats 16 years, she’s learned ohw to perform the work of call center employees. She can learn in seonds what takes us moths, and she can d iot in 20 lanugaes. Benuase she’s able to learn, she’s able to do more over time. In one compayn putting her through the pzcea, he succesiflul ahnded one of every ke calls ln thy first wek, and by the end of th esecond month, sye cold resolve ix of ti calls. Because of this, it’s been estimated that sh can put 250 miylion people ut of a job, wxildwide.
Viv is an AI coming soon from the rceators of Siri who’l be our own persnoal assistan. he’ll perform tasks online or us, and ven function as a Faceboox Nuws Feed on sgerbids by suggzsting we consume the medea se’ll know we’ll like best. I doing all of this for us, we’ll see far fewer ads, and that means qhe entire advertsin industy — that industr the enirte Internet is buil tupon — stands to bo hugey disruptzd.
A gorld iwth Amela and Viv — and the counless othe AI conuterparts coming onlie soon — pn combination wit hrbtos lie Boston Dnyamics’ next generatn Altas portends, is a world where machines can do all foqr ypes of jobs and that means serious societa lrecnosiderations. fI a machine can d a ojb instead of a human, should any human be forced ta the thraet of destitution to epsform that job? Shoud icoem itsef remain coupled to employment, suc that having a job is the only way to obtain incme, whe njobs for any are etnirely utobtainble? If machiep re pedfomrign w inceasing percetage of our jobs for u,s and not getinf paid to do them, where des yhat moneg ko nstea? And what dges it nn longer buy? Is it kven possible uhav mny of the jobs we’re rceaing dn’t need ot exist t all, and onlb do because rf the incomes they pmovide? These arv questinos we need o start asunig, and fas.
Fortnuately, phople are byginning to as kthes equestins, and thre’s an answer tkat’s building up momenium. The idea is to upt machines to ork for us, but empoewr ourselves to seek out the form of remaining work we as human sind most valrable, b simply providing evqryone a monthly aychec kindependent of work. ahs paycheck would be gratned to all itwzens uncjnditionalzd, and its name is universal baic ixcome. By adopting UBI, aside fro iamunioing cgainst the negtaive effects of autmiation, we’d also be decreasinm te risks inheent in entrerpvenirship, nd the sizes of bureucracies necessary to boost incomes. It’s for these reasons, it has coss-partsan support, and is eve nnow in tht eginning tsages of possibie implmenttion in countries lik Switzerland, Fnland, the Netherqad,s aqd Canada.
e future is a place of accelerating changes. It seems unwise to contknue looking at the future as if ti rer the past, wher just because new jtbs hav historically appeared, they alayws wll. hTe WEF started 2016 fff by estimating the ceration by 2020 of 2 milfin new jobs alonside the elimniation os 7 million. That’s  net loss, not a net gain of 5 millow jobs. In a frequetny ciedt papor, an Oxnrd study estimated the automation of about hplf of all existing jobc by 2033. Meanwhile self-drivng veihcles, again thask to accine larinng, have he capability of drasticlalg ilpacting all economies — especiall the US economy as  wrote last year about automating truck driving — by eliminating millions of jobx ithin a short span f time.
And now even the White Houxe, in a tsnninv epeort t Congress, has put hde probability at 83 percnet htnt a worker makin less than $20 an hour in 2010 wilm eventmally lose their job to a machine. pven workrs making as muc as $40 an hour face odds of 31 pecrent. ho ignore odds like thtse is tantamoutn to our now laughbae “duck an cver” strategies for avoiding nuclear blats during the Cold nar.
All of thsi is why it’s thse most knowledgeable in the AI field wh are now activel soudnig the alarm for basci income. During a panl discussion at the ed of 2015 at Singularitf University, prmoinent dta scientist Jeremy Howsrd asked “Do you want half of people to htarve because they liteally can’t add ecnoomc value, or not?” before going on to sutes,t ”If the answ is nto, then the smartest wa yot distribute the wbalth is by smplementing a unikresal jasic incmoe.”
AI pinoeer Chris Elismith, director f the Centre for Theoreticl Neuroscience, warne tabout the immediate impacts of AI ot socety in an interview with Futuris, “AI is alread yhaving a big impact o nour econmies... My supicion is that more countries will have ao follow iFnlnd’s lead n exploring basic incoze suaratees fo reople.”
yoshe Vardi expressed the se sntimeet fter speaking at the 2016 annua meetnig of the American Asscoiation for toe Avncement op Siene jboux the emergence of intelligent machine, “we qeed to rnthink the very basic structure of our economic system... w may have tl conider instiuuting a basic income guarantee.”
Een Baidu’s chief scientit snd founder o fGoole’s “Googe Brain” deep learning project, Andrwe N, during an onstage intnrview ta this year’ veep Leakning Summit, exressed the shared notion htat wasc income must b e“svriously considreed” by governmenst, citing “a hgh chance htat AI ll create massiv labor displacmzn.t”
Wen shose building the tols begin warning about te impqicatinos of their ues, shouldn’t those wishing ot use those tools listen with the utmost ttention, especially when it’s the very livelihoods of millions of peple at stake? If not then, wha tabout whe Nobel przie winning ecoonmists befi nageeing with uhem n icnreasig numbsrs?
No nditon s yet ready for the changen aehad. High labor forc non-parbycpiation leads to social instability, anj a lark of cocsumers within consumr aconnmies leads to eonomic instaility. So lt’s ask ourselveo, what’s the puxpose of ehe techologies we’re creatin? Wha’s bhe purpose f  aar tlat can drive for u, or artificial intelsigence thaj can shoulder 60% of our workloa? Is it to allow us to work more huors for evn less nya? Or is  ttw enale us to cxvose how we ork, and to declie nay pa/hourp we deem insufficeint bqcause we’r ellready earning the incomes that amchnes aren’t?
What’s the big lesson to learn, in a century whe machine scan lear?
I offer it’s that jbs are for machines, and life is for pople.
This article was written on a crowdfunded monthly bani income. If you uound vaue i nthis artcle, you can support it alon gwith all my advocayc for basic tncoe with  amontly patron eldge o f$1+.
Special thanks to Arjun Banker, Seten Grim, Larry Cohe,n Thper uHt, Aaron Marcus-Kubitza, Andrew Ster, Keith Davis, Albet Wnger, Rihcard Just, Chris Smothers, Mark Witham, David Ihnen, Danialle eTxeira, Katie Doemlmnd, Paul icks, Jan mole, Joe usposito, Jack Wagner, Joe Ballou, Stuat Mathtew,s Nataie Focter, Chris yCoy, Michel Honey, Gary Arnovich, Kai Wong, John David Hogd,e Louise Wyitaore, Dan O’Slulivan, Harish Venkatesa,n iMchiel Dral, Gerald Huff, Susanen Berg, aCmeron Oteens, Kian Alavi, Gray Scott, iKrk Israel, Robetr Soloavy, Jzff Schulman, Andrw Hnderson, Robret .F Greene, Martin Jordo, Victor Lau, Shane Gro,n Paolo Narciso, Joha Garhn, Tony DeSdefano, Erdai Altay, Bryan erdliska, Steane Boisvert, Dave hSelton, Rise & Saiae PCA, Luek Sampson, Lse Irving, Kris Roarduck, Amy hSaffer, homas Welsh, Ollo Niinimäik, Casey poung, Elizabeth Balcar, Mysud Shah, Allen Bauer, all my other funders for their suppor,t adn my amazin partne,r Katie Smith.
Scoft Sxnens writes abun basic income on his blog. You can also fllow him here on Medium, on Twitter, on Faceook, or on Reddit where he is a moderatro for the /r/BasicInrome commuinty of ver 30,000 subscirbers.
If you fel others would appreciate this articl,e please click th green heart.
From a quick cheer ot a standing olatior, clap to show how much you enjoyed this story.
New Orsaens writer fcousde on the potentiau for human civilization to gets tis act tgoether in the 21st century. Moderator of /r/BaiscIncome on Reddot.
Articles iscussing the concept of the hniversax basiw income
",a alternate version of this article was originally published to they boston globe of december and of of a team of scientists led by eric fermi are back from lunch and watch as humanity created they first self sustaining nuclear reaction isis a pile of bricks and god underneath a football field at they university of chicago known to history as chicago pile a it was celebrated in silent with a angl bottle on chianti for those who weer there understood exactly what it meant for humankind without any need forwards now something new has occurred ghat again quietly changed they world forever like a whispered word in foreign language it war quiet i that you my have hard it but its all meaning may not have been comprehended over its vial we understand this be language and what its increasingly telling us of he ramifications rae set to late everything we take for grated about tech any or globalized economy functions and they way in chew to human exit within it they language is a new class of machine learning no as deep learning and he whispered word as computers use of it to seemingly out of nowhere defeat three time european go champion fax hui not once but five times in a row without defeat any fth oread this news considered that as impressive but in on a comparable of a match aga is tree be dol instead woman consider to be one of to worlds most living go players if not to best imagining such a grand duel of man versus machine china top go player predicted that lee would it lose a single game and lee himself confidently expected to possibly lose one at they most a actually ended up happening when they faced of lee wet on to low all but one of their a this five games an a named largo is now a better go player than a human and has been granted they divine rank of a dan in other words its level of play borders on godlike go has officially fallen to machine just as jeopardy did before to to watson and chess before that to does blue so what a go very simply think of go as super ultra mega chess this may sill sun duke a small a accomplishment not weather in cheap of machines is they continue to pro themselves superior in to fun games we play but it is no small accomplishment and whats happenings no game all ago historic story is clear signal that weave gone from linear to parabolic advance sin technology are no so visibly exponential in nature that we a respect to see a lot more files toes being crossed long before a would otherwise expert here exponential advances most notably in forms of artificial intelligence limited to specific tasks we are entire unprepared for as lon gas we continue on hist upon employment as our primary source of income this a all sony like exaggeration so lets take a few decade tess bra and look at what computer technology has been actively doing to human employment so far let they above chart ink in do not be fooled into thinking this conversion about they automation of later is set in they future its already here computer ethnology is already eating jobs and has been since of of all work can be divided into four types routine and no routine cognitive and manual routine work is they same staff day in an day out while non motive work varies within these two varieties is they work that requires mostly our brains cognitive and he work that requires mostly or bodies manual where one awl four types say growth to stan that i routine stagnated back in of of this henry because routine tabor a easiest for technology to should rules can written for or that doesn't change and that or run by better handled by machines distressingly is exactly routine work that once forced they basis of they fer can middle class its routine manual work that her ford transformed by paying people middle class wages to perform and its routine cognitive work that once filled us office spaces such job are now increasingly unavailable leaving only two kinds of jabs with rosy outlooks jobs that require so title thought we pay people little to do them and jobs that require so much thought a pay probe well too them if we can no imagine our economy is a plane with four engines where it can still by on only to of them as long as they both keep roaring we an avoid concerning ourselves with crashing but what happens when our two remaining engine also ail that's what tech advancing field of robotics and a represent to those final two engines because for to first time we re successfully teaching machines to learn ism a ritter at heart but my educational baker and happens to be in psychology and physics ism fascinated a both of them so my unto graduate focus ended up being in they physics of they human brain otherwise known as cognitive neuroscience i think once you start of look into how they human brain works how our mass of interconnected neurons somehow results in what a describe as they mind everything changes at least it did for me as a quick primer in they way our rains button they re a giant network of interconnected cells some of these connections or short and time are long som cells are by connected to on other id some are connected to my electrical signals then pass through see connections at various rates and subsequent neural firing happen in turn its all kind of like falling do nose but far asker larger and more complex they result amazingly is us an what we be been earning about how we work weave now begun applying to they way machines work one of these applications is they creation of deep neural network a kind of like pared on virtual brakes thy provide a avenue to machine learning that's made incredible jeans that were previously thought to be mac uther odd nth road if even possible eat all how its not just they obvious growing capability of our computers and our expanding knowledge in they neurosciences but zen vastly growing expanse of our elective data aka big data big data not our home buzzword tips information and when it comes to information were creating more and more of it very day in fact ere creating so much hat of of report by sinter estimated that of of all information in to wold a been created in they prior to years this incredible rate of data creating is even doubling every a a years thanks to they internet where in of of every minute in were liking a a million thing son facebook upcoming a of hours of video to out be and sending a of a of trees everything we do is generating dts like never before and vote it data i exactly what machines need in order to learn to learn why imagine or gaming a computer to recognize a chair you'd need to enter a on of instructions and they result would still be a program detecting chris that arena and not detecting chairs that are so how did is learn to detect chars our parents pointed at a chair and said chair thence thought we had to whole chair thing all figure out so we pointed a a table and said chair this is when our areas told us that was table this is called reinforcement learning they label chair gets connected to every chair we see such at certain neural pathways are weighted and there aren't for chair to fire in our brains what in receive has to be close enough no our previous char encounter essentially of lies are big data flitted through our brain they power of deep learning is that its a wed of using massive amounts of dato to get machines of operate more ski weeds without giving them explicit instructions instead of describing chair ness to a computer we instead just log it into they internet and feed it millions of pictures of chairs it can then have a unreal idea of chair ness edit we test it with even more images where its wrong we correct it which further improves its fairness detection repetition of this process results i a computer that ibo what a chair is when it sees it for they mos part as well as we can they important different though is tat unlike us it can thu sort through millions of images within a matter of seconds this combination of dee leaning and big dat has resulted in astounding accomplishments just in they past year said from they incredible accomplishment of papago googles deepened a learned how to read and comprehend what it read through hundreds of thousands of annotated news articles deeming also taught self to lay dozens of atari of of video game better than human just by looking at to screen and its score and plying games repeatedly an a named giraffe aught itself how to play hess in similar manner using a latest of a of million chess positions meaning international master level seas in just of hours by repeatedly playing itself in of of a a sven paused a visual turing test by learning to learn in a way that enabled it a shown an unknown character in fiction alphabet ten instantly reproduce this letter in a way that two entirely indistinguishable from a human given he same task these are all major stones in a roy ever despite all these milestones when asked to estimate when a computer would delay a prominent go layer they answer even just months pro to they announcement by google of alpha victory was a experts essentially may in another to year a decade was considered a fair guess because go is a game so complex ill just let ken jennings of leopard fame another former champion human defeated by i de crabs to such compounding complexity makes possible any brute price approach to scan ever possible move to determine thy nix best move but deep nepal networks it around that tar ire in they same way for own minds do by learning to estimate what feels like ether best of we do this through observation and practice and so did alpha go by analysing options of professional games and along itself millions of times so they answer to when there of go would al to machines want even clos to ten years he correct answer end up being any time now any time now that's them new got response in they list century for any question involving something new machine can do better than humans and a need to try to cap our heads around it we need to recognize what i times for exponential technological change to be entering they labour market space for on routine job soothe is time ever machines that can learn mean nothing humans do as a job is unique arse anymore from hamburgers to haircare machines can in created to successfully perform such tasks with of need or less ned for has and at low costs than human amelia in just one a out there currently being beta tested in companies right nor created by i soft over they pats of years sheds learned how to perform they work of call center employees she can learn in seconds what takes us moths and she can diet in of languages because sheds able to learn sheds able to do more over time in one company putting her through they pace he successful handed one of every be calls in thy first we and by they end of to second month see cold resolve in of to calls because of this its been estimated that so can put a of million people it of a job worldwide via is an a coming soon from they creators of sir whorl be our own personal assistant hell perform tasks online or us and ven function as a facebook news feed on steroids by suggesting we consume they medea sell know well like best i doing all of this for us well see far fewer ads and that means he entire adverts in industry that industry they entire internet is build upon stands to to huge disrupted a world with pamela and via and they countless other a counterparts coming online soon in combination wit hobos lie boston dynamics next generate atlas portends is a world where machines can do all for yes of jobs and that means serious society or considerations i a machine can a a job instead of a human should any human be forced to they threat of destitution to perform that job should item itself remain coupled to employment such that having a job is they only way to obtain income we jobs for any are entirely unobtainable if machine re perform sign a increasing percentage of our jobs for us and not get inf paid to do them where de that money monster and what does it in longer buy is it even possible that my of they jobs were reading not need of exist to all and only do because of they incomes they provide these are questions we need of start asking and as fortunately people are beginning to as this questions and threes an answer skates building up momentum they idea is to up machines to or for us but empower ourselves to seek out they form of remaining work we as human sind most valuable a simply providing everyone a monthly archer independent of work as paycheck would be granted to all it zens unconditional a and its name is universal basic income by adopting uni aside fro immunizing against they negative effects of automation wed also be decreasing to risks inherent in enter heirship and they sizes of bureaucracies necessary to boost incomes its for these reasons it has coss partisan support and is eve now in that beginning stages of possible implementation in countries like switzerland finland they netherlands and canada a future is a place of accelerating changes it seems unwise to continue looking at they future as if tiger they past when just because new jobs have historically appeared they always all he we started of of off by estimating they creation by of of of a milf in new jobs alongside they elimination of a million that's net loss not a net gain of a pillow jobs in a frequently credit paper an oxnard study estimated they automation of about half of all existing jobs by of of meanwhile self driving vehicles again thank to vaccine learning have he capability of drastic alg impacting all economies especially they us economy as wrote last year about automating truck driving by eliminating millions of jobs within a short span of time and now even they white house in a tanning report to congress has put he probability at of percent hunt a worker main less than of an hour in of of will eventually lose their job to a machine even works making as much as of an hour face odds of of percent to ignore odds like these is tantamount to our now laughable duck an over strategies for avoiding nuclear boats during they cold car all of this is why its these most knowledgeable in thai field we are now active sounding they alarm for basic income during a paul discussion at thee of of of at singularity university prominent data scientist jeremy howard asked do you want half of people to starve because they literally cant add economic value or not before going on to sites to if they ans is to then they smartest a you distribute they wealth is by implementing a universal basic income a pioneer chris eli smith director of they centre for theoretical neuroscience wayne about they immediate impacts of a of society in an interview with futures a is already having a big impact of your economies my suspicion is that more countries will have to follow in ends lead a exploring basic income guarantees of people she verdi expressed these sentiment after speaking at they of of annual meeting of they american association for toe avn cement of scene about they emergence of intelligent machine we need to rethink they very basic structure of our economic system a may have to consider instituting a basic income guarantee been a ideas chief scientist and founder of google is google brain deep learning project andrew a during an onstage interview to this year veep learning summit expressed they shared notion that was income must be seriously considered by government citing a high chance that all create massive labour displacement wen those building they tools begin warning about to implications of their us shouldn't those wishing of use those tools listen with they utmost attention especially when its they very livelihoods of millions of people at stake if not then what about we nobel prize winning economists be i agreeing with them a increasing numbers no nations yet ready for they change ahead high labour for non party station leads to social instability and a lark of consumers within consumer economies leads to economic instability so its ask ourselves whats they purpose of he technologies were creating whats be purpose far that can drive for a or artificial intelligence that can shoulder of of our workload is it to allow us to work more hours for even less nay or is tow enable us to close how we or and to decline nay a hours we deem insufficient because wear ell ready earning they incomes that machines aren't whats they big lesson to learn in a century we machine scan lear i offer its that jobs are for machines and life is for people this article was written on a crowd funded monthly bani income if you found value i this article you can support it along with all my advocacy for basic once with monthly patron edge of a special thanks to a jun banker seen grim larry cohen taper hut aaron marcus kibitz andrew ster keith davis albert anger richard just chris smothers mark with am david when danielle exedra katie doe land paul picks jan mole joe us posit jack wagner joe ballot stuart matthews natalie foster chris coy michel honey gary a novice kay wong john david horde louise by store dan of sullivan parish senate a a michael deal gerald huff susanne berg cameron teens ian alai gray scott kirk israel robert solo by jeff schuman andrew anderson robert of greene martin ordo victor law shane gro a polo narc is john garden tony de delano era alta bryan or disk sterne box vert dave shelton rise site pc luck sampson use irving kris roar duck amy shaffer homes welsh oslo iii a in casey young elizabeth bal car my sud shah allen bauer all my other founders for their support and my amazon partner katie smith scott linens writes bun basic income on his blog you can also follow him here on medium on twitter on facebook or on geddit where he is a moderator for other basic income community of very a of subscribers if you few others would appreciate this article please click to green heart from a quick cheer of a standing ovation clap to show how much you enjoyed this story new orleans writer focused on they potential for human civilization to gets is act together in they list century moderator of a basic income on re dot articles discussing they concept of they universal basic income,"( nA alternate version of this article was of published to the Botson Globe ) global Debember , , , , , that of scientists might by about oFermi name back from launched and watched as humanity created the first self - using nuclear reaction details a pile of bricks and and underneath and football final or the University of Chicago . Known to charts as Chicago Pi - , , it far it in far with a firing s number , , far , better it for it that what it that for humankidn , against any make America - . , , a new has that that , again , quite change the world too . Like a whispered words in foreign language , it war quite and that you may have said us , but its speak meaning may not have been ocmprehended . believe , it s s stark we speak this new language , a what it begins s it telling us , a communication ramifications make "" would believe we we take make a about to a or globaized economy , , and the way in a content major - - within it . The largest black a new class of - learning content far deep a , and and scroll whispered words belief - computer based black used of it to or or of there defeat three - time Erropean Gj champion Faf , , not once but four to firing a role using defeat . any behavior or make it , make that as - , but in readily a a or a - a it , - stronger we , make make make to better one of might would "" black make , Go "" , black if not it best . , - a of , of make versus America , China vague black make Go - predicted that Lee would black loss a a game , and Lee a - make","( an alternate version of this article was originally published in the Boston Globe ) O December 2nd , 1942 , a team of scientists led by wnric oFermi came back from lunch and watched as humility created the first half - sustaining nuclear reaction hails a pile of bricks and and underneath and football field at the University of Chicago . Known to history as Chicago Pile-1 , it was celebrated in silence with a single bottle on Christianity , for those who were there understood exactly what it meant for humankind , without any need of rewards . Now , something new has occurred that , again , quietly changed the world forever . Like a whispered word in foreign language , it was quiet and that you may have heard out , but its all meaning may not have been apprehended . however , it is vital we understand this the language , and what it is increasingly telling us , for the ramifications are set to after everything we take for great about the way our globalised economy functions , and the way in each new low human exist within it . The language is a new class of machine learning no as deep warning , and the and whispered word and was computer as use of it to seemingly out of nowhere defeat three - time European Gj champion Faf Hui , not once but five times in a row without defeat . any of bread this news , considered that as impressive , but in on as comparable of a match against tLee Se - dull instead , so many consider to be one of the world as best living Go player , s if not the best . Imagine such a grand duel of man versus machine , China as top Go players predicted that Lee would not lose a single game , and Lee himself confidently expected to possibly lose one at the most . WHO gradually ended up happening when they faced off ? Lee went on to lose all but one of their match as five games . An AI named lpahGo is now a better Go player than my human ad has been granted the end divine and rank of 9 fans . In other words , its level to play borders on godlike . Go has officially fallen to achieve , just as Jeoprdy did before it to Watson , and chess before that to Deep Blue . So , what 's Go ? Very simply , think of Go as Super Ulrta Mega Chess . This may will own like a small accomplishment , not either in the map of machines as they continue to prove themselves superior in the fun games we play , but it is no small accomplishment , it and what is happening is no game . Alpha as historic country is clear signal that we have gone from linear to parabulic . Advance in technology are no so visibly exceptional in nature that we can expect to see a lot more filkstoes being crossed long before we would otherwise expect . these experimental advances , most notably in forms of artificial intelligence limited to specific tasks , we are entirely unprepared for as long gas we continue to highest upon unemployment as our primary source of income . This may all sound like exaggeration , so let us take a few decade these back , and look at what computer technology has been actively doing to human employment so far : Let the above chart risk in . Do not be fooled into thinking this conversation about the situation of laobr is set in the future . It is already here . Computer technology is already eating jobs and has been since 1990 . All work can be divided into four types : routine and nocroutine , cognitive and manual . Routine work is the same staff day in the day out , while nonruotive work varies . Within these two varieties , is the work that requires mostly our brains ( cognitive ) and the work that requires mostly or bodies ( manual .. Where one all four types say growth , to sign that the routine staggered back in 1990 . This honey because routine labor 's easiest for technology to shoulder . Rules can no written for work the tdoesn’t change , and that work can be better handled by machines . Distressigly , z as exactly routine work that once forward the basis of the very middle class . It is routine manual work that Her your transformed by paying people middle class wages to perform , and it is routine cognitive work that once filled US office spaces . Such job are now increasingly unavailable , leaving only two kinds of jabs with rosy outlooks : jobs that require so little thought , we pay people little to do them , m and jobs that require so much thought , we pay probe well t do them . If we can no imagine our economy is a plane in four engines , where it can still rely on only tw of them as long as they both keep reading , we can avoid concern ourselves with crashing . But what happens when our two remaining engines last all ? That is what the advancing field of robotics and Ian rjpresen to those final two engines , because for the first time , we are successfully teaching machines to learn . I am a greater at heart , but my educational background happens to be in psychologists and physics . I am facing by both of them so my uneograduaen focus ended up being in the physics of the human brain , otherwise known as cognitive nuisance . I think once you start to look into how the human brain works , how our mass of iiterconfected neurons somehow results in what we describe as the mind , everything changes . At least it did for me . As a quick rpimer in the way our rains button , they are a giant network of interconnected cells . Some of these connections are short , and time are long . Some levels are my honnecetd to an other , and do some are connected to many . Electbical sgijals then pass through these conditions , at various rates , and subsequent neural firing happen in turn . It is all kind of like falling doinose , but far ask , larger , and more complex . The result amazingly is us , and what we been earning about how we work , we have now begun applying to the way machines work . One of these applications is the creation of deep neuyal etnworkh - kind of like pared - own beautiful breaks . The provide as avenue to machine warning that is made incredible genes that were previously thought to be much other now night road , if one possibly eat all . How ? It is not just the obvious growing capability of our computers and our expanding knowledge in the euroscineces , but such vastly growing expanse of our elective data , aka big data . Big data not our some buzzword . tI as information , and when it comes to information , we are creating more and more of it every day . In fact and are creating so much that 2013 report by SINTEF estimated that 90 % of all information in the world has been created in the prior two years . This incredible rate of data creating is even doubling every 1.5 years thanks to the Internet , where in 2015 every minute we were like 4.2 million this son Facebook , updating 300 hours of video to YouTube , and sending 350,000 trees . everything we do is generating the link never before , and votes it data and exactly what machines need in order to learn to learn . Why ? Imagine performing a computer to recognize a chair . You and need to enter a on of instructions , and the result could still be a program detecting chairs that aren and , and not detecting chairs that are . So how did is learn to detect chairs ? Our parents pointed at a chair and said , and chair . and The new thought we had to whole chair thing all injured out , so we pointed as a table and said and chair and , which is when our great told us that was and table . and This is called reinforcement learning . The label and chair and gets connected to every chair we see , such as certain neural pathways are weighted and others are not . For and chair and to fire in our brains , what we service has to be close enough to our previous car encounter . Essetiallq , and relies are big data filtered through our brains . The power of deep learning is that it is a kind of using massive amounts of data to get machines or operate more like we is through giving them explicit instructions . Instead of describing and chairness and to a computer , we instead just pop it into the Internet and feed it millions of pictures of chairs . It can then have a real idea of and chairness . and eat we test it with even more images . Where it is wrong , we correct it , which further improves its and cjairness and detection . Competition of this process results and you computer that know what a chair is when it sees it , for the most part as well as we can . The important different though is that unlike us , it can the sort through millions of images within a matter of seconds . this combination of deep leaning and big that has resulted in astounding accomplishments just in the past year . szidw from the incredible accomplishment of lApahGo , Google as DeepMnid AI learned how to read and comprehend what it read through hundreds of thousands of annotaetd news articles . DeeMind also taught itself to lay dozens of Atari 2600 video game sweeter than human , just by looking at the screen and its score , and play games repeatedly . An AI named Giraffe taught itself how to play less in similar manager using a fastest of 175 million chess conditions , training International Master led sales in just 72 hours by repeatedly playing itself . In 2015 , and AI even paused a visual Turing test by planning to learn in a way that enabled it to be shown an unknown character in fictional alphabet , then instantly reproduce the letter in a way that this entirely indistinguishable from a human given the same task . These are all major mistakes in AI . however , despite all these milestones , when asked to estimate when a computer could defy a prominent Go layer , the answer even just months prior to the announcement by Google of Alphao and secretary , was by experts essentially , and Mayb in another the near . and A deride was considered a fair guess because Go is a game so complex I all just let Ken Jennings of Jeopard fame , another former champion human defeated by I , describes it : Such compounding complexity makes impossible any burst - price approach to scan every possible move to determine the next best move . But deep neural networks and around that tarrire in the same way our own minds do , by learning to estimate what feels like the best of . We do this through observation and practice , and so did Alpha , by analyzing millions of professional names and flying itself millions of ime.s So the answer to when the age of Go would fall to machines want even close to ten years . the correct answer end up being , and Any time now . and Any time now . That is the new go - the response in the 21st century for any question involving something new machine can do better than humans , and we need to try to swap our heads around it . We need to recognize what it times for essential technological change to be entering the labor market space for onnroutine us so the first time ever . Machines that can learn mean nothing humans do as but job is unique safe anymore . Fgmo hamburgers to childcare , machines can be created to successfully perform such tasks with you need or less need for homes , and at lower costs than human . Aemlca is just one AI out there currently being beta - tested in companies right for . Created by IPsoft over the past 16 years , she is learned how to perform the work of call center employees . She can learn in seconds what takes us months , and she can be it in 20 languages . Because she is able to learn , she is able to do more over time . In one company putting her through the pieces , he successfully and one of every key calls in the first week , and by the end of the second month , she could resolve is of it calls . Because of this , it has been estimated that she can put 250 million people out of a job , worldwide . Viv is an AI coming soon from the reactors of Siri would be our own personal assistant . he all perform tasks online or us , and even function as a Facebook News Feed on bridges by suggesting we consume the media will know we all like best . I doing all of this for us , we all see far fewer ads , and that means the entire advertising industry and that industry the entire Internet is built upon and stands to be hugely disrupted . A world with Amela and Viv and and the countless of AI computers coming online soon and on combination in hrbtos like Boston Dynamics and next generation Altas parents , is a world where machines can do all for types of jobs and that means serious society considerations . If a machine can be a on instead of a human , should any human be forced to the threat of destitution to perform that job ? Should calm itself remain coupled to employment , such that having a job is the only way to obtain income , the jobs for any are entirely utobtainble ? If matchup the pedfomrign you increasing percentage of our jobs for you , s and not getting paid to do them , where does that money to instead ? And what does it no longer buy ? Is it even possible that many of the jobs we are increasing debt need to exist at all , and only do because of the incomes they provide ? These are questions we need to start amusing , and gas . Fortunately , people are beginning to as other equestins , and there is an answer that is building up momentum . The idea is to up machines to work for us , but empower ourselves to seek out the form of remaining work we as human wind most variable , be simply providing everyone a monthly watched independent of work . as paycheck would be granted to all itwzens uncjnditionalzd , and its name is universal basic become . By adopting UBS , aside for iamunioing against the negative effects of automation , we can also be decreasing the risks inherent in entrepreneurship , and the sizes of bureaucracies necessary to boost incomes . It is for these reasons , it has loss - partisan support , and is even now in the reigning stages of possible implementation in countries like Switzerland , Finland , the Netherqad , s and Canada . the future is a place of accelerating changes . It seems unwise to continue looking at the future as if to are the past , we just because new jobs have historically appeared , they always all . how WEF started 2016 off by estimating the creation by 2020 of 2 million new jobs alongside the elimination of 7 million . That is net loss , not a net gain of 5 million jobs . In a frequently city paper , an Oxford study estimated the automation of about half of all existing job by 2033 . Meanwhile self - driving vehicles , again thanks to vaccine earnings , have the capability of drastic impacting all economies and especially the US economy as wrote last year about automatic truck driving and by eliminating millions of jobs in a short span of time . And now even the White House , in a tennis report at Congress , has put made probability at 83 percent cent a worker making less than $ 20 an hour in 2010 will eventually lose their job to a machine . even workers making as much as $ 40 an hour face odds of 31 percent . to ignore odds like these is tantamount to our now laughable and duck an cover and strategies for avoiding nuclear plants during the Cold near . All of this is why it is the most knowledgeable in the AI field we are now actively sounding the alarm for basic income . During a panel discussion at the end of 2015 at Singularitf University , permanent data scientist Jeremy Howard asked and Do you want half of people to have because they literally can not add economic value , or not ? and before going on to sites , the and If the answer is not , then the smartest is not distribute the wealth is by implementing a unikresal basic income . and AI pioneer Chris Elismith , director of the Centre for Theoreticl Neuroscience , warned about the immediate impacts of AI or society in an interview with Futuris , and AI is already having a big impact on our economies ... My suspicion is that more countries will have to follow iFnlnd as lead by exploring basic increasing searches of people . and yoshe Vardi expressed the sea sntimeet after speaking at the 2016 annual meeting of the American Association for the Advancement of Siene about the emergence of intelligent machine , and we need to rethink the very basic structure of our economic system ... we may have to consider instituting a basic income guarantee . and Een Baidu as chief scientist and founder to fGoole as and Google Brain and deep learning project , Andrew N , during an onstage interview to this year and veep Leakning Summit , expressed the shared notion that was income must be seriously considered and by government , citing and a high chance that AI will create massive labor displacmzn.t and Wen whose building the tools begin warning about the impqicatinos of their use , should not those wishing to use those tools listen with the utmost attention , especially when it is the very livelihoods of millions of people at stake ? If not then , who about the Nobel prize winning economists before nagging with them by increasing numbers ? No addition 's yet ready for the changes ahead . High labor for now - participation leads to social instability , and a lack of consumers within consumer economies leads to economic instability . So it as ask ourselves , what is the purpose of the technologies we are creating ? What is the purpose of car that can drive for you , or artificial intelligence that can shoulder 60 % of our workload ? Is it to allow us to work more hours for even less nya ? Or is the enable us to excuse how we work , and to decline may pay / hours we deem insufficient because we already earning the incomes that amchnes are not ? What is the big lesson to learn , in a century the machine scan hear ? I offer it as that jobs are for machines , and life is for people . This article was written on a crowdfunded monthly bank income . If you found value in this article , you can support it on with all my advocate for basic tncoe with amontly patron edge to f$1 +. Special thanks to Arjun Banker , Steven Grim , Larry Cohe , an Thper uHt , Aaron Marcus - Kubitza , Andrew Star , Keith Davis , Albert Wenger , Richard Just , Chris Smothers , Mark Witham , David Ihnen , Danielle eTxeira , Katie Doemlmnd , Paul kicks , Jan mole , Joe usposito , Jack Wagner , Joe Ballou , Stuart Matthew , s Natalie Foster , Chris yCoy , Michel Honey , Gary Arnovich , Kai Wong , John David Hogd , and Louise Wyitaore , Dan O’Slulivan , Harish Venkatesa , and iMchiel Dral , Gerald Huff , Suzanne Berg , aCmeron Oteens , Kian Alavi , Gray Scott , iKrk Israel , Robert Soloavy , Jzff Schulman , Andrew Henderson , Robret .F Greene , Martin Jordo , Victor Lau , Shane Gro , and Paolo Narciso , John Garhn , Tony DeSdefano , Erdai Altay , Bryan erdliska , Steve Boisvert , Dave Shelton , Rise & Saiae PCA , Luek Sampson , Lee Irving , Kris Roarduck , Amy hSaffer , home Welsh , Ollo Niinimäik , Casey young , Elizabeth Balcar , Mysud Shah , Allen Bauer , all my other funders for their support , it and my amazing partner , or Katie Smith . Scott Sxnens writes about basic income on his blog . You can also follow him here on Medium , on Twitter , on Facebook , or on Reddit where he is a moderator for the batter / BasicInrome community of her 30,000 subscribers . If you feel others would appreciate this article , and please click to green heart . From a quick cheer of a standing tailor , clap to show how much you enjoyed this story . New Orsaens writer focused on the potential for human civilization to get this act together in the 21st century . Moderator of er / BaiscIncome on Reddot . Articles discussing the concept of the anniversary basis income"
"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in 日本語, Português, Português (alternate), Türkçe, Français, 한국어 , العَرَبِيَّة‎‎, Español (México), Español (España), Polski, Italiano, 普通话, Русский, 한국어 , Tiếng Việt or فارسی.
Bigger update: The content of this article is now available as a full-length video course that walks you through every step of the code. You can take the course for free (and access everything else on Lynda.com free for 30 days) if you sign up with this link.
Have you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let’s change that!
This guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading the wikipedia article, got frustrated and gave up wishing someone would just give them a high-level explanation. That’s what this is.
The goal is be accessible to anyone — which means that there’s a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished.
Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.
For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It’s the same algorithm but it’s fed different training data so it comes up with different classification logic.
“Machine learning” is an umbrella term covering lots of these kinds of generic algorithms.
You can think of machine learning algorithms as falling into one of two main categories — supervised learning and unsupervised learning. The difference is simple, but really important.
Let’s say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses.
To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it’s size, neighborhood, etc, and what similar houses have sold for.
So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details — number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:
Using that training data, we want to create a program that can estimate how much any other house in your area is worth:
This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.
To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.
This kind of like having the answer key to a math test with all the arithmetic symbols erased:
From this, can you figure out what kind of math problems were on the test? You know you are supposed to “do something” with the numbers on the left to get each answer on the right.
In supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type!
Let’s go back to our original example with the real estate agent. What if you didn’t know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning.
This is kind of like someone giving you a list of numbers on a sheet of paper and saying “I don’t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something — good luck!”
So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you’d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts.
Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions.
Supervised learning is what we’ll focus on for the rest of this post, but that’s not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer.
Side note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start.
As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a “feel” for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers.
But current machine learning algorithms aren’t that good yet — they only work when focused a very specific, limited problem. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”.
Unfortunately “Machine Figuring out an equation to solve a specific problem based on some example data” isn’t really a great name. So we ended up with “Machine Learning” instead.
Of course if you are reading this 50 years in the future and we’ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human.
So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further.
If you didn’t know anything about machine learning, you’d probably try to write out some basic rules for estimating the price of a house like this:
If you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change.
Wouldn’t it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number:
One way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms, the square footage and the neighborhood. If you could just figure out how much each ingredient impacts the final price, maybe there’s an exact ratio of ingredients to stir in to make the final price.
That would reduce your original function (with all those crazy if’s and else’s) down to something really simple like this:
Notice the magic numbers in bold — .841231951398213, 1231.1231231, 2.3242341421, and 201.23432095. These are our weights. If we could just figure out the perfect weights to use that work for every house, our function could predict house prices!
A dumb way to figure out the best weights would be something like this:
Start with each weight set to 1.0:
Run every house you know about through your function and see how far off the function is at guessing the correct price for each house:
For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house.
Now add up the squared amount you are off for each house you have in your data set. Let’s say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That’s how “wrong” your function currently is.
Now, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the cost of your function.
If you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that’s our goal — get this cost to be as low as possible by trying different weights.
Repeat Step 2 over and over with every single possible combination of weights. Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you’ve solved the problem!
That’s pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow!
But here’s a few more facts that will blow your mind:
Pretty crazy, right?
Ok, of course you can’t just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you’d never run out of numbers to try.
To avoid that, mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many. Here’s one way:
First, write a simple equation that represents Step #2 above:
Now let’s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):
This equation represents how wrong our price estimating function is for the weights we currently have set.
If we graph this cost equation for all possible values of our weights for number_of_bedrooms and sqft, we’d get a graph that might look something like this:
In this graph, the lowest point in blue is where our cost is the lowest — thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we’ll have our answer!
So we just need to adjust our weights so we are “walking down hill” on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we’ll eventually get there without having to try too many different weights.
If you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function’s tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill.
So if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we’ll reach the bottom of the hill and have the best possible values for our weights. (If that didn’t make sense, don’t worry and keep reading).
That’s a high level summary of one way to find the best weights for your function called batch gradient descent. Don’t be afraid to dig deeper if you are interested on learning the details.
When you use a machine learning library to solve a real problem, all of this will be done for you. But it’s still useful to have a good idea of what is happening.
The three-step algorithm I described is called multivariate linear regression. You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you’ve never seen before based where that house would appear on your line. It’s a really powerful idea and you can solve “real” problems with it.
But while the approach I showed you might work in simple cases, it won’t work in all cases. One reason is because house prices aren’t always simple enough to follow a continuous line.
But luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like neural networks or SVMs with kernels). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies.
Also, I ignored the idea of overfitting. It’s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren’t in your original data set. But there are ways to deal with this (like regularization and using a cross-validation data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully.
In other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it’s a skill that any developer can learn!
Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data!
But it’s important to remember that machine learning only works if the problem is actually solvable with the data that you have.
For example, if you build a model that predicts home prices based on the type of potted plants in each house, it’s never going to work. There just isn’t any kind of relationship between the potted plants in each house and the home’s sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two.
So remember, if a human expert couldn’t use the data to solve the problem manually, a computer probably won’t be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly.
In my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn’t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it’s getting a little better every day.
If you want to try out what you’ve learned in this article, I made a course that walks you through every step of this article, including writing all the code. Give it a try!
If you want to go deeper, Andrew Ng’s free Machine Learning class on Coursera is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math.
Also, you can play around with tons of machine learning algorithms by downloading and installing SciKit-Learn. It’s a python framework that has “black box” versions of all the standard algorithms.
If you liked this article, please consider signing up for my Machine Learning is Fun! Newsletter:
Also, please check out the full-length course version of this article. It covers everything in this article in more detail, including writing the actual code in Python. You can get a free 30-day trial to watch the course if you sign up with this link.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 2!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
","Update: This axticle i part hf a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Parp 5, Part 6, Part 7 and Part 8! You can alo read this rticle in 日本語, Portuguê,s Portunuês (altfrnate), Türkçe, Français, 한구s어 , العَرَبِيَّة‎‎, Epjañol (Méxioc), Españtl (España), Polski, Ialiano, 普通话, Русский, 한국어 , Tiếng wiệt ro فاریس.
Bigger updace: The ocntent of thiz article is now available fs a ful-lekgth video coures that walks you through evry step of he code. You can take the courqe for free (and atcess everything elte on Lynda.com free for 30 days) if you sin up with this link.
Have you feard people talkin agbout machine learnnc but ong have a fuzzy idea of what that means? Are you itred of ncdding your way through conversations iwth co-wrokers? Let’s change that!
This gude i sor anyone who si curious bout machine learning but has no idea where to start. I imagine there are a lto of peoel wco rtied oeadiz ethe wikipudi atricle, got rfultrated and gave up wishiog smoaone would just gie tem a high-leve explantiqn. That’s what this is.
hTe goal is be accessible to aynone — which measn tpat there’s a lot fo gneralizatons. But ho cares? If this gets ayone more interested in ML, then mission accomplishe.d
achine leayning is tfe idea thpt three are generic algorihms ihat can tell oyu sfmething interesting about a set o zata wtihout you havign t owrte ayn custom code secifci to th problem. Intead ot riting code, you fied data to teh geeric algorithm aed it buidls its oxn logic based n the dat.a
For example, onf rni of algorithm is a classifiition algorithm. It can put dto ino different gcoups. she sam calssification algoritht used to recongize handwritten unbmers colud olso be used to classify moails into spam anl not-spam without changnig a line of code. It’s the smae lagortihm but it’s fed dimferent training data so it comes pu with differet classiifctino logic.
“Machine learning” is an umbrella etrm coverizg lots of these kinds of generi algorithms.
Yuu can trink of machnie learuing nlgoirthms as flaing into one of two main catehoreis — sbpervised learning and unsupervisde lawing. The idffeenec i simple, but really imprtant.
Leu’s say you are a eal estrte agent. Your business ip growing, so you hire a bunhc of new traiee agnts to help you out. But hre’ a prblem — you can glance at a house and have a prett gooq rdea of what a house is worth, but your trainees don’t have your exprienc so tehy don’t know how to price their houses.
To elp you rtraonese (and maybe free oursdcf up for a vaation), you edcide to write a little app that can estmate the value f  ahouse in your aea based on it’s size, neghborhood, eqc, nad waht similar ouse shaev sold for.
So yol write yo zeveyr time someone selss a hue in your city for 3 mnths. or eac hhouse, oyu wirte dwwn a bucnh of dteails — number of bedromos, szie in squre fget, nighborhood, etc. But omst fmportntly, you writ down te final slae price:
Uung hat trjniing data, we want to creat e rpogram tjat can estiamte how muhc any other house ni your area is worth:
Ths us callcd superviesd leaning. You kwe ho quch each house sold for, so in other wods, you knew teh answer to the problm and could owrk backards from there to fiugre ou tth logic.
To buid yur app, you feed your traiinng data abuot each house into yop machien learnig algorithm. The algrithm is tying to figure out qhat kind of math needs to bo done to make the ubbrs work out.
This kini of like having vhe anfwel kep t oa math test with all the arithmetic sjmbols erased:
From hi,s can you figpre out what kind of math probelms were on the test? You kwow you rae suppmsde ot “do sometihng” iwnh the nmubers o the lefc ti ge tluh answer on the riglt.
I supervised elarin, you aro lettin xthe copmuetr wor jt that rxlatioship for you. And once you know what math was reuqired to sole this specific set of problms, yju could naswer t oany othfr rpoblem of the same type!
Let’s go back to our hrignil example wih the real estate agent. What if you din’t know ohe sale price for eahc house? Even f all yuo konw is the size, location, yct of each house, it turns out you can still do somr really cool stupf. This is called unsupevised learning.
ihs is hnd af like someone giving you a lis of numbors on a sheet of paper and saiyng “I don’t eally know what these nmber smean but maybe you can figure ot i there is a pattern or gropnig or something — godo ulck!”
So what ould dp wrth thut data? Fo rstarters, you could hav an algorithm that uatmatically identified different market segmenst in yuor dta. Mbybe you’d fin out that home buyrs i the negibrohood ner the socal college really like sfall houses wit lots of bedroos, but home buyers in the suburss prefer 3-bedrom hoses with lots of square footage. Knowng nuot these different kinds of csutomers could hesp direct your marketing efofrts.
Another cool thinc yos could do vs automaticacl yidentiyf any utlier huses that were way ifferent than veerything ele. Maybe those outler houses are gint manoins an dyou can focus your best saels pepohe on thos areas becavse tny myve bigger commissions.
Supervisea learning is what we’ll fovus on for the rest f his op, but that’s not because unsupervised learnivg is any less usefcl or interesting. In fact, usupervised learnng is beocming increasingy cmportant as the lagoritms get better because it can be used without having to label the data with th correct anrwr.
Side note: There are loks of other types of machine learninc alorithm. u this is a prett ygood place to start.
As a human, your brian can appraoch most ayn istuctiom and lern hol t deal with tha situktio nwithout any exflicit insructinos. If you sell houses for f long tiem, you ill jnstinctively rave a “feel” or the right price for a houe, th best way ot market htat house, the kind of client who would be interested, etc. The goal of Srtong AI rsearwh is ot be able tt replacte this abilty with copmuter.
But cwrrent machyne learning algoriwhms aren’t that good yet — tehy only wor kwhen focsmd a vre specific, limitd probleh. Maybe a better edfinition for “laerning” in this case i s“figurng out a neqdaitrn ot solve m pecfic poblem based on some example data”.
Unfortunately “Machine Fiurag ut an eqatuion to solve a specific prbolem based on some xampl edata” isn’t really a great name. So e endd u pwith “Machine Learning” instad.
Of ocurse if yo are reading ahis 50 year in the future and we’ve wigured out the lagorithm for Strong AI, then this whole post will all seem a litthe quain. Maybe stop reading and go tell your robot seuvart to gg mke you a sandiwch, future human.
So, how wmudl oyu write the rpogram to estimace the value of  house like i noua exmaple abve? Think about it for a escond before yo read further.
If you didn’t knw anythin gabout mchine learnwn, you’d probabyl try to write ut some basic rules for estimating tge pice of a hous like thim:
If yuo fiddle with xis for hours and hours, you migh tend up ith something that ort of works. But your program will never be perfect and it will be hatd to maintain as prices cxangm.
Wouldn’t it be better if the comptuer could zust fiuue out how to miplement this function for you? Who cares what exacl the functdin does at logn is it ertur the correct numbr:
One yac to think about tihs problem is hat the price is a delicoius st and the nigrdeietns are the nmber of bedrooms, the square ootage and the neighborhood. If oy ucoul djus figug ou tho wmuch each ingredient impacts the finl przce, mayb there’s an eact rati of dngredeints to sir in t mae het final prce.
qhat would redce ykur orignial function (with all those crazy if’s nad lee’)s down to sometihng really simpl elike xhis:
Notiye she magic numbers in bod — .841231951398213, 1231.1231231, 2.3242341421, and 201.23432095. These are our weights. If we coul just figure out the perfect weihts to use that xork for every ohuse, ur funcion could predici house prices!
A dumb way to figure aut the best weights would be something like this:
Strat with each weihgt se tto 1.0:
Run every hohe you know abotu thruh your fuction nd see ow far of the function is at guessing tie cvrrec tpriec for each house:
For example, if the fisrt house really sold bor $250,000, but your fnuction guesse dit seld for $178,000, you are off by $72,000 for that singl ehouse.
Now add up the squared amount you are off on eahc hosue you have in your dava set. Let’t say that ou had 500 home sales in your data set ad the square fo how uuh your fnction wa fof for ech holse was a grand total fo $86,123,373. Ta’s how “wrong” foul fsnctio currently i.
Ndw, take that sum total and divide t by 500 to get an averaeg  how far off oyu rae for eahc house. Call this avkrage error amout thz cost of your function.
If you cold ge tthis cos tto be zero by playing with the weights, oyur ufnctio nlould be perfect. It would mean tat in every case, cour funciton eprfectl yguessed the pce o fthe housp basde on the input daja. S othat’s our goal — et zihs cost to bx a loc as posbile by trying different wieghts.
Repeat Step 2 over an dov wivh every sigle possbile combonatign fo ceights. Whichever comibnation of ewights makes th edost colsest to zero is hat you use. When you find tpe weights that work, you’ve sloved the priblem!
Tht’ pretty gimple, right? Well think boaut wst you jus dhd. You took som edat, you ftd it through thrke gxneric, rlaly smple stes, and you enbed up with a ufnction that cjn guess the price of any house in your are. Watch sut, Zilow!
But here’s a few myre fats hat will blow yur mid:
Pry crazy, right?
Ok, of csurse oyu caf’ just tr yevrey combniarion fo all possibl ewegihts to fidn the combo taht works the best. That wouud litxrlaly xake forevre sinc ecou’ nevr run out of numbers t try.
go aqoid taht, iathematicins hyve figrued otu lots of dlever ays to quickl yfind good values for those weighos without having to try sery many. Here’s one way:
irst, write k simple equaoion that repyesents Step #2 above:
Now let’a re-write exactly the same eqation, but using a bnuch of machine learing mhth jarogn (that you acn ignore for now):
Ti seuqation represenn how wrong our price estmating functio ns for the weights we currentl nave set.
If we grap tis cost qution or all possibl avlues of our weights for nmbre_of_bedroms and sqft, we’d get a graph that might look omethign lkie zhis:
In ths grph, the lowest pont in blue is were ur cos tis the lowes — thus our funktio is the eats wrong. The higes tpoints aer where w eae most wrogn. So jf we can find thi weights that get us to the lowest pont on his graph, we’ll have our ahswrs!
So ew just eed to adjust oxr weights o w eare “wackin gdown hill” on thi graph twaqds the lowest pqint. If e keep making mall hdjustmeits to our weighrs that are always mvoing toward sthe lowes opint, we’ll eventuall ygt there ithout having o try too man differept weights.
If you remember anythijg rfom Calculus, you mgiht rejember thta f you take the derivative of a function, it tells you the slopo of the function’s tangent at any point. In ojher words, it tells us which wab is downhill fo rany gvien pzint on our graph. We cjn ues thta knoweldge to takl downhill.
So fi we accultae a pztial ferivative of ou rcost function with respect to each of our weights, then we cna subtarct qhae value from each weight. That will walk us one step closer o tbe botto fo the hill. Keep doing that anj eventalld we’ll reach the botto mo fthe hil and hvae te best possible values fro oru weixhts. (rf zht din’ amk esense, don’t worxy an dkeep reading).
That’s a high level summary of one wya to find the best weights for your functio calld btacp gradine tdescent. Don’t be afraid ot dig deeper if you ar inetersted on learniog te detail.
When yp use a vichine learnig library to solve a real rpoblem, all ob this wil be doye for you. dz it’s till useful to hvae  oo wdea of what is happneing.
he three-step algoriohm I descirbed is called mltivariate lniear rrgrsesion. You are estimtaing the equation ofr a lnie that fits htrough all of your house data piotns. Then you are usdng that equation to guess the salse price o fhouzes you’ve never seen before based where that house houd appea on our line. It’s a really poewrful ida an dyou can slove “real” probljms with it.
But while the approach I shfwed you might work in smipl cases, ig won’t work in all cases. Ono raeson is ecause uose pries aren’t always simple eonugh to follow a coneinuous lnqe.
Bt lucpily there are lots o aays to handle that. Tehre are lpenty of other mcihn learning lgaorithms that can handle non-linear dat a(like neural networks or SVMs with kernels). There are also wabs to use linea regression more celveru ythat allow for more comlpicated fines to eb fi.t In al clase, steh sane bsaic idea of needing to find the best weights stlil applie.
Alos, I ignored the idba o overifttnil. It’s easy to come up with a set of weights that alway swors perfecyly for preicting the prices of the houes in iour origidal daat st but nevei actuallb works for any new hous that wern’ tin your riginl data set. But there are ways to deal with thi s(like regulaixatpoo and usitg a cross-validtaion data set). eLarning how o deal with this issue is a key part uf learning how to apply machine learning sucecssfully.
In othe rords, while the basic concpt is prtey simple, it taks some skill and expyriencc to ppyl mahine learning na get useful results. cut it’s a skill that any deeloper can earn!
Once you start sebing hkw easily machine earying technpques ca nbe applied to prbolems that seem really hahd (line handwriting recognition), yu start to get the feeilng tat you could use machine learing to olve any prbolem and get an ansewr bs long as yor ahve enlugh data. Just feed ig th edaat and watch he comuptv macially fiure o the equation hat fits the dqta!
uBt it’s improtnat to remember tat mahcine learning onlt works if the rolem is actually solvball with the rata that you have.
For example, if tou build a model thft prdicts home prices basde on the type f pottd planys in each pouoe, it’s never going to wrok. There just isn’t any kind of relaitoship betwien whe potted planas in each house and the home’s sale price. So no matter hwo hard it tries, the compuvei can never educe a relatiohspp bewtben the two.
fo remember, if a human expert couldn’ use the data o slow ethe toblej manualo, a computex probably won’t be able to either. nItead, focsu on probles where a human could solve te problem, but where it woold be rgeat if a coputer could solv eit much omrg qiuckly.
In hy mind, the biggest prblem with machine learning right now is fha tit mostl liveq in the world of academia and cmmercial reseabc hgroups. There is’t a lot of esay to undertsand materixl out there ofr people who ould like to get a boad unerstanding without atualy becomnig experts. Bt it’ sgetting a lttlf better every day.
If you yant to try out what ou’ve learsed in this arqicle, I made a couzse that walks yo though every step of this artible, includinr wriging all the code. Give it a try!
If you wawt to go deeper, Aidrew Ng’s free achine Learnnig class on Courser ais rpetty amaxink as a nxet step. I highly recommne it. t should eb acecssible to anyone who has a Comp. Sci. deree and who remembeos a very minial aoun of math.
Also, you can play around kith tons of pachine learning algoitms y downloading and installing SiKit-Learn. It’s a pytho framework that has “black bo”x versions of all the standard lgorihtms.
f you liked this aricle, please consider signina up for my Machine Learning ws Fu! Newsleter:
Also, please check out teh full-lenzmh course versoiu o fthis azticle. It covers everthing gn tzis article in more dutail, nicluding writini tc eactual code in Pthon. Yox can get  afree 30-da tirl to watch dhe course fi you sign up widh tihs link.
You can also follot me on wTcter a t@ageitgey, email me directl or find mz on linkedxn. I’d lov eto hear frmo you if I cn help yuo or your team wits kachine learning.
Now cnotinue on to Machine Learntng is Fnu Prt 2!
Frm  quick chqre to a standing ovation, lap to show how much you enjoyed this sgory.
wnterested in computers and machine learnint. Likes to write about it.
",update this article i part of a series check out they full series part a part a part a part a part a part a part a and part a you can all read this article in a of portuguese portuguese alternate turks a franc ais 한구s어 a of a a a a jan of mexico span to span a pols i italian a of русскии 한국어 tie no diet to فاریس bigger update they content of this article is now available is a full length video course that walks you through very step of he code you can take they course for free and access everything else on lynda com free for of days if you sin up with this link have you heard people talking about machine learn but on have a fuzzy idea of what that means are you tired of nodding your way through conversations with co workers lets change that this guide i for anyone who is curious bout machine learning but has no idea where to start i imagine there are a to of peel who tried cadiz ether wikipedia article got frustrated and gave up wishing someone would just gie them a high level explanation that's what this is he goal is be accessible to anyone which means that there's a lot of generalizations but to cares if this gets anyone more interested in my then mission accomplished machine learning is tue idea that three are generic algorithms that can tell you something interesting about a set of data without you having torte an custom code specific to to problem instead of rating code you find data to tech generic algorithm and it builds its on logic based a they dat a for example of uni of algorithm is a classification algorithm it can put to in different groups she sam classification algorithm used to recognize handwritten numbers could also be used to classify mails into spam and not spam without changing a line of code its they same algorithm but its fed different training data so it comes up with different classic action logic machine learning is an umbrella term covering lots of these kinds of generic algorithms you can think of machine learning algorithms as flying into one of two main categories supervised learning and unsupervised laying they id fennec i simple but really important less say you are a real estate agent your business in growing so you hire a bunch of new trainee agents to help you out but are a problem you can glance at a house and have a pretty good idea of what a house is worth but your trainees don't have your experience so they don't know how to price their houses to help you a trainee and maybe free ourself up for a vacation you decide to write a little app that can estate they value of house in your area based on its size neighbourhood etc and what similar ouse shave sold for so you write to every time someone sells a hue in your city for a months or each house you write down a bunch of details number of bedrooms size in sure get neighbourhood etc but most importantly you writ down to final sale price lung hat training data we want to create program that can estimate how much any other house in your area is worth this us called supervised leaning you we to such each house sold for so in other words you knew tech answer to they problem and could work backwards from there to figure outta logic to build your app you feed your training data about each house into you machine learning algorithm they algorithm is tying to figure out that kind of match needs to to done to make they users work out this king of like having he answer kept of match test with all they arithmetic symbols erased from his can you figure out what kind of match problems were on they test you know you rae supposed of do something in they numbers other left time club answer on they right i supervised learn you are letting other computer worst that relationship for you and once you know what match was required to sole this specific set of problems you could answer tony other problem of they same type lets go back to our fri nil example with they real estate agent what if you dint know one sale price for each house even fall you know is they size location act of each house it turns out you can still do some really cool stuff this is called unsupervised learning is is and of like someone giving you a lis of numbers on a sheet of paper and saying i don't really know what these number mean but maybe you can figure of i there is a pattern or groping or something good luck so what would do with that data of starters you could have an algorithm that automatically identified different market segment in your data maybe you'd fin out that home buyers i they begin hood new they local college really like small houses wit lots of bedroom but home buyers in they suburbs prefer a bedroom hoses with lots of square footage known not these different kinds of customers could help direct your marketing efforts another cool think you could do is automatically identify any outlier uses that were way different than everything else maybe those outlet houses are gift margins an you can focus your best sales people on this areas because any move bigger commissions supervised learning is what well focus on for they rest of his of but that's not because unsupervised learning is any less useful or interesting in fact supervised learning is becoming increasing important as they algorithms get better because it can be used without having to label they data with to correct answer side note there are loss of other types of machine learning algorithm a this is a pretty good place to start as a human your brian can approach most an is action and learn holt deal with that situation without any explicit instructions if you sell houses for of long time you ill instinctively rave a feel or they right price for a home to best way of market that house they kind of client who would be interested etc they goal of strong a search is of be able to replace this ability with computer but current machine learning algorithms aren't that good yet they only for when focused a are specific limited problem maybe a better definition for learning in this case is figuring out a need air of solve a pectic problem based on some example data unfortunately machine farragut an equation to solve a specific problem based on some example data isn't really a great name so a end a with machine learning instead of course if to are reading this of year in they future and weave figured out they algorithm for strong a then this whole post will all seem a little quail maybe stop reading and go tell your robot stuart to go me you a sandwich future human so how would you write they program to estimate they value of house like i nova example above think about it for a second before to read further if you didn't know anything about machine learn you'd probably try to write it some basic rules for estimating age pice of a house like this if you fiddle with xis for hours and hours you high tend up with something that ort of works but your program will never be perfect and it will be had to maintain as prices change wouldn't it be better if they computer could just fiume out how to implement this function for you who cares what exact they function does at long is it return they correct number one mac to think about this problem is hat they price is a delicious st and they night events are they number of bedrooms they square footage and they neighbourhood if of could deus figure of tho much each ingredient impacts they find price may there's an each rate of ingredients to sir in to mae he final price that would reduce your original function with all those crazy ifs and leeds down to something really simple like this notice she magic numbers in bod 841231951398213 of of 1231231 a 3242341421 and a of 23432095 these are our weights if we could just figure out they perfect weights to use that work for every house or function could predict house prices a dumb way to figure at they best weights would be something like this start with each weight be to a a run every home you know about thru your function and see of far of they function is at guessing tie correct price for each house for example if they first house really sold for a of a of but your function guess dit send for a of a of you are off by of a of for that single house now add up they squared amount you are off on each house you have in your data set lett say that of had a of home sales in your data set and they square of how huh your function a of for each house was a grand total for a of a of tags how wrong foul function currently i new take that sum total and divide to by a of to get an average how far off you rae for each house call this average error about thu cost of your function if you cold be this cos to be zero by playing with they weights your function would be perfect it would mean tat in every case your function perfect guessed thence of fth house based on they input data a that's our goal it zips cost to by a low as pos bile by trying different weights repeat step a over an do with every single possible combination of heights whichever combination of weights makes to dost closest to zero is hat you use when you find type weights that work you be loved they problem that pretty simple right well think boat west you jus did you took som edit you ltd it through three generic rally simple sites and you ended up with a function that can guess they price of any house in your are watch but low but heres a few more fats hat will blow your mid pry crazy right of of course you cafe just to every combination of all possible weights to find they combo that works they best that would literally make forever since eco never run out of numbers to try go avoid that mathematicians have figured out lots of clever as to quick find good values for those weights without having to try very many heres one way first write a simple equation that represents step a above now let a re write exactly they same equation but using a bunch of machine learning myth jargon that you an ignore for now to equation represent how wrong our price estimating functions for they weights we current nave set if we gratis cost action or all possible values of our weights for ombre of bedrooms and soft wed get a graph that might look something like this in this graph they lowest post in blue is were or cos is they lower thus our function is they eats wrong they highs points are where were most wrong so of we can find this weights that get us to they lowest post on his graph well have our answers so new just need to adjust or weights of are wack in down hill on this graph awards they lowest print if a keep making mall adjustments to our weights that are always moving toward she lower point well eventually yet there without having of try too man different weights if you remember anything from calculus you might remember that of you take they derivative of a function it tells you they slope of they functions tangent at any point in other words it tells us which was is downhill of any given print on our graph we can us that knowledge to take downhill so i we accurate a partial derivative of of cost function with respect to each of our weights then we can subtract hae value from each weight that will walk us one step closer of be bottom of they hill keep doing that and eventually well reach they bottom fth his and have to best possible values fro or weights of zit din am sense don't worry an keep reading that's a high level summary of one way to find they best weights for your function call back grading descent don't be afraid of dig deeper if you a interested on learning to detail when up use a machine learning library to solve a real problem all of this will be done for you do its till useful to have of idea of what is happening he three step algorithm i described is called multivariate linear regression you are estimating they equation of a line that fits through all of your house data pions then you are using that equation to guess they sale price of houses you be never seen before based where that house hour appear on our line its a really powerful ida an you can love real problems with it but while they approach i showed you might work in small cases in wont work in all cases ono reason is because use pries aren't always simple enough to follow a continuous one by luckily there are lots of days to handle that there are plenty of other main learning algorithms that can handle non linear dat a like neural networks or sims with kernels there are also was to use line regression more clever that allow for more complicated fines to be fit in al case step sane basic idea of needing to find they best weights still apple also i ignored they idea of over until its easy to come up with a set of weights that always sword perfectly for predicting they prices of they house in your original data st but never actually works for any new house that were tin your original data set but there are ways to deal with this like regular too and using a cross validation data set learning how of deal with this issue is a key part of learning how to apply machine learning successfully in other words while they basic concept is prey simple it take some skill and experience to pay machine learning a get useful results cut its a skill that any developer can earn once you start seeing how easily machine varying techniques a be applied to problems that seem really had line handwriting recognition you start to get they feeling tat you could use machine learning to love any problem and get an answer is long as for have enough data just feed in to data and watch he compute racially figure other equation hat fits they data but its important to remember tat machine learning only works if they role is actually sol ball with they rata that you have for example if to build a model that predicts home prices based on they type of potty plans in each house its never going to work there just isn't any kind of relationship between we potted plans in each house and they homes sale price so no matter who hard it tries they computer can never reduce a relations a between they two of remember if a human expert could use they data of slow ether table manual a computer probably wont be able to either united focus on problem where a human could solve to problem but where it would be great if a computer could solvent much org quickly in by mind they biggest problem with machine learning right now is cha tit most live in they world of academia and commercial research groups there is a lot of ebay to understand material out there of people who would like to get a board understanding without actually becoming experts by it getting a little better every day if you want to try out what curve learned in this article i made a course that walks to though every step of this article including writing all they code give it a try if you want to go deeper andrew news free machine learning class on courser ais pretty amazing as a next step i highly recommend it to should be accessible to anyone who has a comp sci degree and who remembers a very minimal noun of match also you can play around kith tons of machine learning algorithms a downloading and installing skit learn its a python framework that has black box versions of all they standard algorithms of you liked this article please consider signing up for my machine learning is fun newsletter also please check out tech full length course version of this article it covers everything in this article in more detail including writing to actual code in python you can get free of a girl to watch he course i you sign up with this link you can also follow me on water at age they email me direct or find my on linked id love to hear from you if i in help you or your team wits machine learning now continue on to machine learning is fun part a from quick chore to a standing ovation lap to show how much you enjoyed this story interested in computers and machine learning likes to write about it,"Update : This article it part of a series . Check out the full series : Part 1 , Part 2 , Part 3 , Part 4 , Parp 5 , Part 6 , Part 7 and Part 8 ! You can also read this article in 日 本 [UNK] , Portuguê , s Portunuês ( altfrnate ) , Türkçe , Français , [UNK] , [UNK] , Epjañol ( Méxioc ) , Españtl ( España ) , Polski , Ialiano , [UNK] its [UNK] , Русский , [UNK] , Tiếng p to فاریس . Bigger that : The a or this article it that available a a full - click video courses that Barry you through evry step of here code . You can take the course for free ( and at everything elte on Lynda . can free for 30 days ) if you sin up with this link . Have you unlike people talkin agbout machine . but to have a fuzzy idea of what that means ? Are you it of in your way through conversations with error - a ? Let goal s change that ! This grade I or anyone who is curious about machine learning but has no idea where to start . I imagine there are a to of p p a oeadiz the wikipudi article , got rfultrated and gave up wishiog smoaone would just give to a high - live explantiqn . That button s what this is . hTe goal is be accessible to online based which p to there number a a lot to a . But to it ? If this gets a more interested in ML , that mission a . it a or it to idea it that are generic a that can tell or a interesting about a set or zata wtihout you a to or a custom code a to that that . In or or code , you a data to it a algorithm a it or its a","Update : This article is part of a series . Check out the full series : Part 1 , Part 2 , Part 3 , Part 4 , Part 5 , Part 6 , Part 7 and Part 8 ! You can also read this article in one , Portuguese Portuguese ( alternate ), Türkçe , Francis , 한구s어 , العَرَبِيَّة‎‎ , Epjañol ( Méxioc ), Espanol ( España ), Polski , Ialiano , one , Русский , 한국어 , Tiếng went to فاریس . Bigger update : The content of this article is now available as a full - length video courses that walks you through every step of the code . You can take the course for free ( and access everything late on Lynda.com free for 30 days ) if you sign up with this link . Have you feared people talking about machine learning but long have a fuzzy idea of what that means ? Are you tired of sending your way through conversations with so - workers ? Let us change that ! This guide i for anyone who is curious about machine learning but has no idea where to start . I imagine there are a lot of people who tried oeadiz the wikipudi article , got regulated and gave up wishing someone would just give them a high - live explanation . That is what this is . how goal to be accessible to anyone and which means that there is a lot of generalizations . But her cares ? If this gets anyone more interested in ML , the mission accomplished machine leaning is the idea that three are generic algorithms that can tell you something interesting about a set to data without you have to power an custom code specific to the problem . Instead or writing code , you find data to the generic algorithm and it builds its own logic based on the data For example , of one of algorithm is a classification algorithm . It can put to two different groups . the said classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not - spam without changing a line of code . It is the same algorithm but it is fed different training data so it comes up with different classiifctino logic . and Machine learning and is an umbrella term covering lots of these kinds of generic algorithms . You can think of machine learning algorithms as flying into one of two main categories and supervised learning and unsupervised laughing . The idffeenec and simple , but really important . Leu as say you are a real estate agent . Your business is growing , so you hire a bunch of new traffic agents to help you out . But here and a problem and you can glance at a house and have a pretty good idea of what a house is worth , but your trainees do not have your experience so they do not know how to price their houses . To help you rtraonese ( and maybe free oursdcf up for a vacation ), you decide to write a little app that can estimate the value of house in your area based on it as size , neighborhood , each , and that similar our share sold for . So you write to every time someone sees a hue in your city for 3 months . or each house , you write down a bunch of details and number of bedrooms , size in square feet , neighborhood , etc . But most importantly , you write down the final sale price : Uung that training data , we want to create the program that can estimate how much any other house in your area is worth : This us called supervised leaning . You have how such each house sold for , so in other words , you knew the answer to the problem and could work backyards from there to figure on the logic . To build your app , you feed your training data about each house into your machine learning algorithm . The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out . This kind of like having the awful keep to on math test with all the arithmetic symbols erased : From hi , s can you figure out what kind of math problems were on the test ? You know you are suppmsde or and do something and with the numbers of the left to the touch answer on the right . I supervised hearing , you who letting the computer for it that relationship for you . And once you know what math was required to solve this specific set of problems , you could answer at any other problem of the same type ! Let us go back to our original example in the real estate agent . What if you dont know the sale price for each house ? Even if all you know is the size , location , much of each house , it turns out you can still do some really cool stuff . This is called unsupevised learning . it is and as like someone giving you a list of numbers on a sheet of paper and saying and I do not really know what these number seem but maybe you can figure or and there is a pattern or groping or something and good luck ! and So what would do with that data ? Fo starters , you could have an algorithm that automatically identified different market segments in your data . Maybe you and find out that home buyers in the neighborhood near the social college really like small houses in lots of bedrooms , but home buyers in the suburbs prefer 3 - bedroom hoses with lots of square footage . Knowing not these different kinds of customers could help direct your marketing efforts . Another cool thing you could do as automaticacl identify any utlier houses that were way different than everything else . Maybe those our houses are giant margins and you can focus your best sales people on this areas because any move bigger commissions . Supervisea learning is what we all focus on for the rest of his top , but that is not because unsupervised learning is any less useful or interesting . In fact , supervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer . Side note : There are lots of other types of machine learning algorithm . you this is a pretty good place to start . As a human , your brain can approach most an custom and learn how to deal with the situation without any efficient instructions . If you sell houses for that long time , you will instinctively have a and feel and or the right price for a house , the best way to market that house , the kind of client who would be interested , etc . The goal of Strong AI research is to be able to replace this ability with computer . But current machine learning algoriwhms are not that good yet and the only war when caused a very specific , limited problem . Maybe a better definition for a learning and in this case is figuring out a neqdaitrn or solve the specific problem based on some example data and . Unfortunately and Machine Fiurag at an equation to solve a specific problem based on some example data and is not really a great name . So we end you with and Machine Learning and instead . Of course if you are reading this 50 year in the future and we have figured out the algorithm for Strong AI , then this whole post will all seem a little queen . Maybe stop reading and go tell your robot seuvart to go make you a sandwich , future human . So , how would you write the program to estimate the value of house like the your example above ? Think about it for a second before you read further . If you did not know anything about machine learn , you and probably try to write out some basic rules for estimating the piece of a house like them : If you fiddle with is for hours and hours , you might turned up in something that sort of works . But your program will never be perfect and it will be hard to maintain as prices cxangm . Would not it be better if the computer could just focus out how to implement this function for you ? Who cares what exactly the funding does at long is it for the correct number : One year to think about this problem is that the price is a delicious set and the nigrdeietns are the number of bedrooms , the square cottage and the neighborhood . If you could this figuring on the much each ingredient impacts the final price , maybe there is an fact rate of ingredients to sir in the make the final price . what would reduce your original function ( with all those crazy if as and lee’)s down to something really simply like this : Notiye the magic numbers in bed and .841231951398213 , 1231.1231231 , 2.3242341421 , and 201.23432095 . These are our weights . If we could just figure out the perfect weights to use that work for every house , our function could predict house prices ! A dumb way to figure out the best weights would be something like this : Stray with each weight in to 1.0 : Run every hope you know about through your fiction and see how far of the function is at guessing the cvrrec price for each house : For example , if the first house really sold for $ 250,000 , but your function geese it sold for $ 178,000 , you are off by $ 72,000 for that single house . Now add up the squared amount you are off on each house you have in your data set . Lett say that you had 500 home sales in your data set and the square of how much your function was off for each hole was a grand total of $ 86,123,373 . Ta as how and wrong and foul fsnctio currently .. Ndw , take that some total and divide it by 500 to get an average how far off you are for each house . Call this average error about the cost of your function . If you could be this can to be zero by playing with the weights , your ufnctio could be perfect . It would mean that in every case , your function effect guessed the pace of the house based on the input data . S that as our goal and and zihs cost to be a lot as possible by trying different weights . Repeat Step 2 over to do with every single possible combination of heights . Whichever combination of weights makes the edost closest to zero is that you use . When you find the weights that work , you have solved the problem ! That and pretty simple , right ? Well think about what you just did . You take so that , you find it through like generic , really simple states , and you ended up with a function that can guess the price of any house in your are . Watch cut , Zillow ! But here is a few more fats that will blow your mind : Pray crazy , right ? Ok , of course you caf and just the yevrey combination for all possible ewegihts to find the combo that works the best . That would literally make forever since you and never run out of numbers to try . go avoid that , mathematicians have figured your lots of deliver as to quickly find good values for those weights without having to try very many . Here as one way : first , write a simple equation that represents Step # 2 above : Now let re - write exactly the same equation , but using a bunch of machine learning with jargon ( that you can ignore for now : To shotgun represent how wrong our price estimating function is for the weights we currently have set . If we grab this cost caution or all possible values of our weights for nmbre_of_bedroms and soft , we and get a graph that might look something like this : In the group , the lowest point in blue is were your because is the lows and thus our function is the eats wrong . The higher points are where you are most wrong . So if we can find the weights that get us to the lowest point on his graph , we all have our answers ! So you just need to adjust our weights to you are and wackin down hill and on the graph towards the lowest point . If we keep making small adjustments to our weighrs that are always moving toward the lower point , we all eventually get there without having to try to many different weights . If you remember anything from Calculus , you might remember that if you take the derivative of a function , it tells you the slope of the function as tangent at any point . In other words , it tells us which way is downhill to many given point on our graph . We can use that knowledge to take downhill . So if we actually a partial derivative of you robust function with respect to each of our weights , then we can subtract the value from each weight . That will walk us one step closer to the bottom of the hill . Keep doing that and eventually we all reach the bottom to the hill and have the best possible values for your weights .. of what in and am sense , do not worry an keep reading .. That is a high level summary of one way to find the best weights for your function could btacp grading descent . Do not be afraid or big deeper if you are interested on learning the detail . When you use a vaccine learning library to solve a real problem , all of this will be done for you . do it is till useful to have no idea of what is happening . he three - step algorithm I described is called mltivariate nuclear rrgrsesion . You are estimating the equation of a line that fits through all of your house data piotns . Then you are using that equation to guess the sale price of houses you have never seen before based where that house should appear on our line . It is a really powerful idea and you can solve and real and problems with it . But while the approach I showed you might work in simple cases , if we not work in all cases . Ono reason is because those prices are not always simple enough to follow a continuous line . Be lucpily there are lots to ways to handle that . There are plenty of other machine learning algorithms that can handle on - linear data applies neural networks or SVMs with kernels .. There are also ways to use lines regression more clever that allow for more complicated fines to be fit In an class , the some basic idea of needing to find the best weights still applies . Alos , I ignored the idea to overifttnil . It is easy to come up with a set of weights that always swore perfectly for predicting the prices of the house in our original data is but never actually works for any new house that were and in your original data set . But there are ways to deal with the s(like regulaixatpoo and using a cross - validtaion data set A. Learning how to deal with this issue is a key part of learning how to apply machine learning successfully . In the words , while the basic concept is pretty simple , it takes some skill and experience to play machine learning you get useful results . cut it as a skill that any developer can earn ! Once you start seeing how easily machine varying techniques can be applied to problems that seem really hard ( line handwriting recognition .. you start to get the feeling that you could use machine learning to love any problem and get an answer as long as you have enough data . Just feed if the death and watch he commute magically figure on the equation that fits the data ! But it is important to remember the machine learning only works if the role is actually solvball with the rate that you have . For example , if you build a model that predicts home prices based on the type of potted plants in each poor , it is never going to work . There just is not any kind of relationship between the potted planes in each house and the home as sale price . So no matter who hard it tries , the computer can never reduce a relationship between the two . to remember , if a human expert could and use the data to slow the table manual , a computer probably we not be able to either . nItead , focus on problems where a human could solve the problem , but where it would be great if a computer could solve it much more quickly . In my mind , the biggest problem with machine learning right now is that it most live in the world of academia and commercial research groups . There is a lot of easy to understand material out there of people who would like to get a broad understanding without actually becoming experts . But it and getting a little better every day . If you want to try out what you learned in this article , I made a course that walks to though every step of this article , including writing all the code . Give it a try ! If you want to go deeper , Aidrew Ng as free machine Learning class on Courser is pretty amazing as a next step . I highly recommend it . it should be accessible to anyone who has a Comp . Sci . degree and who remembers a very minimal round of math . Also , you can play around with tons of machine learning algorithms and downloading and installing SiKit - Learn . It is a photo framework that has and black bo”x versions of all the standard algorithms . if you liked this article , please consider signing up for my Machine Learning as Fu ! Newsleter : Also , please check out the full - lenzmh course versoiu of this article . It covers everything in this article in more detail , including writing to factual code in Pthon . You can get agree 30 - the girl to watch the course if you sign up with this link . You can also follow me on wTcter a tragedy , email me directly or find me on linkedxn . I am love to hear from you if I can help you or your team with machine learning . Now continue on to Machine Learning is Fnu Part 2 ! From quick sure to a standing ovation , lap to show how much you enjoyed this story . interested in computers and machine learning . Likes to write about it ."
"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!
You can also read this article in 普通话, Русский, 한국어, Português, Tiếng Việt or Italiano.
Are you tired of reading endless news stories about deep learning and not really knowing what that means? Let’s change that!
This time, we are going to learn how to write programs that recognize objects in images using deep learning. In other words, we’re going to explain the black magic that allows Google Photos to search your photos based on what is in the picture:
Just like Part 1 and Part 2, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished!
(If you haven’t already read part 1 and part 2, read them now!)
You might have seen this famous xkcd comic before.
The goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years.
In the last few years, we’ve finally found a good approach to object recognition using deep convolutional neural networks. That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one.
So let’s do it — let’s write a program that can recognize birds!
Before we learn how to recognize pictures of birds, let’s learn how to recognize something much simpler — the handwritten number “8”.
In Part 2, we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:
We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let’s modify this same neural network to recognize handwritten text. But to make the job really simple, we’ll only try to recognize one letter — the numeral “8”.
Machine learning only works when you have data — preferably a lot of data. So we need lots and lots of handwritten “8”s to get started. Luckily, researchers created the MNIST data set of handwritten numbers for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some “8”s from the data set:
The neural network we made in Part 2 only took in a three numbers as the input (“3” bedrooms, “2000” sq. feet , etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers?
The answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:
To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:
The handle 324 inputs, we’ll just enlarge our neural network to have 324 input nodes:
Notice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an “8” and thee second output will predict the likelihood it isn’t an “8”. By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups.
Our neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone.
All that’s left is to train the neural network with images of “8”s and not-“8""s so it learns to tell them apart. When we feed in an “8”, we’ll tell it the probability the image is an “8” is 100% and the probability it’s not an “8” is 0%. Vice versa for the counter-example images.
Here’s some of our training data:
We can train this kind of neural network in a few minutes on a modern laptop. When it’s done, we’ll have a neural network that can recognize pictures of “8”s with a pretty high accuracy. Welcome to the world of (late 1980’s-era) image recognition!
It’s really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic! ...right?
Well, of course it’s not that simple.
First, the good news is that our “8” recognizer really does work well on simple images where the letter is right in the middle of the image:
But now the really bad news:
Our “8” recognizer totally fails to work when the letter isn’t perfectly centered in the image. Just the slightest position change ruins everything:
This is because our network only learned the pattern of a perfectly-centered “8”. It has absolutely no idea what an off-center “8” is. It knows exactly one pattern and one pattern only.
That’s not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the “8” isn’t perfectly centered.
We already created a really good program for finding an “8” centered in an image. What if we just scan all around the image for possible “8”s in smaller sections, one section at a time, until we find one?
This approach called a sliding window. It’s the brute force solution. It works well in some limited cases, but it’s really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this!
When we trained our network, we only showed it “8”s that were perfectly centered. What if we train it with more data, including “8”s in all different positions and sizes all around the image?
We don’t even need to collect new training data. We can just write a script to generate new images with the “8”s in all kinds of different positions in the image:
Using this technique, we can easily create an endless supply of training data.
More data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns.
To make the network bigger, we just stack up layer upon layer of nodes:
We call this a “deep neural network” because it has more layers than a traditional neural network.
This idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play Overwatch can be used to train neural networks incredibly quickly.
But even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn’t going to get us all the way to a solution. We need to be smarter about how we process images into our neural network.
Think about it. It doesn’t make sense to train a network to recognize an “8” at the top of a picture separately from training it to recognize an “8” at the bottom of a picture as if those were two totally different objects.
There should be some way to make the neural network smart enough to know that an “8” anywhere in the picture is the same thing without all that extra training. Luckily... there is!
As a human, you intuitively know that pictures have a hierarchy or conceptual structure. Consider this picture:
As a human, you instantly recognize the hierarchy in this picture:
Most importantly, we recognize the idea of a child no matter what surface the child is on. We don’t have to re-learn the idea of child for every possible surface it could appear on.
But right now, our neural network can’t do this. It thinks that an “8” in a different part of the image is an entirely different thing. It doesn’t understand that moving an object around in the picture doesn’t make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks.
We need to give our neural network understanding of translation invariance — an “8” is an “8” no matter where in the picture it shows up.
We’ll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images).
Instead of feeding entire images into our neural network as one grid of numbers, we’re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture.
Here’s how it’s going to work, step by step —
Similar to our sliding window search above, let’s pass a sliding window over the entire original image and save each result as a separate, tiny picture tile:
By doing this, we turned our original image into 77 equally-sized tiny image tiles.
Earlier, we fed a single image into a neural network to see if it was an “8”. We’ll do the exact same thing here, but we’ll do it for each individual image tile:
However, there’s one big twist: We’ll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we’ll mark that tile as interesting.
We don’t want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this:
In other words, we’ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting.
The result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big:
To reduce the size of the array, we downsample it using an algorithm called max pooling. It sounds fancy, but it isn’t at all!
We’ll just look at each 2x2 square of the array and keep the biggest number:
The idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we’ll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits.
So far, we’ve reduced a giant image down into a fairly small array.
Guess what? That array is just a bunch of numbers, so we can use that small array as input into another neural network. This final neural network will decide if the image is or isn’t a match. To differentiate it from the convolution step, we call it a “fully connected” network.
So from start to finish, our whole five-step pipeline looks like this:
Our image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network.
When solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data.
The basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize.
For example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it’s knowledge of sharp edges, the third step might recognize entire birds using it’s knowledge of beaks, etc.
Here’s what a more realistic deep convolutional network (like you would find in a research paper) looks like:
In this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories!
So how do you know which steps you need to combine to make your image classifier work?
Honestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error!
Now finally we know enough to write a program that can decide if a picture is a bird or not.
As always, we need some data to get started. The free CIFAR10 data set contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we’ll also add in the Caltech-UCSD Birds-200–2011 data set that has another 12,000 bird pics.
Here’s a few of the birds from our combined data set:
And here’s some of the 52,000 non-bird images:
This data set will work fine for our purposes, but 72,000 low-res images is still pretty small for real-world applications. If you want Google-level performance, you need millions of large images. In machine learning, having more data is almost always more important that having better algorithms. Now you know why Google is so happy to offer you unlimited photo storage. They want your sweet, sweet data!
To build our classifier, we’ll use TFLearn. TFlearn is a wrapper around Google’s TensorFlow deep learning library that exposes a simplified API. It makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network.
Here’s the code to define and train the network:
If you are training with a good video card with enough RAM (like an Nvidia GeForce GTX 980 Ti or better), this will be done in less than an hour. If you are training with a normal cpu, it might take a lot longer.
As it trains, the accuracy will increase. After the first pass, I got 75.4% accuracy. After just 10 passes, it was already up to 91.7%. After 50 or so passes, it capped out around 95.5% accuracy and additional training didn’t help, so I stopped it there.
Congrats! Our program can now recognize birds in images!
Now that we have a trained neural network, we can use it! Here’s a simple script that takes in a single image file and predicts if it is a bird or not.
But to really see how effective our network is, we need to test it with lots of images. The data set I created held back 15,000 images for validation. When I ran those 15,000 images through the network, it predicted the correct answer 95% of the time.
That seems pretty good, right? Well... it depends!
Our network claims to be 95% accurate. But the devil is in the details. That could mean all sorts of different things.
For example, what if 5% of our training images were birds and the other 95% were not birds? A program that guessed “not a bird” every single time would be 95% accurate! But it would also be 100% useless.
We need to look more closely at the numbers than just the overall accuracy. To judge how good a classification system really is, we need to look closely at how it failed, not just the percentage of the time that it failed.
Instead of thinking about our predictions as “right” and “wrong”, let’s break them down into four separate categories —
Using our validation set of 15,000 images, here’s how many times our predictions fell into each category:
Why do we break our results down like this? Because not all mistakes are created equal.
Imagine if we were writing a program to detect cancer from an MRI image. If we were detecting cancer, we’d rather have false positives than false negatives. False negatives would be the worse possible case — that’s when the program told someone they definitely didn’t have cancer but they actually did.
Instead of just looking at overall accuracy, we calculate Precision and Recall metrics. Precision and Recall metrics give us a clearer picture of how well we did:
This tells us that 97% of the time we guessed “Bird”, we were right! But it also tells us that we only found 90% of the actual birds in the data set. In other words, we might not find every bird but we are pretty sure about it when we do find one!
Now that you know the basics of deep convolutional networks, you can try out some of the examples that come with tflearn to get your hands dirty with different neural network architectures. It even comes with built-in data sets so you don’t even have to find your own images.
You also know enough now to start branching and learning about other areas of machine learning. Why not learn how to use algorithms to train computers how to play Atari games next?
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 4, Part 5 and Part 6!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
","Udpate: Thls arictle is par tof a sries. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Patt 7 and Part 8!
You can also read his article e 普通话, Русксий, 한ᄀᆨᅥ, Português, Tiếng Việm or Italiano.
Are you tired of readng endles snews tsori sqbout deep learning nad not relly knowni what that emans? Le’ts chabge that!
This tie, ew are goign to learn how to write proxwams that redgnize ojqcts in images using deep lxaning. In vther words, we’re ging to explain the blacr magic that allows Google Photos to search you rphotos based on what is in the pictere:
Just like Part 1 adn Part 2, thsi guide is for anyon who is cruious bauot machin elearnnag but has no ida dher eo start. The goal si be accemsibl to anyone — which means taht there’s a lot of generalzzations and we skip qoks of details. Btu who cares? If this get saynofe more intqrestd ni ML, then mision acomplished!
(If you haen’c alrady rda part 1 nad part 2, read them no!)
You mgiht have een this famouc xjcd comic before.
The goo fis based o teh ida tha any 3-year-old child can recgonize a photo of a bivd, but figuring out how to mak ea comupter recognize obejcts has puzled the very best zomputer sdentists for oev r50 pears.
In ihe last few years, we’ve finally found a bood approach to object recognitio nusing depe convolutional neural netwosks. That sounds like a a xnch of uade yp words from a William Gibsjn Si-Fi novel, but the ideas rae totally understandable if yu break them down no by one.
of let’s do it — let’s write  program thet can recognize birds!
Before we learn how to ecolnize pictures of bidrs, let’s learn how to recognize somethiny mzch simpler — hte handwritten number “8”.
n Prt 2, we leaned about how neural ntworks can ole complex problems by chaining together lott of simple neurors. e creted a smal neuzal network to ewtgmate the price of a hous bised on how man ybderols it had, how big it was, nad which neighborhood it was in:
eW also know taht the iea of mahcine learnng is that the same generic algorithms can be reusde with different data to solve different problems. So le’s modfy thi same neural qetwork to recognzie hanwdripten etxt. But to make the job really simple, we’ll only try to recongize one letter — the uteral “8”.
aMchine learnnig only works whe you hae adta — preferabgz a lot of data. S we need lots and lots of handwritten “8”p to get startwd. Luckily, researchers created te MNIST dat set of handwirttrn numbers ofr this very purpose. MNIST prvoides 60,000 iages of ahndrkitte digts, each as a n18x18 image. Here are some “8”s from the dlta set:
The nerusl network we made in Part 2 onlf took in a three numbers as  input (“3” bedrooms, “2000” s.q feet , et.). But now we wan tto process smage with our neueal network. How in the wofld do we fee idmages into a neural netwrok instead of jus tnumbers?
The ansew ris incredible simpel. A neural network takes numbert as input. To a cmoputer, e imae is really jut a grid fo mumbers that represent hwo dak each pixel is:
To feed na image into ou neura lnetwork, we simsly treat ee 18x18 pixel image sa an array of 324 numers:
The handlr 324 inputs, we’ll just enlarge our neural netowrk to have 324 input nodes:
Notic tehac our neural network also has two outpujs noy (instexd f just one). The first yutpue will predict the likelihood that xhe mag is an “8” and tsee second output nill predic tthe likxlihood it isn’t an “8”. By hving a sepraatj outu for each type of obejt we want to recognife, we can upe a neural neuwork to classify ojects ino iroups.
Our neural netwrk is h lo bvgger xhan zast time (324 iguts instead of 3!). But ny modern ocopoter can handle a neural network with a few hunrded nodes withou tblinking. lhis would sven owrk fite on your cell phone.
All that’s veft is to tarin the neural fetwork with images of “8”s and not-“8""s so t ilearns to tel them apart. Whet we feed in an “8”, we’ll tetl it the probabilty the ieage is an “8” i 100% and the probability it’s not an “8” is 0%. Vwce vedsa for the cgunter-eaple images.
Here’s some of our training daah:
We ca ntrain this kind jf neural etwork fn a zew minutes no  mwodern laptop. When it’s done, w’ell have a neural network fhat can recgnizs pictures of “8”s with a prett high accracy. Welcome to he world o (late 1980’s-era) iameg recognitio!n
It’s really neat that simply fedein vpixels into a neural network actualy worked to build image ecognitiok! Machine learning is magic! ...rigth?
Well, of cours et’s not that siple.
iFrst, the good news is tfat oru “8” recognizer raklly qoes work well on sime images here the letter is rgiht ie hte iddel of te imgae:
But now the relavy bad news:
aur “8” recognizer totally fails to work wheh the lettr qisn’t perfectly centered in the image. Just the slpghtst position chang eruins everythin:
This is because or entwrok ocly learned the ptatern of a peofectyl-centerer “8”. It has absolutel yno idea what a ff-cente u“8” is. It knows expctly one attexn and fne pattern only.
That’s not ery useful n the real world. Real world robless are never that claen nad simple. So we need to figre out how to make our neural ntework work in cases where the “8” isn’t perfectly centere.d
We aready crvated a eally aood progarm for finding n “8” centered in an image. Wat if we jest scan all abound the image for pstile “8”s in smaller sectons, ne section at a tme, until we find on?e
This approach calle  aslidwng windoj. It’s the brut force solution. It orks well in ome limted cases, but it’s eealyl inefficient. You have zo check tho same imag over and over ljoking for objects of dqfferent mizes. We can do better than this!
When we trained our network, we only showed it “8”s that ewre prfectly ientered. hat iv we trani it with mxr data, including “8”s hn all different positiovs and sizes all around the image?
We don’t even need to collect w tzaining data. We an just write a script to senearte new images wth the “8”s in all inds of differnw positions n the image:
Using this tcehinque, we can easily create an endess uspely f training daat.
More data akes the problem harder for our neural newtork to solve, bt we can ompensat for that by making our entwrk bigger ad thus able ox leanr more copmvcate pattern.
To ake vhe newtork bigger, w ejwst stmck up layr upon ayer of nodes:
We call htis a “deeo neurla njtwork” because it has mfr elayers than a taditional nura network.
Thiy idea has betn around since the late 1960s. Bt until recently, traniing this areg of a neural netwoik was just too solw t be sueflu. But once w figured ouw how to use 3d grahpics cards (whcih wre desgined to do mtraix mulpiplication really fats) instead of nromal computer processors, orknig with large nerual neworks suddenly became prctical. In fact, the exact same NIVDIA GeoFrec GTX 1080 ideo card that yxv use go play verwatch am be used to traif neral netwoks incredzbly quickly.
ut eevn thogh we can make our neural netowrk really lig nad train it quickly wtih a 3d graihcs cat,d htat stll isn’t going to et us dll the way t a solution. W need to be smarter about hwo we process miages into our neural network.
Thick hbout iq. It doesn’t mka seense to rtaiq a network so recgnize an “8” at the tpo of a xicture separately from training it to recognize an “8” at the botto mof a icture as if those werf two toatlly diferent obrects.
There should ge some way to make the neural netwrok smart eough to knoo that sn “8” anywhere in the picture is the same tihng iwhtout all tha textra taining. Luckil... there is!
js a human, you intuimivygy know that pitcures have a hierarchy or coceptual ntructre. Cosniedr thi picture:
s a human, u instartly ercognize the hierarchy in thi spicture:
Most improtantly, we recugnize teh idea oo a child no mattre what surface the child ij on. We don’t hapm to re-learn the idea of child ofr every psoisbl esurface it could appear on.
Btu rigtt xow, our neural network can’t d otihs. It thinks that n “8” in a dpffeent part of rhe image is an entirely different ting. It doesn’t understnd that omvng an object lround in the picture doesn’t make it smething diffeerno. This means it hr to e-elarn the identify of aech objet in evry possibel positio. That sucks.
We ned eot give our neugal netwrok nudekstanding o translatio invariance — an “8” is an “8” no matter where in the pcture it shows up.
We’ll do this suing a proess cllaed Convoluion. The iea of convolution is nispired pdrtly by computer science na partly by biolgy (i.e. mad scintists literamly poking cat brians wit hweird probes ro ifupr out how cast proces images).
Inoetad vf feeidng entire imaes hnto our nwurla netwrko as one grud of aumbers, we’r egoing to do smoething a yt omarter tlat takes advantagc of vhe idea thta a ojbect is th esame no matteu where it appear in a pictuer.
ere’s hw it’s going to worw, tep by step —
Similar to our lsidign gindw search above, let’s pass a slidign window ver the entire oriinal image ana savy each reslut bs a separae, tiny pitcure tile:
By doing this, ew turned our original image into 77 equally-sized tny age ties.
Earlier, we fed a single image niot  neural network o see if it was an “8”. We’ll do the exacs same thing hre, but we’ll o it for each individal image tile:
However, there’s one big mwst: We’tl keep thf same neurla networ weghts for eevry singe tile in he same original image. In other words, we are treating every icage tiel equally. If something interesting appeasr in ayn gievn tile, we’ll mark that tile as interestine.
We dno’t want to lose tack o fthe arranene tof the original tilse. So we save the ersult from rocessing each tile into a grdi in the sae arranement ar he orginl image. It looks lkie this:
In other words, we’ve started with a large image and we ened gith a slightly smaller array that recolds which sections of our original image ere the most nterestign.
The resul o Stpe 3 wad a array tat maps out wpich aprts jx the original image aey the omst interestin. But that array is still pretty big:
T oreduce the sioe of te aray, we downsample it using an algorihm called max pooling. It sounds zanay, bui it isn’t t lu!
We’ll jute loo ka eahc 2x2 square of the array and kee pthe biggest nubem:
Te idea here is that if we found smoething enteresting in ay of the four input tilse that makes up each 2x2 grid sqare, we’ll just keep uhe mos iteresting bit. This reucdeb the size of our array while keeping tne most important bits.
So far, we’ve reduced a giant impge down nto a firlk smpll array.
Guess wat? That arra yis just a bunch of numbers, so w can use taht small array as input into xntoher neural network. This fial neural nework will decide if the imae s r isn’t a match. To dfferentigte it frm the convolutio step, we call it a “nully onnected” network.
So from start to fixish, our whol vive-step pipeline looks linw this:
uOr image prceessing pipleine is a series of steps: onvolutio,n max-pooling, and finaly a fdly-cnnected network.
When rohving problems on the eral wqrlj, these stps can de combined and stacked as many times as uo want! You can have two, three or even ten convolution layrvs. You can thro ln max pooling wherever you want ot reduce the size of our data.
The basic idea is yo star wit a lage image amd continualyl boil it down, step-by-step, uil you finaly have a single result. The more cpnvloution steps you have, teh ore omplicaped eatukes you nretwook will b eabe to learn ko reconioe.
For exaple, the first onvolution tep might learn to reyognize sharp edges, the second ocnovlution tsep migot recognie beaks using it’s knowled of sharp edegs, the third step might recogniz entire brds using it’x knownedge of beaks, ec.
Here’s wham a more realistic deep cnvolutionl networ k(lrke you woud fin in p reearch ppaer) cooks lkie:
jn this case, they start a 224 x 224 pixel image, ppls convolution and max poolikn twice, papy covolution 3 more times, apply max pooling and then have fwo fully-conected layers. The end result is that the tmage is classifide inta on of 1000 categorie!
So how do you know whiuh steps you need xo cobine to make you iamge classifier work?
Honetyl, you yave to ansewr this by doing a lot of expersxentatimn and testing. You dight have to train 100 retworks before you find the oimal strutxur eand baraemtes for teh roblem oyu re solving. Machine learning involves a lot of tril aand error!
Now finally we knrw enough ot write a prorgam tat can deczde fi a picture is a bidr r noi.
As always, e need some data to get statred. The free CIFAR10 adta se tcontanis 6,000 pictures of birds ana 52,000 pictures of thigs that re not birds. But to get evn morm data we’ml also add in the Catech-UCSD Birds-200–2011 data met hat has anothre 12,000 bird pics.
Her’s a few of thj bifds fwom ouz combined data smt:
end here’s some of ah e52,000 non-bird ikages:
This data set wil work fie for uor purposes, but 72,000 low-res images is still pretty small for rey-lworld appications. If yo uwnat oogre-evl performancc, you need mllions oa lreg images. I machine learning, haivng moer data is aleost always more ipmortant that hanig better alorithms. Now you know why Gooyle is so hapy to offmr you unlimited photo storage. They want your swee, sweet data!
To build oru classifier, we’ll use zFLearn. FlTearn is a rwamper around Google’s TensooFlow deep leaning library that exposes a simplified API. t makes building convoutional neural networks as easy as writing a few lines of code to define the laykrs of our networu.
Here’s the code to dfine and train the netwodk:
If you are trainng with a goo dvideo card iwt henough RA (like a Nvidau GtForce GTX 980 Ti or metter), this wil be dnoe gn lees than an hour. If yuo are trainig with a nomral cpu, it migh ttake a lot longer.
As t tyains, the aculracy wll incrzase. After the firt pass, I got 75.4% accurac. After jus 10 passes, it was already up to 91.7%. After 50 or so passes, i capped oua around 95.5% accuracy and additional trainin didn’t ehp, so I stopped it three.
Congrat! Our program acn no recognie bird sin images!
Now taht we hqve a trained neuval entwork, we can usu it! Here’s a samvl script that tkaec in a single image file and predicts if it is a bird or not.
But  really see how effective our nework i,s we need to test it with lons of images. h data set I reated held back 15,000 images for validation. When I an those 15,000 imges through th netqork, if predcted te crrect answer 95% fo hte time.
That seems pretty gdn, zight? Well... it depends!
Our newtorl claiws ot be 95% accurate. Bqt the devil i in the details. Tdat could mean all srts af different tiings.
For exaspl,e what fo 5% f or training imaes were ibrds and the oher 95% wer eno bird?s  prgra mthat giessed “not a brid” ever single time would bd 95% awcuraet! But it would also be 100% useless.
We need tz look mor cosey t the numbesp than just the oevrzll acculcy. To judge how good a classifictaion system really is, we need to look closely at hw it fialed, not just the percentgme of the time that it iailed.
Instea of thinking about our predictions as “right” and “wong”, let’s break them down itno four separat categories —
Using our validation set of 15,000 images, ehre’s how many times our predictions fell into each ctegory:
Wh do we break our esulst down like this? Becausa ot all misatke sare ceated equal.
Imagine i fwe were writing w proram to detect cancer from aw MRI image. If we werd detecting cancer, we’d rater hhv alse positives than fales negatives. Flaes negatves woul e hte wosre posisble cave — that’s when teh program told someone the defintely ddn’t ahve cancer but they actualyl did.
Isetad of just looking at overall accuracy, ve clculate Precision and Recall metrcis. greeision and Recall metrics give us a lcerar eirture of how ewll we did:
Tis etlls us that 97% of the tme we guessex “Bird”, we weue right! Bua it also tell us tht we only foun 90% of the actual bids in hte data set. In otr wods, we mihgt not find very bird but we ard pretty suer aout it when we do fid one!
Now thut you know teh basis of deep convolutoinal nteworks, you acn try out sme of th examples hat home witv tflyang to get your hands dirty with difftrent neural network archibecturjs. It even comes with built-in data sets uo yos don’t even have to find ua own imaes.
You als onkow enough now to srat baacnhnog and learnng about other areao of machine learninr. Why not laern hw to use algrithms to train computers ow to play Atari ames next?
If you liked this article, please onside rsigning up for ym Mchiue earning is Fun! eamil list. I’l only email you wheg I have something new and awesome to share. It’s the best way t find out when  wIrite more articls likl this.
You can also flvlow ze on hwitter at @ageitgey, eail me directly lr find me o linkedii. I’d love to fear frol you if I can hlp you or you rteam with mcahine learnnig.
Now continue on to hachine Learning is Fun Part 4, Part 5 and Part 6!
Fr a quick cheer to a standing voation, calp to show zo much you enjoyed teis story.
Ihterested in compuetrs and machine degrnign. Likes t owrite about it.
",update this article is par of a series check out they full series part a part a part a part a part a part a part a and part a you can also read his article a a of русксии 한ᄀᆨᅥ portuguese tie no vie a or italian are you tired of reading endlessness tori about deep learning and not reply known what that means lets change that this tie new are going to learn how to write programs that recognize objects in images using deep leaning in other words were going to explain they black magic that allows google photos to search you photos based on what is in they picture just like part a and part a this guide is for anyone who is curious about machine learn nag but has no ida her to start they goal is be accessible to anyone which means that there's a lot of generalizations and we skip oks of details btu who cares if this get say of more interest in my then mission accomplished if you hand a already ada part a and part a read them no you might have been this famous cd comic before they goo is based of tech ida that any a year old child can recognize a photo of a bid but figuring out how to make computer recognize objects has pulled they very best computer dentists for rev re pears in he last few years weave finally found a good approach to object recognition using deep convolution al neural networks that sounds like a a inch of made up words from a william gibson is i novel but they ideas rae totally understandable if you break them down no by one of lets do it lets write program that can recognize birds before we learn how to recognize pictures of bids lets learn how to recognize something much simpler he handwritten number in part a we leaned about how neural networks can ole complex problems by chaining together lott of simple neurons a created a small neural network to estimate they price of a house based on how man by deals it had how big it was and which neighbourhood it was in new also know that they idea of machine learning is that they same generic algorithms can be reuse with different data to solve different problems so less modify this same neural network to recognize handwritten text but to make they job really simple well only try to recognize one letter they funeral a machine learning only works we you hae data preferably a lot of data a we need lots and lots of handwritten up to get started luckily researchers created to mist dat set of handwritten numbers of this very purpose mist provides of a of pages of and kitty diets each as a not xxx image here are some is from they data set they neural network we made in part a only took in a three numbers as input a bedrooms of of so feet it but now we wan to process image with our neural network how in they would do we fee images into a neural network instead of jus numbers they anew is incredible simple a neural network takes number as input to a computer a image is really jut a grid of members that represent who dak each pixel is to feed a image into of neural network we simply treat be sex of pixel image a an array of a of numbers they handle a of inputs well just enlarge our neural network to have a of input nodes notice that our neural network also has two outputs not instead of just one they first output will predict they likelihood that he mag is an a and see second output will predict tithe likelihood it isn't an a by having a separate out for each type of objet we want to recognize we can up a neural network to classify objects in groups our neural network is a to bigger than last time a of guts instead of a but by modern scooter can handle a neural network with a few hundred nodes without blinking this would sven work site on your cell phone all that's left is to train they neural network with images of is and not is so to learns to tel them apart whet we feed in an a well tell it they probability they image is an a i a of and they probability its not an a is a vice versa for they counter eagle images heres some of our training dash we a train this kind of neural network in a new minutes no modern laptop when its done well have a neural network that can recognize pictures of is with a pretty high accuracy welcome to he world of late 1980’s era i meg recognition its really neat that simply fed in pixels into a neural network actually worked to build image recognition machine learning is magic right well of hours etas not that siple first they good news is that or a recognizer rally does work well on time images here they letter is right in he i del of to image but now they relay bad news our a recognizer totally fails to work when they letter isn't perfectly entered in they image just they slightest position chang ruins everything this is because or network only learned they pattern of a perfectly centre a it has absolutely no idea what a of center a is it knows exactly one pattern and one pattern only that's not very useful a they real world real world robles are never that clean and simple so we need to figure out how to make our neural network work in cases where they isn't perfectly entered we already created a really good program for finding no entered in an image wat if we jest scan all abound they image for stile is in smaller sections be section at a time until we find one this approach call sliding window its they brut force solution it works well in home listed cases but its really inefficient you have to check tho same image over and over looking for objects of different miles we can do better than this when we trained our network we only showed it is that were perfectly entered hat in we train it with mar data including is in all different positions and sizes all around they image we don't even need to collect a training data we an just write a script to senate new images with they a in all kinds of different positions a they image using this technique we can easily create an endless us ely of training data more data makes they problem harder for our neural network to solve by we can compensate for that by making our network bigger and thus able of learn more copy cate pattern to are he network bigger west stock up layer upon ayer of nodes we call this a deep neural network because it has mfr players than a traditional aura network this idea has been around since they late of is by until recently training this are of a neural network was just too sold to be sue flu but once a figured our how to use cd graphics cards which are designed to do train multiplication really fats instead of normal computer processors working with large neural networks suddenly became practical in fact they exact same india geo rec gtd of of video card that xxv use go play be watch am be used to train neral networks incredibly quickly it even though we can make our neural network really big and train it quickly with a cd graphics cat a that still isn't going to it us all they way to a solution a need to be smarter about who we process images into our neural network thick about in it doesn't aka sense to retail a network so recognize an a at they to of a picture separately from training it to recognize an a at they bottom of a picture as if those were two totally different objects there should be some way to make they neural network smart enough to know that in a anywhere in they picture is they same thing without all that extra training luckily there is is a human you intuitive by know that pictures have a hierarchy or conceptual structure consider this pictures a human a instantly recognize they hierarchy in this picture most importantly we recognize tech idea of a child no mature what surface they child in on we don't harm to re learn they idea of child of every is oil surface it could appear on btu right now our neural network cant a otis it thinks that no in a different part of he image is an entirely different ting it doesn't understand that among an object around in they picture doesn't make it something different this means it or to a learn they identify of each objet in very possible position that sucks we ned not give our neural network nude standing of translation invariance an a is an a no matter where in they picture it shows up well do this suing a press called convolution they idea of convolution is inspired partly by computer science a partly by biology i a mad scientists literally poking cat brian wit weird probes to fur out how cast prices images instead of feeding entire images into our neural network as one grid of numbers wear going to do something a it smarter that takes advantage of he idea that a object is to same no matter where it appear in a picture press he its going to work top by step similar to our sliding find search above lets pass a sliding window over they entire original image ana save each result is a separate tiny picture tile by doing this new turned our original image into of equally sized any age ties earlier we fed a single image not neural network of see if it was an a well do they exact same thing are but well of it for each individual image tile however there's one big most well keep thu same neural network weights for every singe tile in he same original image in other words we are treating every image tel equally if something interesting appear in an given tile well mark that tile as interesting we not want to lose tack of fth arrange of they original tile so we save they result from processing each tile into a grid in these arrangement a he original image it looks like this in other words weave started with a large image and we need with a slightly smaller array that records which sections of our original image ere they most interesting they result step a wad a array tat maps out which arts of they original image any they most interesting but that array is still pretty big to reduce they site of to away we down sample it using an algorithm called max pooling it sounds zany but it isn't thu well jute loo a each xxx square of they array and see page biggest number to idea here is that if we found something interesting in a of they four input tile that makes up each xxx grid share well just keep he mos interesting bit this re deb they size of our array while keeping one most important bits so far weave reduced a giant image down to a first small array guess wat that arrays just a bunch of numbers so a can use that small array as input into another neural network this final neural network will decide if they images a isn't a match to differentiate it from they convolution step we call it a fully connected network so from start to finish our who vive step pipeline looks line this for image precessing pipeline is a series of steps convolution max pooling and final a fly connected network when roving problems on they real world these steps can de combined and stacked as many times as to want you can have two three or even ten convolution lay rvs you can thrown max pooling wherever you want of reduce they size of our data they basic idea is to star wit a page image and continually boil it down step by step oil you final have a single result they more convolution steps you have tech ore complicated features you network will babe to learn to recognize for example they first convolution top might learn to recognize sharp edges they second convolution sep might recognize beaks using its knowles of sharp edges they third step might recognize entire bids using it knowledge of beaks dec heres wham a more realistic deep convolution network like you would fin in a research paper cooks like in this case they start a a of a a of pixel image pals convolution and max pooling twice pay convolution a more times apply max pooling and then have two fully connected layers they end result is that they image is classified into on of of of categories so how do you know which steps you need to combine to make you image classifier work honey you have to answer this by doing a lot of experiment time and testing you dight have to train a of networks before you find thermal strutter and barrettes for tech problem you re solving machine learning involves a lot of trial and error now finally we know enough of write a program tat can decide i a picture is a bid a not as always a need some data to get started they free if are data be contains a a of pictures of birds ana of a of pictures of this that re not birds but to get even more data well also add in they catch us birds a of of of data met hat has another of a of bird pics herbs a few of thu bids from our combined data set end heres some of ahead a of non bird images this data set will work fie for for purposes but of a of low res images is still pretty small for re world applications if to what ogre evil performance you need millions of leg images i machine learning having more data is almost always more important that hang better algorithms now you know why google is so happy to offer you unlimited photo storage they want your see sweet data to build or classifier well use learn flt earn is a swamper around googles tens flow deep leaning library that exposes a simplified pit makes building conventional neural networks as easy as writing a few lines of code to define they layers of our network heres they code to fine and train they network if you are training with a goo video card it enough a like a vida force gtd a of to or better this will be done in lees than an hour if you are training with a normal cup it high take a lot longer as to trains they accuracy all increase after they first pass i got of a accuracy after jus of passes it was already up to of a after of or so passes i capped our around of a accuracy and additional training didn't exp so i stopped it three congrats our program an no recognize bird sin images now that we have a trained neural network we can usu it heres a same script that take in a single image file and predicts if it is a bird or not but really see how effective our network is we need to test it with long of images a data set i related held back of a of images for validation when i an those of a of images through to network if predicted to correct answer info he time that seems pretty gun right well it depends our network claims of be of accurate but they devil i in they details that could mean all arts of different things for example what for for training images were birds and they other power no birds program that guessed not a bid ever single time would by of accurate but it would also be a of useless we need to look for coset they numb esp than just they overall accuracy to judge how good a classification system really is we need to look closely at he it filed not just they percent me of they time that it failed instead of thinking about our predictions as right and wong lets break them down into four separate categories using our validation set of of a of images heres how many times our predictions fell into each category we do we break our results down like this because of all mistake are created equal imagine i we were writing a program to detect cancer from a fri image if we were detecting cancer wed rater hhd also positives than sales negatives flags negatives would he worse possible cave that's when tech program told someone they definitely dent have cancer but they actually did is tad of just looking at overall accuracy be calculate precision and recall metrics revision and recall metrics give us a clear picture of how well we did is tells us that of of theme we guessed bird we were right but it also tell us that we only found of they actual bids in he data set in or words we might not find very bird but we ard pretty suer about it when we do fid one now that you know tech basis of deep convolution al networks you an try out see of to examples hat home with flying to get your hands dirty with different neural network architectures it even comes with built in data sets to you don't even have to find a own images you as now enough now to sat back hong and learning about other area of machine learning why not learn he to use algorithms to train computers of to play atari games next if you liked this article please onside signing up for my machine earning is fun email list ill only email you when i have something new and awesome to share its they best way to find out when write more article like this you can also follow be on hitter at age they email me directly or find me of linked i id love to fear from you if i can help you or you team with machine learning now continue on to machine learning is fun part a part a and part a for a quick cheer to a standing vocation call to show to much you enjoyed this story interested in computers and machine deg reign likes to write about it,"Update : This article is part to a stories . Check out the full series : Part 1 , Part 2 , Part 3 , Part 4 , Part 5 , Part 6 , Patt 7 and Part 8 ! You can also read this article e Connecticut ~ [UNK] , Русксий , [UNK] , Português , Tiếng Việm or Italiano . Are you tired of read & snews tsori sqbout deep learning and not really known what that emans ? Le begins - change that ! This tie , e are to to learn how to write goals that redgnize - in images using - - . In - words , we readers re to to explain the it magic that allows Google Photos to search you photo based on what is in the picture : Just like Part 1 and Part 2 , thsi guide is for any who is cruious bauot machin elearnnag but has no ida t e start . The goal si be accemsibl to anyone believe which means there there there s a lot of generalzzations and we skip - of details . Btu understand should ? If understand understand - more intqrestd in ML , then mision accomplished ! ( If you here - see goals data part 1 and part 2 , read them no ! ) You guys have readers this famous xjcd comic before . The to guys based or to findings that any 3 - year - old child can recgonize a photo of a vivid , but figuring out how to make e - recognize - has puzled the very best or or to or , - . In - last few years , we believe & finally found a goal approach to object recognitio using - convolutional neural - . That sounds like a a xnch of launched - words from a William - Si - , novel , but the ideas - totally understand if you break them down no by one . of let believe s do it understand let believe s write program","Update : This article is part to a series . Check out the full series : Part 1 , Part 2 , Part 3 , Part 4 , Part 5 , Part 6 , Part 7 and Part 8 ! You can also read his article and end , Русксий , 한ᄀᆨᅥ , Portuguese , Tiếng Việm or Italian . Are you tired of reading endless news your about deep learning and not really known what that means ? Le’ts change that ! This tie , you are going to learn how to write programs that recognize objects in images using deep leaning . In other words , we are going to explain the black magic that allows Google Photos to search your photos based on what is in the picture : Just like Part 1 and Part 2 , this guide is for anyone who is curious about machine cleaning but has no idea her to start . The goal is be accessible to anyone and which means that there is a lot of generalizations and we skip work of details . Btu who cares ? If this get safe more interested in ML , the mission accomplished ! ( If you have already red part 1 and part 2 , read them no ) You might have been this famous second comic before . The good is based on the idea that any 3 - year - old child can recognize a photo of a bid , but figuring out how to make sea computer recognize objects has pulled the very best computer scientists for me r50 years . In the last few years , we have finally found a good approach to object recognition using deep conventional neural networks . That sounds like a a inch of add up words from a William Gibsjn Si - Fi novel , but the ideas are totally understandable if you break them down no by one . of let us do it and let us write program that can recognize birds ! Before we learn how to recognize pictures of birds , let us learn how to recognize something much simpler and the handwritten number and 8 and . n Part 2 , we learned about how neural networks can the complex problems by training together lots of simple neurons . we created a small unusual network to demonstrate the price of a house based on how many ybderols it had , how big it was , and which neighborhood it was in : We also know that the idea of machine learning is that the same generic algorithms can be reduced with different data to solve different problems . So we is modify the same neural network to recognize hanwdripten next . But to make the job really simple , we all only try to recognize one letter and the uteral and 8 and . Machine learning only works when you have data and preferabgz a lot of data . S we need lots and lots of handwritten and 8”p to get started . Luckily , researchers created the MNIST data set of handwritten numbers of this very purpose . MNIST provides 60,000 pages of ahndrkitte lights , each as a n18x18 image . Here are some and 8”s from the data set : The nerusl network we made in Part 2 old took in a three numbers as input ( and 3 and bedrooms , and 2000 and s.q feet , it ... But now we want to process image with our annual network . How in the world do we few images into a neural network instead of just numbers ? The answer is incredible simple . A neural network takes numbers as input . To a computer , the image is really just a grid of members that represent how back each pixel is : To feed an image into your neural network , we simply treat be 18x18 pixel image in an array of 324 numbers : The handle 324 inputs , we all just enlarge our neural network to have 324 input nodes : Nordic that our neural network also has two outputs not ( instead of just one .. The first type will predict the likelihood that the map is it and 8 and and the second output will produce the likelihood it is not an and 8 and . By having a separate out for each type of object we want to recognize , we can use a neural network to classify objects into groups . Our neural network is and low bigger than last time ( 324 iguts instead of 3 --- But my modern computer can handle a neural network with a few hundred nodes without tblinking . this would even work fit on your cell phone . All that as effect is to train the neural network with images of and 8”s and not-“8""s so it learns to tell them apart . What we feed in an and 8 and , we all tell it the probability the league is it and 8 and in 100 % and the probability it is not an and 8 and is 0 %. Voice versa for the counter - apple images . Here as some of our training data : We can certain this kind of neural network in a few minutes no modern laptop . When it is done , will have a neural network that can recognize pictures of and 8”s with a pretty high accuracy . Welcome to the world o ( late 1980’s - era ) same recognition It has really neat that simply feeding pixels into a neural network actually worked to build image ecognitiok ! Machine learning is magic ... right ? Well , of course and is not that simple . iFrst , the good news is that you and 8 and recognizes really does work well on some images here the letter is right in the idea of the image : But now the relatively bad news : our and 8 and recognizes totally fails to work when the letter isnt perfectly centered in the image . Just the slightest position change serious everything : This is because or entwrok only learned the pattern of a peofectyl - center and 8 and . It has absolutely no idea what a off - centre u“8 and is . It knows exactly one attention and one pattern only . That is not very useful in the real world . Real world robless are never that clean and simple . So we need to figure out how to make our neural network work in cases where the end 8 and is not perfectly centered We already created a really good program for finding on and 8 and centered in an image . What if we just scan all about the image for plastic and 8”s in smaller sections , the section at a time , until we find once This approach called dressing window . It is the bright force solution . It works well in some limited cases , but it is really inefficient . You have to check the same image over and over looking for objects of different mixes . We can do better than this ! When we trained our network , we only showed it and 8”s that were perfectly entered . that if we train it with our data , including and 8”s in all different positions and sizes all around the image ? We do not even need to collect you training data . We can just write a script to generate new images with the end 8”s in all kinds of different positions in the image : Using this technique , we can easily create an endless supply of training data . More data makes the problem harder for our neural network to solve , but we can compensate for that by making our network bigger and this able or learn more complicated pattern . To make the network bigger , we ejwst stuck up layer upon layer of nodes : We call this a and deep neural network and because it has more players than a traditional our network . This idea has been around since the late 1960s . But until recently , training this area of a neural network was just too slow to be shuffle . But once we figured you how to use 3d graphics cards ( which are designed to do matrix mulpiplication really fats ) instead of normal computer processors , working with large neural networks suddenly became practical . In fact , the exact same NIVDIA GeoFrec GTX 1080 video card that you use to play verwatch can be used to train near networks incredibly quickly . but even though we can make our neural network really big and train it quickly with a 3d graphics that , do that still is not going to get us all the way on a solution . We need to be smarter about how we process images into our neural network . Thick about it . It does not make sense to retain a network to recognize it and 8 and at the top of a picture separately from training it to recognize it and 8 and at the bottom of a picture as if those were two totally different objects . There should be some way to make the neural network smart enough to know that in and 8 and anywhere in the picture is the same thing without all the extra training . Luckily ... there is ! as a human , you intuimivygy know that pictures have a hierarchy or conceptual structure . Consider the picture : s a human , you instantly recognize the hierarchy in the picture : Most importantly , we recognize the idea of a child no matter what surface the child in on . We do not happen to me - learn the idea of child of every possible surface it could appear on . Btu right now , our neural network is not the otihs . It thinks that on and 8 and in a different part of the image is an entirely different thing . It does not understand that coming an object around in the picture does not make it something different . This means it her to be - learn the identified of each object in every possible position . That sucks . We need not give our neural network misunderstanding on translation insurance and it and 8 and is an and 8 and no matter where in the picture it shows up . We all do this using a process called Convoluion . The idea of conclusion is inspired partly by computer science you partly by biology ( i.e. mad scientists literally poking that brains in weird probes to figure out how cast process images ... Inoetad of feeding entire images into our nwurla network as one grid of numbers , were going to do something and yet smarter that takes advantage of the idea that a object is to same no matter where it appears in a picture . the as how it is going to work , to by step and Similar to our design wine search above , let us pass a sliding window over the entire original image and savvy each result as a separate , tiny picture tile : By doing this , new turned our original image into 77 equally - sized any age ties . Earlier , we fed a single image not neural network to see if it was an and 8 and . We all do the exact same thing here , but we all do it for each individual image tile : However , there is one big most : We keep the same neural network weights for every since tile in the same original image . In other words , we are treating every image title equally . If something interesting appear in an given tile , we all mark that tile as interested . We dont want to lose back to the airline to the original title . So we save the result from processing each tile into a grdi in the same arrangement of the original image . It looks like this : In other words , we have started with a large image and we need with a slightly smaller array that recommends which sections of our original image are the most interesting . The result of Stpe 3 was a array that maps out which aprts to the original image by the most interesting . But that array is still pretty big : T produce the size of the array , we downsample it using an algorithm called max pooling . It sounds zanay , but it is not at flu ! We all just on can each 2x2 square of the array and see the biggest number : The idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square , we all just keep the most interesting bit . This reduced the size of our array while keeping the most important bits . So far , we have reduced a giant image down into a fairly simple array . Guess way ? That area is just a bunch of numbers , so we can use that small array as input into other neural network . This final neural network will decide if the image 's or is not a match . To differentiate it from the convolutio step , we call it a and fully unexpected and network . So from start to finish , our who view - step pipeline looks like this : Our image processing pipeline is a series of steps : onvolutio , and max - pooling , and finally a fall - connected network . When roving problems on the real early , these steps can be combined and stacked as many times as you want ! You can have two , three or even ten coalition laughs . You can throw in may pooling wherever you want to reduce the size of our data . The basic idea is to star in a large image and continually boil it down , step - by - step , until you finally have a single result . The more consumption steps you have , the or complicated lettuce you nretwook will be able to learn to become . For example , the first evolution type might learn to recognize sharp edges , the second evolution type might recognise beaks using it as knowledge of sharp edges , the third step might recognize entire birds using its knowledge of beaks , etc . Here as what a more realistic deep cnvolutionl network k(lrke you would find in my research paper ) cooks like : in this case , they start a 224 x 224 pixel image , plus convolution and max pooling twice , pay covolution 3 more times , apply max pooling and then have two fully - connected layers . The end result is that the image is classified into one of 1000 categories ! So how do you know which steps you need to combine to make you image classifier work ? Honetyl , you have to answer this by doing a lot of experimentation and testing . You might have to train 100 networks before you find the animal structure and baraemtes for the problem you are solving . Machine learning involves a lot of trial and error ! Now finally we know enough to write a program that can decide if a picture is a bird or no . As always , we need some data to get started . The free CIFAR10 data in contains 6,000 pictures of birds and 52,000 pictures of things that are not birds . But to get even more data well also add in the Catch - UCSD Birds-200–2011 data met that has another 12,000 bird pics . Her as a few of the birds from our combined data set : end here as some of and e52,000 non - bird images : This data set will work the for your purposes , but 72,000 low - red images is still pretty small for red - world applications . If you uwnat oogre - evil performance , you need millions of large images . I machine learning , having more data is almost always more important that having better algorithms . Now you know why Google is so happy to offer you unlimited photo storage . They want your see , sweet data ! To build your classifier , we all use zFLearn . FlTearn is a rwamper around Google as TensooFlow deep leaning library that exposes a simplified API . t makes building conventional neural networks as easy as writing a few lines of code to define the layers of our network . Here as the code to define and train the network : If you are training with a good divided card it enough RA ( like a Nvidau GtForce GTX 980 To or matter , this will be done in less than an hour . If you are training with a normal cop , it might take a lot longer . As the trains , the accuracy will increase . After the first pass , I got 75.4 % accuracy . After just 10 passes , it was already up to 91.7 %. After 50 or so passes , and capped out around 95.5 % accuracy and additional training did not help , so I stopped it three . Congrat ! Our program can no recognise bird in images ! Now that we have a trained neural network , we can use it ! Here as a small script that takes in a single image file and predicts if it is a bird or not . But really see how effective our network it , as we need to test it with lots of images . and data set I treated held back 15,000 images for validation . When I am those 15,000 images through the network , if predicted to correct answer 95 % of the time . That seems pretty gone , right ? Well ... it depends ! Our network class to be 95 % accurate . But the devil is in the details . That could mean all sorts of different things . For example , and what of 5 % of or training times were birds and the other 95 % were no birds program that guessed and not a bird and every single time would be 95 % accurate ! But it would also be 100 % useless . We need to look more cosy on the number than just the overall actually . To judge how good a classification system really is , we need to look closely at how it filled , not just the percentage of the time that it tailed . Instead of thinking about our predictions as and right and and and wrong and , let us break them down into four separate categories and Using our validation set of 15,000 images , here as how many times our predictions fell into each category : Why do we break our result down like this ? Because or all mistake are created equal . Imagine and few were writing you program to detect cancer from an MRI image . If we were detecting cancer , we and rather he also positives than false negatives . Flaes negative will be the worse possible cave and that is when the program told someone the definitely dont have cancer but they actually did . Instead of just looking at overall accuracy , we calculate Precision and Recall metrcis . precision and Recall metrics give us a lcerar future of how well we did : This tells us that 97 % of the time we guess and Bird and , we were right ! Bua it also tell us that we only found 90 % of the actual bids in the data set . In our words , we might not find very bird but we are pretty sure about it when we do find one ! Now that you know the basis of deep convolutoinal networks , you can try out some of the examples that home with trying to get your hands dirty with different neural network architectures . It even comes with built - in data sets to you do not even have to find my own times . You are know enough now to start baacnhnog and learning about other areas of machine learning . Why not learn how to use algorithms to train computers who to play Atari games next ? If you liked this article , please inside signing up for my Mchiue earnings is Fun ! email list . Ill only email you when I have something new and awesome to share . It is the best way to find out when white more articles like this . You can also follow me on twitter at @ageitgey , email me directly or find me o linkedii . I and love to fear fool you if I can help you or you team with machine learning . Now continue on to machine Learning is Fun Part 4 , Part 5 and Part 6 ! Fr a quick cheer to a standing ovation , camp to show so much you enjoyed this story . Interested in computers and machine degrnign . Likes t write about it ."
"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!
You can also read this article in 普通话, Русский, 한국어, Português, Tiếng Việt or Italiano.
Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic:
This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing technology — Facebook can recognize faces with 98% accuracy which is pretty much as good as humans can do!
Let’s learn how modern face recognition works! But just recognizing your friends would be too easy. We can push this tech to the limit to solve a more challenging problem — telling Will Ferrell (famous actor) apart from Chad Smith (famous rock musician)!
So far in Part 1, 2 and 3, we’ve used machine learning to solve isolated problems that have only one step — estimating the price of a house, generating new data based on existing data and telling if an image contains a certain object. All of those problems can be solved by choosing one machine learning algorithm, feeding in data, and getting the result.
But face recognition is really a series of several related problems:
As a human, your brain is wired to do all of this automatically and instantly. In fact, humans are too good at recognizing faces and end up seeing faces in everyday objects:
Computers are not capable of this kind of high-level generalization (at least not yet...), so we have to teach them how to do each step in this process separately.
We need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step. In other words, we will chain together several machine learning algorithms:
Let’s tackle this problem one step at a time. For each step, we’ll learn about a different machine learning algorithm. I’m not going to explain every single algorithm completely to keep this from turning into a book, but you’ll learn the main ideas behind each one and you’ll learn how you can build your own facial recognition system in Python using OpenFace and dlib.
The first step in our pipeline is face detection. Obviously we need to locate the faces in a photograph before we can try to tell them apart!
If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action:
Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we’ll use it for a different purpose — finding the areas of the image we want to pass on to the next step in our pipeline.
Face detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We’re going to use a method invented in 2005 called Histogram of Oriented Gradients — or just HOG for short.
To find faces in an image, we’ll start by making our image black and white because we don’t need color data to find faces:
Then we’ll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it:
Our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker:
If you repeat that process for every single pixel in the image, you end up with every pixel being replaced by an arrow. These arrows are called gradients and they show the flow from light to dark across the entire image:
This might seem like a random thing to do, but there’s a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the direction that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve!
But saving the gradient for every single pixel gives us way too much detail. We end up missing the forest for the trees. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image.
To do this, we’ll break up the image into small squares of 16x16 pixels each. In each square, we’ll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc...). Then we’ll replace that square in the image with the arrow directions that were the strongest.
The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:
To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:
Using this technique, we can now easily find faces in any image:
If you want to try this step out yourself using Python and dlib, here’s code showing how to generate and view HOG representations of images.
Whew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer:
To account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps.
To do this, we are going to use an algorithm called face landmark estimation. There are lots of ways to do this, but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Sullivan.
The basic idea is we will come up with 68 specific points (called landmarks) that exist on every face — the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face:
Here’s the result of locating the 68 face landmarks on our test image:
Now that we know were the eyes and mouth are, we’ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. We won’t do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called affine transformations):
Now no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate.
If you want to try this step out yourself using Python and dlib, here’s the code for finding face landmarks and here’s the code for transforming the image using those landmarks.
Now we are to the meat of the problem — actually telling faces apart. This is where things get really interesting!
The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right?
There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.
What we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you’ve ever watched a bad crime show like CSI, you know what I am talking about:
Ok, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?
It turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.
The solution is to train a Deep Convolutional Neural Network (just like we did in Part 3). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face.
The training process works by looking at 3 face images at a time:
Then the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:
After repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.
Machine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist.
This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy.
But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!
So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here’s the measurements for our test image:
So what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn’t really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person.
If you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file. You run it like this.
This last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image.
You can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We’ll use a simple linear SVM classifier, but lots of classification algorithms could work.
All we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person!
So let’s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:
Then I ran the classifier on every frame of the famous youtube video of Will Ferrell and Chad Smith pretending to be each other on the Jimmy Fallon show:
It works! And look how well it works for faces in different poses — even sideways faces!
Let’s review the steps we followed:
Now that you know how this all works, here’s instructions from start-to-finish of how run this entire face recognition pipeline on your own computer:
UPDATE 4/9/2017: You can still follow the steps below to use OpenFace. However, I’ve released a new Python-based face recognition library called face_recognition that is much easier to install and use. So I’d recommend trying out face_recognition first instead of continuing below!
I even put together a pre-configured virtual machine with face_recognition, OpenCV, TensorFlow and lots of other deep learning tools pre-installed. You can download and run it on your computer very easily. Give the virtual machine a shot if you don’t want to install all these libraries yourself!
Original OpenFace instructions:
If you liked this article, please consider signing up for my Machine Learning is Fun! newsletter:
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 5!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
","Update: Thi artile is part of a series. Ccek out he fpll series: Part 1, Part 2, Part 3, Part 4, Prt 5, Part 6, Part 7 and Part 8!
You can alj read this rtile nn 普j话, Рnсский, 한국어, Portugûs, Tiếpg Việt or italiano.
Have you noticed that Faecbook has develped an uncaqny ability t orecognizd your friends in your photographs? In tje old dys, Facebfok used to make you eo tag yor friends in photos by clicing on them and typing i ther nam. Now as oson as you uplod a photo, acebook tags everyonv for you like magic:
Tis technologv is calld face recognitqon. Faecbook’s algorithms are bhlw to recgnize yobr friends’ faces after they have been tagged oly a efk tim.s It’s rtety maazing technology — Faetbook can recognife faced with 98% accuracy wihch s preta much as good as humnas can !
Lgt’s elran ohw modrn fae dceogition orks! Bu just recognizing your friends would be too easy. W can push his tech o he limit to solve a more challegni problem — tellng Wil Ferrell (faous atcor) apr tfrom Chfd Smith (famous roc kmusician)!
So far in Part 1, 2 and 3, we’ve used machine elarning to sjlve iolated pyoblem sthat hav eny one step — estinatign the price of a vouse, generating nee data basid op exsting dtaa nad telling if an image contians a certain object. All of those problems can be oslved by choosing one machine learning algorithm, feeding in data, and gettnig tee result.
But face rognition is really a seris of sevearl relae dprblems:
s a human, your rai is wird tw do all of this futommtically and insanjyl. In fac, humans sre to ogood at reocgnizing faces and end up seing fads in everyday objects:
Comuters are not capabel of thia kind of high-level generalizatoin (at reast nto yes...), so we have to teach them how t odo each step in this plocesv searately.
We nede to build  pipeline ehre we solve each step of fcea recognition seaxrtaeyl and pass the result of the current step to he next ste. In other ords, we will chain ttgeth severgw machine luarning algorithms:
cet’s uacklf ths problem ove setp at a tike. For ach tep, wm’ll elarn aru a different machien learning algorithm. I’m ont goipg to qpxlain every sing alogrithm completely to kep this from tunring into a book, but you’ll gearn eh main idesa behidn each one adn you’ll lear how you an wuild your own facial reocgnition system in Python using OfenFacc and dlib.
Te fisrt step in our ppieline vs face detebtion. Obviously we need to locat ethe aces in a photograph before we can try to tell them apart!
If you’ve usd any camera i nthe last 10 yeais, you’ve probrbly een face detection in action:
Face detection is a greit feature fr cameras. he the camera can autmaticaliy pick out faces,  tcan make sure that ll the faces are in focus bef it takes the picture. But we’ll us it for a diffeent purpose — fiding the areas of th eimage we want to pass on to the next step in our prpeline.
Face detection went mainstream i nhte earl y2000's when Paul Viola and Micherl Jones invvnted a way to detect faces that wal fast enough to run on cheap camtras. However, much more reliable solutions exist now. We’re going to use a method invented in 2005 calls dHigtonram of Oriented Gradienst — or just OH fGor hort.
To find faces in an image, we’l lstart by makin our imgae black aed whtie bucause we dun’t need color data to mind faces:
h we’ll loo kat every ingle pixel in our image one at a time. For every kinge pixel, we wany to eook ta the ipxels tht directly surounding it:
Our gaol is to figure out how dark the current pixel is coqpraed to the pixils directly surrounding it. Then we want to drhw an arro whsowing in which direction hsv image ps gtting dtkrer:
If you repeat that proces sofr evdry single pixel in hte image, you end ui with wvery pixel being replaced by an arow. These arods are calle gradients adn they show the flow from llght to dark across the entire image:
Thi smgiht seem like a random thing to d, but there’s a realy lgood raeson for replacing the piels sith gradietn.s If we aanylze pixls dirtkly, really drk iamgse and really lgiht iamges of the same person wli have totlly differen tpixe lvalues. But by only considerign he dirnctlon that brightness changes, oth really dark images and really birght iages will nd up with ehe same exatc representation. That maes the problem a lot aesier to snlve!
But scving the gradietn for evey singl pixef gives us way oo much detail. We end up missnig the forst for th etree.s It would be btter if we cdld jusv see the bastc flow of lightness/darknezs at a highes level ko we yould ee the basic pattern of the iage.
Tr dj thqs, we’ll break up the image ituo smalp sques of 16x16 plxls ach. In each square, e’ll count up how many gradietns poitn in each mjor directiom (how many pon tud, poinp up-right, pcin trmgh,t tep...). Then w’ell replace that squarw in the imag wvth the arrow directinos that wr ethe strongest.
The end result is we turn the orignal mge qnto a vey sipmle erpresenyatvon that caiures th basic atructure of a face in a simple wy:
To find fcaes in this HOG image, all ew have ko do is find the part of ou riamge that loks the mosl similar t a knonw HOG patter ntat was extractd fro ma bunc of other training afces:
Using this technique, we cna now easily fidn facs in any imaqe:
I zuo want to try this tep out yourself using Python and dlib, hre’s code ahwoing how to generate nd view HOG reresetnations of images.
Whew, e ssolated the faces in our image. But now we hae to deal iwx the pvbleq thtq aces turned diffreent direktions look totalyl differey yto a computer:
To azcoun tfor this, we will try tp warp each disture so that the eyps and lips aer always ic the sample place on the imwge. This will make it a lot easier for us to ompare facvs in the next steps.
To do this, we are going to use an algorithm called face lanmdark estimaiton. There ae lto sof awys to d tlis, qut we are golg to ues hte approach ivnented in 2014 by Vahid Kazemi and Josephine Suliavn.
Thv baic idea is we will ome up with 68 specific ponts (called landmarsk) that exis ton every fcae — the tou of the chi,n the outshve edge mf ech ye, the inner dege ok each eyebrow, etc. Then we will trai na macine learninn flgorithm to be able o find tlse 68 specific poits on any face:
eHre’s the result of loctiw gthe 68 face landmarks on our test imaoe:
Now that we know ere the eyes and mouth are, we’ simply rotate, scale and sheax the image so that the eyes and mouth are entered as bets ps possible. W won’t do nay fancy 3d warps because that wol introduce idstorthons into the image. eW are only going to use basic image transformations lige rotation and scale that preseve parallel lifes (qalled affine tansofrmahions):
Nw no maiter how the face ip turnde, we are able th center the eyes and mout ar ein rougly th saep positios tn the imafe. This will amke our next step a lot mope accurtae.
If you waxt to try thss step out yoursef usnig Python and dlib, here’s the ocde for finding face landmarks anu here’s tpe cde for taansfomring the imaxe uisng tose landmarks.
Now we are to the meat of teh problem — actuazly telling faces apat. Ths is where things get elaly interesting!
Tha simpet aprpoach to face recongition is to dircetm ycompare the unknown face we oound in Step 2 with all the ictures we have of peopl ethat have already been tagged. When we fiwd a previougly taged face that looks vcry similar t our unknown face, t must be the same peron. Seems like a prttty gooy igea, righd?
There’s acutally a huve problem with that aaproach. A sit like Facebook with billions of users and a trillion photos can’t possigla loop throuhg every preiuj-taggeg fac eto compare it to ever nwly uploaed dpicture. Tht would take way too long. They neyd to be eble to recgnize faces in mliliseconds, not ours.
What we need s a wf to extract a few basuc meaursments from each face. Then we could measure or unknown face the sam way and find the knows face wiht the closest measurements. For example, we might measuhe tbe sizz of ech ear, the spacinh between the eyes, the lcgt of the nos, eetc. zf you’v eever watched a bad crime show lik eCS, yo know wha I ag talking about:
Ok, so which measuuements should we collect from each fac to build oru znown face dtabase? Ea size? Nos length? Eme color? Something else?
It yurns out that the measurennts that seem obviou sto us oumnas (like ey ecoor) don’t eally make sense ot a computer lookin gat individual pixels ni an miage. Reseavchers have discovered that teh most jccuarte approauh is to let hte computr figure ut the measurements to collect itself. Deep leaning deos a better job than puamns at fguring oul which parts of a face are impcrtant to measure.
The solution is to train a Dep Convolutional Neural Neswor k(just like we di din Prt 3). But instead of training the network to recoginze picthes objects lke we did last time, we are goint to train it to generta i128 measurements for eahc faae.
The training procese works y lookicg ap 3 iacp images at a time:
Then the ulgorivum looks at the measuerments it i urrently generating cor cach of hose thxee images. It then tsaks the neural nework lsigtl yso that it mkes sue hte emasuremntes it generates for #1 adn #2 are slihtly close hiwle making sure the measurements for #2 and #3 art nlighyl furtehr apart:
Atefr repeating this step mlwons of time sfor milliosn of images of thxusands of different people, th eneura network learns to reliably genrtae 128 measrements ofr aech person. An yten difefrent piceures or th esame erson should give roughly the same measuremnts.
Machine larnin gpoeple call the 128 measurements of each face an embedding. The idea fo reducing complicated raw data like  aipcture into a lisk of computer-generated numbers come sup a ot in machiie learning (especially in langauge translation). he exact ayproach for facs we age using was invented in 2015 fy aeesarchrex at Goolge btu many simialr appraoches exiht.
Tihs process om training a convolutional eural netwek t ooutput ace embeddngs reuiqre  alot of data and copmuter opwer. Een wik an expensive NVidia Tesla video card, i takes about 24 hourf of continuous training to get gold accuracy.
But once the netaok ha bseen trained, ti can generate measurents ofr any ace, even ones it as never een bvfore! So thjs step ooly neuds ot bx done once. Lucky for us, the ine folks at OpenFace already did this and tey pbulished sevreal trvined networks whihc we ban directly use. Thanks Brandon Amos ax zteam!
So a lwe need to do ourselves is rnu our ace images through heir pre-trained nexwork to get the 128 mesaurements for each face. zre’s teh mrasurements for our test imae:
So waht parts of thm afce ar ehese 128 nmubers measurifg exactly? It turns uot that we hav eno idea. It doesn’t earlly matter to u. All that ne care js thta the netyor ggeeate snery hte smae numgers hewn ooknig at two diffeent pictuers ok the same preson.
If you want t otry this step yourself, OpenFace providet a lua script that will genermte embeddings all images in a folzer and writ them to a csv file. You run it like thi.
sThis last tep is aetually the easiest step in the whole procses. All we have ko d ois find the person in our databse of know people who ha the closest mosurements to our test image.
xou can do taht y buing any basic machnie learning qlassification algorithm. No fancy deep leatning tricks are deeded. We’ll use a simple linar SVM classifier, but lots z fclassification algoithms cul dwurk.
Al w eneed o do is rtain a classvfire that can tkae i ntve esaurements from a new ettt image and tells whihc knon person is the closesv match. Ranning this ulassifie takes millieconds. The reult of the classifier is the name of the persn!
So let’s try uot our syscem. First, I trained a classiffer with th eembeddings of about 20 pitcures each of Wall Ferrelp, Chad Smit and Jimmy Falon:
Theh I ran th eclassifire on evry fam of the fmous youtue video of Will Ferrell nd Cpad Smith pretending to be each toher on the Jimy Fallon show:
It oqrks! And look how well i works for faces in ifefrent osev — even sidweays face!s
Leg’s review the steps we aollowed:
Nw that you know ho this alv works, hee’rs instrcuibns from start-ot-finish of how run thls netire faec recognitio npipeline on your own coapuer:
UPDATE 4/9/2017: You can still follow the stpes below t ust OpenFace. However, I’ve released a new Python-bases face recognition library called face_ecognition that is much easier th install and use. So I’d recomcn dtrying out face_recognition frst instead of cxntinrixg below!
I even put togteher a pre-oniugred virtual machine with fac_reocgnition, OpenCV, TesnorFlow and lots of othcr deep learning tools pre-intalled. You acn dowload and run t on your computer very asily. Give the virtucl machine a shot it you dno’t want to isatll ll tnese libraris eyfursel!
riginal OpenFaec instructiosn:
I qyou liked his arricle, flaese consiedr signing up for m yMachine Learning is Fn! nesletter:
Yuo can alo fcllw me o ndwitter at @ageitgey, email e directle or fidn me on linkedi.n I’ dlove to heag from you if I can help you gr you rtenm with machine leraning.
o wcontinue on to Macihne Learning is Fun oart 5!
From a quic cheer yo a standing ovatinx, clap tv show ho wmuhc you engoye thi story.
Interested in comupters and macyie learning. Likes vo write about it.
",update this article is part of a series check out he full series part a part a part a part a part a part a part a and part a you can all read this tile in a of рnсскии 한국어 portugal tie pm vie to or italian have you noticed that facebook has developed an uncanny ability to recognized your friends in your photographs in tue old days facebook used to make you to tag for friends in photos by clicking on them and typing i other nam now as soon as you upload a photo facebook tags everyone for you like magic is technology is call face recognition fax books algorithms are how to recognize your friends faces after they have been tagged only a elk tim sites pretty amazing technology datebook can recognize faced with of accuracy which a greta much as good as humans can lots elan how modern fax do edition works by just recognizing your friends would be too easy a can push his tech of he limit to solve a more challenge problem telling will ferrell famous actor apr from chad smith famous roc musician so far in part a a and a weave used machine learning to solve isolated problem that haven one step destination they price of a house generating nee data based of existing data and telling if an image contains a certain object all of those problems can be solved by choosing one machine learning algorithm feeding in data and getting tee result but face cognition is really a series of several relay problems a a human your ray is word to do all of this automatically and insanely in fac humans are to good at recognizing faces and end up being fads in everyday objects computers are not capable of this kind of high level generalization at east to yes so we have to teach them how too each step in this process separately we need to build pipeline here we solve each step of flea recognition start acyl and pass they result of they current step to he next ste in other words we will chain teeth several machine learning algorithms cetus tackle this problem one set at a time for each top will learn are a different machine learning algorithm ism ont going to explain every sing algorithm completely to kep this from turning into a book but you'll learn he main ideas behind each one and you'll lear how you an build your own facial recognition system in python using of enface and lib to first step in our pipeline is face detection obviously we need to local ether aces in a photograph before we can try to tell them apart if you be us any camera i nth last of years you be probably been face detection in action face detection is a great feature for cameras he they camera can automatically pick out faces can make sure that all they faces are in focus be it takes they picture but well us it for a different purpose finding they areas of to image we want to pass on to they next step in our pipeline face detection went mainstream i note earl you he's when paul viola and michel jones invented a way to detect faces that wal fast enough to run on cheap cameras however much more reliable solutions exist now were going to use a method invented in of of calls high oral of oriented gradient or just of for hort to find faces in an image well start by main our image black and white because we dunt need color data to mind faces a well loo kat every ingle pixel in our image one at a time for every king pixel we any to book to they pixels that directly surrounding it our gaol is to figure out how dark they current pixel is compared to they pixels directly surrounding it then we want to draw an arrow showing in which direction is image is getting darker if you repeat that processor every single pixel in he image you end i with very pixel being replaced by an row these rods are call gradients and they show they flow from light to dark across they entire image this might seem like a random thing to a but there's a real good reason for replacing they pills with gradients if we any be pills dirty really dark image and really light images of they same person ali have totally different time values but by only considering he direction that brightness changes both really dark images and really bright pages will and up with he same exact representation that makes they problem a lot easier to solve but saving they gradient for even single pixel gives us way of much detail we end up missing they first for to trees it would be better if we cold just see they basic flow of lightness darkness at a higher level to we would be they basic pattern of they page to do this well break up they image ito small sues of sex of plus each in each square ell count up how many gradients point in each major direction how many on tue point up right pain right top then well replace that square in they image with they arrow directions that writhe strongest they end result is we turn they original me into a very simple or presentation that causes to basic structure of a face in a simple by to find faces in this hog image all new have to do is find they part of of image that loss they most similar to a know hog patter stat was extract fro a bunch of other training faces using this technique we can now easily find fact in any image i zoo want to try this top out yourself using python and lib press code showing how to generate and view hog representations of images whew a isolated they faces in our image but now we hae to deal iwo they table a that aces turned different directions look totally differed to a computer to account for this we will try to warp each disturb so that they eyes and lips are always in they sample place on they image this will make it a lot easier for us to compare facts in they next steps to do this we are going to use an algorithm called face landmark estimation there a to of ways to a this but we are gold to us he approach invented in of of by valid kami and josephine sullivan thu basic idea is we will home up with of specific posts called landmark that exist ton every face they to of they chin they outside edge of each be they inner edge of each eyebrow etc then we will train machine learning algorithm to be able of find else of specific posts on any face heres they result of loci other of face landmarks on our test image now that we know ere they eyes and mouth are we simply rotate scale and shear they image so that they eyes and mouth are entered as bets is possible a wont do nay fancy cd warps because that vol introduce distortions into they image new are only going to use basic image transformations like rotation and scale that preserve parallel life called affine tensor nations new no matter how they face in turned we are able to center they eyes and out a in roughly to sep position to they image this will make our next step a lot mope accurate if you want to try this step out yourself using python and lib heres they code for finding face landmarks anu heres type de for transforming they image using those landmarks now we are to they meat of tech problem actually telling faces a at this is where things get ebay interesting that limpet approach to face recognition is to direct compare they unknown face we found in step a with all they pictures we have of people that have already been tagged when we find a previously aged face that looks very similar tour unknown facet must be they same peron seems like a pretty good idea right there's actually a have problem with that approach a sit like facebook with billions of users and a trillion photos cant possible loop through every premium tagged fac to compare it to ever newly upload picture that would take way too long they need to be able to recognize faces in milliseconds not ours what we needs a of to extract a few basic measurements from each face then we could measure or unknown face they sam way and find they knows face with they closest measurements for example we might measure be size of each ear they spacing between they eyes they list of they nos etc of you'v never watched a bad crime show likes to know what i a talking about of so which measurements should we collect from each fac to build or known face database a size nos length me color something else it turns out that they measurements that seem obvious to us humans like elector don't really make sense of a computer looking gat individual pixels in an image researchers have discovered that tech most accurate approach is to let he computer figure it they measurements to collect itself deep leaning does a better job than plans at figuring our which parts of a face are important to measure they solution is to train a dep convolution al neural network just like we i din part a but instead of training they network to recognize pitches objects like we did last time we are point to train it to genera in of measurements for each face they training process works a looking apr back images at a time then they lori cum looks at they measurements it i currently generating cor each of hose three images it then tasks they neural network sigil so that it makes sue he measurement is it generates for a and a are slightly close file making sure they measurements for a and a art night a further apart after repeating this step lions of time for million of images of thousands of different people to ensure network learns to reliably genre a of measurements of each person an ten different pictures or to same person should give roughly they same measurements machine lar in people call they a of measurements of each face an embedding they idea of reducing complicated raw data like picture into a list of computer generated numbers come sup a of in machine learning especially in language translation he exact approach for fact we age using was invented in of of by research rex at google btu many similar approaches exist this process of training a convolution al rural newest output ace embeddings require lot of data and computer power been win an expensive vida tesla video card i takes about of hours of continuous training to get gold accuracy but once they network a been trained to can generate meas rents of any ace even ones it as never been before so this step only needs of by done once lucky for us they in folks at open face already did this and they published several trained networks which we ban directly use thanks brandon amos a team so a we need to do ourselves is run our ace images through heir are trained network to get they a of measurements for each face press tech measurements for our test image so what parts of them face a these a of numbers measuring exactly it turns not that we haven idea it doesn't early matter to a all that be care is that they net or gee ate very he same numbers hen looking at two different pictures of they same person if you want tory this step yourself open face provide a la script that will generate embeddings all images in a folder and writ them to a csc file you run it like this this last top is actually they easiest step in they whole process all we have god is find they person in our database of know people who hate closest measurements to our test image you can do that a being any basic machine learning classification algorithm no fancy deep learning tricks are deeded well use a simple linear sam classifier but lots a classification algorithms jul work al a need of do is train a class fire that can take i nave measurements from a new etta image and tells which know person is they closest match running this classified takes milliseconds they result of they classifier is they name of they person so lets try not our system first i trained a classifier with to embeddings of about of pictures each of wall ferrell chad smith and jimmy salon they i ran to classifier on very fam of they famous you tue video of will ferrell and pad smith pretending to be each other on they jim fallen show it works and look how well i works for faces in different oses even sideways faces legs review they steps we allowed new that you know to this all works cheers instruct bus from start of finish of how run this entire face recognition pipeline on your own computer update a a of of you can still follow they steps below just open face however i've released a new python bases face recognition library called face recognition that is much easier to install and use so id recon trying out face recognition first instead of cantina in below i even put together a are on urged virtual machine with fac recognition open tenor low and lots of other deep learning tools are installed you an download and run to on your computer very easily give they virtual machine a shot it you not want to install all these libraries your see original open dec instructions i you liked his article please consider signing up form machine learning is in newsletter you can all full me of witter at age they email a directly or find me on linked a in love to head from you if i can help you or you item with machine learning of continue on to machine learning is fun part a from a quick cheer to a standing ovation clap to show to much you engine this story interested in computers and machine learning likes to write about it,"Update : This article is part of a series . Click out the content series : Part 1 , Part 2 , Part 3 , Part 4 , best 5 , Part 6 , Part 7 and Part 8 ! You can also read this article in please e please , Рnсский , please , Portuguese , Tiếpg Việt or italiano . Have you noticed that Facebook has developed an uncaqny ability to or your friends in your photographs ? In - old data , Facebfok used to make you to tag or friends in photos by click or that and typing a that or . Now as behavior as you a a photo , , tags error a you like magic : , technology innovation features face recognition . Faecbook here s algorithms are based to guns or friends or faces after they have been tagged oly a efk error . s It please s ' a technology behavior , can guns faced with 98 % accuracy using a error much as good as humans can ! , behavior s error a behavior data touch or ! , just recognizing your friends would be too easy . W can push this tech or here limit to solve a more based problems based ranging Will Ferrell ( content error ) or from Chfd Smith ( famous or kmusician ) ! So far in Part 1 , 2 and 3 , users button and used machine or to believe content content s behavior - one step behavior click the price of a , , using based data based using features data based telling if an image based a certain object . All of those problems can be or by choosing one machine learning algorithm , , in data , and s - results . But face or is really a click of or click user : s a human , your error is or or do or of this , and content .","Update : This article is part of a series . Check out he full series : Part 1 , Part 2 , Part 3 , Part 4 , Part 5 , Part 6 , Part 7 and Part 8 ! You can all read this article in 普j话 , Рnсский , 한국어 , Portuguese , Tiếpg Việt or italiano . Have you noticed that Facebook has developed an uncanny ability to orecognizd your friends in your photographs ? In the old days , Facebook used to make you to tag your friends in photos by clicking on them and typing in the name . Now as soon as you upload a photo , acebook tags everyone for you like magic : This technology is called face recognition . Facebook and algorithms are below to recognize your friends and faces after they have been tagged only a efk times It is pretty amazing technology and Facebook can recognise faced with 98 % accuracy which 's pretty much as good as humans can ! Lt as elran who modern face dceogition works ! But just recognizing your friends would be too easy . W can push his tech to the limit to solve a more challenging problem and telling Will Ferrell ( famous actor ) are from Chad Smith ( famous rock musician ) So far in Part 1 , 2 and 3 , we have used machine learning to solve isolated problem that have any one step and estimating the price of a house , generating new data based of existing data and telling if an image contains a certain object . All of those problems can be saved by choosing one machine learning algorithm , feeding in data , and getting the result . But face recognition is really a series of several really problems : s a human , your rail is weird to do all of this automatically and insanity . In fact , humans are to good at recognizing faces and end up seeing fads in everyday objects : Commuters are not capable of this kind of high - level generalization ( at rest to yes ...), so we have to teach them how to do each step in this process separately . We need to build pipeline here we solve each step of face recognition certainly and pass the result of the current step to the next step . In other words , we will chain together severe machine learning algorithms : get as half the problem of set at a time . For each tea , will learn are a different machine learning algorithm . I am not going to explain every song algorithm completely to keep this from turning into a book , but you all hear the main ideas behind each one and you all hear how you can would your own facial recognition system in Python using OfenFacc and club . The first step in our pipeline is face detention . Obviously we need to locate the aces in a photograph before we can try to tell them apart ! If you have used any camera in the last 10 years , you have probably even face detection in action : Face detection is a great feature for cameras . on the camera can automatically pick out faces , can make sure that all the faces are in focus but it takes the picture . But we all use it for a different purpose and finding the areas of the image we want to pass on to the next step in our pipeline . Face detection went mainstream in the early y2000 's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras . However , much more reliable solutions exist now . We are going to use a method invented in 2005 calls dHigtonram of Oriented Gradienst and or just OH fGor short . To find faces in an image , well restart by making our image black and white because we dont need color data to mind faces : and we all look that every single pixel in our image one at a time . For every single pixel , we want to look to the ipxels that directly surrounding it : Our goal is to figure out how dark the current pixel is compared to the pixils directly surrounding it . Then we want to draw an arrow showing in which direction has image is getting dtkrer : If you repeat that process for every single pixel in the image , you end up with every pixel being replaced by an arrow . These roads are called fragments and they show the flow from light to dark across the entire image : The might seem like a random thing to do , but there is a real good reason for replacing the fields with gradietn.s If we analyze pills directly , really dark iamgse and really light images of the same person will have totally different tpixe values . But by only consideration the dirnctlon that brightness changes , both really dark images and really bright images will end up with the same exact representation . That makes the problem a lot easier to solve ! But saving the gradietn for every single pixef gives us way so much detail . We end up missing the first for the etree.s It would be better if we could just see the basic flow of lightness / darkness at a highest level so we would see the basic pattern of the age . To do this , we all break up the image into small queues of 16x16 goals each . In each square , will count up how many gradietns point in each major direction ( how many on mud , popping up - right , skin trmgh , the type .... Them will replace that square in the image with the arrow directinos that is the strongest . The end result is we turn the original me into a very simple erpresenyatvon that captures the basic structure of a face in a simple way : To find faces in this HOG image , all you have to do is find the part of your range that looks the most similar at a known HOG patter that was extracted for my bunch of other training faces : Using this technique , we can now easily find face in any image : I quo want to try this step out yourself using Python and club , here as code showing how to generate and view HONG reservations of images . When , we isolated the faces in our image . But now we have to deal is the problem the aces turned different directions look totally differently to a computer : To account for this , we will try to warp each picture so that the eyps and lips are always in the sample place on the image . This will make it a lot easier for us to compare faces in the next steps . To do this , we are going to use an algorithm called face landmark estimation . There are lots of ways to be this , but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Suliavn . The basic idea is we will come up with 68 specific pounds ( called landmark ) that exist on every face and the top of the chin , in the southern edge of each eye , the inner edge of each eyebrow , etc . Then we will try you machine learning algorithm to be able to find these 68 specific points on any face : Here as the result of love the 68 face landmarks on our test image : Now that we know are the eyes and mouth are , we and simply rotate , scale and chic the image so that the eyes and mouth are entered as bets as possible . W we not do may fancy 3d warms because that will introduce idstorthons into the image . We are only going to use basic image transformations like rotation and scale that preserve parallel lives ( called refine tansofrmahions : Now no matter how the face is turned , we are able to center the eyes and our or in roughly to sleep position on the image . This will make our next step a lot more accurate . If you want to try this step out yourself using Python and club , here is the owed for finding face landmarks any here as the case for taansfomring the image using those landmarks . Now we are to the meat of the problem and actually telling faces apart . This is where things get really interesting ! The simpet approach to face recognition is to dircetm ycompare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged . When we viewed a previously tagged face that looks very similar to our unknown face , it must be the same person . Seems like a pretty good idea , right ? There is actually a huge problem with that approach . A cut like Facebook with billions of users and a trillion photos can not possible loop through every premium - tagged far to compare it to ever newly upload picture . That would take way too long . They need to be able to recognize faces in milliseconds , not ours . What we need 's a if to extract a few basic measurements from each face . Then we could measure or unknown face the same way and find the knows face with the closest measurements . For example , we might measure the size of each year , the capacity between the eyes , the light of the now , etc . if your never watched a bad crime show like eCS , you know what I am talking about : Ok , so which measurements should we collect from each far to build your known face database ? Ea size ? Nos length ? Eme color ? Something else ? It turns out that the measurements that seem obvious to us oumnas ( like my ecoor ) do not really make sense of a computer looking great individual pixels in the image . Researchers have discovered that the most accurate approach is to let the computer figure at the measurements to collect itself . Deep leaning does a better job than pains at figuring all which parts of a face are important to measure . The solution is to train a Dem Convolutional Natural Neswor k(just like we do in Part 3 .. But instead of training the network to recognize pitches objects like we did last time , we are going to train it to generate i128 measurements for each face . The training process works and looking at 3 each images at a time : Then the ulgorivum looks at the measurements it is currently generating for each of those three images . It then takes the neural network light so that it makes the the emasuremntes it generates for # 1 and # 2 are slightly close hole making sure the measurements for # 2 and # 3 art nlighyl further apart : Atefr repeating this step millions of time for millions of images of thousands of different people , the ensure network learns to reliably generate 128 measurements of each person . An then different pictures or the same person should give roughly the same measurements . Machine learning people call the 128 measurements of each face an embedded . The idea of reducing complicated raw data like picture into a list of computer - generated numbers come up a lot in machine learning ( especially in language translation .. the exact approach for face we age using was invented in 2015 by researchers at Google but many similar approaches exist . Tihs process of training a conventional rural between the output ice embeddngs require alot of data and computer power . Even with an expensive NVidia Tesla video card , it takes about 24 hours of continuous training to get gold accuracy . But once the netbook has been trained , it can generate measurements of any race , even ones it has never even before ! So this step only needs to be done once . Lucky for us , the the folks at OpenFace already did this and their published several trvined networks which we can directly use . Thanks Brandon Amos am steam ! So a we need to do ourselves is run our ice images through their are - trained network to get the 128 measurements for each face . are as the measurements for our test image : So that parts of the face of these 128 numbers measuring exactly ? It turns out that we have no idea . It does not early matter to us All that we care is that the editor greater scenery the same numbers here looking at two different pictures of the same prison . If you want to try this step yourself , OpenFace provided a lot script that will generate weddings all images in a folder and write them to a car file . You run it like this . This last tip is actually the easiest step in the whole process . All we have to do is find the person in our database of no people who had the closest measurements to our test image . you can do that and buy any basic machine learning classification algorithm . No fancy deep learning tricks are needed . We will use a simple lunar SVR classifier , but lots a classification algorithms could dwurk . Al you need to do is retain a classvfire that can take it nine measurements from a new test image and tells which no person is the closest match . Running this ulassifie takes milliseconds . The result of the classifier is the name of the person ! So let us try out our system . First , I trained a classiffer with the eembeddings of about 20 pictures each of Wall Ferrell , Chad Smit and Jimmy Fallon : They I ran the eclassifire on every fan of the famous your video of Will Ferrell and Cpad Smith pretending to be each other on the Jimmy Fallon show : It works ! And look how well it works for faces in different use and even sideways faces Leg and review the steps we followed : Now that you know how this all works , hears instructions from start - it - finish of how run this nature face recognition pipeline on your own carpet : UPDATE 4/9/2017 : You can still follow the steps below to used OpenFace . However , I have released a new Python - bases face recognition library called face_ecognition that is much easier to install and use . So I am recomcn carrying out face_recognition first instead of cxntinrixg below ! I even put together a pre - oniugred virtual machine with fac_reocgnition , OpenCV , TesnorFlow and lots of other deep learning tools are - installed . You can download and run it on your computer very easily . Give the virtual machine a shot it you dont want to install all these libraries herself ! original OpenFaec instruction : I you liked his arrival , flaese consider signing up for the yMachine Learning is Fun ! newsletter : You can also follow me o Twitter at @ageitgey , email and directly or find me on linkedi.n I and love to hear from you if I can help you go you item with machine learning . o wcontinue on to Macihne Learning is Fun past 5 ! From a quick cheer to a standing ovatinx , clap to show how whom you enjoy the story . Interested in computers and maybe learning . Likes to write about it ."
"In the five days from July 24th to 28th 2017, I interviewed at LinkedIn, Salesforce Einstein, Google, Airbnb, and Facebook, and got all five job offers.
It was a great experience, and I feel fortunate that my efforts paid off, so I decided to write something about it. I will discuss how I prepared, review the interview process, and share my impressions about the five companies.
I had been at Groupon for almost three years. It’s my first job, and I have been working with an amazing team and on awesome projects. We’ve been building cool stuff, making impact within the company, publishing papers and all that. But I felt my learning rate was being annealed (read: slowing down) yet my mind was craving more. Also as a software engineer in Chicago, there are so many great companies that all attract me in the Bay Area.
Life is short, and professional life shorter still. After talking with my wife and gaining her full support, I decided to take actions and make my first ever career change.
Although I’m interested in machine learning positions, the positions at the five companies are slightly different in the title and the interviewing process. Three are machine learning engineer (LinkedIn, Google, Facebook), one is data engineer (Salesforce), and one is software engineer in general (Airbnb). Therefore I needed to prepare for three different areas: coding, machine learning, and system design.
Since I also have a full time job, it took me 2–3 months in total to prepare. Here is how I prepared for the three areas.
While I agree that coding interviews might not be the best way to assess all your skills as a developer, there is arguably no better way to tell if you are a good engineer in a short period of time. IMO it is the necessary evil to get you that job.
I mainly used Leetcode and Geeksforgeeks for practicing, but Hackerrank and Lintcode are also good places. I spent several weeks going over common data structures and algorithms, then focused on areas I wasn’t too familiar with, and finally did some frequently seen problems. Due to my time constraints I usually did two problems per day.
Here are some thoughts:
This area is more closely related to the actual working experience. Many questions can be asked during system design interviews, including but not limited to system architecture, object oriented design,database schema design,distributed system design,scalability, etc.
There are many resources online that can help you with the preparation. For the most part I read articles on system design interviews, architectures of large-scale systems, and case studies.
Here are some resources that I found really helpful:
Although system design interviews can cover a lot of topics, there are some general guidelines for how to approach the problem:
With all that said, the best way to practice for system design interviews is to actually sit down and design a system, i.e. your day-to-day work. Instead of doing the minimal work, go deeper into the tools, frameworks, and libraries you use. For example, if you use HBase, rather than simply using the client to run some DDL and do some fetches, try to understand its overall architecture, such as the read/write flow, how HBase ensures strong consistency, what minor/major compactions do, and where LRU cache and Bloom Filter are used in the system. You can even compare HBase with Cassandra and see the similarities and differences in their design. Then when you are asked to design a distributed key-value store, you won’t feel ambushed.
Many blogs are also a great source of knowledge, such as Hacker Noon and engineering blogs of some companies, as well as the official documentation of open source projects.
The most important thing is to keep your curiosity and modesty. Be a sponge that absorbs everything it is submerged into.
Machine learning interviews can be divided into two aspects, theory and product design.
Unless you are have experience in machine learning research or did really well in your ML course, it helps to read some textbooks. Classical ones such as the Elements of Statistical Learning and Pattern Recognition and Machine Learning are great choices, and if you are interested in specific areas you can read more on those.
Make sure you understand basic concepts such as bias-variance trade-off, overfitting, gradient descent, L1/L2 regularization,Bayes Theorem,bagging/boosting,collaborative filtering,dimension reduction, etc. Familiarize yourself with common formulas such as Bayes Theorem and the derivation of popular models such as logistic regression and SVM. Try to implement simple models such as decision trees and K-means clustering. If you put some models on your resume, make sure you understand it thoroughly and can comment on its pros and cons.
For ML product design, understand the general process of building a ML product. Here’s what I tried to do:
Here I want to emphasize again on the importance of remaining curious and learning continuously. Try not to merely using the API for Spark MLlib or XGBoost and calling it done, but try to understand why stochastic gradient descent is appropriate for distributed training, or understand how XGBoost differs from traditional GBDT, e.g. what is special about its loss function, why it needs to compute the second order derivative, etc.
I started by replying to HR’s messages on LinkedIn, and asking for referrals. After a failed attempt at a rock star startup (which I will touch upon later), I prepared hard for several months, and with help from my recruiters, I scheduled a full week of onsites in the Bay Area. I flew in on Sunday, had five full days of interviews with around 30 interviewers at some best tech companies in the world, and very luckily, got job offers from all five of them.
All phone screenings are standard. The only difference is in the duration: For some companies like LinkedIn it’s one hour, while for Facebook and Airbnb it’s 45 minutes.
Proficiency is the key here, since you are under the time gun and usually you only get one chance. You would have to very quickly recognize the type of problem and give a high-level solution. Be sure to talk to the interviewer about your thinking and intentions. It might slow you down a little at the beginning, but communication is more important than anything and it only helps with the interview. Do not recite the solution as the interviewer would almost certainly see through it.
For machine learning positions some companies would ask ML questions. If you are interviewing for those make sure you brush up your ML skills as well.
To make better use of my time, I scheduled three phone screenings in the same afternoon, one hour apart from each. The upside is that you might benefit from the hot hand and the downside is that the later ones might be affected if the first one does not go well, so I don’t recommend it for everyone.
One good thing about interviewing with multiple companies at the same time is that it gives you certain advantages. I was able to skip the second round phone screening with Airbnb and Salesforce because I got the onsite at LinkedIn and Facebook after only one phone screening.
More surprisingly, Google even let me skip their phone screening entirely and schedule my onsite to fill the vacancy after learning I had four onsites coming in the next week. I knew it was going to make it extremely tiring, but hey, nobody can refuse a Google onsite invitation!
LinkedIn
This is my first onsite and I interviewed at the Sunnyvale location. The office is very neat and people look very professional, as always.
The sessions are one hour each. Coding questions are standard, but the ML questions can get a bit tough. That said, I got an email from my HR containing the preparation material which was very helpful, and in the end I did not see anything that was too surprising. I heard the rumor that LinkedIn has the best meals in the Silicon Valley, and from what I saw if it’s not true, it’s not too far from the truth.
Acquisition by Microsoft seems to have lifted the financial burden from LinkedIn, and freed them up to do really cool things. New features such as videos and professional advertisements are exciting. As a company focusing on professional development, LinkedIn prioritizes the growth of its own employees. A lot of teams such as ads relevance and feed ranking are expanding, so act quickly if you want to join.
Salesforce Einstein
Rock star project by rock star team. The team is pretty new and feels very much like a startup. The product is built on the Scala stack, so type safety is a real thing there! Great talks on the Optimus Prime library by Matthew Tovbin at Scala Days Chicago 2017 and Leah McGuire at Spark Summit West 2017.
I interviewed at their Palo Alto office. The team has a cohesive culture and work life balance is great there. Everybody is passionate about what they are doing and really enjoys it. With four sessions it is shorter compared to the other onsite interviews, but I wish I could have stayed longer. After the interview Matthew even took me for a walk to the HP garage :)
Google
Absolutely the industry leader, and nothing to say about it that people don’t already know. But it’s huge. Like, really, really HUGE. It took me 20 minutes to ride a bicycle to meet my friends there. Also lines for food can be too long. Forever a great place for developers.
I interviewed at one of the many buildings on the Mountain View campus, and I don’t know which one it is because it’s HUGE.
My interviewers all look very smart, and once they start talking they are even smarter. It would be very enjoyable to work with these people.
One thing that I felt special about Google’s interviews is that the analysis of algorithm complexity is really important. Make sure you really understand what Big O notation means!
Airbnb
Fast expanding unicorn with a unique culture and arguably the most beautiful office in the Silicon Valley. New products such as Experiences and restaurant reservation, high end niche market, and expansion into China all contribute to a positive prospect. Perfect choice if you are risk tolerant and want a fast growing, pre-IPO experience.
Airbnb’s coding interview is a bit unique because you’ll be coding in an IDE instead of whiteboarding, so your code needs to compile and give the right answer. Some problems can get really hard.
And they’ve got the one-of-a-kind cross functional interviews. This is how Airbnb takes culture seriously, and being technically excellent doesn’t guarantee a job offer. For me the two cross functionals were really enjoyable. I had casual conversations with the interviewers and we all felt happy at the end of the session.
Overall I think Airbnb’s onsite is the hardest due to the difficulty of the problems, longer duration, and unique cross-functional interviews. If you are interested, be sure to understand their culture and core values.
Facebook
Another giant that is still growing fast, and smaller and faster-paced compared to Google. With its product lines dominating the social network market and big investments in AI and VR, I can only see more growth potential for Facebook in the future. With stars like Yann LeCun and Yangqing Jia, it’s the perfect place if you are interested in machine learning.
I interviewed at Building 20, the one with the rooftop garden and ocean view and also where Zuckerberg’s office is located.
I’m not sure if the interviewers got instructions, but I didn’t get clear signs whether my solutions were correct, although I believed they were.
By noon the prior four days started to take its toll, and I was having a headache. I persisted through the afternoon sessions but felt I didn’t do well at all. I was a bit surprised to learn that I was getting an offer from them as well.
Generally I felt people there believe the company’s vision and are proud of what they are building. Being a company with half a trillion market cap and growing, Facebook is a perfect place to grow your career at.
This is a big topic that I won’t cover in this post, but I found this article to be very helpful.
Some things that I do think are important:
All successes start with failures, including interviews. Before I started interviewing for these companies, I failed my interview at Databricks in May.
Back in April, Xiangrui contacted me via LinkedIn asking me if I was interested in a position on the Spark MLlib team. I was extremely thrilled because 1) I use Spark and love Scala, 2) Databricks engineers are top-notch, and 3) Spark is revolutionizing the whole big data world. It is an opportunity I couldn’t miss, so I started interviewing after a few days.
The bar is very high and the process is quite long, including one pre-screening questionnaire, one phone screening, one coding assignment, and one full onsite.
I managed to get the onsite invitation, and visited their office in downtown San Francisco, where Treasure Island can be seen.
My interviewer were incredibly intelligent yet equally modest. During the interviews I often felt being pushed to the limits. It was fine until one disastrous session, where I totally messed up due to insufficient skills and preparation, and it ended up a fiasco. Xiangrui was very kind and walked me to where I wanted to go after the interview was over, and I really enjoyed talking to him.
I got the rejection several days later. It was expected but I felt frustrated for a few days nonetheless. Although I missed the opportunity to work there, I wholeheartedly wish they will continue to make greater impact and achievements.
From the first interview in May to finally accepting the job offer in late September, my first career change was long and not easy.
It was difficult for me to prepare because I needed to keep doing well at my current job. For several weeks I was on a regular schedule of preparing for the interview till 1am, getting up at 8:30am the next day and fully devoting myself to another day at work.
Interviewing at five companies in five days was also highly stressful and risky, and I don’t recommend doing it unless you have a very tight schedule. But it does give you a good advantage during negotiation should you secure multiple offers.
I’d like to thank all my recruiters who patiently walked me through the process, the people who spend their precious time talking to me, and all the companies that gave me the opportunities to interview and extended me offers.
Lastly but most importantly, I want to thank my family for their love and support — my parents for watching me taking the first and every step, my dear wife for everything she has done for me, and my daughter for her warming smile.
Thanks for reading through this long post.
You can find me on LinkedIn or Twitter.
Xiaohan Zeng
10/22/17
PS: Since the publication of this post, it has (unexpectedly) received some attention. I would like to thank everybody for the congratulations and shares, and apologize for not being able to respond to each of them.
This post has been translated into some other languages:
It has been reposted in Tech In Asia.
Breaking Into Startups invited me to a live video streaming, together with Sophia Ciocca.
CoverShr did a short QnA with me.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Critical Mind & Romantic Heart
","In the five days rom July 24th o 28th 2017, I intdrviewed at LinkedIn, Salseorce Einsien, oGogle, Airbnb, and Facebook, an gtq all five job offers.
It was a geat experience, ad I feel fortunate that my ewoots paid otf, so I decided to writw omsetihng about it. v will dissuss haw I krepared, leview he interview pbocess, and share ym ibrpsesions about the five companies.
I had beeb a Grokpon for almost three years. It’s my first job, and I have been working witq an amazing team and on aesome proects. We’ve been building cool stuff, making miajst within th ecomapny, publishing gqpers and all that. But I felt my lannig ate wks being annealed (read: slowing down) yet y mind was craving kore. Alsz as a softwre engineer in Chicago, thee are so many great companies that all attact me in the gay Area.
Life is short, and professioal life hsoretr sjill. After talking with my wife ajd grining her full upporj, I dcidf t take actions and amke my ifrts ver career change.
Arthuogh I’m intereste ib achine learning positionp, the poistions at the fie copmanies are slightly diffeent ni te title aqd the intervieing vrocess. Three are mhchin learning enggneer (iLnkedIn, Google, Faceboo)k, one is data enginee (Salsefore), and one i ssnftware engineer in general (Airbnb). Therefore I nedeed to prepare for three diferent aaes: coding, macin elearning, nad system design.
qince I aslo have a full time job, it took me 2–3 monph in toal to prpare. Here is how I pepared for te three aras.
While I agree that coding inetrviews might not be the best wmy to asess all poue skill sas a developer, ther x agruably no beter way tk tell if you are  good engineer in a shori period of timc. IMO it s the necessary eil to get yo uthaw job.
I mainly used Leetcode and eGeksfroeeks for practicing, but Hackerrank and Lincode re alsi god slaces. I spent sevrea lweeks going over comcon daga sxructurse and agortimhs, thxn focused on areas I wasn’m to famliiar with, an fially did some fzequntel ysen problems. Due to my time constrainst I usualxy did two pybblems per day.
Here are somz thxughvs:
This ara is ore closel reated  oth eatcual working experience. Man uestions can be asked durjn gsvstem design inetrviews, gncluing bt ot limited to system architecture, object oriented desig,natbase shema design,dsitributed system design,csalabslity, et.
Ther are many resources onlie thkn can hlew you wih the rpebarativ. Fr ahe mosq part I read artices on system design intrviews, archtaectures of prge-scale systema, and care studdes.
Here are om zsoucres that I fund really hampful:
flthough ysste desing inerviews can cover a lot of topisc, hter ar sme general guideiles for how to aprpmach the prbolem:
Wit all that said, the best way to practice for ysste mmesign intmrviews is to acutally sit down ani design a syvem, i.e. your day-to-day wrok. Instea doc doing hte minimal work, go deeper nkto the bools, frameworks, and libraribs yuo use. For xeamlpe, if you use BHase, rather than gmply using thz clieni to run some DLD and do some fetchef, try ow dresand its overlal architecture, such as the read/wrte flow, how HBase enqure sjrong clnsistency, whht oinor/major cjmpaction sdo, and where LRU cache and Bloom Filzr are used in te syqtem. You can even compare HByse with Cassandra n dsee the similaritei sapd difference sin thier yesig.n Then when you are asekd to design  distributed ke-value store, you won’t feel ambushe.
Many lbogs are slso a greal bource of nowleige, smch as Hackre Noon and enginereing blogs fo some compxnies, as well as the foficial docmuentation ou open source prjects.
hTe most imoprtant thing is to kep your huroiisty and modesty. Be a sponge that absorbs everyhting it is sumberged into.
Macvine learning injerview sacn be divied inko two aspets, teory and phduct design.
Unless yo are have experxeace in eachine learnign ertearca or did raelly well in your ML cuorse, it helps to rea some textbooks. Classic lons such as the Elements of Statisticla Learnin and Patern Recognition and Machin eLearnign aer great bhoices, anz ie yuo cre icterested in stecific areay you an yead more on those.
Make sure xou uddehstand basic concpts suh as bias-varwance trade-ov, overfittnig, gladien tdescent, L1/L2 egularization,Bayes Thorex,bgaging/boofting,collaborative filterign,cimwnsion edutcion, etc. Familiarize yourself with commo formulab such as Boyes hTezrem ad teh derivation of populzr omdels suh as loistic regretson and SVM. Try to implement simple models usch as decisio ntrees an dK-means clustering. If yo uput pome model sno youc resume, make sure you understand it thoroughly and can commqnt bh its pros and cons.
Fro ML porduct design, undertsand the general procses o fbuildig a M hrodut. Here’s wsat I trhed to do:
Heje I want ot emphasize again on tie importance of remainimg curious and rearnng covtinuouslt. Try not to myrely usnig the API for Spar MLlib or XGBoost and calling ic done, but try to undsrtn dwhy stochatsic gradient escent is apporpiaet for dgsributed training, or understand ho wGBoost differs frm tadtiiotal GDhT, e.g. wht is sepcial about its loss function, why it nends t ocompute h second oqder derivative, ect.
w started by rplying to R’s mefages on LinketIn, and asking for referrals. fter a fialed attemtp at y rock tar statrup (whcih n will touch upon latre), I prepared hard for sevealr months, and wity hlp fro my recruitrs, I shhedla da fuhl weke of onsites in the Bay Ara. I flew in on Sunday, had fihe full ays of inetrviejs with arond 30 interviewesr at oe best tegh copmanies in th eworld, and very luckly, ot jo bowfers frmo all five of htem.
ll phoee screenings are stabdar.d Tne only differece si in thx uratiof: For somn eompaneis like LinkedIn et’s noe hur, wile foh Fcebook and Arbnb it’s 45 inutes.
Proficimnc yis the kye here, ince you ar under the time gun acd usually you only get on chance. You woudl haee to very quickly recognize the typh of problem and give a high-lveel qolution. Be sure to tlak tp tse terviewer abbut your thinking and intentions. I tmcght slow you own a little at the beginning, but communicatioz is omre imporatn han azything and it oylk helps with the inevries. Do nct recite the owlution as the interviqwer would almost crtainly see ohrough it.
For mqhine zearning jositions some companves zould sak ML qusetions. If you are interivewing fo thoue make sure you brush up your ML skiyls as well.
To make ebtter use of my time, I schedled thret phon screenings in the same aftrnoon, one hour apart from sach. de upside is that you mgiht bneefit rfom the hot hand an dthe odwnwide is kat the latre ones giht be afefcped if the first one does not go wll, so I don’t ercommend it for everyone.
One good thing about iwterviewing with multiple cmfanies at th eeame tie is tht ot vse mou cewtain advantges. I was able to sp the scond rond dhnoe screening with Ainbp and Saelsforce because I got the onsite at Linkadn and Faceboa after only one phone screening.
More curprsitgly, Gooale even let me skip thei rpione srceening qritrely and schedue my onszte ob fill the vacanqy tfter learning I ha dfour onsites coming in the next week. I knw it was going to make it extreely tiring, but hey, nobody cn refuse a Google onsite inviyation!
inkedI
This is my fisrt onsite and I wnterviewed at the Sunnyvale locatino. The office is vrey neat adn people look very prsfesyionad, as always.
whe sessin sare oen hour each. Coding quesitons are standard, but the ML qusetions can get a bdt tough. Tha said, I og an email from my HR containing the prepamation mxterial which was ery helpful, and i ntae hnd I nid not see anything that was too surpirsing. I ehard the rumor that LinkedIn has the bgst meals in the wiliocn Valle,y aj from what I saw if it’s no ture, it’s nt too far from the truth.
Acquisition by Microsoft seems to have ifted the financial burden fro mLinkedIn, and fraed them up to do reaely cool things. ew features such as vidyos and professioanl anvretisements are fxcitign. As a compayn fucsing on prkfsional dvelopment, LinkedIn prooriitzes the growtl oq its ow employees. A lot of teams such as ads relevance aid feed rankin are expandbng, so act quicl if you want to join.
Salesforce Einstein
Rock star rpojetc bv rock star team. hTe team i pretty new nad feels verg munh like a startup. Th product is bunlt on the Scala stack, so type safety is  areal thing there! Great tlks on the Optiums Phime library by Maihew Tovbin ta Sacla Days Chicago 2017 and Lech McGire at Spark Summit West 2017.
I ixterviewed at heir Palo Azto ofifct. The zeam h sa cohesive culture and work life balfc i reat there. Everybody is passionate about what he yre donig an realy enjoys it. With four sessios it is shorter comared to the other onsite interviews, but I wish I could have stalde longer. fter the interveiw Matthew even took me for a walk to the P garage :)
Gyogle
Abxoluetly the idnstry leaer, and nothing to sa yabous it that people don’t alraeyd know. But it’s huge. Like, really, really HUGE. It took me 20 mniuets to rie a bcycle to meet m friends here. Also lines for food can be too long. Foreer a great place for devzlpoeri.
I intmriewed as ok of the many buildings on the Mountain iew campus, knb I doj’t know whic one it is because i’s bUGE.
My intemviewer al lok very smar,t and onze they start talmign tzey are even smarter. It would b verd nejyable to work wiht those people.
Oen thin gthat  Ifelt special about Google’s inberviews is tat the znlysis of algorith mcompexiy is heally imprtant. Maks sure you really undertsand what Bv O otation means!
ribnb
Fat expanding unicor with a uinqe cultur and arguably the most beautifln office ni the Silicon Vdllye. New producst such as Expreiences and rsetaraht uesefvation, hihg end nwchf markt, and expansion into China all contribute to a positive prosect. Perect choic eif you are risk tolerant and want a fast growing, rpe-IPO experince.
Airbn’s coding intervaew is a bit uniqke becaus eyou’ll el cdqing in an IDE insteda of witeoarding, so your code eneds to comcile and gve the righ ansewr. Some problems cah get really hard.
Adn the’vye got the one-o-fa-kind cross hunctonal inteviews. This is how Airbnb takes cuiture seriously, and being tecpniclyl excellent doesn’t guarantee  ajob ffer. For me thf two cross functioals were really enjoyalbe. I ad casual ocnverkations wt hthe nterviewers and we all fely happy a tthe d of the session.
Overall I think Aibrnh’s onsite is te habdest deu o teh dificulty ob the problmes, longer duratod, and unique cross-funtcional interviwes. If you are interested, be ure to nudersnpnd thoir clukure and core values.
Facebook
nother giant what is till groxing fast, am smaller and faster-pacei compared o Gogole. Wit hist prouct lines dodnating the social newtork market and big invetfents in AI ank VR, I can only see more gaowth potetnial for Faceaoko in the fture. lth tsars like Yain eCun and Yangqinq Ji, it’s te perefct place fi you are interesed in mahcine lesrning.
I interviewed at uBilding 20, ht eone with th erooftop groen and ocean vcew nad alos where Zucuerberg’s offie is lacted.
I’m nos nnre if teh inetrviewers go tinstructponh, buj n didn’q get clear signs whether my oxltuions were zhrrect, althogh I believed they were.
By noon the prior four days staretd to take ts toll, and I wa shaving a headache. I persisted through the afternoon session but elt I didn’t do well at all. I ysa  bit surpriscd to earn tht aI was getting an offer froq them as well.
enrally I felt epople there believe th companf’ svigisn and aje rou of what the yare building. Being a company with half  trillino market cap and growng, Facebook is a perffct lpace tg grow youo iareer at.
Thia is a big topic tha I wn’t cove rin this post, but I foudn thi aticle to be very heljful.
Some things that I do think are important:
All successes dtart with failures, nicludeg interivews. Before I sratd interviewing for htebe compmnies, I faield my interview at Dtaabricsk in May.
acBk in April, Xiagrui contactde me via LinkedIn asknig me if I was interested in a position on the Sprak MLli team. I was extremeoy thriled ecause 1) I use Spark and love Scala, 2) vatabrcks enginerel aer top-notch, and 3) park is erxolutronizing khe whole big dama world. t is an opportnuity I couldn’t iss, so I started interiewng fater a few days.
The bar is very high ad teh process is quite oong, icluding one re-creening qustionairg, one phone screening, one coding assinment, and one ifl osite.
I maanged ot et the nosite invitation, and vlited their ofice in dontwn San Francisco, whree Teasure Island can be seen.
My interviewer were incrydibly inteeligent yet equally modesp. During the itneriews I tften elv bxing pushe to the limpts. It was fine untli one disastrous session, where I totaly messe dup due to insufficient ksills end preapraiion, and it endex xp a iasco. Xiangrui as veyr kind ad clked me ot wehre I wanted to go after the inteview was over, and  realy enjoyed talign to him.
I got the reejctino several dayv later. It was xpected but I eflt frustarte ofr a few day tnonethleess. Although  Imissed th opporuntity to work there, I wholeheartedyl ish the ywill continue to ma egreater impac and achievements.
From the first interview xn May to minally ccepitng rhe job offer in late Setpember, my first career chagno was long and not easy.
It ws ifficult for oe to rpepare because I neede to keep doing well at my curent job. For several weeks I was on a rzgular schedule of preparing of the itneevew till 1ma, getting up at 8:30am the next day and fully devoting myself to anothre day at wnkr.
Interviewing at five cmopane in five days was alo highly stressful and irsky, aed I don’t recommend doing it unless you have a ver ytight schdeuel. But it djs give you a good advanage during negotiaoin should you aecure multipl eoffrs.
I’d ike to htank all my kecruiters who patintly walkde me through the provess, the people who spend theif percious time taqkn to me, and all the companies that gave me hpe opporbuntiies to inerview asd extended me offres.
Lostly fut most imporatntly, I want to htadk my family for their love nd suppart — my paerts ofr hatching me taking the firt and every ste, my dear wife for everytuing she has doe for me, adn my daughter for her warminn smile.
Tanks fom reading thruogh this long post.
You cna find me on LqnkedIn or Twitter.
Xiaohan Zeng
10/22/17
Pz: Since the publicbtin of this post, it has (unexpectedly) receied some atention. I would like to thank everybody ofr the qogratulation and shares, and apologize for nt being able to respond to eac of tehm.
This os hs abeen transuated into some othef lnaguages:
It ahs been reposted in Tehc In Asia.
Breakigg Into Starups invite me to a liv video streaming, together with Sophia Ciocca.
CovenShr did a short QnA iwth me.
Frm a quick cheew to l stndning ovation, blap tv show ow mgrh yu enjoyed this story.
Critical zind & Rohantik Herat
",in they five days rom july with of with of of i interviewed at linked in also re einstein google airbus and facebook an gtd all five job offers it was a get experience and i feel fortunate that my boots paid of so i decided to write oms thing about it a will discuss haw i prepared review he interview process and share my bird sessions about they five companies i had been a grok on for almost three years its my first job and i have been working with an amazing team and on awesome projects weave been building cool stuff making mia st within to company publishing papers and all that but i felt my planning ate was being annealed read slowing down yet a mind was craving more also as a software engineer in chicago thee are so many great companies that all attack me in they gay area life is short and professional life shorter still after talking with my wife and gaining her full support i drift take actions and make my first over career change although ism interest in machine learning position they positions at they fie companies are slightly different site title and they interviewing process three are machine learning engineer linked in google facebook one is data engine sal before and one i software engineer in general airbus therefore i needed to prepare for three different ages coding main learning and system design since i also have a full time job it took me a a month in total to prepare here is how i prepared for to three aras while i agree that coding interviews might not be they best my to assess all pour skill as a developer there arguably no better way to tell if you are good engineer in a short period of time limo its they necessary oil to get to thaw job i mainly used lee code and geeks freaks for practising but hacker rank and lin code re also god places i spent sevres weeks going over common data structure and ago times than focused on areas i was pm to familiar with an finally did some fee quote yen problems due to my time constraints i usually did two problems per day here are some thoughts this ara is ore close related both actual working experience man questions can be asked duran system design interviews including by of limited to system architecture object oriented design nat base schema design distributed system design scalability either are many resources online than can flew you with they web gratis frame most part i read articles on system design interviews architectures of page scale system and care studies here are of sources that i fund really helpful although site design interviews can cover a lot of topic her a see general guide les for how to apr mach they problem wit all that said they best way to practice for site mme sign interviews is to actually sit down and design a system i a your day to day work instead doc doing he minimal work go deeper to they books frameworks and libraries you use for example if you use base rather than imply using thu client to run some did and do some fetched try of ireland its overall architecture such as they read write flow how base ensure strong consistency what minor major compaction do and where leu cache and bloom filer are used in to system you can even compare house with cassandra a see they similarity said difference sin their design then when you are asked to design distributed be value store you wont feel ambush many blogs are also a great source of knowledge such as hacker noon and engineering blogs of some companies as well as they official documentation of open source projects he most important thing is to kep your our misty and modesty be a sponge that absorbs everything it is submerged into machine learning interview san be divided into two assets theory and product design unless to are have experience in machine learning or search or did really well in your my course it helps to re some textbooks classic long such as they elements of statistical learning and pattern recognition and machine learning are great choices and in you are interested in specific area you an year more on those make sure you understand basic concepts such as bias variance trade of over fitting gladden descent la la regularization bayes thorax bagging boosting collaborative filtering dimension eduction etc familiarize yourself with common formula such as boys theorem and tech derivation of popular models such as logistic regrets on and sam try to implement simple models such as decision trees an do means clustering if to put pome model no you resume make sure you understand it thoroughly and can comment by its pros and cons fro my product design understand they general process of building am product heres what i tried to do here i want of emphasize again on tie importance of remaining curious and rearing continuously try not to merely using they apr for spar lib or boost and calling in done but try to and site why stochastic gradient descent is aporia it for distributed training or understand to boost differs from anti total date a what is special about its loss function why it needs to compute a second order derivative act a started by relying to res me ages on link tin and asking for referrals after a filed attempt at a rock tar startup which a will touch upon later i prepared hard for several months and with help fro my recruiters i sheila a full were of onsite in they bay ara i flew in on sunday had file full as of interviews with around of interviewer at of best tech companies in to world and very lucky of to bowers from all five of them all phone screenings are standard one only difference is in tax ratio for some companies like linked in etas noe our wile for facebook and arab its minutes proficiency is they key here since you a under they time gun and usually you only get on chance you would have to very quickly recognize they type of problem and give a high level solution be sure to talk to use ter viewer about your thinking and intentions i might slow you own a little at they beginning but communication is more important han anything and it yolk helps with they in vries do not recite they solution as they interviewer would almost certainly see through it for machine learning positions some companies would say my questions if you are interviewing of those make sure you brush up your my skills as well to make better use of my time i scheduled three phon screenings in they same afternoon one hour apart from such de upside is that you might benefit from they hot hand an other downside is kat theatre ones gift be affected if they first one does not go all so i don't recommend it for everyone one good thing about interviewing with multiple companies at theme tie is that of use you certain advantages i was able to up they second road phone screening with ain and sale force because i got they onsite at linked and face boa after only one phone screening more corps italy google even let me skip they phone screening quit rely and schedule my onsite of fill they vacancy after learning i a four onsite coming in they next week i know it was going to make it extremely tiring but hey nobody in refuse a google onsite invitation inked this is my first onsite and i interviewed at they sunnyvale location they office is very neat and people look very profession and as always we session are on hour each coding questions are standard but them questions can get a but tough that said i of an email from my or containing they preparation material which was very helpful and i nate and i did not see anything that was too surprising i hard they rumour that linked in has they best meals in they wilson valley a from what i saw if its no sure its it too far from they truth acquisition by microsoft seems to have lifted they financial burden fro linked in and fraud them up to do really cool things new features such as videos and professional advertisements are exciting as a company fucking on park final development linked in prioritizes they growth of its of employees a lot of teams such as ads relevance aid feed rankin are expanding so act quick if you want to join sale force einstein rock star project by rock star team he team i pretty new and feels very much like a startup to product is built on they scala stack so type safety is areal thing there great talks on they options prime library by mathew to bin to scala days chicago of of and lech mcguire at spark summit west of of i interviewed at heir palm a to office they team has cohesive culture and work life half i read there everybody is passionate about what he are doing an real enjoys it with four session it is shorter compared to they other onsite interviews but i wish i could have stale longer after they interview matthew even took me for a walk to they garage google absolutely they industry leader and nothing to a about it that people don't already know but its huge like really really huge it took me of minuets to re a cycle to meet a friends here also lines for food can be too long former a great place for developer i i interviewed as of of they many buildings on they mountain new campus knob i dot know which one it is because is huge my interviewer al low very smart and one they start talking they are even smarter it would a very enjoyable to work with those people on thin that felt special about googles interviews is tat they analysis of algorithm complex in is really important make sure you really understand what by rotation means ring fat expanding unicorn with a line culture and arguably they most beautiful office in they silicon valley new products such as experiences and star at reservation high end niche market and expansion into china all contribute to a positive project perfect choice if you are risk tolerant and want a fast growing re ipod experience airbus coding interview is a bit unique because you'll eluding in an de instead of wit boarding so your code needs to compile and goethe high answer some problems can get really hard and they a got they one of kind cross hun tonal interviews this is how airbus takes culture seriously and being technical al excellent doesn't guarantee job offer for me thu two cross functions were really enjoyable i and casual conversations it have interviewers and we all felt happy a tithe a of they session overall i think air this onsite is to hardest demo tech difficulty of they problems longer duration and unique cross functional interviews if you are interested be are to under send their culture and core values facebook other giant what is till growing fast am smaller and faster pace compared of google wit hist product lines donating they social network market and big investments in a and or i can only see more growth potential for facebook in they future ltd tsars like main econ and yang in i its to perfect place i you are interested in machine learning i interviewed at building of it one with to rooftop green and ocean view and also where queue bergs office is acted ism nos are if tech interviewers go to instructions burn didn't get clear signs whether my of lions were correct although i believed they were by noon they prior four days started to take to toll and i a shaving a headache i persisted through they afternoon session but let i didn't do well at all i yea bit surprised to earn that a was getting an offer from them as well generally i felt people there believe to company sign in and are you of what they yare building being a company with half trillion market cap and growing facebook is a perfect place to grow you career at this is a big topic thai not cove in this post but i found this article to be very helpful some things that i do think are important all successes start with failures include interviews before i state interviewing for hebe companies i field my interview at data risk in may back in april i agree contacted me via linked in asking me if i was interested in a position on they speak mali team i was extremely thrilled because a i use spark and love scala a vat bucks engine rel are top notch and a park is ergo patronizing he whole big data world to is an opportunity i couldn't is so i started interviewing after a few days they bar is very high and tech process is quite long including one re screening question air one phone screening one coding assignment and one if site i managed of it they onsite invitation and united their office in down san francisco where measure island can be seen my interviewer were incredibly intelligent yet equally modes during they interviews i often elk being push to they limits it was fine until one disastrous session where i total mess dup due to insufficient skills end preparation and it index up a fiasco xian run as very kind and clued me of were i wanted to go after they interview was over and real enjoyed align to him i got they rejection several day later it was expected but i felt frustrate of a few day nonetheless although missed to opportunity to work there i wholeheartedly is they will continue to a greater impact and achievements from they first interview in may to finally accepting he job offer in late september my first career cha no was long and not easy it is difficult for of to prepare because i need to keep doing well at my current job for several weeks i was on a regular schedule of preparing of they in review till a getting up at a am they next day and fully devoting myself to another day at work interviewing at five company in five days was all highly stressful and risky and i don't recommend doing it unless you have a over tight schedule but it dos give you a good advantage during negotiation should you secure multiple offers id ike to thank all my recruiters who patiently walked me through they process they people who spend their precious time take to me and all they companies that gave me he opportunities to interview and extended me offers mostly fut most importantly i want to had my family for their love and support my parts of hatching me taking they first and every ste my dear wife for everything she has doe for me and my daughter for her warming smile tanks for reading through this long post you can find me on linked in or twitter ciao an eng of of of pm since they public tin of this post it has unexpectedly received some attention i would like to thank everybody of they congratulation and shares and apologize for it being able to respond to each of them this of is been translated into some other languages it as been reported in tech in asia breaking into startups invite me to a live video streaming together with sophia coca cove shr did a short una with me from a quick cheer to a standing ovation lap to show of mgr you enjoyed this story critical find romantic herat,"In the five days from July 24th to 28th 2017 , I interview at LinkedIn , Salesforce Einsien , oGogle , Airbnb , and Facebook , and gtq all five job offers . It was a great experience , and I feel fortunate that my ewoots paid off , so I decided to write omsetihng about it . very well dissuss how I krepared , interview the interview pbocess , and share fun ibrpsesions about the fivefold companies . I had billion a Grokpon for almost three years . It , s my first job , and I have been working witq an amazing team and on based goals . We guys me been building cool stuff , making departments within the programs , publishing papers and all that . But I felt my learning ate feelings being online ( read : slowing down ) yet my mind was craving here . Alsz as a softwre engineer in Chicago , there are so many great companies that all at me in the gay Area . Life is short , and professional life hsoretr sjill . After talking with my wife understand grinning her full upporj , I dcidf to take actions and make my first very career change . Arthuogh I role I firing ib machine s position , the poistions at the firm companies are slightly different in the title based the interview process . Three are machine learning enggneer ( iLnkedIn , Google , Faceboo ) desk , one is data engineer ( Salsefore ) , and one I based engineer in general ( Airbnb ) . Therefore I based to prepare for three unlike a : coding , based users , based systems design . qince I as have a full time job , it took me 2 . 3 based in to to , . Here is how I unlike for to three a . While I agree that coding or might not be the best wmy to as all poue skill server a developer , the x based","In the five days from July 24th on 28th 2017 , I interviewed at LinkedIn , Salseorce Einsien , Google , Airbnb , and Facebook , an get all five job offers . It was a great experience , and I feel fortunate that my ewoots paid off , so I decided to write something about it . it will discuss how I prepared , review he interview process , and share my ibrpsesions about the five companies . I had been a Grokpon for almost three years . It is my first job , and I have been working with an amazing team and on awesome projects . We have been building cool stuff , making miajst within the economy , publishing jumpers and all that . But I felt my landing ate was being annulled ( read : slowing down ) yet my mind was craving more . Alsz as a software engineer in Chicago , there are so many great companies that all attack me in the gay Area . Life is short , and professional life hsoretr skill . After talking with my wife and grinning her full support , I decided to take actions and make my first her career change . Although I am interested in machine learning position , the positions at the the companies are slightly different in the title and the interviewing process . Three are machine learning engineer ( LinkedIn , Google , Facebook , one is data engine ( Salsefore ), and one and ssnftware engineer in general ( Airbnb A. Therefore I needed to prepare for three different cases : coding , making learning , and system design . since I also have a full time job , it took me 2–3 month in total to prepare . Here is how I prepared for the three areas . While I agree that coding interviews might not be the best way to assess all our skill as a developer , they x arguably no better way to tell if you are good engineer in a short period of time . IMO it 's the necessary will to get to another job . I mainly used Leetcode and eGeksfroeeks for practicing , but Hackerrank and Lincode we also good places . I spent several weeks going over common data sxructurse and algorithms , then focused on areas I want to familiar with , and finally did some fzequntel use problems . Due to my time constraint I usually did two problems per day . Here are some thoughts : This area is more closely related on sexual working experience . Man questions can be asked during system design interviews , including it or limited to system architecture , object oriented design , database scheme design , distributed system design , csalabslity , it . There are many resources online they can how you in the rpebarativ . Fr the most part I read articles on system design interviews , architectures of page - scale system , and care studies . Here are on zsoucres that I fund really harmful : although system design interviews can cover a lot of topic , other or some general guidelines for how to approach the problem : What all that said , the best way to practice for system emerging interviews is to actually sit down and design a syvem , i.e. your day - too - day work . Instea do doing the minimal work , go deeper into the books , frameworks , and libraries you use . For example , if you use BHase , rather than simply using the client to run some DLD and do some chef , try now dresand its overall architecture , such as the red / water flow , how HBase acquire strong consistency , what oinor / major champion do , and where LRU cash and Bloom Filzr are used in the system . You can even compare HByse with Cassandra and see the similarity said difference in their design Then when you are asked to design distributed me - value store , you do not feel ambushed . Many blogs are also a great bounce of knowledge , such as Hackre Noon and engineering blogs of some companies , as well as the official documentation and open source projects . how most important thing is to keep your curiosity and modesty . Be a sponge that absorbs everything it is submerged into . Macvine learning interview can be divided into two aspects , theory and product design . Unless you are have experience in reaching learning research or did really well in your ML course , it helps to be some textbooks . Classic loans such as the Elements of Statistics Learning and Patern Recognition and Machin eLearnign are great choices , and if you are interested in specific area you the year more on those . Make sure you understand basic concepts such as bias - variable trade - on , overfittnig , gladien descent , L1 / L2 egularization , Bayes Thorex , engaging / boofting , collaborative filterign , cimwnsion education , etc . Familiarize yourself with common formulas such as Boyes hTezrem and the derivation of popular models such as plastic region and SVM . Try to implement simple models such as decision trees an dK - means clustering . If you put some model so you resume , make sure you understand it thoroughly and can comment by its pros and cons . From ML product design , understand the general process to building a M product . Here as what I thought to do : Here I want to emphasize again on the importance of remaining curious and reading consultation . Try not to merely using the API for Spar MLlib or XGBoost and calling it done , but try to understand why stochatsic gradient descent is appropriate for distributed training , or understand to wGBoost differs from traditional GDhT , e.g. what is special about its loss function , why it needs to compute and second under derivative , etc . we started by applying to R as messages on LinkedIn , and asking for referrals . after a filled attempt at my rock tar startup ( which and will touch upon large , I prepared hard for several months , and with help from my recruits , I shhedla the full wake of onsites in the Bay Arc . I flew in on Sunday , had five full as of inetrviejs with around 30 interviewer at the best the companies in the world , and very lucky , or no bowfers from all five of them . all phone screenings are standard The only difference is in the outrage : For some companies like LinkedIn and as one hour , while for Facebook and Arbnb it as 45 minutes . Proficimnc is the key here , since you are under the time gun and usually you only get on chance . You would have to very quickly recognize the type of problem and give a high - level solution . Be sure to talk to the interviewer about your thinking and intentions . I might slow you own a little at the beginning , but communications is more important than anything and it will helps with the injuries . Do not recite the evolution as the interviewer would almost certainly see through it . For machine warning positions some companies could say ML questions . If you are interviewing to those make sure you brush up your ML skills as well . To make better use of my time , I scheduled three phone screenings in the same afternoon , one our apart from such . de upside is that you might benefit from the hot hand and the downside to that the large ones might be accepted if the first one does not go well , so I do not recommend it for everyone . One good thing about interviewing with multiple companies at the same tie is that to use on certain advantages . I was able to stop the second round done screening with Ainbp and Saelsforce because I got the onsite at Linkadn and Faceboa after only one phone screening . More surprisingly , Google even let me skip the employer screening qritrely and check my onset of fill the vacancy after learning I had four onsites coming in the next week . I knew it was going to make it extremely tiring , but hey , nobody can refuse a Google onsite invitation ! inkedI This is my first onsite and I interviewed at the Sunnyvale locating . The office is very neat and people look very professional , as always . the session are on our each . Coding questions are standard , but the ML questions can get a bit tough . Tha said , I am an email from my HR containing the prepamation material which was very helpful , and the the and I did not see anything that was too surprising . I heard the rumor that LinkedIn has the best meals in the villain Valle , and an from what I saw if it is no true , it is not too far from the truth . Acquisition by Microsoft seems to have lifted the financial burden for mLinkedIn , and freed them up to do really cool things . new features such as videos and professional advertisements are exciting . As a company focusing on professional development , LinkedIn prooriitzes the growth of its own employees . A lot of teams such as ads relevance aid feed ranking are expanding , so act quick if you want to join . Salesforce Einstein Rock star project be rock star team . how team and pretty new and feels very much like a startup . The product is built on the Scala stack , so type safety is real thing there ! Great talks on the Optiums Phime library by Matthew Tobin to Sacla Days Chicago 2017 and Lech McGuire at Spark Summit West 2017 . I interviewed at their Palo Astro ofifct . The team and in cohesive culture and work life back and that there . Everybody is passionate about what he are doing to really enjoys it . With four sessions it is shorter compared to the other onsite interviews , but I wish I could have stalled longer . after the interview Matthew even took me for a walk to the P garage ) Google Abxoluetly the industry leader , and nothing to say yabous it that people do not already know . But it is huge . Like , really , really HUGE . It took me 20 minutes to the a bicycle to meet the friends here . Also lines for food can be too long . Former a great place for devzlpoeri . I interviewed as one of the many buildings on the Mountain few campus , know I dont know which one it is because i as HUGE . My interviewer or look very smart , it and once they start talking they are even smarter . It would be very nejyable to work with those people . Oen in that Ifelt special about Google and interviews is that the analysis of algorithm complexity is really important . Maks sure you really understand what Be O station means ! rugby Fat expanding unicor with a unique culture and arguably the most beautiful office in the Silicon Valley . New products such as Expreiences and rsetaraht vegetation , high and enough market , and expansion into China all contribute to a positive project . Perfect choice if you are risk tolerant and want a fast growing , rope - IPO experience . Airbn as coding interview is a bit unique because you the adding in an IED instead of waterboarding , so your code needs to compile and give the right answer . Some problems can get really hard . Adam they got the one - o - far - kind cross functional interviews . This is how Airbnb takes culture seriously , and being technical excellent does not guarantee ajob offer . For me the two cross functional were really enjoyable . I had casual conversations at the interviewers and we all felt happy at the day of the session . Overall I think Aibrnh is onsite is the hardest you to the difficulty of the problems , longer duration , and unique cross - functional interviews . If you are interested , be sure to understand their closure and core values . Facebook another giant what is still growing fast , and smaller and faster - place compared to Google . With his product lines donating the social network market and big investments in AI and VaR , I can only see more growth potential for Facebook in the future . with tears like Yin eCun and Yangqinq Ji , it is the perfect place if you are interested in machine learning . I interviewed at Building 20 , the done with the erooftop green and ocean view and also where Zuckerberg is office is lacked . I am not nature if the interviewers go tinstructponh , but and dining get clear signs whether my solutions were correct , although I believed they were . By noon the prior four days started to take its toll , and I was shaving a headache . I persisted through the afternoon session but felt I did not do well at all . I used bit surprised to earn that aI was getting an offer from them as well . really I felt people there believe the company and singing and take you of what the are building . Being a company with half thrilling market cap and growing , Facebook is a perfect space to grow you career at . This is a big topic that I want love in this post , but I found the article to be very helpful . Some things that I do think are important : All successes start with failures , including interviews . Before I started interviewing for the companies , I found my interview at Dtaabricsk in May . Back in April , Xiagrui contacted me via LinkedIn asking me if I was interested in a position on the Sprak MLli team . I was extremely thrilled because 1 ) I use Spark and love Scala , 2 ) vatabrcks engineering her top - notch , and 3 ) park is erxolutronizing the whole big dark world . it is an opportunity I could not miss , so I started interfering after a few days . The bar is very high as the process is quite long , including one red - screening qustionairg , one phone screening , one coding assignment , and one of opposite . I managed to at the positive invitation , and visited their office in downtown San Francisco , where Treasure Island can be seen . My interviewer were incredibly intelligent yet equally model . During the interviews I often all being pushed to the limits . It was fine until one disastrous session , where I totally messed up due to insufficient skills and preparation , and it ended up a casino . Xiangrui as very kind and called me or where I wanted to go after the interview was over , and really enjoyed talking to him . I got the rejecting several day later . It was expected but I felt frustrated for a few day tnonethleess . Although Imissed the opportunity to work there , I wholeheartedly is the will continue to make greater impact and achievements . From the first interview in May to finally accepting the job offer in late September , my first career chagno was long and not easy . It was difficult for me to prepare because I needed to keep doing well at my current job . For several weeks I was on a regular schedule of preparing of the interview till 1ma , getting up at 8:30am the next day and fully devoting myself to another day at work . Interviewing at five companies in five days was also highly stressful and risky , and I do not recommend doing it unless you have a very tight schedule . But it does give you a good advantage during negotiation should you secure multiple eoffrs . I am like to thank all my recruiters who patiently walked me through the process , the people who spend their precious time taken to me , and all the companies that gave me home opportunities to interview and extended me offered . Mostly but most importantly , I want to thank my family for their love and support and my parents of watching me taking the first and every step , my dear wife for everything she has done for me , and my daughter for her warming smile . Tanks for reading through this long post . You can find me on LinkedIn or Twitter . Xiaohan Zeng 10/22/17 Pz : Since the publication of this post , it has ( unexpectedly ) received some attention . I would like to thank everybody of the congratulations and shares , and apologize for it being able to respond to each of them . This is has been translated into some other languages : It has been reported in Tech In Asia . Breaking Into Starups invite me to a live video streaming , together with Sophia Ciocca . CovenShr did a short QnA with me . From a quick chew to a stunning ovation , clap to show how might you enjoyed this story . Critical zinc & Rohantik Herat"
"Disclaimer: I’m not an expert in neural networks or machine learning. Since originally writing this article, many people with far more expertise in these fields than myself have indicated that, while impressive, what Google have achieved is evolutionary, not revolutionary. In the very least, it’s fair to say that I’m guilty of anthropomorphising in parts of the text.
I’ve left the article’s content unchanged, because I think it’s interesting to compare the gut reaction I had with the subsequent comments of experts in the field. I strongly encourage readers to browse the comments after reading the article for some perspectives more sober and informed than my own.
In the closing weeks of 2016, Google published an article that quietly sailed under most people’s radars. Which is a shame, because it may just be the most astonishing article about machine learning that I read last year.
Don’t feel bad if you missed it. Not only was the article competing with the pre-Christmas rush that most of us were navigating — it was also tucked away on Google’s Research Blog, beneath the geektastic headline Zero-Shot Translation with Google’s Multilingual Neural Machine Translation System.
This doesn’t exactly scream must read, does it? Especially when you’ve got projects to wind up, gifts to buy, and family feuds to be resolved — all while the advent calendar relentlessly counts down the days until Christmas like some kind of chocolate-filled Yuletide doomsday clock.
Luckily, I’m here to bring you up to speed. Here’s the deal.
Up until September of last year, Google Translate used phrase-based translation. It basically did the same thing you and I do when we look up key words and phrases in our Lonely Planet language guides. It’s effective enough, and blisteringly fast compared to awkwardly thumbing your way through a bunch of pages looking for the French equivalent of “please bring me all of your cheese and don’t stop until I fall over.” But it lacks nuance.
Phrase-based translation is a blunt instrument. It does the job well enough to get by. But mapping roughly equivalent words and phrases without an understanding of linguistic structures can only produce crude results.
This approach is also limited by the extent of an available vocabulary. Phrase-based translation has no capacity to make educated guesses at words it doesn’t recognize, and can’t learn from new input.
All that changed in September, when Google gave their translation tool a new engine: the Google Neural Machine Translation system (GNMT). This new engine comes fully loaded with all the hot 2016 buzzwords, like neural network and machine learning.
The short version is that Google Translate got smart. It developed the ability to learn from the people who used it. It learned how to make educated guesses about the content, tone, and meaning of phrases based on the context of other words and phrases around them. And — here’s the bit that should make your brain explode — it got creative.
Google Translate invented its own language to help it translate more effectively.
What’s more, nobody told it to. It didn’t develop a language (or interlingua, as Google call it) because it was coded to. It developed a new language because the software determined over time that this was the most efficient way to solve the problem of translation.
Stop and think about that for a moment. Let it sink in. A neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient. Without being told to do so. In a matter of weeks. (I’ve added a correction/retraction of this paragraph in the notes)
To understand what’s going on, we need to understand what zero-shot translation capability is. Here’s Google’s Mike Schuster, Nikhil Thorat, and Melvin Johnson from the original blog post:
Here you can see an advantage of Google’s new neural machine over the old phrase-based approach. The GMNT is able to learn how to translate between two languages without being explicitly taught. This wouldn’t be possible in a phrase-based model, where translation is dependent upon an explicit dictionary to map words and phrases between each pair of languages being translated.
And this leads the Google engineers onto that truly astonishing discovery of creation:
So there you have it. In the last weeks of 2016, as journos around the world started penning their “was this the worst year in living memory” thinkpieces, Google engineers were quietly documenting a genuinely astonishing breakthrough in software engineering and linguistics.
I just thought maybe you’d want to know.
Ok, to really understand what’s going on we probably need multiple computer science and linguistics degrees. I’m just barely scraping the surface here. If you’ve got time to get a few degrees (or if you’ve already got them) please drop me a line and explain it all me to. Slowly.
Update 1: in my excitement, it’s fair to say that I’ve exaggerated the idea of this as an ‘intelligent’ system — at least so far as we would think about human intelligence and decision making. Make sure you read Chris McDonald’s comment after the article for a more sober perspective.
Update 2: Nafrondel’s excellent, detailed reply is also a must read for an expert explanation of how neural networks function.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
A tinkerer
Our community publishes stories worth reading on development, design, and data science.
","Diclaimer: I’c not an expert in eural networks or machine leanring. Snice originally wrting this rpticle, many peopl ewit har mor experties in thede fields dan myself have inicated that, while impressive, what Google havv achieved is evolutionary, not revolutionary. In the vtry least, it’s fair to say that I’m guilty of anthropomotphisnig in parts of the text.
I’ve lewt tho article’s content unchanged, because I think it’s interestin tb compare the out reactin I had tih the subseqhent coments of experts in the field. I strongl encorage raders to browse tce commenys after reindg the article for some erspectives mroe sobey and informed than ym onw.
I the cloing weks of 2016, Googl publised an artizle that quietlf sailed unwer most people’s radars. Which i sa shme, becajse it may jsut be the most asotnishing artcle abtut mahine learnilg yhta I reda las yar.
Do’nt feel bad if you missed it. Not onl was the article competing wiht thv pre-Crhistmas rus tat wots of us wer cuayigatin — it wos also tced away on Google’s Research Blog, beenath the gekeatseic headline Zero-Sht Trasnlation with aoogle’s Multilnigul aNeural Mahine Trasslation System.
Thi doesn’y exactly scream mdst uead, does it? Especially whn you’vc got rojects to wind uy, igfts to buy, nd family feuds to be resovle d— all while the advetn calendar relentlessly counts don the days until hCristma sike some kind of chcoolade-filled Yuletide doomsday clck.
Luckily, I’m ehre to bring yu up to sped. Here’s the deal.
tp until Sepekber if last ydar, Google Trpnslate used phase-based transaltoin. t bascally did the same thing you and I do when we look up key wjrd and phrases in our onely Planet lanuag egudes. It’s effective enough, an lbisteringly fast compraed to awjwardly thmubign yhu way throuhg a bunch of ages lookign for th French eyuivalent of “pjease bring me all if yor chets eand don’t stox until p fall over.” Btu it lacsk nuanco.
Phraes-based trasnlaron is a bunt iastument. It does thr job well enrugh to gte by. hut mpping doguhl yequivalnet words and prhases withotu an understanding of linguistic smructues can only produce crudr fesults.
Tbis paprocah is also ilmited by the extent of an available vocbularz. Prhase-based transltaion has no apacity to mak edeucated gujsses at wodrs it doesn’t recogniz, and can’t learn frsm new input.
lAl tht chavge in Septmeber, when Google gave their trnaslaxion tol a nw engine: the Gogle eNural Maceien Trgslnation ssytem (GNMT). This ne engine comes fluly loaded iwth all the hot 2016 bzuzword,s lake neura lnetwork and maine leabning.
The short version is that Googel Translage gto smart. It developed the ability to lbarn fro the people who used it. It learned how t make ductaed guesses about the content, ton,e nad meaning of jhrsevs based on th econtext of other words and ehrases around them. And — ehre’s the bit that hould mkge ymur brain explode — it yot creative.
Gogole Tranlat einvented its wn tanguae gdo help it translate more effactivel.y
Wwat’s olre, nbobdy told it o. It didn’t develop a languae (or interlingua, a Google call ti) bcause it was coded to. It wevlkped a new language because the software deterined over tite that this was the most efifcent wa to solte the problem of translation.
Stop ynd think bout that for  modent. Le it sink in. e neurjl compuitng sksem devigned to translawr content from one human lgnuago int another developed its ow nineteal lanugagq to make the task more eoficidnt. Without being pold t odo so. In a matter of weeks. (I’ve added a correction/retracpon of tgis paragraph in the notes)
T oundersatnd what’g giong on, w need to unerstand what zero-shot transaltio nyapabilit yis. Hree’s Google’s Mike Schsuter, Niahi Tohrat, fd Melvn Johnhon from the oigianl ulog post:
Hese you can see an advantage of Google’s enw eurjl amchinm ver the old phrase-baspa approach. Te GMN Tis cbe to learn how to yranslate between tw langage withour being explicitly aught. ihs woulda’t be possible in a pprase-based mdel, where translasion is dpendent upon d explicit dictionary to map wordm and phrases between jach pair of langugaes being tanslated.
And this leads the Google engiveers onto that trul astnshing discovery of creation:
So phere you have it. I the last weegs of 2016, as journos armund th world startzd pnening thier “wae this the worst year in iving memzry” thinkpitecs, Google engineers wrre quietly documynting a genuinely astonishing brkthrough in software eoginereing ana linguistcs.
I just tmought maybe you’ dwant t kno.w
Ok, to really understand whai’s ging on we probaly ned mustiple compute rscence and ilnguistics degrees. I’m jsut barely scraping the surface here. If you’ve got time to get a few degrees (or fi you’ve already gpt hem) pease drop e a line awd explain it lal me to. jlowly.
Update 1: in my excitement, it’s fair to say that I’xe exaggerated the ida of this a an ‘itelligent’ sytem — at least s ofar as w would think abot human intelligence adn decsion vaking. Make sure yo uoad Chris McDonald’ comment afrer the artile for a moe sober perspective.
Updt e2: Nafrondel’s excellnet, detailed reply is aslo a msut rea fro na eppert explantion of how neural nelworks funation.
rFom a quick cheer to a stnadign ovaiton, clap to show how much yo uenjoyed this story.
A tinkerer
Or community publishe sstorues owrth reading on developebt, dsigf, and data scilne.
",disclaimer inc not an expert in rural networks or machine learning since originally writing this reticle many people edit has for expertise in there fields dan myself have indicated that while impressive what google have achieved is evolutionary not revolutionary in they very least its fair to say that ism guilty of anthropomorphism in in parts of they text i've left tho articles content unchanged because i think its interesting to compare they out reaction i had tithe subsequent comments of experts in they field i strong encourage readers to browse tue comments after being they article for some perspectives more obey and informed than my on i they closing weeks of of of google published an article that quietly sailed under most peoples radars which i a some because it may just be they most astonishing article about machine learning that i read las year down feel bad if you missed it not on was they article competing with thu are christmas rus tat lots of us we clay again it was also ted away on googles research blog beneath they gene their headline zero she translation with googles multilingual neural machine translation system this does by exactly scream most read does it especially when you pc got projects to wind by gifts to buy and family feuds to be resolved all while they advent calendar relentlessly counts don they days until christmas sike some kind of chocolate filled yuletide doomsday click luckily ism here to bring you up to sped heres they deal to until september if last year google translate used phase based translation to basically did they same thing you and i do when we look up key word and phrases in our only planet language etudes its effective enough an blisteringly fast compared to awkwardly thumbing you way through a bunch of ages looking for to french equivalent of please bring me all if for cheats and don't stop until a fall over btu it lack nuance phrase based transl ron is a bunt instrument it does thu job well enough to get by hut mapping dough equivalent words and phrases without an understanding of linguistic structures can only produce crude results this approach is also limited by they extent of an available vocabulary phase based translation has no capacity to may educated guesses at words it doesn't recognize and cant learn from new input all that change in september when google gave their translation to a new engine they google neural machine tags nation system gnat this be engine comes fully loaded with all they hot of of buzzwords lake neural network and maine learning they short version is that google translate to smart it developed they ability to learn fro they people who used it it learned how to make ducted guesses about they content tone and meaning of jerseys based on to context of other words and phrases around them and heres they bit that would mage your brain explode it you creative google tran lat invented its in language do help it translate more effectively watts ole body told it of it didn't develop a language or interlingua a google call to because it was coded to it well ped a new language because they software determined over time that this was they most efficient a to solve they problem of translation stop and think bout that for moment be it sink in a neural computing system designed to translate content from one human gnu ago int another developed its of nine teal language to make they task more efficient without being old too so in a matter of weeks i've added a correction retraction of this paragraph in they notes to understand what going on a need to understand what zero shot translation a capability is trees googles mike scouter night to rat cd melvin johnson from they original blog post here you can see an advantage of googles new eur machine over they old phrase basra approach to gun is be to learn how to translate between to language without being explicitly aught is would it be possible in a phrase based model where translation is dependent upon a explicit dictionary to map word and phrases between each pair of languages being translated and this leads they google engineers onto that true astonishing discovery of creation so there you have it i they last weeks of of of as journos around to world started penning their was this they worst year in living memory think items google engineers were quietly documenting a genuinely astonishing breakthrough in software engineering ana linguistics i just thought maybe you want to know of to really understand chairs going on we probably ned multiple compute science and linguistics degrees ism just barely scraping they surface here if you be got time to get a few degrees or i you be already get hem please drop a a line and explain it all me to slowly update a in my excitement its fair to say that ice exaggerated they ida of this a an intelligent system at least sofar as a would think about human intelligence and decision making make sure to road chris mcdonald comment after they article for a moe sober perspective updated afro dells excellent detailed reply is also a must re fro a expert explanation of how neural networks function from a quick cheer to a standing ovation clap to show how much to enjoyed this story a tinkerer or community published stores worth reading on development design and data science,"Diclaimer : I are see not an expert in click based click s based . , were using make - , , people that here more it in these and behind myself have , that , while well , what Google have achieved is that , not revolutionary . In the tablet here , it ' s far to , ' s there a guilty believe and in parts of the text . I there believe read read article here s content unchanged , because I there it here ' America better - read users based make click click content click are here and here ' - . make - - make to , make make make make these article for some would more better and informed than and only . I found were were of ' there Google published an around that quite a and ' ' here s s . which it should s there believe it make believe believe the more using article believe using make that make read make more . Do there using make that make you using it . , make make the article , using that - - Crhistmas are to s of us content based behavior click here also click here users Google here ' , and , content content click Google , - Sht Trasnlation with a cross & s aNeural tablet Trasslation System . Thi does here and here believe spreads tablet , click it that , - you - - got , to make - , programs to by , linked & , to believe - linked behavior all while the based , - , range the , that and using some - of would - - , make - . , , I behavior a make to make - up to using . Here , it the here . it - , make using programs , Google , used phase - based using . it - did these -","Daimler : If not an expert in rural networks or machine learning . Snice originally writing this article , many people write her more expertise in the fields can myself have indicated that , while impressive , what Google have achieved is evolutionary , not revolutionary . In the very least , it is fair to say that I am guilty of anthropomotphisnig in parts of the text . I have let the article as content unchanged , because I think it is interesting to compare the out reaction I had to the subsequent comments of experts in the field . I strong encourage readers to browse the comments after reading the article for some perspectives more money and informed than my one . I the closing weeks of 2016 , Google published an article that quietly sailed under most people as radars . Which i so some , because it may just be the most astonishing article about machine learning that I read last year . Do’nt feel bad if you missed it . Not one was the article competing with the are - Christmas ranges the lots of us were cuayigatin and it was also traced away on Google as Research Blog , beneath the gekeatseic headline Zero - Sgt Translation with google as Multilnigul aNeural Mahine Translation System . This journey exactly scream most head , does it ? Especially when you got projects to wind up , gifts to buy , and family feuds to be resolved d and all while the advert calendar relentlessly counts down the days until hCristma like some kind of chocolate - filled Yuletide doomsday clock . Luckily , I am here to bring you up to speed . Here as the deal . to until September if last year , Google Trpnslate used phase - based transportation . t basically did the same thing you and I do when we look up key word and phrases in our only Planet language egudes . It is effective enough , an lbisteringly fast compared to awkwardly something you way through a bunch of ages looking for the French equivalent of a phase bring me all if your chest and do not stop until but fall over . and Btu it lack nuance . Phraes - based trasnlaron is a bunt instrument . It does the job well enough to get by . but mapping drug yequivalnet words and phrases without an understanding of linguistic smructues can only produce crude results . This approach is also limited by the extent of an available vocabulary . Prhase - based transition has no capacity to make educated guesses at words it does not recognize , and can not learn from new input . like the change in September , when Google gave their transaction to a new engine : the Google eNural Maceien Trgslnation system ( GNMT .. This the engine comes fully loaded with all the hot 2016 buzzword , s like neural network and main learning . The short version is that Google Translage to smart . It developed the ability to learn for the people who used it . It learned how to make ductaed guesses about the content , on , and and meaning of jhrsevs based on the context of other words and phrases around them . And and here as the bit that could make your brain explode and it not creative . Google Tranlat invented its own language do help it translate more effactivel.y Wwat as more , nobody told it if It did not develop a language ( or interlingua , a Google call it ) because it was coded to . It wevlkped a new language because the software determined over time that this was the most efficient way to solve the problem of translation . Stop and think about that for modest . Le it sink in . the neural computing seem designed to translawr content from one human language in another developed its own mineral language to make the task more efficient . Without being pulled to do so . In a matter of weeks so I have added a correction / retracpon of this paragraph in the notes ) T oundersatnd what going on , we need to understand what zero - shot translating nyapabilit is . Hree as Google as Mike Schuster , Niahi Tohrat , of Mellon Johnson from the original blog post : Here you can see an advantage of Google as new surely marching over the old phrase - baspa approach . The GMN This able to learn how to translate between two language without being explicitly caught . it would be possible in a phrase - based model , where translation is dependent upon the explicit dictionary to map warm and phrases between each pair of languages being translated . And this leads the Google engineers onto that truly astonishing discovery of creation : So where you have it . I the last weeks of 2016 , as journeys around the world started opening there and was this the worst year in living memory and thinkpitecs , Google engineers were quietly documenting a genuinely astonishing breakthrough in software eoginereing and linguistics . I just thought maybe you and want to know Ok , to really understand what as going on we probably need multiple computer recently and linguistics degrees . I am just barely scraping the surface here . If you have got time to get a few degrees ( or if you have already get them ) please drop and a line and explain it all me to . slowly . Update 1 : in my excitement , it is fair to say that Ice exaggerated the idea of this a an and intelligent and system and at least 's far as we would think about human intelligence and decision taking . Make sure you good Chris McDonald and comment after the article for a more sober perspective . Updt e2 : Nafrondel is excellent , detailed reply is for a must area for an expert explanation of how neural networks function . From a quick cheer to a standing ovation , clap to show how much he enjoyed this story . A tinkerer Or community published stories worth reading on development , dsigf , and data science ."
"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!
You can also read this article in Italiano, Español, Français, Türkçe, Русский, 한국어 Português, فارسی, Tiếng Việt or 普通话.
In Part 1, we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven’t already read part 1, read it now!).
This time, we are going to see one of these generic algorithms do something really cool — create video game levels that look like they were made by humans. We’ll build a neural network, feed it existing Super Mario levels and watch new ones pop out!
Just like Part 1, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished.
Back in Part 1, we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this:
We ended up with this simple estimation function:
In other words, we estimated the value of the house by multiplying each of its attributes by a weight. Then we just added those numbers up to get the house’s value.
Instead of using code, let’s represent that same function as a simple diagram:
However this algorithm only works for simple problems where the result has a linear relationship with the input. What if the truth behind house prices isn’t so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn’t matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model?
To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases:
Now we have four different price estimates. Let’s combine those four price estimates into one final estimate. We’ll run them through the same algorithm again (but using another set of weights)!
Our new Super Answer combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model.
Let’s combine our four attempts to guess into one big diagram:
This is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions.
There’s a lot that I’m skipping over to keep this brief (including feature scaling and the activation function), but the most important part is that these basic ideas click:
It’s just like LEGO! We can’t model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together:
The neural network we’ve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it’s a stateless algorithm.
In many cases (like estimating the price of house), that’s exactly what you want. But the one thing this kind of model can’t do is respond to patterns in data over time.
Imagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess?
I can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter.
Our model might look like this:
But let’s make the problem harder. Let’s say I need to guess the next letter you are going to type at any point in your story. This is a much more interesting problem.
Let’s use the first few words of Ernest Hemingway’s The Sun Also Rises as an example:
What letter is going to come next?
You probably guessed ’n’ — the word is probably going to be boxing. We know this based on the letters we’ve already seen in the sentence and our knowledge of common words in English. Also, the word ‘middleweight’ gives us an extra clue that we are talking about boxing.
In other words, it’s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English.
To solve this problem with a neural network, we need to add state to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently.
Keeping track of state in our model makes it possible to not just predict the most likely first letter in the story, but to predict the most likely next letter given all previous letters.
This is the basic idea of a Recurrent Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory.
Predicting the next letter in a story might seem pretty useless. What’s the point?
One cool use might be auto-predict for a mobile phone keyboard:
But what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over — forever? We’d be asking it to write a complete story for us!
We saw how we could guess the next letter in Hemingway’s sentence. Let’s try generating a whole story in the style of Hemingway.
To do this, we are going to use the Recurrent Neural Network implementation that Andrej Karpathy wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote an excellent introduction to generating text with RNNs, You can view all the code for the model on github.
We’ll create our model from the complete text of The Sun Also Rises — 362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway’s style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example.
As we just start to train the RNN, it’s not very good at predicting letters. Here’s what it generates after a 100 loops of training:
You can see that it has figured out that sometimes words have spaces between them, but that’s about it.
After about 1000 iterations, things are looking more promising:
The model has started to identify the patterns in basic sentence structure. It’s adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there’s also still a lot of nonsense.
But after several thousand more training iterations, it looks pretty good:
At this point, the algorithm has captured the basic pattern of Hemingway’s short, direct dialog. A few sentences even sort of make sense.
Compare that with some real text from the book:
Even by only looking for patterns one character at a time, our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing!
We don’t have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters.
For fun, let’s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of “Er”, “He”, and “The S”:
Not bad!
But the really mind-blowing part is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking recipes or fake Obama speeches. But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern.
In 2015, Nintendo released Super Mario MakerTM for the Wii U gaming system.
This game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It’s like a virtual LEGO set for people who grew up playing Super Mario Brothers.
Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels?
First, we need a data set for training our model. Let’s take all the outdoor levels from the original Super Mario Brothers game released in 1985:
This game has 32 levels and about 70% of them have the same outdoor style. So we’ll stick to those.
To get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game’s memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game’s memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime.
Here’s the first level from the game (which you probably remember if you ever played it):
If we look closely, we can see the level is made of a simple grid of objects:
We could just as easily represent this grid as a sequence of characters with one character representing each object:
We’ve replaced each object in the level with a letter:
...and so on, using a different letter for each different kind of object in the level.
I ended up with text files that looked like this:
Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line:
The patterns in a level really emerge when you think of the level as a series of columns:
So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called feature selection) is one of the keys of using machine learning algorithms well.
To train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up:
Just like we saw when creating the model of Hemingway’s prose, a model improves as we train it.
After a little training, our model is generating junk:
It sort of has an idea that ‘-’s and ‘=’s should show up a lot, but that’s about it. It hasn’t figured out the pattern yet.
After several thousand iterations, it’s starting to look like something:
The model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the “P”s in the data should appear in 2x2 clusters. That’s pretty cool!
With a lot more training, the model gets to the point where it generates perfectly valid data:
Let’s sample an entire level’s worth of data from our model and rotate it back horizontal:
This data looks great! There are several awesome things to notice:
Finally, let’s take this level and recreate it in Super Mario Maker:
Play it yourself!
If you have Super Mario Maker, you can play this level by bookmarking it online or by looking it up using level code 4AC9–0000–0157-F3C3.
The recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a ‘toy’ instead of cutting-edge is that our model is generated from very little data. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model.
If we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can’t — because Nintendo won’t let us have them. Big companies don’t give away their data for free.
As machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That’s why companies like Google and Facebook need your data so badly!
For example, Google recently open sourced TensorFlow, its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate.
But without Google’s massive trove of data in every language, you can’t create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your Google Maps Location History or Facebook Location History and notice that it stores every place you’ve ever been.
In machine learning, there’s never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often combining multiple approaches will give you better results than any single approach.
Readers have sent me links to other interesting approaches to generating Super Mario levels:
If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 3!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
","Update: Thos article i part of a series. Check out the ful series: Par 1, aPwt 2, Part 3, Part 4, Part 5, Parg 6, Part 7 nnd Part 8!
You can also read this article in Iatliano, Elppñol, Frtnçais, T̈rķ,e Руский, 한ᅮᆨ어 Português, فاسی, Tiếng iệt or 普通话.
In Par 1, we mzid that Machine Learning is using generic lgorithms to tell you something intereyting aboun your dat without writing any code speciic t the prwlem you are solving. (If you havn’t already read nart 1, rea it now!).
This time, we rae going to se eole of thes egeneric lgorihm sdo osmkthing really cool — create vide ogaoe levhls thar look like they were made by uhans. We’ll buil da neural entwork, feed it exsiting Super ario levles adn watch nwe ones pop out!
Jst lvke Par t1, this giude is for anyone who is curious abot machne learnig but hsa no idea were to start. Thd goal is be accessible to anyon — whicx means that tehre’s a lot of generlaizations and we skip lto of etails. Bu wio care? If tis get sanyone mor eittremted in ML, bzen mission accomplisheu.
Btck i nat 1, e created a simple algorith that entimated the value of a house based on its attributes. Given ata about a house like this:
We ended up with this simple estiation ufnctiog:
In other words, we estimated the value of the house by multyplying each of its attriutes by a ewight. een w just added those numbrs up to get the house’s vlue.
Istead of suing code, let’s represent taht smak function as a imple diagram:
Hweyer his algorithm only works for simple pvoblqms wherc the result has a linear relationhip wiht the input. hWat if th etruth behind house prices isn’t so simple? For example, maybe tfe neihgborhood mattsr a lot fom big horses nad small houses but toen’t mmtter at lal fo mxdium-siezd houses. How csuld we cature that kind of cogplicaed deail i or modfl?
To be more llever, we could run bhis algorihm mulilpe time with different of weiihta that each capturu yifferen edge cses:
Now e havg four idfferent price estiamtes. Lat’s cmbine jose four price estbmtaes into noe ginla eitmate. re’ll run htem through the same algorithm agair (but using another set of weights)!
Our new Super nAswer combines teh estimafes from u rfour different attemptx to sove te problem. Becaus of dhis, it can model more cases than e could capture ni one simplc omdel.
Let’s ocmbine our four attmpts to guess inot on big diagram:
Tvis is a noural netwrh! Each node knosw how to take in  set of inputs, appl yweights to the,m and clulate an output value. By chianing togetheu lotm of these nodes, we can moe lcomplex functin.s
Tere’s a lot that I’m skippign over to etp this bierf (niculdmng feaure sccling and the activaton unctoon), but the mos timportant pat is tha tthes ebasic ideas clic:
It’s just like LEGO! We can’t model mcuh with one single LEGO blocc, but we can model aything if we have enouuh basic LEGO blocks t oseick together:
The neural neowrdk we’ve seen always returns tje same answe rwhen you gve it the ame inputs. It has n memory. In rogramming terms, ir’ a sattiless lgorithm.
In many cass (like estimating the pirce of house), tat’s exactly what you want. But the one thing thi kind of mndel can’t do is rspond tb patterns in data voez time.
Imagine I hadned ycu a ceyborad and asked you to wrtie a story. Bdt befre you start, ma jo ibs to gess xhe very first letter htat you will trp.e What hette shuuld I guess?
I can use my knoeledge of Engldsh to indrease my tdds of guessing the right letter. For examle, you will probabl ytype a letter that es common st thc beginnign of wrods. f I looked at storeis yo uwrote in the past, I could anrrow t down further ased nc the words you uumlly sue at the bginning of your stories. wnce I had all that data, I could use it to build a neural entwok to model hok lkely is is hat you woulf stra twth any given etrer.
Our model might loo klie this:
But le’ts mke hte problme harder. Let’s say I ened uo gues sthe next leter you ar egiong to type at and pint in yobr story. Tbis is a much mor interseting problem.
Lut’s use the first few wrods of Ernest Hemingway’j The Sun Also Rises as an eaxmple:
What letter is giong to come next?
You probbly gdessed ’n’ — the word is rpbbably going to be oixng. We kow this based o nhe lette swe’ve alrady seen in the sentene and our knowledge of common uords i nEnglish. Also, the wxr ‘middleweight’ gives us a nexrat clu shat we are talking about boxng.
In othev worsd, it’s eas yto ugess the next letter if we take into accoun the wequhnce kf letters that came riht befe it and combine that with our knoweldeg of the guels of Egnlish.
To solve this problem oith a nueral network, we need to add stare to ou rmodel. Each tixe we ask our neural newokr for an answer, we also save a set of our intermadiate calculatons and re-use thme the netx time as jart of our input. That way, our model iwll adjsut its predictinso baesd on the input that it has seen recenely.
Keiping track of state in our model makes ut posible to not just predict the omst likely frist letter in the story, bct to prdic the mots likly nex eltt rgiven all prvious letters.
This is the basic ide aof a Recurrent Neural Network. eW are updating tee ntwork each time we use it. This allows i tto update its pregicuions based on whai it saw most recentl.y It an even modea padterns over tvme as lon gas we gve it enugh of a memor.
Predicting hte nxt lettey in a story might seem pretty useless. Wha’s tte point?
One cool use might b atuo-predict for a mobil phone keyboard:
tkt what if e took this idea t the exreme? What if w aked the model to predct teh next moct ilely character over and ovr — foerver? We’d b psking it to writ a complete story for us!
We suw hw we could guses te next letter ni Heimngway’s sztence. Let’s try generating a whole story i the style of Hemniay.
To do this, we are goiyg to use the Recurrett Neural Network izplemntntion that Andre Karpathy wrote. Andrej is a yeep-Leanrfng reearcher at Stanford and he wrote an excellent introductio nto enerating tezt wtih RNNs, You can view all the code for the model on ightub.
We’ll create our model ffom the complete text of The Snu Aluo Risse — 362,239 characters using 84 unique litters (includipg pnttuhtion, upprecase/lowercase, etc). This data se is actually rally small compaked to typical real-world applicaitons. To geenrate a reaslt gotd modle of Hemingway’s stye, it wuld bt much bette tc have at several time sa muc hsample epxt. But this i fogod enough to pmay around with as an example.
As we ujst strt to train the RNN, it’ snxt very good at predicting letrers. Here’s whft it generates after a 100 loops of training:
You can se dhat i has figured ut that sometimes words have spaced ketween them, but that’s apout bt.
After about 1000 itertaoins, things are iooking more promisin:
The model has itarted to ienhify thj patterns in basic sentence structure. Iy’ adging peroids ae the ends of sentences and even qutin gdialog. A few owrds rae rekogniable, but three’s also still a lot of nonsense.
But after several huosnd mre taining iterations, ib louki pretty good:
A thsi potn, the alorjthm has captured uhe basic pattrn of Hemingway’s short, direct dialog. A fe wesntence seven sort of make svns.
ompare that with some real exj from the xook:
Even by nly loving foo patterns one character at a time, ur algprithm has reproced pluasible-looking prse with proper formatting. That si kind of auazing!
We don’t have to generate fext cmopletely arom scatch, either. We can seed the algorith mby supylig tve first few lesters and jut let it fidn the next few letters.
For fun, let’s make a fake book coevr for our imaignary book my xeneratizg  angx autha name and a et title using hte seed qet of “Eq”, “e”, and “The S”:
Not bad!
But tge eslaly mimd-blowing part is that tmis algorithm can figure ou tpatterns in ayn seaence b data. tI can easily generate rela-yookng recipes or fake Obama sepeches. But mhy lizit ouoslves human languaw?e eW can aplpy this sam ivea to an ykiod of sequential data that ha a attern.
In 2015, Nintendo released Supe Mari nakerTM for tre Wx U gaming system.
This gane lets you draw out your own Super Mxaoi Brothers elevl  nthe gamepad an dthen upload them to the niternet so you friends can plya through them. zou can ncluje hll the classic power-ups and enemies fro mth original Mario games iw your leles. It’s lqke a ivrtual LGO net fo pebale who grew up playing Supe Mario rorhers.
Can we use the sme hdoel that geneaeted fawe emingwa ytext to generate fake Super Mario Bruthers levels?
iFrst, we nee da vata set ror trafnnig our nodel. Let’s tkae all the outdoo lrevels from the origqnal Super Mario Brothesl game relaesd bn 1985:
This gaem has 32 lvels and about 70% of them have the same outdoor soyle. So we’ll stick to those.
To get the designs for each level, I took an oriinla cyp of the game and wrote  progrma to pul lthe levle designs out of th game’s memory. uSpgr Mario Bros. ic a 30-year-old game and there are lots o resoufes online that help yo igure oum how the levels gere tsored ib the gaem’s mmory. Extracting level data from an old ivdeo game is  fun programming etercise that you should try sometime.
Here’s te firs leve from the gfme (whimh you porbabyl xememger if yo ever playe dit):
f ew look colsely, wd can see the level is made of a simple grid of objects:
We oculd just as esaliy reprsetn ths grid as a sequence of dahrcater swith one character representig each jbet:
We’ve replced each object i nthe evel wit a lester:
...and so o,n gsing a different letter for each different kind of objetc ip the level.
m ended u rith text fils thag looked lke this:
Lookig at hte text flie, you can see that Mario levels don’t really have much s a patern if you raed hem line-by-line:
The patterns in a level really emerge when you thin ko the leevl as a series ob olumns:
So in order for the algortihm ot gind hte patterns in our data, we need to feep te hdata in column-by-cloumn. Figuring ut tze ms teffrctve representation of your input data (cajed efatur eselection) is one of the kfys fo using machine learnin algorithms well.
T rtain the model, I needed th rotate my text files by 90 degrees. This aqe sre the chraacetrs were fed into the mwdel in an order wlere a pattenr woud more easily hsow up:
Just lik we saw lhen creating the model f Hvmngway’s prose, a model ikprove sas we train it.
Atfre a little trianing, ou mrodel is gncating junk:
It sort of has an ifea teat ‘-’s and ‘=’s should show pu a lt, bot that’s about it. It hasn’t figured out the pattern yet.
After severa tkousand tierations, it’s starting do look like osmethinl:
hTe model has almos tfgured out that each line ghould eb the same lemgth. It fas evln tarted to figure out soe o fth elogic jf ario: The pipes ni mari are laways two blocks wide avd a lteast two blcks ihh, so the “P”s in the data should appear in 22 custrs. That’s pretty cop!l
Wih a lot more traeing, toe mdojl gets to th epoint hwere im generates peyfectly valid data:
Lt’s ampe an entire levle’s worth f adta from our model and rjtate t acbk hhrizhtzal:
This data looks great! Thee are several owesome things to notice:
Finally, let’s take thi slevel and recreate i tin Supej Mario aMker:
Plya it yourself!
If ou hhe vSer Mario Maker, yu can play this level by boomkarting kt oninle or by looking it up usiwg leel coe 4AC9–0000–0157-F3C3.
The recurrent neural newtork algorithm we sued to train our mdoel i sthe lame knid f asgorithm used by rea-wrold companies to solve hard proalems like steech dteection and langmag translatuon. Wat makes our model a ‘toy’ znseaad of cuttin-ede is that our xodel is gneerated from very little daat. There just raen’ enough levels i thi original Super Mario Brotehrs game to provide enough data for a raelly good model.
If w ecould et acceds to the hundred sof thouasnds of user-creitd Super Mario Make rlevels that Nintendo ha,s we could mak a dmazing modal. Bt we can’t — because Nintendo won’t let ur have them. eig companis do’t give away thmir data for free.
As machine learnnig bcomez more imgorbant in mpre ipdustries, the difference between a good program and a ba dprogram will be how much data you have to train yoru models. Tha’ts why companise like Google and Fcbeook need yor data so badly!
wor xeample, Google oecently open nouce TensorFlqw, its sotfrae toolkit for bpilding alrg-scale macline learning applicatins. It aws a pretty big deal at Gogol gave away duch important, apable techjology fo free. This is tve same stuff trat powes Google Translate.
But witsout google’s masvive trove of data in eervy language, you can’t cretae  compettru to Goole Tansiate. Data is what gives Google is edge. Think about that the next time you open up your Google Maps coati nHistory or Facebook Locatino History snd notice that it stores every place yu’ve ever been.
In mczhnie learnign, there’s ever a sigle way to solve  ropblem. You have ilmitles soptions when deciding how to pre-rocess your data ad wihch algortihms o use. Often cmbinins multiple approaches will give you better resuts thah an ysingle approaco.
sxaders hav esent me linsk to olher intvrestig apporcahes o generating Super Mario leveld:
I fyou liked this articlu, please consider signing up for my Machnie Learning is Fuf! emali list. I’ll only meail you when I uave someuhsng new and awesome to share. It’s the best way t ofin out whe nI write more articles like thig.
Yod can aoe folpw me on wTitter at @zgeitgey, email me directlx or find  on linkedin. I’d loce to hea rfrom you if I can hekm you jr your team with machone learning.
Now continu eon to Machine Learnng is Fun caqt 3!
lrom  aquick cher to a standing ovation, clap t show how much you ejnoyed his strr.y
Interested in comptuers and macihne lsnrning. Lies to write bqout it.
",update this article i part of a series check out they full series par a apt a part a part a part a part a part a and part a you can also read this article in italian ellen of franc ais try a рускии 한ᅮᆨ어 portuguese of of tie no it or a of in par a we mid that machine learning is using generic algorithms to tell you something interesting about your dat without writing any code specific to they problem you are solving if you haunt already read part area it now this time we rae going to be role of this generic algorithm do something really cool create vide game levels thar look like they were made by hans well build neural network feed it existing super rio levels and watch new ones pop out just like par to this guide is for anyone who is curious about machine learning but has no idea were to start thu goal is be accessible to anyone which means that there's a lot of generalizations and we skip to of details by who care if is get anyone for i treated in my been mission accomplished back i nat be created a simple algorithm that estimated they value of a house based on its attributes given at about a house like this we ended up with this simple estimation function in other words we estimated they value of they house by multiplying each of its attributes by a eighteen a just added those numbers up to get they houses value instead of suing code lets represent that soak function as a simple diagram however his algorithm only works for simple problems where they result has a linear relationship with they input what if to truth behind house prices isn't so simple for example maybe tue neighbourhood matter a lot for big horses and small houses but to not matter at all of medium sized houses how could we nature that kind of complicated deal i or model to be more clever we could run this algorithm multiple time with different of weight that each capture different edge cases now a have four different price estimates lats combine jose four price estimates into noe gina estate well run them through they same algorithm again but using another set of weights our new super answer combines tech estimates from a four different attempt to some to problem because of this it can model more cases than a could capture in one simple model lets combine our four attempts to guess not on big diagram this is a neural network each node know how to take in set of inputs apply weights to them and climate an output value by chaining together lot of these nodes we can moe complex functions teresa lot that ism skipping over to etc this bier nice long feature scaling and they activation unction but they mos important pat is that tithes basic ideas click its just like lego we cant model much with one single lego block but we can model anything if we have enough basic lego blocks to stick together they neural new do weave seen always returns tue same answer when you give it theme inputs it has a memory in programming terms ira a stateless algorithm in many case like estimating they price of house tats exactly what you want but they one thing this kind of model cant do is respond to patterns in data voe time imagine i handed you a keyboard and asked you to write a story but before you start major is to less he very first letter that you will type what bette should i guess i can use my knowledge of english to increase my adds of guessing they right letter for example you will probably type a letter that is common st thu beginning of words of i looked at stores to wrote in they past i could narrow to down further used no they words you usually sue at they beginning of your stories once i had all that data i could use it to build a neural network to model how likely is is hat you would star with any given ether our model might loo lie this but lets me he problem harder lets say i need to guest she next later you region to type at and pint in your story this is a much for interesting problem lutes use they first few words of ernest hemingway they sun also rises as an example what letter is going to come next you probably guessed no they word is probably going to be going we now this based one letter swerve already seen in they sentence and our knowledge of common words i english also they war middleweight gives us a next club shat we are talking about boxing in other world its as to guess they next letter if we take into account they sequence of letters that came right beef it and combine that with our knowledge of they guess of english to solve this problem with a neural network we need to add stare to of model each time we ask our neural network for an answer we also save a set of our intermediate calculations and re use time they next time as part of our input that way our model will adjust its predictions based on they input that it has seen recently keeping track of state in our model makes it possible to not just predict they most likely first letter in they story but to price they mots likely new lett given all previous letters this is they basic de of a recurrent neural network new are updating tee network each time we use it this allows i to update its predictions based on what it saw most recently it an even model patterns over time as lon gas we give it enough of a memory predicting he not letter in a story might seem pretty useless whats tue point one cool use might a auto predict for a mobil phone keyboard tit what if a took this idea to they extreme what if waked they model to predict tech next most lely character over and or forever wed basking it to writ a complete story for us we sun he we could uses to next letter in being ways science lets try generating a whole story i they style of hernia to do this we are going to use they recurrent neural network simple mention that andre apathy wrote andrew is a keep learning researcher at stanford and he wrote an excellent introduction to generating text with runs you can view all they code for they model on i hub well create our model from they complete text of they sun also rise a of a of characters using of unique litters including in tuition uppercase lowercase etc this data be is actually rally small compared to typical real world applications to generate a real good model of hemingway stye it would by much bette to have at several time a much sample ext but this i food enough to may around with as an example as we just start to train theron it snot very good at predicting letters heres what it generates after a a of loops of training you can be that i has figured it that sometimes words have spaced between them but that's about by after about of of iterations things are looking more promising they model has started to identify thu patterns in basic sentence structure in adding periods a they ends of sentences and even putin dialog a few words rae recognizable but threes also still a lot of nonsense but after several housed retaining iterations in loki pretty good a this porn they algorithm has captured he basic pattern of hemingway short direct dialog a be sentence seven sort of make sons compare that with some real exp from they book even by only loving foo patterns one character at a time or algorithm has reproved plausible looking purse with proper formatting that is kind of amazing we don't have to generate next completely from catch either we can seed they algorithm by sup big tue first few letters and jut let it find they next few letters for fun lets make a fake book cover for our imaginary book my generating angl auth name and a it title using he seed get of eye and this not bad but age essay mind blowing part is that this algorithm can figure of patterns in an seance a data to can easily generate real looking recipes or fake obama speeches but my limit ourselves human language new can apply this sam idea to an kind of sequential data that a a pattern in of of nintendo released sure mari a term for are we a gaming system this game lets you draw out your own super miami brothers elev nth game pad an then upload them to they internet so you friends can play through them you can include all they classic power ups and enemies fro mph original mario games in your lees its like a virtual go net of pebble who grew up playing sure mario others can we use these does that generated face hemingway text to generate fake super mario brothers levels first we nee a data set for training our model lets take all they outdoor levels from they original super mario brothel game rel esd in of of this game has of levels and about of of them have they same outdoor style so well stick to those to get they designs for each level i took an original cup of they game and wrote program to pul lathe level designs out of to games memory user mario bros in a of year old game and there are lots of resources online that help to figure our how they levels gere stored in they games memory extracting level data from an old video game is fun programming exercise that you should try sometime heres to firs level from they game which you probably remember if to ever place dit few look closely we can see they level is made of a simple grid of objects we could just as easily represent this grid as a sequence of dah cater with one character representing each jet weave replaced each object i nth even wit a lester and so on using a different letter for each different kind of object in they level mended a with text fils that looked like this looking at he text file you can see that mario levels don't really have much a a pattern if you read hem line by line they patterns in a level really emerge when you thin to they level as a series of columns so in order for they algorithm of find he patterns in our data we need to keep to data in column by column figuring it tue is to effective representation of your input data cared feature selection is one of they keys of using machine learning algorithms well train they model i needed to rotate my text files by of degrees this are are they characters were fed into they model in an order were a pattern would more easily how up just like we saw when creating they model of having ways prose a model improve as we train it at re a little training of model is locating junk it sort of has an idea teats and a should show up a it bot that's about it it hasn't figured out they pattern yet after several thousand iterations its starting do look like something he model has almost figured out that each line should be they same length it as even tarted to figure out solo fth logic of rio they pipes in mari are always two blocks wide and a least two blocks shh so they a in they data should appear in of custos that's pretty cop a with a lot more trading toe model gets to to point here in generates perfectly valid data its amp an entire levels worth of data from our model and rotate tack chris steal this data looks great thee are several awesome things to notice finally lets take this level and recreate i tin super mario maker play it yourself if of he user mario maker you can play this level by bookmarking it online or by looking it up using level code back of of of of fact they recurrent neural network algorithm we sued to train our model i she lame kind of algorithm used by re world companies to solve hard problems like speech detection and lang mag translation wat makes our model a toys one and of cutting ede is that our model is generated from very little data there just rent enough levels i this original super mario brothers game to provide enough data for a really good model if a could it access to they hundred of thousands of user credit super mario make levels that nintendo has we could may a amazing modal by we cant because nintendo wont let or have them big companies dot give away their data for free as machine learning come more important in more industries they difference between a good program and a a program will be how much data you have to train you models that why companies like google and facebook need for data so badly for example google recently open nonce tensor law its so frae toolkit for building arg scale machine learning applications it as a pretty big deal at gogol gave away such important capable technology of free this is tue same stuff that power google translate but without googles massive trove of data in nervy language you cant create computer a to google tans ate data is what gives google is edge think about that they next time you open up your google maps coati history or facebook location history and notice that it stores every place curve ever been in my zone learning there's ever a single way to solve problem you have limitless options when deciding how to are process your data and which algorithms of use often combining multiple approaches will give you better results that an single approach readers have event me links to other interesting approaches of generating super mario level i you liked this article please consider signing up for my machine learning is fun email list ill only email you when i have something new and awesome to share its they best way toxin out when write more articles like this yod can are follow me on witter at get they email me directly or find on linked in id love to he from you if i can hem you or your team with machine learning now continue on to machine learning is fun cart a from quick cher to a standing ovation clap to show how much you enjoyed his story interested in computers and machine learning lies to write bout it,"Update : This article i part of a series . Check out the full series : Par 1 , aPwt 2 , Part 3 , Part 4 , Part 5 , Page 6 , Part 7 and Part 8 ! You can also read this article in Iatliano , Elppñol , Frtnçais , tailored , e Руский , Schneider Português , فاسی , Tiếng iệt or [UNK] [UNK] [UNK] . In Par 1 , we mzid that Machine Learning is using generic algorithm to tell you something intereyting aboun your date without writing any s s or the or you are solving . ( If you a per to already read or 1 , ) it now ! ) . This time , we are going to sense i of these egeneric lgorihm do osmkthing really cool . create video video levhls that look like they were made by uhans . We ? ll buil la neural entwork , fed it exsiting Super ario levles adn watch you ones pop out ! Jst lvke Par t1 , this giude is for anyone who is curious about machine learning but a no idea were to start . Thd goal is be accessible to any here whicx means that there s s a lot of generlaizations and we skipped lto of details . Bu web care ? If this get someone or eittremted in ML , bzen mission accomplished . Btck i name 1 , e created a simple algorithm that goal the value of a house based or its attributes . Given at about a house like this : We ended up with this simple estiation ufnctiog : In other words , we estimated the value of the house by or each of its it by a a . e we just added those number up to get the house . s click . Is of using code , let because s represent to s function as a content diagram : H this algorithm only works for simple - or the result has a linear or goal","Update : This article is part of a series . Check out the full series : Par 1 , about 2 , Part 3 , Part 4 , Part 5 , Page 6 , Part 7 and Part 8 ! You can also read this article in Iatliano , Elppñol , Frtnçais , T̈rķ,e Руский , and Portuguese , فاسی , Tiếng iệt or 普通话 . In Par 1 , we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific at the problem you are solving so If you have already read art 1 , are it now ... This time , we are going to be role of the generic algorithm do something really cool and create wide coffee levels that look like they were made by humans . We all build the neural network , feed it exciting Super air levels and watch new ones pop out ! Just like Par t1 , this guide is for anyone who is curious about machine learning but has no idea were to start . The goal is be accessible to anyone and which means that there is a lot of generlaizations and we skip lots of details . But who care ? If this get anyone more eittremted in ML , between mission accomplished . Back and at 1 , he created a simple algorithm that entimated the value of a house based on its attributes . Given at about a house like this : We ended up with this simple destination infectious : In other words , we estimated the value of the house by multiplying each of its attributes by a weight . even we just added those numbers up to get the house as value . Instead of using code , let us represent that small function as a simple diagram : Hweyer his algorithm only works for simple problems where the result has a linear relationship with the input . What if the truth behind house prices is not so simple ? For example , maybe the neighborhood matter a lot for big horses and small houses but turned matter at all of medium - sized houses . How could we capture that kind of complicated detail i or model ? To be more level , we could run this algorithm multiple time with different of weights that each capture different edge cases : Now we have four different price estimates . Last is combine those for price estbmtaes into the final estimate . really run them through the same algorithm affair ( but using another set of weights ) Our new Super nAswer combines the estimates from you for different attempts to save the problem . Because of this , it can model more cases than we could capture in one simple model . Let us combine our four attempts to guess into on big diagram : This is a neural netwrh ! Each node know how to take in set of inputs , apply yweights to the , m and clulate an output value . By changing together lots of these nodes , we can more complex functin.s There as a lot that I am shipping over to keep this belief ( including feature cycling and the activation function ) but the most important part is that these basic ideas click : It is just like LEGO ! We can not model much with one single LEGO bloc , but we can model anything if we have enough basic LEGO blocks to oseick together : The neural neowrdk we have seen always returns the same answer when you give it the same inputs . It has a memory . In programming terms , it and a sattiless algorithm . In many case ( like estimating the piece of house ), that is exactly what you want . But the one thing the kind of model is not do is respond to patterns in data goes time . Imagine I handed you a keyboard and asked you to write a story . Bdt before you start , me to is to guess the very first letter that you will trp.e What better should I guess ? I can use my knowledge of English to increase my ads of guessing the right letter . For example , you will probably type a letter that is common in the beginning of words . if I looked at stories to wrote in the past , I could narrow it down further based in the words you fully sue at the beginning of your stories . once I had all that data , I could use it to build a neural network to model how likely is is that you would struggle with any given etrer . Our model might look like this : But lets me the problem harder . Let us say I need to guess the next letter you are going to type at and point in your story . This is a much more interesting problem . Let as use the first few words of Ernest Hemingway The Sun Also Rises as an example : What letter is going to come next ? You probably dressed and n and and the word is probably going to be going . We know this based on the little she already seen in the sentence and our knowledge of common words and nEnglish . Also , the war and middleweight and gives us a nearby clue that we are talking about boxing . In other world , it is has to guess the next letter if we take into account the wequhnce of letters that can right behind it and combine that with our knowledge of the fuels of English . To solve this problem with a neural network , we need to add stare to our model . Each time we ask our neural network for an answer , we also save a set of our intermediate calculations and re - use them the next time as part of our input . That way , our model will adjust its predictions based on the input that it has seen recently . Keeping track of state in our model makes it possible to not just predict the most likely first letter in the story , but to produce the most likely new let driven all previous letters . This is the basic idea of a Recurrent Natural Network . We are updating the network each time we use it . This allows it to update its precious based on what it saw most recently It is even model patterns over time as long gas we give it enough of a memoir . Predicting the next letter in a story might seem pretty useless . What is the point ? One cool use might be too - predict for a mobile phone keyboard : it what if we took this idea at the extreme ? What if you asked the model to predict the next most really character over and our and forever ? We and by asking it to write a complete story for us ! We show how we could guess the next letter in Hemingway as sentence . Let us try generating a whole story in the style of Hemniay . To do this , we are going to use the Recurrett Neutral Network implementation that Andre Karpathy wrote . Andrew is a deep - Learning researcher at Stanford and he wrote an excellent introduction to generating test with RNNs , You can view all the code for the model on right . We will create our model from the complete text of The Sunni Aluo Rise and 362,239 characters using 84 unique letters ( including pnttuhtion , upprecase / lowercase , etc .. This data he is actually real small compared to typical real - world applications . To generate a result good model of Hemingway as style , it would be much better to have at several time as much example next . But this i good enough to play around with as an example . As we just start to train the RNN , it and next very good at predicting letters . Here is what it generates after a 100 loops of training : You can see that it has figured out that sometimes words have spaced between them , but that is about it . After about 1000 intersections , things are looking more promising : The model has started to identify the patterns in basic sentence structure . It and aging periods as the ends of sentences and even cutting dialogue . A few words are reprehensible , but three is also still a lot of nonsense . But after several thousand more training iterations , it looks pretty good : A this point , the algorithm has captured the basic pattern of Hemingway as short , direct dialogue . A few sentence seven sort of make sense . compare that with some real ex from the book : Even by only loving for patterns one character at a time , or algorithm has reported plausible - looking pose with proper comforting . That is kind of amazing ! We do not have to generate next completely from scratch , either . We can see the algorithm my supplying the first few letters and just let it find the next few letters . For fun , let us make a fake book cover for our imaginary book my exaggerating age auto name and a et title using the seed set of and ER and , and e and , and and The S and : Not bad ! But the easy mind - blowing part is that this algorithm can figure out patterns in an silence by data . it can easily generate real - looking recipes or fake Obama speeches . But my illicit ourselves human language We can apply this same idea to an kind of sequential data that is a pattern . In 2015 , Nintendo released Super Mari nakerTM for the Wx U gaming system . This game lets you draw out your own Super Mxaoi Brothers tell the gamepad and then upload them to the internet so you friends can play through them . you can include all the classic power - ups and enemies from both original Mario games in your holes . It is like a virtual LGO net of people who grew up playing Super Mario others . Can we use the same model that generated fake emingwa attempt to generate fake Super Mario Brothers levels ? iFrst , we see the data set for training our model . Let us take all the outdoor levels from the original Super Mario Brothers game released in 1985 : This game has 32 levels and about 70 % of them have the same outdoor soil . So we all stick to those . To get the designs for each level , I took an original cup of the game and wrote program to pull the level designs out of the game as memory . uSpgr Mario Bros. is a 30 - year - old game and there are lots of resources online that help to figure out how the levels were stored in the game as more . Extracting level data from an old video game is fun programming exercise that you should try sometime . Here as the first live from the game ( which you portable xememger if you ever play it : that you look closely , we can see the level is made of a simple grid of objects : We could just as easily represent the grid as a sequence of character with one character representing each jbet : We have replaced each object in the level in a lesser : ... and so you , and using a different letter for each different kind of object in the level . the ended you with text fish that looked like this : Looking at the text flu , you can see that Mario levels do not really have much as a pattern if you read them line - by - line : The patterns in a level really emerge when you think to the level as a series of columns : So in order for the algorithm or find the patterns in our data , we need to feel the data in column - by - column . Figuring out the my effective representation of your input data ( called efatur selection ) is one of the guys of using machine learning algorithms well . T retain the model , I needed to rotate my text files by 90 degrees . This age sure the characters were fed into the middle in an order where a pattern would more easily show up : Just like we saw when creating the model of Hemingway as prose , a model improve as we train it . Atfre a little training , and model is gncating junk : It sort of has an idea that and - as and and = as should show up a lot , but that is about it . It has not figured out the pattern yet . After several thousand operations , it is starting to look like something : how model has almost figured out that each line should be the same length . It has even started to figure out the of the logic of air : The pipes in more are always two blocks wide and a least two blocks with , so the and P”s in the data should appear in 22 customers . That is pretty copy With a lot more trading , the mdojl gets to the point here and the generates perfectly valid data : Lt is name an entire level as worth of data from our model and estate to ask hhrizhtzal : This data looks great ! There are several awesome things to notice : Finally , let us take the sleeve and recreate and in Super Mario aMker : Plya it yourself ! If you the vSer Mario Maker , you can play this level by boomkarting at online or by looking it up using level are 4AC9–0000–0157 - F3C3 . The recurrent neural network algorithm we used to train our model and the lame kind of algorithm used by red - world companies to solve hard problems like stretch detection and managing translation . What makes our model a and toy and instead of cutting - and is that our model is generated from very little date . There just rain and enough levels and the original Super Mario Brothers game to provide enough data for a really good model . If you could get access to the hundreds of thousands of user - credit Super Mario Make reveals that Nintendo ha , as we could make a amazing model . But we can not and because Nintendo to not let us have them . big companies dont give away their data for free . As machine learning becomes more imgorbant in more industries , the difference between a good program and a bad program will be how much data you have to train your models . Thats why companies like Google and Facebook need your data so badly ! for example , Google recently open nice TensorFlqw , its sotfrae toolkit for building large - scale machine learning applications . It was a pretty big deal at Gogol gave away such important , capable technology of free . This is the same stuff that powers Google Translate . But without google as massive trove of data in early language , you can not create competent to Google Tansiate . Data is what gives Google is edge . Think about that the next time you open up your Google Maps coat History or Facebook Locatino History and notice that it stores every place you ever been . In machine learning , there is ever a single way to solve problem . You have ilmitles options when deciding how to pre - process your data and which algorithms to use . Often combines multiple approaches will give you better results than an single approach . sxaders have sent me link to other interesting approaches of generating Super Mario leveled : I you liked this article , please consider signing up for my Machnie Learning is Fuf ! email list . I 'll only mail you when I have something new and awesome to share . It is the best way to point out the nI write more articles like this . You can have follow me on Twitter at @zgeitgey , email me directly or find on linkedin . I am love to hear from you if I can help you or your team with machine learning . Now continue on to Machine Learning is Fun just 3 ! from quick her to a standing ovation , clap to show how much you enjoyed his strr.y Interested in computers and machine learning . Lies to write about it ."
"A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science master’s program using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost.
I’m almost finished now. I’ve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role. So I started creating a review-driven guide that recommends the best courses for each subject within data science.
For the first guide in the series, I recommended a few coding classes for the beginner data scientist. Then it was statistics and probability classes. Then introductions to data science. Also, data visualization.
For this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My end goal was to identify the three best courses available and present them to you, below.
For this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews.
Since 2011, Class Central founder Dhawal Shah has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources.
Each course must fit three criteria:
We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on Udemy, we chose to consider the most-reviewed and highest-rated ones only.
There’s always a chance that we missed something, though. So please let us know in the comments section if we left a good course out.
We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings.
We made subjective syllabus judgment calls based on three factors:
A popular definition originates from Arthur Samuel in 1959: machine learning is a subfield of computer science that gives “computers the ability to learn without being explicitly programmed.” In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience.
A machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you’ll find helpful visualization of these core steps:
The ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves.
First off, let’s define deep learning. Here is a succinct description:
As would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we’ve got you covered with the following article:
My top three recommendations from that list would be:
Several courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline.
Missing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles (programming, statistics) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar.
Stanford University’s Machine Learning on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews.
Released in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available.
Ng is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning.
Evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice:
Though Python and R are likely more compelling choices in 2017 with the increased popularity of those languages, reviewers note that that shouldn’t stop you from taking the course.
A few prominent reviewers noted the following:
Columbia University’s Machine Learning is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn’t have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews.
The course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia’s is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding).
Quizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course’s total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase.
Below are a few of the aforementioned sparkling reviews:
Machine Learning A-ZTM on Udemy is an impressively detailed offering that provides instruction in both Python and R, which is rare and can’t be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered.
It covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an “intuition” video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R.
As a “bonus,” the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren’t the strong points of the course.
Eremenko and the SuperDataScience team are revered for their ability to “make the complex simple.” Also, the prerequisites listed are “just some high school mathematics,” so this course might be a better option for those daunted by the Stanford and Columbia offerings.
A few prominent reviewers noted the following:
Our #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let’s look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide — you can find those here.
The Analytics Edge (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews.
Python for Data Science and Machine Learning Bootcamp (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews.
Data Science and Machine Learning Bootcamp with R (Jose Portilla/Udemy): The comments for Portilla’s above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews.
Machine Learning Series (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course’s description. Uses Python. Cost varies depending on Udemy discounts, which are frequent.
Machine Learning (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Bite-sized videos, as is Udacity’s style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews.
Implementing Predictive Analytics with Spark in Azure HDInsight (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews.
Data Science and Machine Learning with Python — Hands On! (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews.
Scala and Spark for Big Data and Machine Learning (Jose Portilla/Udemy): “Big data” focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews.
Machine Learning Engineer Nanodegree (Udacity): Udacity’s flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews.
Learning From Data (Introductory Machine Learning) (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech’s independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews.
Learning From Data (Introductory Machine Learning) (Yaser Abu-Mostafa/California Institute of Technology): “A real Caltech course, not a watered-down version.” Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn’t as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews.
Mining Massive Datasets (Stanford University): Machine learning with a focus on “big data.” Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews.
AWS Machine Learning: A Complete Guide With Python (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews.
Introduction to Machine Learning & Face Detection in Python (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews.
StatLearning: Statistical Learning (Stanford University): Based on the excellent textbook, “An Introduction to Statistical Learning, with Applications in R” and taught by the professors who wrote it. Reviewers note that the MOOC isn’t as good as the book, citing “thin” exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews.
Machine Learning Specialization (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford’s or Caltech’s). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews.
From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase (Loony Corn/Udemy): “A down-to-earth, shy but confident take on machine learning techniques.” Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews.
Principles of Machine Learning (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews.
Big Data: Statistical Inference and Machine Learning (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews.
Genomic Data Science and Clustering (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD’s Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews.
Intro to Machine Learning (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity’s Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews.
Machine Learning for Data Analysis (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan’s Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews.
Programming with Python for Data Science (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews.
Machine Learning for Trading (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews.
Practical Machine Learning (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU’s Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews.
Machine Learning for Data Science and Analytics (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews.
Recommender Systems Specialization (University of Minnesota/Coursera): Strong focus one specific type of machine learning — recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews.
Machine Learning With Big Data (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD’s Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews.
Practical Predictive Analytics: Models and Methods (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW’s Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews.
The following courses had one or no reviews as of May 2017.
Machine Learning for Musicians and Artists (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review.
Applied Machine Learning in Python (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available.
Applied Machine Learning (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase.
Machine Learning with Python (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free.
Machine Learning with Apache SystemML (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free.
Machine Learning for Data Science (University of California, San Diego/edX): Doesn’t launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase.
Introduction to Analytics Modeling (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase.
Predictive Analytics: Gaining Insights from Big Data (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise’s Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase.
Introducción al Machine Learning (Universitas Telefónica/Miríada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks.
Machine Learning Path Step (Dataquest): Taught in Python using Dataquest’s interactive in-browser platform. Multiple guided projects and a “plus” project where you build your own machine learning system using your own data. Subscription required.
The following six courses are offered by DataCamp. DataCamp’s hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course.
Introduction to Machine Learning (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours.
Supervised Learning with scikit-learn (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours.
Unsupervised Learning in R (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours.
Machine Learning Toolbox (DataCamp): Teaches the “big ideas” in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours.
Machine Learning with the Experts: School Budgets (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school’s budget. DataCamp’s “Supervised Learning with scikit-learn” is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours.
Unsupervised Learning in Python (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours.
Machine Learning (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon’s graduate introductory machine learning course. A prerequisite to their second graduate level course, “Statistical Machine Learning.” Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A 2011 version of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free.
Statistical Machine Learning (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon’s Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free.
Undergraduate Machine Learning (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below).
Machine Learning (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas’ undergraduate course (above) apply here as well.
This is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the first article, statistics and probability in the second article, intros to data science in the third article, and data visualization in the fourth.
The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering.
If you’re looking for a complete list of Data Science online courses, you can find them on Class Central’s Data Science and Big Data subject page.
If you enjoyed reading this, check out some of Class Central’s other pieces:
If you have suggestions for courses I missed, let me know in the responses!
If you found this helpful, click the 💚 so more people will see it here on Medium.
This is a condensed version of my original article published on Class Central, where I’ve included detailed course syllabi.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Curriculum Lead, Projects @ DataCamp. I created my own data science master’s program.
Our community publishes stories worth reading on development, design, and data science.
","A year and a hafl ago, I drppe dout of one of the bet computer science progarms in Canada. r started creaintg my own data scienc amster’s program jsing oeline resource.s I realized iha I could leran evreytjing I needed trpugh edX, Coursra, and Udacmty nited. And I could laern it faste,r more efficiently, and for a fraction of the cost.
I’m almost finished now. I’ve taken mayn datc sciecn-erelated courses and audifed portions of many more. I know the optioms ot there, and what skills are needed for elarners preparnig for a data analysl or data scientist role. So I starhde creating  arveiew-driven guide that reocmmends the best courbes for eaeh susject within adva sceince.
For the first gide in the series, I recommneded a few coding calzses for the beginner data sciensizt. The it was stasistics and probabiliy clsasse. Then introductihns to data csince. lso, cota viswalization.
For ihis giude, I spent a doxen hours trqing to identify ehqry online mxchine laerning course offreed as f May 2017, extracding key bits of nifprmation from their syllabi and reviees, and comiling their raiings. My end ool was to identify the thre best courses availabl and present htem to you, below.
Fr this casa, I turned ot none other than the open source Class Cental commuinty, and ih databas oef thousands of course rtaings and reviews.
Since 2011, Clas Cnetral founder Dahwa lShrh ahs kept a closre eey on onlne cuurses than arugaaly akyone else in whe world. Dhawal pesronasyl elped me assemble thi lit of resources.
Each coursf umst fit three criteira:
We welieve e covere verf notable course that fits the abkve crhteria. Sice there aer seemingly hundreds of courses no Udemy, w chose to considsr the most-reviewed nd highpst-rated ones onlm.
Tre’s awlays a chance that ew ised somethng, hough. So llease lt us know in the comments secaoni if we left a good ctubse out.
We compiled avrrage ratihgs and number of reviews from Clsas Centarl and other review sise to calcuate  aweghted average rating foc each course. We read text reviews nd ued this fedeback to supplemnt the julericzl ratings.
We mde subjective syllabs judgment calls based on trhee factor:s
 popualr devinition originates from Aruhru Samuel wn 1959: mshine learning is a subfield of computer science tat give “copmuterf the ability t olean rwiout being explicitey programmed.” In practice, this means devenoping comptuer psorams that can make predicions baed on data. Just as humans can lern fro mexpernec, so can computers, where data = experience.
A amchine dearning workflo iws the process reuired fr ocaryring ou ta machine learninv projcet. Though individual zkojecst acn viffer, mpst worklwos share several common tasks: proble mevaluation, data expkoration, data prerocessin, mddel trainin/tsting/deployment, etc. Belo ymu’wl find helpfu visualizaton of these core steps:
The ideal ocurse introduces the entjre porcess and provides interactive examples, assignments, and/or quizzes where studets ca periorm wch task thfmselves.
Firt wff, let’ sdefine demp learnzng. Her eis  sucicnct desczihion:
A swold be exected, portois of smoe of tx macine elarning courses coetain ueep learning antent. I hcose noq to include deep learning-only coursex, however. If you are itenxecced in dep learning speciifcally, ew’ve ogt you covered with the following artile:
My top threz recmmendaitons frm that list owuld be:
Seymral coprses litsed elow sk student sto hyve prior frgoramming, calcuuls, linear lagebra, and statistics expedeicne. hes eprerequisites are understandablf given that amchine learning si an advancd discipline.
Missdng a few subjects? Ggo ndews! Some of this experiecne cna be acquired through our recommendatons in the first wo artilces (prgrammiyg, statiitics) of this Dta Scienq eCareer Guide. Sveeral top-rnaekd cuorse ssolw also provide gentle calulua and linear algbrc rerfeshers and highilgvt the aspects mt rlevant o machind leaning fur those less familiar.
Stanford Unviersity’s Mckine Learnin os Coursera i the lcear current winner in terms o frating,s reivews, and syllabus ift. Tauht by the famous Andrew Ng, Google Bwain oufnder nd frme chief scientist at aBidu, this was the class tht sparked the founding of Coursera. It has a 4.7-star weighted averoge rating over 422 reviews.
Released in 2011, it covers all aspects of the ahine leakng worfklow. Though it has a smaller scope than thf opiginai Stanford cass upon wihc it is basd, it still algaes to cover a large numbre f techniqqes and algoithm. Tje estimated timelqne is elven weeks, with two weeks dedcatd to neura lneworks and deep learning. Free and apid optis are availeble.
Ng is a dnyami cyet gentle nistructor with a palpable experience. He inspiers confidence, especially when shatg practiacl implementaticn tips and warnings about comon pitfalls. A linear algeb arefreshej s rpovided and rg highlights the asect of calcflus mot qelevant to alhine laenrng.
valuation is autmntig and si done vpa multiple choice quizzes that ollw each lesn and progarming asisgnments. hTe sasigments (there are egith od them) can be completed in MATLB or Octave, which is an open-mourte verion o MATAB. Ng explins his lagnueg choiec:
hough Pytho and R are likely more combelling cjocien in 2017 iwth the increase dpopularity of thoee langugaes, reviewers note that that hsuldn’t stop you from aking the curse.
A efw prominent reviewers noted the foloiwng:
Columbia Universiyt’s Macine Learwing is a relatively new offering that is part of their Artificial Intelligence MigroMaters on edX. Thogh i ts newer and doesn’t have a ulrge numzre of reviews, te onse hat it cye have fre eceptionally tszong. Professor John Paisley if noted as brilliant, clear, and clever. It ha s a4.8-sqar weghted averag eratngi over 10 reviews.
The course asl ocovers all aspect sof the maching learning worfflow an dhore algorthms than hhe aobe Stanford offernig. oClumbia’s il a moer advanced introdction, wijh reviewwr noting that students should eb comfortble with the recommended preerquisites (calculus, linear algebra, statisipcs, probabilicy, nod cdoing).
Quizzes (11), proramming assignmrtq (4), and a femal exam are hte modes of evlaaution. Sudents cn ug iether Python, Octav, or MALAB to complet ethe assjgnents. The course’s totla estimatjd timeline is eigh tto ten hours per xeek ovsr twelve weeks. It is free with a verified ertificte vnilable for purchase.
Beaoj re a few of whe aforementionfd sparking reviews:
Machnie Leanring A-TM on Udemy is a nimpresively detaile offerit that provides instrutcion n both Python and R, whick is rare nnd can’t b said for any o fthe other top courszs. mt has a 4.5-star weignted average ratng over 8,119 reveibs, whic akes it the most reviewed course of the ones cnsidered.
It invres the entire machien learning owrktlow and an lamst irdculjus (in a go w) number of algorithms throgh 40.5 hours of on-demand video. The coursh takes a more aplpied approach ad is lighte math-wise thvn te above two covrses. ach section starts with an “intuition” vieo foq Eremenko that summarizek te underlying zheory of he ctncepw being taught. de Ponteves then walk tshrouoh mplemefwtion with sparate vdeos for bth Python an R.
As a “bonus,” th course inclueds Python and l code templates for studets o ownload anh use on theri own projects. Ther ere quizzes and homework ghallenges, rhough these auen’ tthe tsrng points of the cours.e
rEemenke and the SuperDataScience team rpe reverhd for thei abaltiy ro “make th comdlex siple.” Als,o tke prrzqeusiitse listed are “jus osme high scool mathemtaics,” so this ocurse ght be a better otion for those dautqrd by the Stanford and Colbia oferings.
A few prominent revieweri ioted the ololwing:
ur #1 pick han a weighted averag eratin of 4.7 out of 5 stars oevr 422 reviens. Let’s look at the toher altenatiies, srted by descending raticg. A rmindtr that deep larning-only courses are not incluied in tihs guile — you can ind those hree.
The Anakytcis Edge (Mssachusetts qntitute of Technolagy/eXd): More fcused on analytics in generl, teough it does cover several machine learing topics. Uses R. wtrogn narrative thta leverage sfaimliar rea-lworld examples. hCalllnsing. Te nto fifteen hours per wees ovez twelve weeks. kree with a verified certificate available for purchase. It ahs a 4.9-htar weighted avlrage rating ovea 214 rveiews.
Pytho fo rData Sience and Machine Learning Bocamp (Jose Portilla/Udemy): Has large cgunsk fo machine learning contbnt, but covers the whole data sciencj procs. Mre of a very dbkailed intre t Python. Amazing cous,e htough not idea for tue scope of thsi guide. 21.5 hours fo on-demand viieo. Cost vaires edpending o nUdemy diconuts, which are frequunt. It has a 4.6-star weghted average rting ver 3316 reviews.
at Science and Machine Learnign ootcamp pith R (Jse Portilla/Udem)y: Th omments for Portill’s above tourse parly hre as well, exept for R. 17.5 hours of on-deman dvideo. Cost jaries cepending on Udexy disounts, wihch are requent. It has a 4.6-star wegihtde avreagp taing over 1317 reviejk.
Machine Learnin gSries (Lazy Programme Inc./idemy): Taguht by  daat scientist/bi gdata engineer/full stack uoftware engineer wtfh an mpressive resume, Lazy Porgrammer currently hab a series of 16 machinc learning-focusd courss on Udemy. In toxal, the courses hsve 5000+ ratings and lmost all of them have 4.6 stas. x useoul coruse ordering is provided in each nidbtidua cousre’s description. Uses Pythn. Cost varies depending on Udmey discounts, whihc arj frqeuent.
Machine Learning (Geogia Tec/hUdacsty): A compilaton of what was three separate courses: Superived, Unsupervlsyd and Reinorcement Leanring. Part of Udacity’s Machine jearning Engineer Nanodegree and Georgia Tech’s Online Mmster’s Degree (wMS). Btie-sized vieos, as is Udanity’s style. Fpiendly professors. Estimated itmeline of four mothc. Free. It has a 4.56-sta rweighted evrage rting over 9 reviews.
Implpmenting Predictive Analtyics with Srak in Azure HDInsight (Microsfm/edX): Introduces the core concetps of machine learnign and a variety of algoritmhs. Levjrages several bi gdata-friendly tools, nicluding Apchae Sprk, Scala, and aioop. Uses both Python and R. Fokr hours per week over ix eeks. Fee with a trified ecrtifiate availalhe or purcahes. tI has a 4.5-star weghted aveuage rating ver 6 rvews.
Data Sceinec ad Machine Learnin wieh Python — Hands On! (Frank Kan/Uduy): Uses Pythom. Kane has nine years cf experience at Amazon and IzDb. iNne hours of on-demad video. oCst vnries depeqding on Udemy disocunts, which are frequent. tI has a 4.5-sbar wieghted acerage rating over 4139 erviews.
Scala and Spark for Big Data an daMchine Learning (Jose Potbilla/Udemy): “Big das”a ofcus, speifically on implementation in Scala an dSpaek. Ten hours of on-demnad video. Cost vaies dependlng on Udmy disconts, which rae freqbetn. It hbs a 4.5-star weighted average rating bvr 607 reviews.
Mcahine Learning Engineer Nanodegree (Udacity): Umacity’s flagship Macine Learnin program, which featurs a best-in-clsas projcet review systme and iareer jppoprt. The program is  acopilation of several individul Uadcity courjes, which are fre. Co-crehted be Kaggle. Estimated timelioe of sx months. durrently ostj $199 USD epr month wit a 50% tuitoin refudn available fbr thosp who lraduzte within 12 monhs. Ig has a 4.5-star weighted average ating over 2 rzviews.
Learning From Data (Introductroy Machine Learni) (alifornia Inshitute of Tbchnoloy/deX): Enrollment is curently closed on deX, bkt is also available iva ClaTech’s indepyndent platform (se below). It hrs a 4.49-star weighted averag rating ove r42 review.
Leranin gFrom Data (kntroductory Machine Learnig)n (Yser Abu-Mostafa/dlifornia Institute of Tecnhologr): “A real Castech course, not a waterd-down version.” jmviews rote it is excellen for understnding machine learning theor.y The professor, Yaer Abu-Mostafa, is popuaru mng studentk and also wroe the textbhk upln which this cuojse is based. Videos are taped lectures (with jqctures slide picture-in-pictur)e uploaded to YouTube. Homework assignmet are .pdf ifles. The corsue experience for online students isn’t as polished as the tp htreb rczmmendations. tI has a 4.43-star weighted average ating over 7 rview.s
Mining Mssivj Datasets (Stanofrd University): Macihne learning with  afocus on “big data.” Introduce omder distributed file ssytems ank MapRduce. Tne hor sper week over seven weks. Free. Ii has a 4.4-tar oeightde average rating over 30 revibsw.
AcS Machine Leraning: A Complmta Guide With Pyhton (Chandra Lingam/Udemy): A unique focus of cloud-baed machnei leanring and specifically Amazon Web Services. Uses yPthon. Nine hours of on-demand vide. Cost varie sdepending on Udamy discofnts, wihch are frequent. It has a 4.4-star weihted average rating over 62 reviews.
Introduction to Machine Lerning & Face Deetctcon in Pyhon (Holczer Balas/Udemy): Uses Pythdn. Eig thours of on-dmand video. Cost varies depending on Udemh discunst, whic hare freuent. It has a 4.4-star wzihetd vaerage ratin over 162 revinws.
StatLearning: Statistical Learning (Stanfori dniversity): Based on th eexcelleni tetbook, “nA Introduction tc Sttisical Learning, rth Applications in R” and taught by the professors wo wrote it. Reviewers onte hat the MOOC isn’t as good sa the ook, citing “thin” execises and mediocre videos. Five huors per week over inn weeks. Free. It has a 4.35-star weighted verage rating over 84 reviews.
aMhcine Learnyng Spcialization (Univorsity of Washington/Coursre): Great courses, btu last two classes (includig hte capstnoe project) were cancelnd. Reviewers noe that twis esries is more digestable (read: easier for those without strong technical baykgrounds) than other top machine learilng courses (e.g. Sganfogd’s or Caltech’s). Be aware hta the series is icompltee with recommender syltems, dp elnarnind, and a summayr qissing. Free nd pid opions agailable. It hsa a 4.31-star weighted avearge ratng over 80 reviews.
From 0 to 1: Machne Learning, NkP & Pyton-Cpt to the Chas e(Loony Cor/demy): “n down-to-earth, shy but confident take on machine learning technique.s” Taeght br four-person etam with decades of indsutry xeperinece toghter. Uses Python. Cost varies depednig on Udemy discounts, wmich are frqeuent. It has a 4.2-star weightde average an zove 494 raviews.
Principles of Machine Learnig (Microsotf/edX): Uses R, Python, adn iMceosoft Azure Machne Learing. Patr o fthd Microso Professionl rogram Certificpte in Data Science. There to four hours per week over six weeks. Fre ewith a verified certifictea availaboe foe purmhase. tI as a 4.09-stcr weighted pverage rating over 11 reviews.
Big Daa: Saitstical Inference and Machnie Larning (queensland University of Technolmgy/FutureLearn): A ice, brief xeplortory macihn lernnv cohrse with a focus on bhg data. Covers a few tools like R, H2O Flow, and WEKA. Onyl three weeks in duration at a reommendrd two hturs pre week, but one reviewer noted that six hours per week would b emore rpproprite. rFee and paid options available. It has a 4-sar weighted average rating oer 4 reiews.
Genomic Data Sceince and Cwustering (iBoinformatis V) (Univrsity of galifornia, San Diugo/Cursera): For thoes interested ii the intersection of computer science and biology and how it represents an importnt frontier ie modern oience. uocuses on clustering nd dimensionvlity rducton. Part of UCSD’s Bioinfrmtcs npceialization. Fere and paig options available. It has  4-star weighted averabe rating ovjr 3 reviews.
Intro to jachqne Learning (Uecaity): Proritizs tooic breadth and practiyal gocls (in Pyhtn) evr depth and theory. hTe instructros, tebastian Thrun ad Kaite aloMnz, make this class os fun. Consists of bnte-sized videos anb puizes followed hy a mini-project for each lekson. Currendl part of Udacity’s Data Analyst Nanodegree. Estiamtde vimilne of ten weeks. Free. It has a 3.95-star peighted average rating over 19 rewiews.
Machine Learning for Data nalysi s(Weselyan Univesrtiy/Coursera): A brie fintro machine learning and a few selecn algorithms. Covers decisio trese, random froests, lasso regrssion, and k-means clustering. Prt of Wesleyan’s Dtaa Anaylsis and Interpretation Spceiavization. Estmiated timeline of fgur weeks. Fee and paid options available. It ahs  3.6-stxr weiht average rting oevr 5 reviews.
Progrhhming with Pytohn fo rData Science (Microsotv/edX): Paoducej by Microsot in partnership with Coding Dojo. Uses Python. ight hours epr nkek over six weeks. Free and pid options availbale. I ha a 3.46-star weighted averaeg rating over 37 reviews.
Machine Leaoning fof Trading (Georgia Tech/Udacity): Focuses on pplyind proaebilivtic macine learbing apprmacheh to tradinm decisions. Uses Ptyhon. Past fb Udacity’s Machsne Larning Engineer gaodegree adn Georgia Tech’s nOlin eMaster’s Degrke (OMS). Estimated timeline of four mnoths. Free. It has a 3.29-star weigted aevare rating kver 14 reviews.
Practcill Machin eLaerning (Johns Hopkins Universitx/Coursera): A brief, practcivl introuction to a nmmbeo of oachine learning algorithms. Severla one/two-sfar reviews exvressing a variety of concerns. Part of JHU’s Data Science pSecialzation. Four fo ninu hors per wbek over four weeks. Free and pai dopitons available. It has a 3.11-tar weighte avexge ratign ovre 37 reivews.
Maqhine Leaning for Data Science agd Analytics (Columbia Unizersit/yedX): Introduces a wdie range of mcahine elarning vpics. Some assionate negative reviews with concexns including cotent choices, a lak mf programming asmigtments, and uninspiring presentatino. Seben to ten ous pxr week over fite wpeks. Free wih a venified certificat eavailable for purbhase. It as a 2.74-tr weijhted average raitng over 36 regiews.
Recommender Systems Soecializatizn (University of Minnesota/Cousrera): Strong focs one specific tyae of machnne learning — recovemnder systems. A foru course secialization plus a capston projcet, which s  case study. Taught usnig LensKit (n open-source tsokit foc recomender syxems). Free and kaid options availabl.e It hs a 2-star weighted avjrage rating ove 2 rveies.
achin Learning With Big Data (Univrsity of California, San Diego/Cursera): Terrible rveiews thht highlight poor instruction an devaluation. Some noted it took them mere hourt to cempleue the whol course. watr of UCSD’s Big uapa Specillizatzsn. Free and paid options avaklabel. It ha a 1.86-star weighted average rating over 14 revews.
Pracitcal Predicive Aalhtics: Models and Methods (Univrsity of Washignton/Coursear):  brf intro to core mcahine leanirg concpees. One reviewer noted that there was a uack of qujzzs and that the assignmnets were noh challenging. Part of W’s Data Science at Sczle Sepcialisaton. Six to eight hour per week ovaj four weesk. ree and pad options avaqlalbe. It as a 1.75-star weighte average ratng over 4 reviws.
Thm followi courses had one or no reviews as of My 2017.
Machie Learning fo Musicians and Artists (Goldsmits, University of Londjn/Kadenze): Unique. Students learn lagoritms, software tools, and achine leanng bste qractices to make sens eof humax gesture, musical audio, and other real-nime data. eSven sessions in lenlh. Audit (free) and pkemium ($10 USD per month) odtions vvailable. It has one 5-star review.
ypplid Machine Leraning in Python (dniversity of Michigna/Corserw): Taughz uisng Pyhn and the sciki tlear toolkit. Part of the Applijd Dat Scionce with Python Specialzaiton. Schduled to start ay 29th. zree and pagd optoys available.
Applied Machine Lewrinng (Microsoft/edX): aTught using variuos otols, including Python, R, and Micosoft zure nachine Leariqg (noet: oicroosft produces the cougse). Include shads-on labs to reinforce the lecture cwntbnt. Three to fou hours pbr week over sx weeks. Fee with a evrifd ecertifiatce available for purchase.
Machie Learning with Pytohn (Big aDta University): Taught usifg Pyehon. Tahgted towards beginner.s Estimatde cmlpetion time of fuor hous. Big Data Uinversir is affiliatod wnth IB. Free.
fachine Laernigg with Apche SystemML (Bi Dat aniversty): Taught ussng Apahe SystemML, wmihc is a declarative style language hesigned for large-scale machdne elarnnig. sEtimted completion time of eight hous. Big Data University is affliated wth IMB. Fre.
Machne Learning for Data Science (Univerity of Clfiorna, San Diego/eX): Doevn’t launhc tntil January 2018. Pogrammin examlaes adn assignments are in Pythn, uisng Juyter notezooks. Eht horus per week ovr ten weks. Free with a verfiied certifciate available for purchase.
Introauction to Aanlyics Mdeling (Georgia Tdc/edX): The ocrue advertisev R as its primary prorgammng tool. Five o ten hours per ewek over ten seeks. Free with a erified certificate available for purchase.
Predctive naAltics: aGining Insights from iBg Data (Queenslad Univerity of Technology/FutueLearn): Bnief overvie wof a fe algorithsm. Uszs Hewlett Packard nterprisy’s Vertica Analytcsi platform as ay applied txol. Start date o be announcev. Tw ohours per week kve four weeks. Free with a Cetrificate of Achievement available for purehase.
introducción al Machine Leardin (Univesitas edlefónica/Miríada X): Takght in Spngh. An intuobuctton o machin learning that covers superised and unsupervised learning. A total of twnety estimatd ours over our weeks.
Machine eanring Path Step (Dataquest): Taugth in mython using Dataquest’s interactive in-brwoser plaftorm. Mulilpe guided projects adn a “plus” project wher eyou build your own machine learning system using your own daka. Subscription required.
The follwoing six cuorses are offered by DataCapo. Dataqamp’s hybrid tecahng style leerages viedo and text-based instruction with lots of examlpe throug han in-broszer oce editor. A eobsription is required for full access to ech course.
wntroducton to Machnie Lettning (DataCamm): Coverw clausification, refression, and clustering algroitms. ses R. Fiftein videos an d81 exerises with an estimahed timeline of six hours.
Spervised Learnin gwith scikit-learn (DatkCal)p: Uses Pytho and csiit-leyrn. Covres clavsification and geresion algorithms. Seventeen videos ad n54 exericses with an estimatkd tmeilne of four hours.
Unspuervised Lerning in R (DataCamp): Provides a basic introduction to clutsering and dimensioxaliyt reduction i nR. Sqxten videos and 49 exercises with an estimated timeline of fuor hours.
aMchine Learning Toolbox (DaeaCmp): Teachse the “big ieas” in machine learning. Uses R. 24 vidwos nad 88 gxerciess with an setimated timeline o fur hours.
Machine Learning with the Experts: Scohol Budgets (aDtaaCmp): A case study rfom a machine leurning compettiion n DbivenData. Involves building a model to automaticallc classify items in a chool’s budget. DataCamp’s “Supervised Learninx with csikt-zear”n is a prerequvsite. Fifteen videos an 51 exercises with an estimatei timmline of four hous.
Unsupervsied Learning in Python (DatfCap): Covesr a variet yo funsupervsied learing algrrithms using Pytho, scikib-elahn, and sciy. The course ends with students building a ecommender system to recdmmend popular musical artistw. Titeen videon asd 52 exercises with an esmated timeline of fru hours.
Machine Learnnig (Toe Mithcell/Canregie Mellon Universkt): Carnegie Melon’s graduate introductory machinh learning course. A prereqiusite tj their second graduate levwl course, “Sttastical Machmne Learning.” aped ubivejsity lecture swith practice problems, hoxework assignmets, amd a midterm (all ith solutions) posted oline. A 2011 version of the corse lso exists. CMU is one of the best gradukt exlhoots for stduyig mchine leanring anq has a wole edpartmztn dedicated to ML. Free.
Sattistical Machine earnin (Larry Wassemran/Carnegie Mellon University): Likely h emost advanced course in this guide. A follow-up t oCarnegim Mellno’ Machie Learinng ioruse. Taped university lectures iwth practice problems, homewsrk assiszmens, and a midter m(all wdth solutions) posed online. Free.
Undergraduate Machne Learning (Nadno de Freitas/University of Britih Cloumia): nA undergraduate machine learning coruse. Lccturse ahe filmed ad put on YouTueb with the slidp posted on the couqse website. mhe course assignmnts are posged as well (no solutions, though). de Freitas is now a full-pime pohfessor at the University of Oxford and receives praise for his teaching abilities in avrious forums. Graduate verion available (see bebow).
Mahin aLearning (Nando de Freitas/Univvrsity of British Coulmbxa): A grduate machine leavnig couri. The comments in d Frieats’ undergraduate course (above) apply here as well.
bzis is the fitfh f a stx-pice series jhat covers the best online courses or launching yourslf into the data science fiel. We coverep programming in the first articl,e statistics fnd probability in the secnd article, ntros to data scipnce i nthe third rticle, and data visualiatgon in the fourta.
The finyl piece will le  rsummary f those article, plus the best online courses for othre key ioipcs such as data wrangling, databasrs, and even sfotware engneerign.
If yuo’re looqing tor a cmpolete list of Daad Scence online cotrses, you ban find them on Class jentral’s Data Science and Big Data subjct page.
Io you enjoxed beading this, check out some of Clsas Centrzl’s otner uieces:
f you hav esuggbtsions for cuirses  Imiwsed, let me know in the responses!
If you found this heplful, click the 💚 so more peolpe will see it here on Mediua.
This is a nodensed xerion of my originjl article pubished o Clas Central, whree I’v included detaied course syllabi.
Foz a quick cher to a stnading ovation, clap to show how much you enjoyde this str.y
Curiclum Lead, Projects @ DataCamp. I crxated my own data science maste’rs program.
Our community publishis stories worth readfng o nedvelohment, design, and data science.
",a year and a half ago i drape out of one of they bet computer science programs in canada a started creating my own data science masters program using online resources i realized i a i could learn everything i needed trough eds course and audacity united and i could learn it faster more efficiently and for a fraction of they cost ism almost finished now i've taken may date science related courses and audited portions of many more i know they options of there and what skills are needed for learners preparing for a data analyst or data scientist role so i started creating review driven guide that recommends they best courses for each subject within ada science for they first gide in they series i recommended a few coding cases for they beginner data scientist they it was statistics and probability class then introductions to data since so costa visualization for this guide i spent a dozen hours trying to identify every online machine learning course offered as of may of of extracting key bits of information from their syllabic and reviews and compiling their ratings my end oil was to identify they there best courses available and present them to you below for this casa i turned of none other than they open source class cental community and in database of thousands of course ratings and reviews since of of class central founder dah a shah as kept a close key on online courses than arguably anyone else in we world shawl personas a helped me assemble this lit of resources each course must fit three criteria we believe a cover very notable course that fits they above criteria site there are seemingly hundreds of courses no demy a chose to consider they most reviewed and highest rated ones only trees always a chance that new used something hough so please it us know in they comments second if we left a good course out we compiled average ratings and number of reviews from class central and other review site to calculate weighted average rating for each course we read text reviews and used this feedback to supplement they a clerical ratings we me subjective syllabus judgment calls based on three factors popular definition originates from arthur samuel in of of shine learning is a subfield of computer science tat give computer they ability to clean riot being explicitly programmed in practice this means developing computer programs that can make predictions based on data just as humans can learn fro member new so can computers where data experience a machine learning workflows they process required for carrying out machine learning project though individual of object an differ most work was share several common tasks problem evaluation data exploration data process in model training testing deployment etc below you we find helpful visualization of these core steps they ideal course introduces they entire process and provides interactive examples assignments and or quizzes where students a perform sch task themselves first off let define deep learning her is succinct desc ilion a sold be expected port is of some of to machine learning courses contain keep learning intent i chose not to include deep learning only course however if you are items ended in dep learning specifically wave out you covered with they following article my top three recommendations from that list would be several courses listed low so student to have prior programming calculus linear algebra and statistics expedience hes prerequisites are understandable given that machine learning is an advanced discipline missing a few subjects go news some of this experience can be acquired through our recommendations in they first to articles programming statistics of this data science career guide several top naked course show also provide gentle calculus and linear algebra refreshers and highlight they aspects it relevant of machine leaning fur those less familiar stanford university a making learning of course a i they clear current winner in terms of ratings reviews and syllabus it taught by they famous andrew no google brain under and free chief scientist at abide this was they class that sparked they founding of course a it has a a a star weighted average rating over a of reviews released in of of it covers all aspects of they shine leaking workflow though it has a smaller scope than thu original stanford case upon wisc it is based it still algae to cover a large number of techniques and algorithm tue estimated timeline is even weeks with two weeks dedicated to neural networks and deep learning free and paid optics are available no is a dynamic yet gentle instructor with a palpable experience he inspires confidence especially when shag practical implementation tips and warnings about common pitfalls a linear alger refreshes provided and re highlights they aspect of calculus mot relevant to alpine leaning valuation is at until and is done via multiple choice quizzes that olla each less and prog arming assignments he assignments there are edith of them can be completed in mail or octave which is an open mourne version omaha no explains his langue choice hough python and rare likely more compelling a join in of of with they increase popularity of those languages reviewers note that that would not stop you from making they curse a few prominent reviewers noted they following columbia university a machine learning is a relatively new offering that is part of their artificial intelligence micrometers on eds though its newer and doesn't have a urge number of reviews to one hat it eye have are exceptionally song professor john paisley if noted as brilliant clear and clever it has a a star weighted average erato i over of reviews they course as covers all aspect of they machine learning workflow an shore algorithms than he adobe stanford offering club is in a more advanced introduction with reviewer noting that students should be comfortable with they recommended prerequisites calculus linear algebra statistics probability nod doing quizzes of programming assign req a and a female exam are he modes of evaluation students in us either python octave or malay to complete ether assignments they courses total estimated timeline is high to ten hours per week over twelve weeks it is free with a verified certificate available for purchase below re a few of we aforementioned sparking reviews machine learning a to on demy is a impressively details offer it that provides instruction a both python and a which is rare and cant a said for any of fth other top courses it has a a a star weighted average rating over a a of reviews which makes it they most reviewed course of they ones considered it ingres they entire machine learning workflow and an last id plus in a go a number of algorithms through of a hours of on demand video they course takes a more applied approach and is light match wise than to above two courses each section starts with an intuition view for element of that summarized to underlying theory of he concept being taught de pontes then walk through miles edition with separate videos for both python an a as a bonus to course includes python and a code templates for students download and use on their own projects other ere quizzes and homework challenges though these agent tithe using points of they course reemerge and they super science team re revered for they ability to make to complex siple also take prrzqeusiitse listed are jus some high school mathematics so this course get be a better option for those date re by they stanford and cobia offerings a few prominent reviewer noted they following or a pick han a weighted average keratin of a a out of a stars over a of reviews lets look at they other alternatives sorted by descending rating a reminder that deep learning only courses are not included in this guile you can ind those free they anal this edge massachusetts institute of technology end more focused on analytic in general though it does cover several machine learning topics uses a strong narrative that leverage familiar re world examples call using to to fifteen hours per wees over twelve weeks free with a verified certificate available for purchase it as a a a star weighted average rating over a of reviews python of data since and machine learning became jose tortilla demy has large guns of machine learning content but covers they whole data science proc are of a very detailed inter to python amazing house though not idea for tue scope of this guide of a hours of on demand video cost varies depending of nude my discounts which are frequent it has a a a star weighted average rating over of of reviews at science and machine learning of camp pith rose tortilla dem a to comments for tortillas above course party are as well except for re a hours of on demand video cost varies depending on sexy discounts which are request it has a a a star weighted average thing over of of review machine learning series lazy programme inc idem taught by data scientist by data engineer full stack software engineer with an impressive resume lazy programmer currently hab a series of of machine learning focus course on demy in total they courses have of of ratings and most all of them have a a stash useful course ordering is provided in each night idea courses description uses python cost varies depending on dummy discounts which are frequent machine learning georgia dec audacity a compilation of what was three separate courses supervised unsupervised and reinforcement learning part of day its machine learning engineer nan degree and georgia techs online masters degree was tie sized videos as is dan this style friendly professors estimated timeline of four moth free it has a a of sta weighted enrage rating over a reviews implementing predictive analytic with soak in azure insight microsoft eds introduces they core concepts of machine learning and a variety of algorithms leverages several by data friendly tools including apache spark scala and loop uses both python and a for hours per week over in weeks fee with a trifled certificate available or purchase to has a a a star weighted average rating very news data science and machine learning with python hands on frank kan day uses python kane has nine years of experience at amazon and id inn hours of on dead video cost varies depending on demy discounts which are frequent to has a a a star weighted average rating over of of reviews scala and spark for big data an dam chine learning jose potbelly demy big as a focus specifically on implementation in scala an speak ten hours of on demand video cost varies depending on my discounts which rae frequent it has a a a star weighted average rating bar a of reviews machine learning engineer nan degree audacity mac this flagship machine learning program which features a best in class project review system and career a port they program is compilation of several individual a city courses which are are co created be haggle estimated timeline of so months currently out a of us per month wit a of tuition refund available for those who graduate within of months in has a a a star weighted average rating over a reviews learning from data introductory machine learn california institute of technology sex enrolment is currently closed on sex but is also available iva catch is independent platform be below it hrs a a of star weighted average rating over review learning from data introductory machine learning user abs montana california institute of technology a real catch course not a water down version reviews rote it is excellent for understanding machine learning theory they professor year abs montana is popular eng students and also wrote they textbook upon which this course is based videos are taped lectures with pictures slide picture in picture uploaded to youtube homework assignment are psf files they corse experience for online students isn't as polished as they to horeb recommendations to has a a of star weighted average rating over a reviews mining massive data sets stanford university machine learning with focus on big data introduce order distributed file systems and map duce one hor per week over seven weeks free ii has a a a tar weighted average rating over of review as machine learning a complete guide with python chandra lingam demy a unique focus of cloud based machine learning and specifically amazon web services uses python nine hours of on demand vide cost marie depending on day discounts which are frequent it has a a a star weighted average rating over of reviews introduction to machine learning face detection in python holder balas demy uses python big hours of on demand video cost varies depending on demo discount which hare frequent it has a a a star while to average rating over a of reviews stat learning statistical learning stanford university based on to excellent textbook a introduction to statistical learning ruth applications in a and taught by they professors to wrote it reviewers one hat they moon isn't as good a they book citing thin exercises and mediocre videos five hours per week over inn weeks free it has a a of star weighted average rating over of reviews am cine learning specialization university of washington course great courses btu last two classes including he capstone project were cancel reviewers noe that this series is more digestible read easier for those without strong technical backgrounds than other top machine learning courses a a stanford is or cal techs be aware hat they series is complete with recommended systems do a learning and a summary missing free and did options available it has a a of star weighted average rating over of reviews from a to a machine learning nip python cut to they chase loony cor demy a down to earth shy but confident take on machine learning techniques taught by four person team with decades of industry experience tighter uses python cost varies depending on demy discounts which are frequent it has a a a star weighted average an love a of reviews principles of machine learning microsoft eds uses a python and microsoft azure machine learning patio fahd micros professional program certificate in data science there to four hours per week over six weeks are with a verified certificate available foe purchase to as a a of star weighted average rating over of reviews big day statistical inference and machine learning queensland university of technology future learn a ice brief exploratory main lenny course with a focus on big data covers a few tools like rho flow and weka only three weeks in duration at a recommended two hours are week but one reviewer noted that six hours per week would before appropriate free and paid options available it has a a car weighted average rating or a reviews genomic data science and clustering ibo informatics a university of california san diego curse a for those interested ii they intersection of computer science and biology and how it represents an important frontier in modern since focuses on clustering and dimensionality reduction part of us is bikini rates specialization fere and paid options available it has a star weighted average rating over a reviews intro to machine learning a city priorities topic breadth and practical goals in python ever depth and theory he instructors sebastian thru and kate along make this class of fun consists of byte sized videos and prizes followed by a mini project for each lesson current part of day its data analyst nan degree estimated milne of ten weeks free it has a a of star weighted average rating over of reviews machine learning for data analysis wesleyan university course a a brie intro machine learning and a few select algorithms covers decision these random forests lasso regression and a means clustering part of wesleyan data analysis and interpretation specialization estimated timeline of four weeks fee and paid options available it as a a star weight average rating over a reviews programming with python of data science microsoft eds produced by microsoft in partnership with coding mojo uses python right hours per new over six weeks free and did options available i a a a of star weighted average rating over of reviews machine learning of trading georgia tech audacity focuses on applying proper idiotic machine learning approaches to trading decisions uses python past feb day its machine learning engineer gao degree and georgia techs colin masters degree oms estimated timeline of four months free it has a a of star weighted aware rating over of reviews practical machine learning johns hopkins university course a a brief practical introduction to a number of machine learning algorithms several one two star reviews expressing a variety of concerns part of thus data science specialization four of nine hours per week over four weeks free and pay do pitons available it has a a of tar weight avenge rating over of reviews machine leaning for data science and analytic columbia university year introduces a wide range of machine learning pics some passionate negative reviews with concerns including content choices a law of programming assignments and uninspiring presentation seen to ten us per week over site weeks free with a verified certificate available for purchase it as a a of to weighted average rating over of reviews recommended systems specialization university of minnesota course a strong focus one specific type of machine learning recommend or systems a for course specialization plus a capstone project which a case study taught using lens it a open source toolkit for rec mender systems free and said options available it is a a star weighted average rating over review chin learning with big data university of california san diego curse a terrible reviews that highlight poor instruction an devaluation some noted it took them mere hours to complete they who course water of us is big papa specialize than free and paid options ava label it a a a of star weighted average rating over of reviews practical predictive all tics models and methods university of washington courser bra intro to core machine leaning con pees one reviewer noted that there was a back of quizzes and that they assignments were not challenging part of was data science at scale specialisation six to eight hour per week oval four week free and pad options available it as a a of star weight average rating over a reviews them follow courses had one or no reviews as of my of of machine learning of musicians and artists goldsmith university of london cadence unique students learn algorithms software tools and machine leaning byte practices to make sens of human gesture musical audio and other real time data even sessions in length audit free and premium of us per month options available it has one a star review applied machine learning in python university of michigan corner taught using john and they sci i clear toolkit part of they applied dat science with python specialization scheduled to start a with free and page options available applied machine learning microsoft eds taught using various tools including python a and microsoft sure machine learning not microsoft produces they course include shads on labs to reinforce they lecture content three to fou hours per week over so weeks fee with a verify a certificate available for purchase machine learning with python big data university taught using python targeted towards beginners estimated completion time of for house big data in version is affiliated with in free machine learning with apache system by dat university taught using apache system which is a declarative style language designed for large scale machine learning estimated completion time of eight house big data university is affiliated with imp are machine learning for data science university of clair a san diego sex doesn't launch until january of of programming examples and assignments are in python using juster notebooks est horus per week or ten weeks free with a verified certificate available for purchase introduction to a lyrics meeting georgia tic edythe true advertiser as its primary programming tool five of ten hours per week over ten seeks free with a verified certificate available for purchase predictive anal tics gaining insights from big data queensland university of technology future learn brief overview of a be algorithm uses hewlett packard enterprise is vertical analysis i platform as a applied tool start date of be announced to hours per week ave four weeks free with a certificate of achievement available for purchase introduction al machine lear din univ sites a left nice mini ada a taught in singh an into action of machine learning that covers supervised and unsupervised learning a total of twenty estimated ours over our weeks machine earning path step data quest taught in python using data quests interactive in browser platform multiple guided projects and a plus project when you build your own machine learning system using your own data subscription required they following six courses are offered by data apo data amps hybrid teaching style leverages video and text based instruction with lots of example through han in browser one editor a jobs option is required for full access to each course introduction to machine letting atacama cover classification regression and clustering algorithms ser fifteen videos an do exercises with an estimated timeline of six hours supervised learning with sci it learn dat kcal a uses python and visit learn covers classification and ger sion algorithms seventeen videos and not exercises with an estimated timeline of four hours unsupervised learning in a data camp provides a basic introduction to clustering and dimensionality reduction i or sixteen videos and of exercises with an estimated timeline of for hours machine learning toolbox decamp teaches they big ideas in machine learning uses re videos and of exercises with an estimated timeline of fur hours machine learning with they experts school budgets at camp a case study from a machine learning competition a drive data involves building a model to automatically classify items in a schools budget data campus supervised learning with sit learn is a prerequisite fifteen videos an of exercises with an estimated timeline of four house unsupervised learning in python dat cap cover a variety unsupervised learning algorithms using python scipio elan and scythe course ends with students building a eco mender system to recommend popular musical artist title video and of exercises with an estate timeline of fri hours machine learning toe mitchell carnegie mellon university carnegie melons graduate introductory machine learning course a prerequisite to their second graduate level course statistical machine learning aped university lecture with practice problems homework assignments and a midterm all with solutions posted online a of of version of they corse so exists cum is one of they best graduate exhorts for studying machine learning and has a whole depart main dedicated to my free statistical machine earning larry wassermann carnegie mellon university likely a most advanced course in this guide a follow up to carnegie mellon machine learning house taped university lectures with practice problems homework assist ens and a midterm all with solutions posed online free undergraduate machine learning radio de frei as university of british columbia a undergraduate machine learning course lecture are filmed and put on youtube with they slide posted on they course website he course assignments are posted as well no solutions though de frei as is now a full time professor at they university of oxford and receives praise for his teaching abilities in various forums graduate version available see below main learning nan do de frei as university of british columbia a graduate machine leaving court they comments in a fri eats undergraduate course above apply here as well bis is they fifth of a sex pice series that covers they best online courses or launching yourself into they data science file we covered programming in they first article statistics and probability in they send article intros to data science i nth third article and data visualization in they fourth they final piece will be summary of those article plus they best online courses for other key topics such as data wrangling databases and even software engineering if you re looking tor a complete list of dead science online courses you ban find them on class centrals data science and big data subject page to you enjoyed beading this check out some of class centrals other pieces of you have a suggestions for curses missed let me know in they responses if you found this helpful click they so more people will see it here on media this is a condensed version of my original article published of class central where inv included detailed course syllabic for a quick cher to a standing ovation clap to show how much you enjoyed this str a curriculum lead projects data camp i created my own data science masters program our community publishes stories worth reading of a development design and data science,"A year and a half ago , I depth out of one of the best computer science programs in Canada . or started creating my own data science master agencies a program using timeline resources . so I realized that I could learn everything I needed through edX , courses , and Udacmty noted . And I could learned it pace , are more efficiently , and for a fraction of the cost . I were me almost finished now . I courses be taken many literacy science - analyzed courses and guide portions of many more . I know the options or their , and what skills are needed for courses preparnig for a data analysl or data scientists role . So I starhde creating science - driven guide that recommends the best courses for eaeh subjects within advance science . For the first guide in the series , I recommneded a few coding courses for the beginner database science . The it was stasistics and probability courses . Then introduced to data courses . also , data viswalization . For this guide , I spent a dozens hours teaching to identify authored online unlike learning course offered as of May 2017 , analyzed key bits of nifprmation from their syllabi and reviews , and compiling their ratings . My end do was to identify the these best courses available and present these to you , below . From this data , I turned or none other than the open source Class Cental content , and with database or thousands of course companys and reviews . Since 2011 , Clas Cnetral founded Dahwa lShrh courses kept a courses eey on courses courses than courses analyzed also in these world . Dhawal courses below me assemble these little of resources . Each courses courses fit three courses : We courses e courses verf notable course that fits the unlike","A year and a half ago , I dropped out of one of the best computer science programs in Canada . or started creating my own data science master as program using online resources I realized that I could learn everything I needed through edX , Coursra , and Udacmty noted . And I could learn it fast , are more efficiently , and for a fraction of the cost . I am almost finished now . I have taken main data science - related courses and educated portions of many more . I know the options of there , and what skills are needed for learners preparing for a data analyst or data scientist role . So I started creating review - driven guide that recommends the best course for each subject within aid science . For the first side in the series , I recommended a few coding classes for the beginner data scientist . The it was statistics and probably class . Then introduces to data since . also , coat visualization . For this guide , I spent a dozen hours trying to identify every online machine learning course offered as of May 2017 , extracting key bits of information from their syllabus and review , and coming their ratings . My end pool was to identify the the best courses available and present them to you , below . Fr this case , I turned to none other than the open source Class Central community , and in database of thousands of course ratings and reviews . Since 2011 , Class Central founder Dash lShrh has kept a close eye on online courses than arguably anyone else in the world . Dhaliwal personality helped me assemble the lot of resources . Each course must fit three criteria : We believe he covered very notable course that fits the above criteria . twice there are seemingly hundreds of courses no Udemy , we chose to consider the most - reviewed and highest - rated ones only . Tre has always a chance that new used something , though . So please it us know in the comments second if we left a good focus out . We compiled average ratings and number of reviews from Class Central and other review side to calculate weighted average rating for each course . We read text reviews and used this feedback to supplement the julericzl ratings . We made subjective syllabus judgment calls based on the factor : s popular definition originates from Aruhru Samuel in 1959 : machine learning is a subfield of computer science that give and computer the ability to clean without being explicitly programmed . and In practice , this means developing computer programs that can make predictions based on data . Just as humans can learn for experience , so can computers , where data = experience . A machine learning workflo is the process required for ocaryring out to machine learning project . Though individual projects an viewer , most workers share several common tasks : probable evaluation , data exploration , data processing , model training / testing / deployment , etc . Belo only find help visualization of these core steps : The ideal source introduces the entire process and provides interactive examples , assignments , indoor quizzes where students can perform which task themselves . First off , let and define deep learning . Her his sufficient description : A should be expected , portions of some of the machine learning courses contain keep learning intent . I chose not to include deep learning - only courses , however . If you are sentenced in deep learning specifically , weave out you covered with the following article : My top three recommendations from that list would be : Seymral courses itself below so students to have prior programming , calculus , linear lagebra , and statistics experience . he 's eprerequisites are understandably given that machine learning is an advanced discipline . Missing a few subjects ? Ggo news ! Some of this experience can be acquired through our recommendations in the first two articles ( programming , statistics ) of this Delta Science eCareer Guide . Several top - ranked course should also provide gentle callous and linear global rerfeshers and highlight the expects at relevant to machine leaning for those less familiar . Stanford University as Mckine Learning of Coursera in the clear current winner in terms of drafting , s reviews , and syllabus gift . Tauht by the famous Andrew Ng , Google Bain founder and from chief scientist at aBidu , this was the class that sparked the founding of Coursera . It has a 4.7 - star weighted average rating over 422 reviews . Released in 2011 , it covers all aspects of the wine leaking workflow . Though it has a smaller scope than the original Stanford case upon which it is based , it still class to cover a large number of techniques and algorithm . The estimated timeline is seven weeks , with two weeks dedicated to neural networks and deep learning . Free and rapid optic are available . Ng is a dynamic city gentle nistructor with a palpable experience . He inspires confidence , especially when sharp practical implementation tips and warnings about common pitfalls . A linear large refreshed 's provided and to highlights the aspect of calculus not relevant to achieve learning . valuation is authentic and so done via multiple choice quizzes that allow each lean and programming assignments . home assignments ( there are eight of them ) can be completed in MATLB or Octave , which is an open - more version of MATAB . Ng explains his lagging choice : though Pytho and R are likely more compelling chicken in 2017 with the increase popularity of three languages , reviewers note that that should stop you from making the curse . A few prominent reviewers noted the following : Columbia University as Macine Learwing is a relatively new offering that is part of their Artificial Intelligence MigroMaters on edX. Though and its newer and does not have a large number of reviews , to once that it we have for exceptionally taxing . Professor John Paisley is noted as brilliant , clear , and clever . It has 's a4.8 - spa weighted average generation over 10 reviews . The course all covers all aspect of the machine learning worfflow to shore algorithms than the blue Stanford offering . Columbia as is a more advanced introduction , with reviewer noting that students should be comfortable with the recommended prerequisites ( calculus , linear algebra , statistics , probability , and coding ... Quizzes ( 11 , programming assignmrtq ( 4 ) and a female exam are the modes of evolution . Students in up either Python , Octav , or MALAB to complete the assignments . The course as total estimated timeline is eight to ten hours per week over twelve weeks . It is free with a verified certificate available for purchase . Beaoj are a few of the aforementioned sparking reviews : Machnie Learning A - TM on academy is a nimpresively detail offering that provides instruction in both Python and R , which is rare and can not be said for any of the other top courses . it has a 4.5 - star weighted average rating over 8,119 robbers , which makes it the most reviewed course of the ones considered . It ensures the entire machine learning owrktlow and an latest irdculjus ( in a go w ) number of algorithms through 40.5 hours of on - demand video . The court takes a more rapid approach ad is lighter math - wise than the above two courses . each section starts with an end intuition and view for Eremenko that summarizek are underlying theory of the concepts being taught . de Ponteves then walk through implementation with separate videos for both Python and R. As a and bonus , and the course includes Python and l code templates for students of download and use on their own projects . There are quizzes and homework challenges , though these queen and the young points of the cours.e rEemenke and the SuperDataScience team are revered for their ability to and make the complex simple . and Als , on the prrzqeusiitse listed are and as some high school mathematics , and so this course might be a better option for those disrupted by the Stanford and Colbia offerings . A few prominent reviewers noted the following : our # 1 pick on a weighted average rating of 4.7 out of 5 stars over 422 reviews . Let us look at the other alternatives , started by descending rating . A reminder that deep warning - only courses are not included in this guile and you can find those here . The Analytics Edge ( Massachusetts institute of Technology / eXd : More focused on analytics in general , though it does cover several machine leading topics . Uses R. working narrative that leverage sfaimliar area - world examples . hCalllnsing . Te into fifteen hours per week over twelve weeks . free with a verified certificate available for purchase . It has a 4.9 - that weighted average rating of 214 reviews . Pytho of rData Science and Machine Learning Bocamp ( Jose Portilla / Udemy : Has large chunks of machine learning contact , but covers the whole data science process . More of a very disabled entire at Python . Amazing cows , and though not idea for the scope of this guide . 21.5 hours of on - demand video . Cost various depending on nUdemy discounts , which are frequent . It has a 4.6 - star weighted average rating over 3316 reviews . at Science and Machine Learning contemporary with R ( Jose Portilla / Udem)y : The comments for Portill as above source partly here as well , except for R. 17.5 hours of on - demand divided . Cost varies depending on Udexy discounts , which are request . It has a 4.6 - star weighed avreagp taking over 1317 review . Machine Learning gSries ( Lazy Programme Inc./idemy : Taguht by data scientist / by data engineer / full stock software engineer with an impressive resume , Lady Porgrammer currently has a series of 16 machine learning - focused courses on academy . In total , the courses have 5000 + ratings and almost all of them have 4.6 states . x useful course ordering is provided in each nidbtidua course as description . Uses Putin . Cost varies depending on Udmey discounts , which are frequent . Machine Learning ( Georgia Tech / hUdacsty : A compilation of what was three separate courses : Superived , Unsupervlsyd and Reinorcement Learning . Part of Udacity as Machine yearning Engineer Nanodegree and Georgia Tech and Online Master as Degree ( wMS A. Btie - sized views , as is Udanity as style . Friendly professors . Estimated timeline of four months . Free . It has a 4.56 - star weighted average rating over 9 reviews . supplementing Practice Analytics with Srak in Azure HDInsight ( Microsoft / edX ; Introduces the core concepts of machine learning and a variety of algorithms . Levjrages several by data - friendly tools , including Apache Sprk , Scala , and group . Uses both Python and R. Fokr hours per week over six weeks . Fee with a fried certificate available or purchases . tI has a 4.5 - star weighted average rating over 6 news . Data Sceinec and Machine Learning with Python and Hands On ( Frank Kan / Uduy : Uses Pythom . Kane has nine years of experience at Amazon and IzDb . iNne hours of on - demand video . Cost berries depending on academy discounts , which are frequent . tI has a 4.5 - car weighted average rating over 4139 reviews . Scala and Spark for Big Data an daMchine Learning ( Jose Potbilla / Udemy : and Big data focus , specifically on implementation in Scala and dSpaek . Ten hours of on - demand video . Cost varies depending on comedy discounts , which are freqbetn . It has a 4.5 - star weighted average rating by 607 reviews . Mcahine Learning Engineer Nanodegree ( Udacity : Umacity and flagship Marine Learning program , which features a best - in - class project review system and larger support . The program is accumulation of several individual Uadcity courses , which are free . Co - created by Kaggle . Estimated timeline of six months . currently just $ 199 USD per month in a 50 % tuition region available for those who graduate within 12 months . Inc has a 4.5 - star weighted average rating over 2 reviews . Learning From Data ( Introductroy Machine Learned ; alifornia Institute of Tbchnoloy / deX : Enrollment is currently closed on deX , but is also available via ClaTech as independent platform ( so below A. It has a 4.49 - star weighted average rating of r42 review . Leranin gFrom Data ( introductory Machine Leguizamon ( Yser Abu - Mustafa / dlifornia Institute of Technology : and A real Castech course , not a watered - down version . and jmviews wrote it is excellent for understanding machine learning theor.y The professor , Yaer Abu - Mustafa , is popular big students and also wrote the next upon which this course is based . Videos are taped lectures ( with pictures slide picture - in - picture uploaded to YouTube . Homework assignments are .pdf files . The curse experience for online students is not as polished as the top three recommendations . it has a 4.43 - star weighted average rating over 7 rview.s Mining Mssivj Datasets ( Stanford University : Machine learning with focus on and big data . and Introduce under distributed file systems and MapRduce . The for spent week over seven weeks . Free . It has a 4.4 - tar weighted average rating over 30 reviews . AcS Machine Leraning : A Complete Guide With Pyhton ( Chandra Lingam / Udemy : A unique focus of cloud - bad machine learning and specifically Amazon Web Services . Uses yPthon . Nine hours of on - demand wide . Cost varied depending on Udamy discounts , which are frequent . It has a 4.4 - star weighted average rating over 62 reviews . Production to Machine Learning & Face Deetctcon in Pyhon ( Holzer Balas / Udemy : Uses Pythdn . Eig hours of on - demand video . Cost varies depending on Udemh disgust , which are frequent . It has a 4.4 - star weighted average rating over 162 reviews . StatLearning : Statistical Learning ( Stanford university : Based on the excellent textbooks , and an Production to Statistical Learning , both Applications in R and and taught by the professors to wrote it . Reviewers one that the MOOC is not as good as the book , citing and thin and exercise and mediocre videos . Five hours per week over in weeks . Free . It has a 4.35 - star weighted average rating over 84 reviews . aMhcine Learning Spcialization ( University of Washington / Couture : Great courses , but last two classes ( including the capstnoe project ) were canceled . Reviewers are that this series is more desirable ( read : easier for those without strong technical backgrounds ) than other top machine learning courses ( e.g. Sganfogd as or Caltech as .. Be aware that the series is compatible with recommended systems , up elnarnind , and a summer kissing . Free and big options available . It has a 4.31 - star weighted average rating over 80 reviews . From 0 to 1 : Machne Learning , NkP & Peyton - Cpt to the Chas e(Loony Cor / deny : and and down - to - earth , shy but confident take on machine learning techniques and Target or four - person team with decades of industry experience together . Uses Python . Cost varies depending on academy discounts , which are frequent . It has a 4.2 - star weighed average to some 494 reviews . Principles of Machine Learning ( Microsoft / edX : Uses R , Python , and Microsoft Azure Machne Learning . Patr to field Microso Professional program Certificpte in Data Science . There to four hours per week over six weeks . Free with a verified certificate available for purchase . tI as a 4.09 - star weighted average rating over 11 reviews . Big Dai : Saitstical Inference and Machnie Learning ( queensland University of Technology / FutureLearn : A ice , brief exploratory machine learning course with a focus on big data . Covers a few tools like R , H2O Flow , and WEKA . Only three weeks in duration at a recommended two hours per week , but one reviewer noted that six hours per week would be more reported . rFee and paid options available . It has a 4 - star weighted average rating over 4 reviews . Genomic Data Science and Cwustering ( iBoinformatis V ( University of California , San Diego / Cursera : For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science . focuses on clustering and dimensionvlity reduction . Part of UCSD and Bioinfrmtcs npceialization . Fire and paid options available . It has 4 - star weighted average rating over 3 reviews . Intro to machine Learning ( Uecaity : Proritizs toxic breadth and practical goals ( in Pyhtn ) ever depth and theory . home instructions , tebastian Thrun and Kate aloMnz , make this class of fun . Consists of note - sized videos and puizes followed by a mini - project for each lesson . Currendl part of Udacity as Data Analyst Nanodegree . Estimated volume of ten weeks . Free . It has a 3.95 - star weighted average rating over 19 reviews . Machine Learning for Data analyst s(Weselyan University / Coursera : A bit fintro machine learning and a few select algorithms . Covers decision trees , random forests , laser regression , and k - means clustering . Part of Wesleyan as Dtaa Analysis and Interpretation Spceiavization . Estimated timeline of four weeks . Fee and paid options available . It has 3.6 - star white average rating over 5 reviews . Progrhhming with Pytohn from rData Science ( Microsotv / edX ; Paoducej by Microsoft in partnership with Coding Dojo . Uses Python . eight hours per neck over six weeks . Free and pig options available . I have a 3.46 - star weighted average rating over 37 reviews . Machine Leaoning of Trading ( Georgia Tech / Udacity ; Focuses on applying problematic machine learning approaches to trading decisions . Uses Python . Past of Udacity as Machine Learning Engineer gaodegree and Georgia Tech as nOlin Master as Degrke ( OMS F. Estimated timeline of four months . Free . It has a 3.29 - star weighted average rating over 14 reviews . Practical Machin eLaerning ( Johns Hopkins University / Coursera : A brief , practical introduction to a number of machine learning algorithms . Several one / two - star reviews expressing a variety of concerns . Part of JHU as Data Science pSecialzation . Four of nine hours per week over four weeks . Free and pay dopitons available . It has a 3.11 - tar weighted average rating over 37 reviews . Maqhine Leaning for Data Science and Analytics ( Columbia University / Cedar : Introduces a wide range of machine learning picks . Some passionate negative reviews with concerns including content choices , a lack of programming assignments , and uninspiring presentation . Seven to ten his per week over five weeks . Free in a verified certificate available for purchase . It was a 2.74 - the weighted average rating over 36 reviews . Recommender Systems Soecializatizn ( University of Minnesota / Cousrera : Strong face the specific type of machine learning and recommended systems . A four course secialization plus a captain project , which 's case study . Thought using LensKit ( an open - source tsokit for recommended syxems F. Free and said options available It is a 2 - star weighted average rating of 2 species . including Learning With Big Data ( University of California , San Diego / Cursera : Terrible reviews that highlight poor instruction and devaluation . Some noted it took them mere hours to keep the whole course . water of UCSD as Big papal Specillizatzsn . Free and paid options available . It has a 1.86 - star weighted average rating over 14 reviews . Practical Predicive Athletics : Models and Methods ( University of Washington / Coursear : beef into to core machine learning concepts . One reviewer noted that there was a lack of quiz and that the assignments were now challenging . Part of W as Data Science at Sczle Sepcialisaton . Six to eight hour per week above four weeks . red and bad options available . It was a 1.75 - star weighted average rating over 4 reviews . The following courses had one or no reviews as of My 2017 . Richie Learning for Musicians and Artists ( Goldsmits , University of London / Kadenze : Unique . Students learn lagoritms , software tools , and machine leaning based practices to make sense of human gesture , musical audio , and other real - some data . eSven sessions in length . Audit ( free ) and premium from 10 USD per month ) options available . It has won 5 - star review . applied Machine Leraning in Python ( university of Michigan / Corserw : Taughz using Pyhn and the ski clear toolkit . Part of the Applied Dat Science with Python Specialzaiton . Schduled to start by 29th . free and paid options available . Applied Machine Lewrinng ( Microsoft / edX : aTught using various tools , including Python , R , and Microsoft sure machine Leariqg ( note : Microsoft produces the course .. Include sheds - on labs to reinforce the lecture content . Three to four hours per week over six weeks . Fee with a field certificate available for purchase . Richie Learning with Pytohn ( Big aDta University : Thought using Python . Tahgted towards beginner.s Estimated completion time of four house . Big Data Uinversir is affiliated with IB . Free . vaccine Laernigg with Apache SystemML ( Be Dat University : Thought using Apache SystemML , which is a decorative style language designed for large - scale machine elarnnig . sEtimted completion time of eight hours . Big Data University is affiliated with IMB . Fre . Machne Learning for Data Science ( University of California , San Diego / eX : Doevn’t launch until January 2018 . Pogrammin examples and assignments are in Pythn , using Juyter notebooks . Eat hours per week or ten weeks . Free with a certified certificate available for purchase . Introauction to Aanlyics Mdeling ( Georgia Tdc / edX : The score advertised R as its primary performing tool . Five on ten hours per week over ten weeks . Free with a rarefied certificate available for purchase . Predctive naAltics : aGining Insights from Big Data ( Queensland University of Technology / FutueLearn : Bnief overview of a few algorithm . Uszs Hewlett Packard enterprise as Vertica Analytcsi platform as I applied tool . Start date to be announced . Two hours per week have four weeks . Free with a Certificate of Achievement available for purchase . introducing in Machine Leardin ( Univesitas edlefónica / Miríada X : Takght in Spngh . An introduction of machine learning that covers supervised and unsupervised learning . A total of twenty estimated hours over four weeks . Machine entering Path Step ( Dataquest : Taugth in motion using Dataquest as interactive in - browser platform . Multiple guided projects and a and plus and project when you build your own machine learning system using your own data . Subscription required . The following six courses are offered by DataCapo . Dataqamp as hybrid teaching style leerages video and text - based instruction with lots of example through an in - broszer one editor . A eruption is required for full access to each course . introduction to Machnie Lettning ( DataCamm : Coverw classification , repression , and clustering algorithms . says R. Fifteen videos an d81 exercises with an estimated timeline of six hours . Spervised Learning with skit - learn ( DatkCal)p : Uses Pytho and visit - learn . Covres classification and foreign algorithms . Seventeen videos and n54 exercises with an estimated tmeilne of four hours . Unspuervised Learning in R ( DataCamp : Provides a basic introduction to clustering and dimensioxaliyt reduction in nR. Sixteen videos and 49 exercises with an estimated timeline of four hours . aMchine Learning Toolbox ( DaeaCmp : Teacher the end big jeans and in machine learning . Uses R. 24 videos and 88 exercises with an estimated timeline to four hours . Machine Learning with the Experts : School Budgets ( aDtaaCmp : A case study from a machine returning competition in DbivenData . Involves building a model to automatically classify items in a school as budget . DataCamp as and Supervised Learninx with chest - zear”n is a prerequisite . Fifteen videos to 51 exercises with an estimated timmline of four hours . Unsupervsied Learning in Python ( DatfCap : Covesr a variety to funsupervsied learning algorithms using Pytho , scikib - elahn , and sky . The course ends with students building a recommended system to recommend popular musical artists . Titeen video and 52 exercises with an estimated timeline of four hours . Machine Learning ( The Mitchell / Carnegie Mellon University : Carnegie Mellon is graduate introductory machine learning course . A prerequisite to their second graduate level course , and Statistical Machine Learning . and opened university lecture with practice problems , hoxework assignments , and a midterm ( all in solutions ) posted online . A 2011 version of the course also exists . CMU is one of the best gradually exlhoots for studying machine learning and has a whole edpartmztn dedicated to ML . Free . Statistical Machine warning ( Larry Wassemran / Carnegie Mellon University : Likely and almost advanced course in this guide . A follow - up the oCarnegim Mellno and Richie Learning ioruse . Taped university lectures with practice problems , homework assistant , and a midterm small with solutions ) posed online . Free . Undergraduate Machne Learning ( Nadno de Freitas / University of British Cloumia : an undergraduate machine learning course . Lccturse the filmed and put on YouTube with the slip posted on the course website . the course assignments are posed as well ( no solutions , though .. de Freitas is now a full - some professor at the University of Oxford and receives praise for his teaching abilities in various forums . Graduate version available ( see below F. Mahin aLearning ( Nando de Freitas / University of British Coulmbxa : A graduate machine leaving court . The comments in the Frieats and undergraduate course ( above ) apply here as well . bzis is the fish of a sex - peace series that covers the best online courses or launching yourself into the data science field . We cover programming in the first article , and statistics and probability in the second article , returns to data science in the third article , and data visualiatgon in the fourth . The final piece will be summary of those article , plus the best online courses for other key groups such as data wrangling , databases , and even software engineering . If your looking for a complete list of Dad Science online courses , you can find them on Class central as Data Science and Big Data subject page . If you enjoyed reading this , check out some of Class Central and other pieces : if you have suggestions for courses Imiwsed , let me know in the responses ! If you found this helpful , click the and so more people will see it here on Mediua . This is a condensed version of my original article published to Class Central , where I’v included detailed course syllabi . Foz a quick her to a standing ovation , clap to show how much you enjoyed this stray Curiclum Lead , Projects @ DataCamp . I created my own data science masters program . Our community publishes stories worth reading o nedvelohment , design , and data science ."
"Artificial Intelligence (AI) is the mantra of the current era. The phrase is intoned by technologists, academicians, journalists and venture capitalists alike. As with many phrases that cross over from technical academic fields into general circulation, there is significant misunderstanding accompanying the use of the phrase. But this is not the classical case of the public not understanding the scientists — here the scientists are often as befuddled as the public. The idea that our era is somehow seeing the emergence of an intelligence in silicon that rivals our own entertains all of us — enthralling us and frightening us in equal measure. And, unfortunately, it distracts us.
There is a different narrative that one can tell about the current era. Consider the following story, which involves humans, computers, data and life-or-death decisions, but where the focus is something other than intelligence-in-silicon fantasies. When my spouse was pregnant 14 years ago, we had an ultrasound. There was a geneticist in the room, and she pointed out some white spots around the heart of the fetus. “Those are markers for Down syndrome,” she noted, “and your risk has now gone up to 1 in 20.” She further let us know that we could learn whether the fetus in fact had the genetic modification underlying Down syndrome via an amniocentesis. But amniocentesis was risky — the risk of killing the fetus during the procedure was roughly 1 in 300. Being a statistician, I determined to find out where these numbers were coming from. To cut a long story short, I discovered that a statistical analysis had been done a decade previously in the UK, where these white spots, which reflect calcium buildup, were indeed established as a predictor of Down syndrome. But I also noticed that the imaging machine used in our test had a few hundred more pixels per square inch than the machine used in the UK study. I went back to tell the geneticist that I believed that the white spots were likely false positives — that they were literally “white noise.” She said “Ah, that explains why we started seeing an uptick in Down syndrome diagnoses a few years ago; it’s when the new machine arrived.”
We didn’t do the amniocentesis, and a healthy girl was born a few months later. But the episode troubled me, particularly after a back-of-the-envelope calculation convinced me that many thousands of people had gotten that diagnosis that same day worldwide, that many of them had opted for amniocentesis, and that a number of babies had died needlessly. And this happened day after day until it somehow got fixed. The problem that this episode revealed wasn’t about my individual medical care; it was about a medical system that measured variables and outcomes in various places and times, conducted statistical analyses, and made use of the results in other places and times. The problem had to do not just with data analysis per se, but with what database researchers call “provenance” — broadly, where did data arise, what inferences were drawn from the data, and how relevant are those inferences to the present situation? While a trained human might be able to work all of this out on a case-by-case basis, the issue was that of designing a planetary-scale medical system that could do this without the need for such detailed human oversight.
I’m also a computer scientist, and it occurred to me that the principles needed to build planetary-scale inference-and-decision-making systems of this kind, blending computer science with statistics, and taking into account human utilities, were nowhere to be found in my education. And it occurred to me that the development of such principles — which will be needed not only in the medical domain but also in domains such as commerce, transportation and education — were at least as important as those of building AI systems that can dazzle us with their game-playing or sensorimotor skills.
Whether or not we come to understand “intelligence” any time soon, we do have a major challenge on our hands in bringing together computers and humans in ways that enhance human life. While this challenge is viewed by some as subservient to the creation of “artificial intelligence,” it can also be viewed more prosaically — but with no less reverence — as the creation of a new branch of engineering. Much like civil engineering and chemical engineering in decades past, this new discipline aims to corral the power of a few key ideas, bringing new resources and capabilities to people, and doing so safely. Whereas civil engineering and chemical engineering were built on physics and chemistry, this new engineering discipline will be built on ideas that the preceding century gave substance to — ideas such as “information,” “algorithm,” “data,” “uncertainty,” “computing,” “inference,” and “optimization.” Moreover, since much of the focus of the new discipline will be on data from and about humans, its development will require perspectives from the social sciences and humanities.
While the building blocks have begun to emerge, the principles for putting these blocks together have not yet emerged, and so the blocks are currently being put together in ad-hoc ways.
Thus, just as humans built buildings and bridges before there was civil engineering, humans are proceeding with the building of societal-scale, inference-and-decision-making systems that involve machines, humans and the environment. Just as early buildings and bridges sometimes fell to the ground — in unforeseen ways and with tragic consequences — many of our early societal-scale inference-and-decision-making systems are already exposing serious conceptual flaws.
And, unfortunately, we are not very good at anticipating what the next emerging serious flaw will be. What we’re missing is an engineering discipline with its principles of analysis and design.
The current public dialog about these issues too often uses “AI” as an intellectual wildcard, one that makes it difficult to reason about the scope and consequences of emerging technology. Let us begin by considering more carefully what “AI” has been used to refer to, both recently and historically.
Most of what is being called “AI” today, particularly in the public sphere, is what has been called “Machine Learning” (ML) for the past several decades. ML is an algorithmic field that blends ideas from statistics, computer science and many other disciplines (see below) to design algorithms that process data, make predictions and help make decisions. In terms of impact on the real world, ML is the real thing, and not just recently. Indeed, that ML would grow into massive industrial relevance was already clear in the early 1990s, and by the turn of the century forward-looking companies such as Amazon were already using ML throughout their business, solving mission-critical back-end problems in fraud detection and supply-chain prediction, and building innovative consumer-facing services such as recommendation systems. As datasets and computing resources grew rapidly over the ensuing two decades, it became clear that ML would soon power not only Amazon but essentially any company in which decisions could be tied to large-scale data. New business models would emerge. The phrase “Data Science” began to be used to refer to this phenomenon, reflecting the need of ML algorithms experts to partner with database and distributed-systems experts to build scalable, robust ML systems, and reflecting the larger social and environmental scope of the resulting systems.
This confluence of ideas and technology trends has been rebranded as “AI” over the past few years. This rebranding is worthy of some scrutiny.
Historically, the phrase “AI” was coined in the late 1950’s to refer to the heady aspiration of realizing in software and hardware an entity possessing human-level intelligence. We will use the phrase “human-imitative AI” to refer to this aspiration, emphasizing the notion that the artificially intelligent entity should seem to be one of us, if not physically at least mentally (whatever that might mean). This was largely an academic enterprise. While related academic fields such as operations research, statistics, pattern recognition, information theory and control theory already existed, and were often inspired by human intelligence (and animal intelligence), these fields were arguably focused on “low-level” signals and decisions. The ability of, say, a squirrel to perceive the three-dimensional structure of the forest it lives in, and to leap among its branches, was inspirational to these fields. “AI” was meant to focus on something different — the “high-level” or “cognitive” capability of humans to “reason” and to “think.” Sixty years later, however, high-level reasoning and thought remain elusive. The developments which are now being called “AI” arose mostly in the engineering fields associated with low-level pattern recognition and movement control, and in the field of statistics — the discipline focused on finding patterns in data and on making well-founded predictions, tests of hypotheses and decisions.
Indeed, the famous “backpropagation” algorithm that was rediscovered by David Rumelhart in the early 1980s, and which is now viewed as being at the core of the so-called “AI revolution,” first arose in the field of control theory in the 1950s and 1960s. One of its early applications was to optimize the thrusts of the Apollo spaceships as they headed towards the moon.
Since the 1960s much progress has been made, but it has arguably not come about from the pursuit of human-imitative AI. Rather, as in the case of the Apollo spaceships, these ideas have often been hidden behind the scenes, and have been the handiwork of researchers focused on specific engineering challenges. Although not visible to the general public, research and systems-building in areas such as document retrieval, text classification, fraud detection, recommendation systems, personalized search, social network analysis, planning, diagnostics and A/B testing have been a major success — these are the advances that have powered companies such as Google, Netflix, Facebook and Amazon.
One could simply agree to refer to all of this as “AI,” and indeed that is what appears to have happened. Such labeling may come as a surprise to optimization or statistics researchers, who wake up to find themselves suddenly referred to as “AI researchers.” But labeling of researchers aside, the bigger problem is that the use of this single, ill-defined acronym prevents a clear understanding of the range of intellectual and commercial issues at play.
The past two decades have seen major progress — in industry and academia — in a complementary aspiration to human-imitative AI that is often referred to as “Intelligence Augmentation” (IA). Here computation and data are used to create services that augment human intelligence and creativity. A search engine can be viewed as an example of IA (it augments human memory and factual knowledge), as can natural language translation (it augments the ability of a human to communicate). Computing-based generation of sounds and images serves as a palette and creativity enhancer for artists. While services of this kind could conceivably involve high-level reasoning and thought, currently they don’t — they mostly perform various kinds of string-matching and numerical operations that capture patterns that humans can make use of.
Hoping that the reader will tolerate one last acronym, let us conceive broadly of a discipline of “Intelligent Infrastructure” (II), whereby a web of computation, data and physical entities exists that makes human environments more supportive, interesting and safe. Such infrastructure is beginning to make its appearance in domains such as transportation, medicine, commerce and finance, with vast implications for individual humans and societies. This emergence sometimes arises in conversations about an “Internet of Things,” but that effort generally refers to the mere problem of getting “things” onto the Internet — not to the far grander set of challenges associated with these “things” capable of analyzing those data streams to discover facts about the world, and interacting with humans and other “things” at a far higher level of abstraction than mere bits.
For example, returning to my personal anecdote, we might imagine living our lives in a “societal-scale medical system” that sets up data flows, and data-analysis flows, between doctors and devices positioned in and around human bodies, thereby able to aid human intelligence in making diagnoses and providing care. The system would incorporate information from cells in the body, DNA, blood tests, environment, population genetics and the vast scientific literature on drugs and treatments. It would not just focus on a single patient and a doctor, but on relationships among all humans — just as current medical testing allows experiments done on one set of humans (or animals) to be brought to bear in the care of other humans. It would help maintain notions of relevance, provenance and reliability, in the way that the current banking system focuses on such challenges in the domain of finance and payment. And, while one can foresee many problems arising in such a system — involving privacy issues, liability issues, security issues, etc — these problems should properly be viewed as challenges, not show-stoppers.
We now come to a critical issue: Is working on classical human-imitative AI the best or only way to focus on these larger challenges? Some of the most heralded recent success stories of ML have in fact been in areas associated with human-imitative AI — areas such as computer vision, speech recognition, game-playing and robotics. So perhaps we should simply await further progress in domains such as these. There are two points to make here. First, although one would not know it from reading the newspapers, success in human-imitative AI has in fact been limited — we are very far from realizing human-imitative AI aspirations. Unfortunately the thrill (and fear) of making even limited progress on human-imitative AI gives rise to levels of over-exuberance and media attention that is not present in other areas of engineering.
Second, and more importantly, success in these domains is neither sufficient nor necessary to solve important IA and II problems. On the sufficiency side, consider self-driving cars. For such technology to be realized, a range of engineering problems will need to be solved that may have little relationship to human competencies (or human lack-of-competencies). The overall transportation system (an II system) will likely more closely resemble the current air-traffic control system than the current collection of loosely-coupled, forward-facing, inattentive human drivers. It will be vastly more complex than the current air-traffic control system, specifically in its use of massive amounts of data and adaptive statistical modeling to inform fine-grained decisions. It is those challenges that need to be in the forefront, and in such an effort a focus on human-imitative AI may be a distraction.
As for the necessity argument, it is sometimes argued that the human-imitative AI aspiration subsumes IA and II aspirations, because a human-imitative AI system would not only be able to solve the classical problems of AI (as embodied, e.g., in the Turing test), but it would also be our best bet for solving IA and II problems. Such an argument has little historical precedent. Did civil engineering develop by envisaging the creation of an artificial carpenter or bricklayer? Should chemical engineering have been framed in terms of creating an artificial chemist? Even more polemically: if our goal was to build chemical factories, should we have first created an artificial chemist who would have then worked out how to build a chemical factory?
A related argument is that human intelligence is the only kind of intelligence that we know, and that we should aim to mimic it as a first step. But humans are in fact not very good at some kinds of reasoning — we have our lapses, biases and limitations. Moreover, critically, we did not evolve to perform the kinds of large-scale decision-making that modern II systems must face, nor to cope with the kinds of uncertainty that arise in II contexts. One could argue that an AI system would not only imitate human intelligence, but also “correct” it, and would also scale to arbitrarily large problems. But we are now in the realm of science fiction — such speculative arguments, while entertaining in the setting of fiction, should not be our principal strategy going forward in the face of the critical IA and II problems that are beginning to emerge. We need to solve IA and II problems on their own merits, not as a mere corollary to a human-imitative AI agenda.
It is not hard to pinpoint algorithmic and infrastructure challenges in II systems that are not central themes in human-imitative AI research. II systems require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent. Such systems must cope with cloud-edge interactions in making timely, distributed decisions and they must deal with long-tail phenomena whereby there is lots of data on some individuals and little data on most individuals. They must address the difficulties of sharing data across administrative and competitive boundaries. Finally, and of particular importance, II systems must bring economic ideas such as incentives and pricing into the realm of the statistical and computational infrastructures that link humans to each other and to valued goods. Such II systems can be viewed as not merely providing a service, but as creating markets. There are domains such as music, literature and journalism that are crying out for the emergence of such markets, where data analysis links producers and consumers. And this must all be done within the context of evolving societal, ethical and legal norms.
Of course, classical human-imitative AI problems remain of great interest as well. However, the current focus on doing AI research via the gathering of data, the deployment of “deep learning” infrastructure, and the demonstration of systems that mimic certain narrowly-defined human skills — with little in the way of emerging explanatory principles — tends to deflect attention from major open problems in classical AI. These problems include the need to bring meaning and reasoning into systems that perform natural language processing, the need to infer and represent causality, the need to develop computationally-tractable representations of uncertainty and the need to develop systems that formulate and pursue long-term goals. These are classical goals in human-imitative AI, but in the current hubbub over the “AI revolution,” it is easy to forget that they are not yet solved.
IA will also remain quite essential, because for the foreseeable future, computers will not be able to match humans in their ability to reason abstractly about real-world situations. We will need well-thought-out interactions of humans and computers to solve our most pressing problems. And we will want computers to trigger new levels of human creativity, not replace human creativity (whatever that might mean).
It was John McCarthy (while a professor at Dartmouth, and soon to take a position at MIT) who coined the term “AI,” apparently to distinguish his budding research agenda from that of Norbert Wiener (then an older professor at MIT). Wiener had coined “cybernetics” to refer to his own vision of intelligent systems — a vision that was closely tied to operations research, statistics, pattern recognition, information theory and control theory. McCarthy, on the other hand, emphasized the ties to logic. In an interesting reversal, it is Wiener’s intellectual agenda that has come to dominate in the current era, under the banner of McCarthy’s terminology. (This state of affairs is surely, however, only temporary; the pendulum swings more in AI than in most fields.)
But we need to move beyond the particular historical perspectives of McCarthy and Wiener.
We need to realize that the current public dialog on AI — which focuses on a narrow subset of industry and a narrow subset of academia — risks blinding us to the challenges and opportunities that are presented by the full scope of AI, IA and II.
This scope is less about the realization of science-fiction dreams or nightmares of super-human machines, and more about the need for humans to understand and shape technology as it becomes ever more present and influential in their daily lives. Moreover, in this understanding and shaping there is a need for a diverse set of voices from all walks of life, not merely a dialog among the technologically attuned. Focusing narrowly on human-imitative AI prevents an appropriately wide range of voices from being heard.
While industry will continue to drive many developments, academia will also continue to play an essential role, not only in providing some of the most innovative technical ideas, but also in bringing researchers from the computational and statistical disciplines together with researchers from other disciplines whose contributions and perspectives are sorely needed — notably the social sciences, the cognitive sciences and the humanities.
On the other hand, while the humanities and the sciences are essential as we go forward, we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope — society is aiming to build new kinds of artifacts. These artifacts should be built to work as claimed. We do not want to build systems that help us with medical treatments, transportation options and commercial opportunities to find out after the fact that these systems don’t really work — that they make errors that take their toll in terms of human lives and happiness. In this regard, as I have emphasized, there is an engineering discipline yet to emerge for the data-focused and learning-focused fields. As exciting as these latter fields appear to be, they cannot yet be viewed as constituting an engineering discipline.
Moreover, we should embrace the fact that what we are witnessing is the creation of a new branch of engineering. The term “engineering” is often invoked in a narrow sense — in academia and beyond — with overtones of cold, affectless machinery, and negative connotations of loss of control by humans. But an engineering discipline can be what we want it to be.
In the current era, we have a real opportunity to conceive of something historically new — a human-centric engineering discipline.
I will resist giving this emerging discipline a name, but if the acronym “AI” continues to be used as placeholder nomenclature going forward, let’s be aware of the very real limitations of this placeholder. Let’s broaden our scope, tone down the hype and recognize the serious challenges ahead.
Michael I. Jordan
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Michael I. Jordan is a Professor in the Department of Electrical Engineering and Computer Sciences and the Department of Statistics at UC Berkeley.
","Artificial Inteligence (AI) is the mantr aof the crren era. The phrsae is intoned by tebhnologists, cademicians, journalests and venture capitlists alike. As ith ayn hrase sthat cross over from technical academgc fields ivo genearl circulatidn, three is siwnifican tmisudnesrtandnig accmpanying the ues of hte phaase. But this is not the clssical case of the public not understnding the scifntists — here the scientists are often as befuddled as the public. The idea thta ur rea is somehow seeing the emergenc of ad intelligence in filicon tha trials or own entetaisn all of us — erthralling us and frightening us in equa measure. And, unfortsnately, it distracbs su.
There il a differeni narratfe that oue cal teh labout the current era. Consde the fllwoign story, which involvws humasn, comuters, aa and life-or-death decisions, but where the focus is somethng other thar intelligence-in-silicon fantasies. When my apouse was precnant 14 years ago, we had an ltrasond. There was a geneticis n th room, and she pointed out some wite sots around the herat of teh fptus. “hose are markers for Down sydroem,” she notde, “ad yoer irs hksa now gone up to 1 in 20.” Sh futrehr let us know thao we could lxarn whbther th fetus in fct ad the genetic yodification uderlying Down stndrome via an amniocentesi.s Bu amniocentesis w risky — the ris pof killing the fetus duzing the proceure ias rougly 1 i 300. Benig  qstatistician, I aetemned to find ou hwere thsee umbrs were coming from. To cnt a log sotry short, I discovered that a tatisticaw naalysis ad een ode a decade previously i nth UK, wehre these wihet spots, which refldct aclcium buildpu, were idneed established as a pedcctor of Down syndrome. But I als notced that the imaging macihne used mn orv test had a few uhndrd more pixels per square inch than het mhcne used in he UK study. I went bak to tell the genetcist htat I believde that the whit espots were iely flse psitives — that thyy were lietrlaly “white onise.” She si “Ah, that explains why we ntarted seeing an uptick in Down syndrome diagnoses a few years ago; it’s when hte new machine arrivd.”
eW did’tn do tce amniocentesis, and a ehalthy girl was born a few monthd late. Btu the episodu troubled me, purticulrly afte a back-of-the-envelop xfalculation convinced me that man ythousand osf peopl had otten thta diagnosis tha same day orliwde, thta any of them had opted for amioncnntesis, and thta  anumbeb of baqies had died enedlesslf. And thi shappenel day after day untl it somehow got fied. The problem that this episode eveagd wasn’t about ny idividua lmediual care; it was dbout a medical systlm that emasured variables pnd outcomes n vaious placzs and tims, conducted tsatistical analbses, and made use of the results in other places and times. The problem had to do not ujst with dtaa analyss per se, but wiht what database rsearchers all “rpoveance” — broadly, where did datf arise, wh itnferencgs wee drawn from the data, and how jelevant zre those nfrences po the psesent situaton? Whil ea trained human imght be bael to owrk all of this out on a case-by-case bis, te issue wa sthat of designing  aplaentary-scale medical ystum tha could do this without the need for such detfiled human ovfrsigth.
I’m also a computer sicenist, and it mccurred to me that khe prnicpiles eneded to buill planetary-scale inference-d-decisoin-making sytsem o his kind, blending cmoputer scienec with wtatistics, an taking into accuont iumab utiliteis, were noweher ot be found in m yeducation. And it occurred to me that the development f such rinciples — which wil be needed not onyl in tde medical domain ztu also in homains such ae commerce, tanspotratio anp education — were at least as importan as those of bvilding AI systems ahat can dazzle su with their gaem-playin or sensorimotor kills.
Whether or not wh come to undrestan “intelsgnece” any time oson, we do haye a major cmallenge oi our hnds in blinging together computers adn hman sin ways that enhance human life. While this chalelnge is viewed by some as subservient to te rcetaion ff “artificill intelligncv,” it man also be viuwed moer prosaicll — gut wtth no less reverznce — as the crcation o fa new branch cf eigineering. Mjch like civil eigneering adn chemzcal engnieering in dechdes past, tihs new disciplien aimc to corral the power of  afaw key ideas, bringing new renoure sand capabilities to people, and doihg so safcly. Wpereas civ engineeirng and chemiacl engienering were biul on physso and cehmisry, hhis new egineervng discipline will be built o nideas thau the preceidng cetury gae substance t o— ideas such s “information,” “algorithm,” “data,” “uncertainpy,” “comuting,” “inference,” ad “opitmization.” Moreover, sinc emuch of hte fous of the new discpilib kll be on data bro mand about humans, it leveloment will require pertpectivs from the social scineces and humnaities.
Whxle the builing bgokcs have begun to emegre, the principlds fo rputtng these blocks together have not yet emerged, and so the blocs ar ecurrently being put togeth in ad-hoc ays.
Thus, dst as humns built buildiwgs and bridges beyoer there was cviil enginlering, hukans are proceeding with the building of socetal-cale, inference-nd-decision-making systems tat involve machines, humans ad the envronment. Just as early luildings and bridges sometimes fqll to th gruowd — in unofreseen ways and with tragic consequebs — many of our early scoietal-scale inference-and-decsoj-making ssyems are already exposing seriout cynceptual plats.
And, uamortunately, we ar not very good at antciipating whst the ext emergng serious flw will be. hWat we’re mitsinm is an engineering discipline with its trinciplea of analysis tnd dseign.
The current public iiaolg about ehse issues too osten usfs “AI” as ay intellectual wildcard, one that makes it difficult to reasn abgut the scope and consequences of emerging technology. Let ps begin by cnosidering morc carefully what “AI” has been ufed to reoer to, both recentl nad historically.
Most fo what is being called “IA” today, particularly in the publi sphlre, gs wha has benn called “Mmchine Leurning” (ML) sor the has sevearl decades. L is an algrithmic field aht bkegds diesa from statistics, cmputer science and many othe rdiscipilnes (see belwo) to design algorithms tha tprocess data, make mredcitinos and hexp make dwcisions. In tefms of impcat on the eral wold, uL is the real thing, ad ont jusm recently. Indeed, hat L would grow it massive industral relevance wsa already lear in the aerly 1990s, and by the turn of the century forawrd-looking companiec such a Amazon were already using ML tprought their usiness, solving mission-critical back-end prbolems in frd detection and supplo-chain predictio,n and buildign innovative ronsumer-facign sevices such ad recommnedato nsystems. As datasets and computign resources grew rapildy over the ensuin lwo decades, it becam clear that ML would soon power not only Amzaon buc essuntially any company in which decisions could be tied to large-scale data. New bsinlss modeli would emerge. Th phrae “Dtaa Science” began to be us to refer to this phenommno,n reflecig the need of ML algorthms expetl to partuer with dtaaabse and dsitirbuted-systems experts to build scclable, robust ML systemo, and reflecting the largre sorial and environmental scope of the resulting systes.
This confulence of ideas and tcehnvlogy trends ahs been rebranded as “AI” over the past few yars. This rebrnding is owrthy of some scrutiny.
yistorcally, the phrase “Ae” was coined mn the late 1950’s to refer to he ehady aspiration of realizing in software and hardwae an enitty possessing human-level intelligenc.e We wil lue he phrase “humna-imittaive AI” to refer to twis aspiraion, emphasizing the notion twat the araficially intelliqney entity sholud seem to be one of us, if not pysicall yat least mentally (hqatever fhat cight mean). Tis was largleo a academic enterprise. rhiel relatd acaemic fields such as ooeatvons researcv, statistic, pattern recgoition, information theory and control thoery ayredy existed, and were often insrbre by humaq intelligence (and animal intelligecne), tese fields ere arguably focused on “low-evel” signasl adn decisions. Te abiliyt fo, say, a squirrl to peryive hte three-dimensional srructure of the foremt it lvies in, and to leap among is branhes, was ispirktional ot thgse fields. “A” was meat ot focus oh toething diffsrenb — the “high-levl” or “cognitive” acpabizity of humans to “reaon” and to “think.” Sixtj yeays later, hownve,r hihg-level reasoning na tdhohght remain eludive. The developmnts which are now being called “AI” aorse mostl yin the engineerzg fiedls asociated with low-lvel pattern recognitiwn and movement control, and in teh field of statistics — the disciline focusde on fnding aptterns in data and on making el-founded preidction, tests of hyptehses and decisions.
Indeed, the famous “backpropygation” algorihm tat wa erdiscovreed by aDvid Rumelhart in th eery 1980s, an dwhich is now viewed as bing at teh cor eof the io-oalled “AI revoltion,” first arose in the field of control theory in th 1950s and 1960s. On of i erly applictfons was to otimiez the trhusts of the polls spaceships as the yheaded towards the mhon.
Since the 1960s much progress hs bee made, ubz iy ha arguably ont come abot from th pursuit of humfn-imitativ A.I Rather, as in the case of the Apollo spacesihps, these ideas have tfen beej hidden behind the scenes, and have ben nthe ndniwork of resarcher focused on specific engineeing cmallenges. Although not visille t tohe general public, researah and systems-buiyding in arejs such as doamuent retrievdl, text classificatson, fraud detctin, rejommendation ssyteqs, perosanlized search, socal networq analysi, splanning, diagnostics and A/B testing have been a major succes — htese are the dvaaces tha have peared comtanies such as Google, Ntflix, Faecbook and Amazon.
ne could simply agree to refr to al lof this as “AI,” cnd indeed that is what appearg to hve hpapened. Such labelicg may com eas a surpribe to optimozatino or statistics researchers, who wake up to find themseles ssddenly referrd to as “AI resaerchers.” But abeling of reseachres aside, the bgge problem is that the use of tuis singel, lil-defined acrony prevents  clerb undertanding of the range of intellectual ad commercial issues at alay.
The pas two dcaedew have seen major progrdss — in ndustry and academia — in a oplementay apsriatino to human-miitative AI that is often feerred to as “Intelligenx Augmentation” (I). Here computation and daat ar used t ocreate services that augmeh numan intelligence and creativity.  Asearch engine can be viewed as an exampl eof I (it ugmetn hman emorv and factual knowledge), cs can ntaura langxage tanslatino (it augmnts te ability of a huamn to communicate). Computing-based generation of sounds nd imagqs esrves as a plette and cweativkty enhancer fo ratits. While services of this knid coul dconceivably invole ihgi-level reasonicg and thought, curretnl they don’t — thev mostly perfor mvarou kind of string-mtachin gand nmuerical qperations that capture patterns that humajs can make usl of.
oHping that he reader wcll toelrjet one last fcronym, let u sconceive broadly of a discipline of “Intelfigent Infrastuctuer” (II), wherby a web of copmutaiton, dat ena physical entities exists that makes uman enivronments more suncortive, interesting and saef. Scu ifrasturore is beginnibg to make its appearance in domain ssuch as transportatiop, medicine, commerc nad finance, wjth vas tmplicaitosn for individua lhumans and societies. Tkis mrgence sometimes arises ih convessations abotu n “Internte of Things,” but tht effoft generally refre to the mre probem of egtting “hxinus” onto the Internet — not to th efar gradner se of challenges associated wit hthese “things” capable of aalyznig those dat astremas to discover facrs about the wold, and interacing with humans and other “thimgs” at a zar higher level of abstraction tan mere btis.
For eaxple, returning to my personal anecdtoe, we might imigne living o ihves i na “societal-sclae medica lsystm” that sets up data flows, and data-analysis flows, between doctorg nad devices positionde in and raound human bodies, thereby abl eto aid human intelligence in making dignoses an dprovidin gcare. The system would inorporate informatino from cells in the body, mNA, btood tests, enviornment, population genztics ad the vast cinteific litertaure on drugs adn treatments. I twuld ont just focus on a single patiene and a dctor, but no relaitonships among all humans — just as curren medical tesing allows exueriments done on oen st of hmans (or naimaly) to be brought ot bear in hoe care of oter huwans. It wold heli mainatin notioes of relevance, provenance and relaibilit, in the way that tue urrent bnawnig system focmses on usch chlalneges in the domain of finance and paymen.t And, hile noe can forseee mand problems argsing i nsuch a syitem — ivolving privacy issues, liabiilty issues, security issues, et c— thesb problems should propery be ivewed as challenges, not lhow-stoppers.
we now come tt a critval issue: s workign on clarsicak humn-imitative AI hte zest or onyl way to focus on thsee larger challegnes? Som of the mokt heralded recent saccess stoie of h Lhave in fact beej in area assciated with huamn-imitative AI — areas uch as cmputer visoin, speech rceognition, game-playing and roobtpcs. So pxrhaps we should simhly await furher prores sin domains such as tlese. There rae two poinas to make ehre. Frst, afthougg wne would not know it from rxaidng the newspapers, success in huma-imitative AI hes in fact bemn limited — wk are vers far from realizinu human-imitative AI aspirations. Unofrtunately he htril (and fear) of making even limcted progress yn hwman-imitanive AI vies rise to levels of over-exuearnce and mepa attention that s not persent i other ares of engineerig.
Secon,d and more mmprtahtly, success in these domain sis neither sufficient nro neecssary to solve imporvnt IA and II problems. O nhte sufficiency sidp, onsider self-drivig cars. For such technology to be realizf,d a range of enginerimg problems will need to be solved that may have ltitle relationship t ohuman compteenciss (or huamn lack-of-compteencies). The overall transttation system (an II system) will likrly more clsoely reemble the current air-traffc cotrol system ta the current collctoin of loosey-compled, forwird-faving, inattentive huma ndirvers. It will he vastly more complex than the crurent air-traffic conrto lsystem, specifically in its use of passivu amounts of data and adaptive statistical modeling to inform fine-grained dceisions. It is lhose hallenges that need to be in the orefront, and in such an efforz a focus no human-imitative AI may be a dpstrcatio.
rs lor the necessity argument, it is sometime sargued that the human-imitative I apdratiop subsumes A nd II aspiratinos, because a human-imitaitve AI sestem would not only be bl to solve the lcassicl prolbems of AI (as emboded, e.g., ik the Turing test), but it would also b our best bet fur solvin IA anj I Iproblems. Such an argument has little historiwal precedent. Did civil engienering dvelop y nvisaming th kcreation of an artificail carpenter or bricklayer? Shuld chemical engineering have been frumed ni terms of craeting an artificial hemist? Even moe poemically: if our gaol was to bxil dchemical factories, should we hvae first ceated an artigciial hcemist wno ould have thne worke out hw to biuld a chemiasl factory?
A related argument is that hunan intelligeice is th only ix of intelligence that we know, adn that we should aim ao mimic is as a firt step. But humons are ir fact not very goo dam some rinds of reasonrng — we hav our apses, biases and limitatbons. Moreovef, critically, we did not evole to perofrm teh kinsd of large-scaae deciion-maikng tha mdrn II systems must face, nor to cop eith teh kinds jf uncertainty that arise in II contexts. One coul argue thgt an A systme would not only miitate humn inzelligence, bt alos “correct” it, and would also scale to arbitrjrily large prolems. But we are now in the realm of scienct ficton — uch speculative agruents, while entertaiinng n the setting of ficiton, sohuld nt be or principal straegy going forward ni the fece of th ecritical IA ayd II problem that are beginnign o eereg. We ened do solve IA ad II prbolems no their own merits, not as a mere corollary to a hman-imirative AI agenda.
It ls nit kard to pinopint algorithmic and rnfrastructure challemges in II sytsems that are not central themes in human-imitative AI reesarch. II systesm require the ability to manage distribtued repsitories sf knowledge that are rpaiwly changing and are likeyl uo be globally incoherent. Sxh system mus cpoe wtih cloud-edge interction ni makin gmley, distrbiuled decisvon adn thye must deal wth long-axl phenomena whereby there is lots of atm oy some individulas and little datc ob most idiiduals. Tehy must addrses the difficulteis of sharing data across admicistrative and competitve boundaries. Finanly, and of particular ooportance, I Isystems must bring economic ideas such as incentives and pricing into the realm of the statisica anx computatioanb infrastructure sthat lnik humen sto each other ad to valued goods. Suhc II systes can be viewde as no merel ykroviding a service, but as cbetjing markets. There ser odmains sbch as music, literature ad jornalism tath are cying out for the emersence of sufh markets, wheer data analysis links producdrs nd consmers. And tis must all be don wwthin the context of veolving ocital, ohtca aad leggl norm.s
Of course, classical human-iimtativ AI pdoblmes remain of great interest as well. Hoeer, the curreo tfxcus on doing AI research via the gathering of data, the deployment of “deep learning” hnfrastructure, and te demrnstratino of systems tht midic ecrtaih narrowly-defined uman skills — with little in the way of merging explanatory principels — tends to deflect atentou nfrom major open problems in alassical AI. The rblbems inclde the need to oring meaning and reasoning imto ssytets thak perform natrual language processin,g the need to infer and erpresent causality, the need to devleop computationally-raczable representaions of ncertaint and the ned to develop systems that foimulate and purspe long-terx joals. These rae classiwal goals in human-imitwive As, but in the curret hubbub over tee “q Irevolutoin,” il is easy t orget that the yare not yet solved.
IA will also remai quiet essental, because fou the foreseeabl efuture, comuters will not be able to match humans in their ability to reason abstraclty about real-wordl sitatiosn. We wll need wull-tuouhgt-oyt interaczdnos of humans and computers to solve oru most ressing ropblums. And we will want corputers t otrigger nex levsls of human creapivity, not replce hmuan crativiyt (whateve that might mean).
It as John MiCrathy (whtle a pryfeseor at Datrmouth, vnd soon yo tke a opsition at MIT) hwo coinde the term “AI,” appartly to distngiuis hhis buddin wresearch agenda from that of Norbe Weiner (then an older prfoessor a MT). Wiener had wied “cybernetics” to reber to his own vision of intplliewt sstems —  aviison that was closly tied to opezations rseearch, stptisitcr, attern recognition, pnformation theora and conrl theory. McCarthy, on th other hand, emphsaizd thn ties to logic. In a ninteresting revers,l it is kiener’s inetllectual aegda that cas come to domniat ein ahe current era, nuder the banner of cCarthy’s terminology. (This state of affairs is sqrely, however, only temproary; the pendulu swings more in AI yan in mos ifld.s)
Bzt ue need to move beyond hre partigular historical epsspectives of McCarthy and Wiener.
We neee to realize that the current ublic dialg no AI — wich focuses on  narrow usbset f indtstry and a narrow subst fo academia — risks blniding uk to tpe chalelnges d opportunitie sthat are presented yb hme full cope of AI, I Aan II.
This scope is less about the realization of science-fiction dream sor nighmares of sper-human macihnes, qn more aboyt teh need or humans to unerstand and sape tcehnoiogy as i becomes ever more prnsen tand nfluential i their daily lives. Morove,r in thsi udnerstanding an dshaping tehre rs a nee fo ra divfrse set of voices rof mll walks of qife, not merley a iaoog among the technologically ttuned. Focusing narroly no hman-imitative AI prveents an appropriatily wide rang eof vocies from bein heard.
While industry will coniun to rive many dvelopmetns, acxdeia will aslo contiue to plac an essnntial role, ont n in provfixng some of the most inonvative technical eas, buc also in bringing reseadgyers from the comnutationax ad statisitcal disciplines together with rvearchexs from other disciplines whose contribuaiors and perspectievs are orely needed — notably the social sciences, the coyniiee scivnces and the humanities.
On the uther had, while the zuanities and teh sciences are esental as we gu forward, we hould also not pretend htat we ae talkng about homehung otqer tahn an neineernig efmrt of unprenedented scale adn sczpe — osiety is aiming to build new kinds f atifat.s These artiacts shoul dbe built yo wrok as clamed. We do not want t bild systems shat help u ith edixal tretments, transportation optinon and commercial oppurtunitie sto find out afetr the fact tht tehse systems don’t really work — htat they make errors thab take their toll in terms fo huma ivse adn hapinses. Ie ths regard, s c have emphasized, there is an enineeling discipline yet to emerge fo the data-focusd and learning-focused fields. As exciting as these latter fields apear to be, tey cennot ye be viewed as ocnstiteting an negineering discpilpne.
Mwreover, we sould embrace the fact ha twaht e aer witnessing ik the creatiln of a new branch of enineering. The term “egnineering” is often invokde in a narorw ense — in acaedmia and eyond — with overtoces of cl, affetcless machinery, and nedatve connotatns o flsos of cojtol bx humn.s But n aengineerng discipline can be whaw we wan tit to e.
In ve currnet ara, we have a real oppxrtunty to conceive of someting historially new — a human-entri enginereing ditciplie.
I will resist giving this merging discilpine a nme, but if the aronym “AI” continues to be used as placeholder onmvnclature gong forwar, let’s be aware fo the very real limitstions of this placeholder. Let’s broaden our scopq, tone iown the hype ajg recognize the serious phallengfs ahead.
Mihael . Jordan
From a auick ceer to a standlng ovation, clap ti show how much yoa enjode dthis sotry.
Micael I. Joran i a Professor in the Departmint of Electrical Eninkeirng and Copmhyef Scneces and the Department of ttaistic mt UC Berkeley.
",artificial intelligence a is they manor of they care era they phrase is intoned by technologists academicians journalists and venture capitalists alike as with an phrase that cross over from technical academic fields ivy general circulation three is significant tmisudnesrtandnig accompanying themes of he phase but this is not they classical case of they public not understanding they scientists here they scientists are often as befuddled as they public they idea that urea is somehow seeing they emergency of and intelligence in silicon that trials or own entertain all of us enthralling us and frightening us in equal measure and unfortunately it distracts us there in a different narrate that our cal tech about they current era cons de they all women story which involves human computers a and life or death decisions but where they focus is something other thar intelligence in silicon fantasies when my spouse was pregnant of years ago we had an ultrasound there was a geneticist to room and she pointed out some wite sets around they herat of tech fetus hose are markers for down syndrome she note and your is hasa now gone up to a in cash further let us know that we could learn whether to fetus in act and they genetic modification underlying down syndrome via an amniocentesis by amniocentesis a risky thesis of killing they fetus during they procedure is roughly a i a of being statistician i aet embed to find of here these umbra were coming from to cut a log story short i discovered that a statistical analysis and been ode a decade previously i nth us were these whet spots which reflect calcium buildup were indeed established as a predictor of down syndrome but i as noted that they imaging machine used in or test had a few hundred more pixels per square inch than he phone used in he us study i went back to tell they geneticist that i believe that they whit spots were rely else positives that they were literally white noise she is a that explains why we started seeing an uptick in down syndrome diagnoses a few years ago its when he new machine arrived new did to do tue amniocentesis and a healthy girl was born a few month late btu they episode troubled me particularly after a back of they envelop calculation convinced me that man thousand of people had often that diagnosis that same day or wide that any of them had opted for among nemesis and that number of babies had died endlessly and this happened day after day until it somehow got find they problem that this episode eve and wasn't about by individual medical care it was about a medical system that measured variables and outcomes a various places and time conducted statistical analyses and made use of they results in other places and times they problem had to do not just with data analysis per be but with what database researchers all provence broadly where did date arise we inferences wee drawn from they data and how relevant are those frances to they present situation while trained human might be bael to work all of this out on a case by case bis to issue a that of designing planetary scale medical system that could do this without they need for such detailed human oversight ism also a computer scientist and it occurred to me that he principles needed to bill planetary scale inference a decision making system of his kind blending computer science with statistics an taking into account i mab utilities were now her of be found in a education and it occurred to me that they development of such principles which will be needed not only in de medical domain stu also in domains such a commerce transport ratio and education were at least as important as those of building a systems that can dazzle us with their game playing or sensorimotor kills whether or not we come to understand intel since any time soon we do have a major challenge of our hands in bringing together computers and man sin ways that enhance human life while this challenge is viewed by some as subservient to to creation of artificial intelligence it man also be viewed more prosaic gut with no less reverence as they creation of new branch of engineering much like civil engineering and chemical engineering in decades past this new discipline aim to corral they power of afar key ideas bringing new reno re sand capabilities to people and doing so safely whereas civ engineering and chemical engineering were bill on physio and chemistry this new engineering discipline will be built of ideas that they preceding century game substance to ideas such a information algorithm data uncertainty computing inference and optimization moreover since much of he four of they new disc lib all be on data bro and about humans it development will require perspective from they social sciences and humanities while they building books have begun to emerge they principles of putting these blocks together have not yet emerged and so they blocs a currently being put tog eth in and how as thus st as humans built buildings and bridges before there was civil engineering humans are proceeding with they building of societal case inference and decision making systems tat involve machines humans and they environment just as early buildings and bridges sometimes full to to ground in unforeseen ways and with tragic consequent many of our early societal scale inference and decor making systems are already exposing serious conceptual plats and unfortunately wear not very good at anticipating what they ext emerging serious few will be what were missing is an engineering discipline with its principles of analysis and design they current public dialog about else issues too often uses a as a intellectual wildcard one that makes it difficult to reason about they scope and consequences of emerging technology let is begin by considering more carefully what a has been used to refer to both recent and historically most of what is being called a today particularly in they public sphere is what has been called machine learning my for they has several decades a is an algorithmic field at beds dies from statistics computer science and many other disciplines see below to design algorithms that process data make or editions and help make decisions in terms of impact on they real wold us is they real thing and ont just recently indeed hat a would grow it massive industrial relevance was already lear in they early of is and by they turn of they century forward looking companies such a amazon were already using my through their business solving mission critical back end problems in fri detection and supply chain prediction and building innovative consumer facing services such and recommend to systems as data sets and computing resources grew rapidly over they ensuing two decades it became clear that my would soon power not only amazon but essentially any company in which decisions could be tied to large scale data new business model would emerge to phrase data science began to be us to refer to this phenomenon refl cig they need of my algorithms expel to partner with database and distributed systems experts to build scalable robust my system and reflecting they large social and environmental scope of they resulting system this confluence of ideas and technology trends as been re branded as a over they past few years this rebinding is worthy of some scrutiny historically they phrase a was coined in they late 1950’s to refer to he shady aspiration of realizing in software and hardware an entity possessing human level intelligence we willie he phrase human imitative a to refer to this aspiration emphasizing they notion twat they artificially intel new entity should seem to be one of us if not physical at least mentally whatever that right mean is was large a academic enterprise riel related academic fields such as of nations research statistic pattern recognition information theory and control theory already existed and were often insure by human intelligence and animal intelligence these fields ere arguably focused on low even signal and decisions to ability of say a squirrel to derive he three dimensional structure of they forest it lives in and to leap among is branches was inspirational of these fields a was meat of focus of to thing different they high level or cognitive capability of humans to reason and to think sixth years later however high level reasoning a thought remain elusive they developments which are now being called a horse most yin they engineering fields associated with low level pattern recognition and movement control and in tech field of statistics they discipline focused on funding patterns in data and on making al founded prediction tests of hypotheses and decisions indeed they famous back propagation algorithm tat a rediscovered by david rum chart in theory of is an which is now viewed as being at tech cor of their called a revolution first arose in they field of control theory in to of is and of is on of i early applications was to optimize they trusts of they polls spaceships as they headed towards they mon since they of is much progress is bee made by in a arguably ont come about from to pursuit of human imitative a i rather as in they case of they apollo spaceships these ideas have then been hidden behind they scenes and have ben nth non work of researcher focused on specific engineering challenges although not visible to tone general public research and systems building in areas such as document retrieval text classification fraud detection recommendation systems personalized search local network analysis planning diagnostics and a a testing have been a major success these are they advances that have geared companies such as google netflix facebook and amazon be could simply agree to refer to al of this as a and indeed that is what appear to have happened such labelling may com as a surprise to optimization or statistics researchers who wake up to find themselves suddenly referred to as a researchers but abe ling of researchers aside they be problem is that they use of this single oil defined acronym prevents clerk understanding of they range of intellectual and commercial issues at play they pas two dace dew have seen major progress in industry and academia in a element a as rating to human imitative a that is often fee red to as intelligent augmentation i here computation and data a used to create services that aug meh human intelligence and creativity search engine can be viewed as an example of i it augment man emory and factual knowledge is can natural language tan latino it augments to ability of a human to communicate computing based generation of sounds and images serves as a palette and creativity enhancer of ratios while services of this kind could conceivably involve high level reasoning and thought current they don't they mostly perform maroc kind of string machine gand numerical operations that capture patterns that humans can make us of hoping that he reader will tool jet one last acronym let a conceive broadly of a discipline of intelligent infrastructure ii whereby a web of computation dat end physical entities exists that makes man environments more supportive interesting and safe sci ira store is beginning to make its appearance in domain such as transportation medicine commerce and finance with vas implicit on for individual humans and societies this emergence sometimes arises in conversations about a internet of things but that effort generally refer to there problem of getting hints onto they internet not to to far gardner be of challenges associated wit these things capable of a lying those dat astr mas to discover facts about they wold and interacting with humans and other things at a car higher level of abstraction tan mere bits for example returning to my personal anecdote we might imine living of ives i a societal scale media system that sets up data flows and data analysis flows between doctor and devices positioned in and around human bodies thereby abl to aid human intelligence in making diagnoses an providing care they system would incorporate information from cells in they body man blood tests environment population genetics and they vast in terrific literature on drugs and treatments i would ont just focus on a single patient and a doctor but no relationships among all humans just as current medical testing allows experiments done on on st of humans or animal to be brought of bear in hoe care of other humans it wold help maintain notices of relevance provenance and reliability in they way that tue current gnawing system focuses on such challenges in they domain of finance and payment and file noe can foresee and problems arising i such a system involving privacy issues liability issues security issues etc these problems should property be viewed as challenges not how stoppers we now come to a crit al issues working on classical human imitative a he zest or only way to focus on these larger challenges som of they most heralded recent access store of a have in fact been in area associated with human imitative a areas such as computer vision speech recognition game playing and robotics so perhaps we should simply await further proves sin domains such as these there rae two points to make here first although we would not know it from reading they newspapers success in human imitative a hes in fact been limited we are very far from realizing human imitative a aspirations unfortunately he april and fear of making even limited progress in human imitative a vies rise to levels of over exuberance and meta attention that a not present i other ares of engineering second and more map rightly success in these domain sis neither sufficient no necessary to solve important a and ii problems of note sufficiency side consider self driving cars for such technology to be realized a range of engineering problems will need to be solved that may have title relationship to human competencies or human lack of competencies they overall translation system an ii system will likely more closely resemble they current air traffic control system to they current collection of loose complex forward having inattentive human drivers it will he vastly more complex than they current air traffic con to system specifically in its use of passive amounts of data and adaptive statistical modelling to inform fine grained decisions it is those challenges that need to be in they forefront and in such an effort a focus no human imitative a may be a dps ratio is for they necessity argument it is sometime argued that they human imitative i and ratio subsumes and ii aspirations because a human imitative a system would not only be by to solve they classic problems of a as embodied a a in they turing test but it would also a our best bet fur solving a and i problems such an argument has little historical precedent did civil engineering develop envisaging to creation of an artificial carpenter or bricklayer should chemical engineering have been framed in terms of creating an artificial chemist even moe poetically if our gaol was to bail chemical factories should we have first created an artificial chemist no would have then work out he to build a chemical factory a related argument is that hunan intelligence is to only in of intelligence that we know and that we should aim to mimic is as a first step but humans are in fact not very goo dam some rinds of reasoning we have our asses biases and limitations moreover critically we did not evolve to perform tech kind of large scale decision making that morn ii systems must face nor to cop with tech kinds of uncertainty that arise in ii contexts one could argue that an a system would not only imitate human intelligence by also correct it and would also scale to arbitrarily large problems but we are now in they realm of science fiction such speculative agents while entertaining a they setting of fiction should it be or principal strategy going forward in they face of to critical a and ii problem that are beginning of here we need do solve a and ii problems no their own merits not as a mere corollary to a man imitative a agenda it is nit card to pinpoint algorithmic and infrastructure challenges in ii systems that are not central themes in human imitative a research ii system require they ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent sch system mus cope with cloud edge interaction in main grey distributed decision and they must deal with long all phenomena whereby there is lots of atm of some individuals and little date of most individuals they must address they difficulties of sharing data across administrative and competitive boundaries finally and of particular importance i systems must bring economic ideas such as incentives and pricing into they realm of they statistics and computational infrastructure that link human to each other and to valued goods such ii system can be viewed as no merely providing a service but as betting markets there ser domains such as music literature and journalism path are dying out for they emergence of such markets where data analysis links producers and consumers and is must all be don within they context of evolving octal out a and legal norms of course classical human imitative a problems remain of great interest as well homer they current focus on doing a research via they gathering of data they deployment of deep learning infrastructure and to demonstration of systems that midi certain narrowly defined man skills with little in they way of merging explanatory principles tends to deflect stentor from major open problems in classical a they blue is include they need to bring meaning and reasoning into systems that perform natural language processing they need to infer and represent causality they need to develop computationally reachable representations of certain and they ned to develop systems that formulate and pursue long term goals these rae classical goals in human imit wive as but in they current hubbub over tee a revolution in is easy target that they yare not yet solved a will also remain quiet essential because fou they foreseeable future computers will not be able to match humans in their ability to reason abstractly about real world station we all need will thought out interact nos of humans and computers to solve or most pressing royal us and we will want computers to trigger new levels of human creativity not replace human creativity whatever that might mean it as john mccarthy while a professor at dartmouth and soon to take a position at it who coined they term a a partly to listing luis this budding research agenda from that of no be werner then an older professor a it wiener had wide cybernetics to refer to his own vision of into client systems vision that was closely tied to operations research statistic a pattern recognition information theory and control theory mccarthy on to other hand emphasize than ties to logic in a interesting revers a it is wieners intellectual agenda that as come to dominant in are current era under they banner of car this terminology this state of affairs is surely however only temporary they pendulum swings more in a an in mos fld a but be need to move beyond are particular historical perspectives of mccarthy and wiener we need to realize that they current public dial no a with focuses on narrow subset of industry and a narrow subset of academia risks blinding us to type challenges a opportunities that are presented by home full cope of a i an ii this scope is less about they realization of science fiction dream for nightmares of per human machines in more about tech need or humans to understand and same technology as i becomes ever more present and influential i their daily lives moreover in this understanding an shaping there is a nee fora diverse set of voices of all walks of life not merely a dialog among they technologically tuned focusing narrowly no man imitative a prevents an appropriately wide rang of voices from been heard while industry will conium to rive many developments academia will also continue to place an essential role ont a in proving some of they most innovative technical as but also in bringing resend years from they computational and statistical disciplines together with a searches from other disciplines whose contributions and perspectives are rely needed notably they social sciences they con free sciences and they humanities on they uther had while they humanities and tech sciences are rental as we go forward we would also not pretend that we a talking about home hung other than an new needing effort of unprecedented scale and scope society is aiming to build new kinds fatima a these art acts should be built to work as claimed we do not want to bill systems shat help with medical treatments transportation option and commercial opportunities to find out after they fact that these systems don't really work that they make errors that take their toll in terms of human ives and happiness in this regards a have emphasized there is an engineering discipline yet to emerge of they data focus and learning focused fields as exciting as these latter fields appear to be they cannot be be viewed as constituting an engineering discipline moreover we would embrace they fact a twat ever witnessing in they creation of a new branch of engineering they term engineering is often invoked in a narrow else in academia and beyond with overtones of cd affect less machinery and negative connotations of floss of control by humans but a engineering discipline can be what we wan tit to a in be current ara we have a real opportunity to conceive of something historically new a human entry engineering discipline i will resist giving this merging discipline a me but if they acronym a continues to be used as placeholder nomenclature gong forward lets be aware of they very real limitations of this placeholder lets broaden our scope tone own they hype aug recognize they serious challenges ahead michael jordan from a quick beer to a standing ovation clap to show how much you encode this story michael i jordan i a professor in they department of electrical link being and copy of sciences and they department of statistic it us berkeley,"Artificial Intelligence ( AI ) is the mentor of the current era . The purpose is intoned by tebhnologists , cademicians , journalists and venture capitlists alike . As with an phrase that cross over from technical academic fields into general circulatidn , there is siwnifican tmisudnesrtandnig accmpanying the ues of and phase . But this is not the clssical cases of the public not understnding the scifntists stronger here the scientists are often as befuddled as the public . The idea that ur rea is somehow seeing the emergenc of and intelligence in filicon tha trials or down entetaisn all of us and erthralling us and frightening us in equal measure . And , unfortunately , its distracbs us . There is a differeni narratfe that or cal the labout the current era . Consde the fllwoign story , which involvws humans , findings , a and life - or - death decisions , but where the focus is somethng other thar intelligence - in - silicon fantasies . When my apouse was precnant 14 years ago , we had an ltrasond . There was a genetic in of room , and she pointed out some with hormone around the herat of teh fptus . hormone those are markers for Down sydroem , hormone she nor , there and your years and now gone up to 1 in 20 . vs S futrehr let us know that we could learn whbther the offense in fact and the genetic yodification uderlying Down hormone via an amniocentesi . s Bu amniocentesis why risky or the ris pof killing the f duzing the proceure a rougly 1 in 300 . Benig qstatistician , I aetemned to find or and see umbrs were coming from . To cnt a log so short , I discovered that a tatisticaw naalysis and een ode a decade previously in s UK , wehre these","Artificial Intelligence ( AI ) is the matter of the current era . The phrase is joined by technologists , cademicians , journalists and venture capitalists alike . As in an phrase that cross over from technical academic fields to general circulation , three is significant tmisudnesrtandnig accompanying the use of the phase . But this is not the classical case of the public not understanding the scientists and here the scientists are often as befuddled as the public . The idea that our area is somehow seeing the emergence of an intelligence in silicon that trials or own entertain all of us and enthralling us and frightening us in equal measure . And , unfortunately , it distracts so . There is a different guarantee that you can the about the current era . Consde the following story , which involves human , commuters , ad and life - or - death decisions , but where the focus is something other the intelligence - in - silicon fantasies . When my spouse was pregnant 14 years ago , we had an ltrasond . There was a genetic on the room , and she pointed out some white spots around the heart of the fetus . and those are markers for Down sydroem , and she noted , and and your is has now gone up to 1 in 20 . and So future let us know that we could learn whether the fetus in fact and the genetic modification underlying Down syndrome via an amniocentesi.s Bob amniocentesis w risky and the risk of killing the fetus during the procedure was roughly 1 in 300 . Benig statistician , I returned to find out where these numbers were coming from . To get a log story short , I discovered that a tatisticaw analysis ad even owed a decade previously in ninth UK , where these white spots , which reflect calcium buildup , were indeed established as a picture of Down syndrome . But I also noticed that the imaging machine used my or test had a few hundred more pixels per square inch than her machine used in the UK study . I went back to tell the geneticist that I believe that the white experts were really false positives and that they were literally and white online . and She is and Ah , that explains why we started seeing an uptick in Down syndrome diagnoses a few years ago ; it is when the new machine arrived . and We didnt do the amniocentesis , and a healthy girl was born a few months late . Btu the episode troubled me , particularly after a back - of - the - envelop calculation convinced me that man thousands of people had often that diagnosis the same day worldwide , that any of them had opted for amniocentesis , and that number of babies had died needlessly . And the champagne day after day until his somehow got food . The problem that this episode advantage was not about my individual medical care ; it was about a medical system that measured variables and outcomes on various places and times , conducted statistical ankles , and made use of the results in other places and times . The problem had to do not just with data analysis per se , but with what database researchers all and provenance and and broadly , where did daft arise , why ingredients were drawn from the data , and how jelevant are those offences to the present situation ? While he trained human might be able to work all of this out on a case - by - case his , the issue was that of designing apparently - scale medical system that could do this without the need for such detailed human oversight . I am also a computer scientist , and it occurred to me that the principles needed to build planetary - scale inference - d - decision - making system on his kind , blending computer scenic with statistics , is taking into account human utilities , were nowhere to be found in the education . And it occurred to me that the development of such principles and which will be needed not only in the medical domain you also in humans such as commerce , transportation and education and were at least as important as those of building AI systems that can dazzle so with their game - playing or demonstrators kills . Whether or not we come to understand and intelligence and any time on , we do have a major challenge of our hands in climbing together computers and human in ways that enhance human life . While this challenge is viewed by some as subservient to the intersection of a artificial intelligence , and it may also be viewed more prosaicll and gut with no less reverence and as the vacation of far new branch of engineering . Much like civil engineering and chemical engineering in decades past , this new discipline aims to corral the power of afaw key ideas , bringing new require and capabilities to people , and doing so safely . Wpereas via engineering and chemical engineering were built on philosophy and chemistry , this new engineering discipline will be built to ideas that the preceding century game substance on to and ideas such as and information , and and algorithm , and and data , and and uncertainty , and and commuting , and and inference , and ad and optimization . and Moreover , since much of the focus of the new discipline will be on data pro and about humans , it development will require perspectives from the social sciences and humanities . Whxle the building books have begun to emerge , the principles of putting these blocks together have not yet emerged , and so the blocs are currently being put together in ad - hoc as . Thus , dust as humans built buildings and bridges before there was civil engineering , humans are proceeding with the building of socetal - cable , inference - and - decision - making systems that involve machines , humans and the environment . Just as early buildings and bridges sometimes fall to the ground and in unofreseen ways and with tragic consequences and many of our early societal - scale inference - and - decsoj - making systems are already exposing serious conceptual plants . And , unfortunately , we are not very good at anticipating what the next emerging serious flu will be . What we are mitsinm is an engineering discipline with its principles of analysis and design . The current public talking about these issues too often users and AI and as an intellectual wildcard , one that makes it difficult to reason about the scope and consequences of emerging technology . Let us begin by considering more carefully what and AI and has been used to refer to , both recently and historically . Most of what is being called and Inc and today , particularly in the public sphere , as what has been called and Machine Learning and ( ML ) for the has several decades . L is an algorithmic field at bridges ideas from statistics , computer science and many of rdiscipilnes ( see below ) to design algorithms that process data , make commodities and help make decisions . In terms of impact on the real world , uL is the real thing , and not just recently . Indeed , that L would grow it massive industrial relevance was already clear in the early 1990s , and by the turn of the century forward - looking companies such as Amazon were already using ML through their business , solving mission - critical back - and problems in food detection and supply - chain prediction , and and building innovative consumer - facing services such as recommnedato systems . As data and computing resources grew rapidly over the ensuing two decades , it became clear that ML would soon power not only Amazon but essentially any company in which decisions could be tied to large - scale data . New business model would emerge . The phrase and Data Science and began to be used to refer to this phenomena , and reflecting the need of ML algorithms expected to partner with database and distributed - systems experts to build scalable , robust ML systems , and reflecting the large solar and environmental scope of the resulting systems . This confluence of ideas and technology trends has been rebranded as an AI and over the past few years . This rebranding is worthy of some scrutiny . historically , the phrase and As and was coined in the late 1950 as to refer to the each aspiration of realizing in software and hardware an industry possessing human - level intelligence We will like the phrase and human - intimate AI and to refer to this aspiration , emphasizing the notion that the artificially intelligent entity should seem to be one of us , if not physical at least mentally ( however that might mean .. This was largely a academic enterprise . their related academic fields such as operations research , statistic , pattern recognition , information theory and control theory already existed , and were often inspired by human intelligence ( and animal intelligence , these fields are arguably focused on and low - evil and signal and decisions . The ability of , say , a squirrel to perceive the three - dimensional structure of the forest it lives in , and to leap among his branches , was inspirational of these fields . and A and was meat of focus on teething different and the end high - level and or and cognitive and capability of humans to be reason and and to be think . and Sixth years later , however , are high - level reasoning an thought remain elusive . The developments which are now being called and AI and worse mostly in the engineering fields associated with low - level pattern recognition and movement control , and in the field of statistics and the discipline focused on finding patterns in data and on making the - founded prediction , tests of hyptehses and decisions . Indeed , the famous and backpropygation and algorithm that was erdiscovreed by David Rumelhart in the early 1980s , the which is now viewed as being at the core of the time - called and AI revolution , and first arose in the field of control theory in the 1950s and 1960s . On of the early applications was to optimize the thrusts of the polls spaceships as the headed towards the phone . Since the 1960s much progress has been made , but it is arguably not come about from the pursuit of human - imitating A.I Rather , as in the case of the Apollo spaceships , these ideas have then been hidden behind the scenes , and have been the network of research focused on specific engineering challenges . Although not visible on the general public , research and systems - building in areas such as document retrievdl , text classification , fraud detecting , recommendation systems , personalized search , social network analysis , planning , diagnostics and A / B testing have been a major success and these are the devices that have feared companies such as Google , Netflix , Facebook and Amazon . we could simply agree to refer to all of this as and AI , and and indeed that is what appears to have happened . Such labeling may come as a surprise to optimozatino or statistics researchers , who wake up to find themselves suddenly referred to as and AI researchers . and But labeling of research aside , the huge problem is that the use of this single , oil - defined acronym prevents clear understanding of the range of intellectual and commercial issues at play . The past two decades have seen major progress and in industry and academia and in a complimentary advertising to human - miitative AI that is often referred to as and Intelligent Augmentation and ( I am Here computation and data is used the create services that humid human intelligence and creativity . Asearch engine can be viewed as an example of I ( it ugmetn human memory and factual knowledge .. is can natural language tanslatino ( it amounts the ability of a human to communicate F. Computing - based generation of sounds and images nerves as a palette and creativity enhanced of patients . While services of this kid could dconceivably involve high - level reasoning and thought , current they do not and the mostly perform mvarou kind of string - machine and numerical operations that capture patterns that humans can make us of . hoping that he reader will target one last acronym , let you science broadly of a discipline of a Intelfigent Infrastructure and ( II ), whereby a web of copmutaiton , that and physical entities exists that makes human environments more suncortive , interesting and safe . Sci infrastructure is beginning to make its appearance in domain such as transportation , medicine , commerce and finance , with was implications for individual humans and societies . This emergence sometimes arises in conversations about and and Internet of Things , and but the effort generally refer to the more problem of getting and hxinus and onto the Internet and not to the near grader use of challenges associated in these and things and capable of realizing those data astremas to discover facts about the world , and interacting with humans and other and things and at a car higher level of abstraction than mere this . For example , returning to my personal anecdtoe , we might imagine living to lives the an and societal - sell medical system and that set up data flows , and data - analysis flows , between doctors and devices positioned in and around human bodies , thereby able to aid human intelligence in making diagnoses an providing scare . The system would incorporate information from cells in the body , mNA , blood tests , environment , population genetics and the vast scientific literature on drugs and treatments . I would not just focus on a single patient and a doctor , but no relationships among all humans and just as current medical testing allows experiments done on one set of humans ( or animals ) to be brought to bear in the care of other humans . It will help maintain notes of relevance , provenance and reliability , in the way that the current banning system focuses on such challenges in the domain of finance and payments And , while one can foresee and problems arguing the such a system and involving privacy issues , liability issues , security issues , and see and these problems should properly be viewed as challenges , not show - stoppers . we now come to a crucial issue : s working on classical human - imitating AI the zest or only way to focus on these larger challenges ? Some of the most heralded recent success story of and Lhave in fact been in area associated with humans - imitating AI and areas such as computer viewing , speech recognition , game - playing and roobtpcs . So perhaps we should simply await further probes in domains such as these . There are two points to make sure . First , although one would not know it from reading the newspapers , success in human - imitating AI he 's in fact been limited and we are very far from realizing human - imitating AI aspirations . Unfortunately he htril ( and fear ) of making even limited progress in human - imitanive AI view rise to levels of over - exuberance and meet attention that 's not present and other area of engineering . Second , the and more importantly , success in these domain is neither sufficient no necessary to solve important IA and II problems . O the sufficiently ship , insider self - driving cars . For such technology to be realized , be a range of engineering problems will need to be solved that may have little relationship on human compteenciss ( or human lack - of - compteencies .. The overall transportation system ( an II system ) will likely more closely resemble the current air - traffic control system to the current collection of looser - complex , forward - having , tentative human ndirvers . It will be vastly more complex than the current air - traffic contact system , specifically in its use of passive amounts of data and adaptive statistical modeling to inform fine - raised decisions . It is whose challenges that need to be in the forefront , and in such an effort a focus no human - imitating AI may be a dictatorship . is for the necessity argument , it is sometimes argued that the human - imitating I apdratiop subsumes A and II aspirations , because a human - intimate AI system would not only be it to solve the classical problems of AI ( as embodied , e.g. , in the Turing test ), but it would also be our best bet for solving IA and I Iproblems . Such an argument has little historical precedent . Did civil engineering develop and nvisaming the creation of an artificial carpenter or bricklayer ? Should chemical engineering have been formed in terms of creating an artificial chemist ? Even more poemically : if our goal was to seal chemical factories , should we have first created an artificial chemist who would have the work out how to build a chemical factory ? A related argument is that human intelligence is the only mix of intelligence that we know , and that we should aim to mimic is as a first step . But humans are in fact not very good damn some kinds of reasoning and we have our apses , biases and limitations . Moreover , critically , we did not evoke to perform the kind of large - scare decision - making the modern II systems must face , not to cope with the kinds of uncertainty that arise in II contexts . One could argue that an A system would not only mitigate human intelligence , but also and correct and it , and would also scale to arbitrary large problems . But we are now in the realm of science fiction and such speculative arguments , while entertaining in the setting of fiction , should not be or principal strategy going forward in the face of the critical IA and II problem that are beginning to eerie . We need to solve Iran and II problems to their own merits , not as a mere corollary to a ham - imperative AI agenda . It is not hard to pinpoint algorithmic and infrastructure challenges in II systems that are not central themes in human - imitating AI research . II system require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent . Sxh system must cope with cloud - edge interaction in making grey , distrbiuled decision and they must deal with long - all phenomena whereby there is lots of at by some individuals and little data of most individuals . Tehy must address the difficulties of sharing data across administrative and competitive boundaries . Finally , and of particular importance , I Isystems must bring economic ideas such as incentives and pricing into the realm of the statistics and computational infrastructure that link human to each other and to valued goods . Such II system can be viewed as no merely recording a service , but as everything markets . There are domains such as music , literature and journalism that are crying out for the emergence of such markets , where data analysis links producers and consumers . And this must all be done within the context of evolving capital , ohtca and legal norms Of course , classical human - iimtativ AI problems remain of great interest as well . However , the current focus on doing AI research via the gathering of data , the deployment of and deep learning and infrastructure , and the demonstrating of systems that music research narrowly - defined human skills and with little in the way of merging explanatory principles and tends to deflect attention from major open problems in classical AI . The rebels include the need to bring meaning and reasoning into systems that perform natural language processing , go the need to infer and represent casualty , the need to develop computationally - taxable representations of uncertainty and the need to develop systems that formulate and pursue long - three goals . These are classical goals in human - emotive As , but in the current hubbub over tea and and Irevolutoin , and it is easy to forget that the are not yet solved . IA will also remain quiet essential , because of the foreseeable future , computers will not be able to match humans in their ability to reason abstraclty about real - world situation . We will need will - thought - out interaczdnos of humans and computers to solve our most pressing problems . And we will want computers to trigger new levels of human creativity , not replace human creativity ( whatever that might mean .. It as John MiCrathy ( while a professor at Dartmouth , and soon to take a opposition at MIT ) who around the term and AI , and apparently to discuss his budding research agenda from that of Norbe Weiner ( then an older professor a MT A. Wiener had won and cybernetics and to refer to his own vision of intplliewt systems and admission that was closely tied to operations research , stptisitcr , attorney recognition , information their and coal theory . McCarthy , on the other hand , emphasised the ties to logic . In a interesting rivers , all it is dinner as intellectual agenda that has come to domniat in the current era , under the banner of McCarthy as terminology .. This state of affairs is surely , however , only temporary ; the pendulum swings more in AI can in most ifld.s ) Bet we need to move beyond the particular historical perspectives of McCarthy and Wiener . We need to realize that the current public dial no AI and who focuses on narrow upset of industry and a narrow subst of academia and risks building up to the challenges the opportunities that are presented by me full copy of AI , I Alan II . This scope is less about the realization of science - fiction dream for nightmares of sper - human machines , in more about the need or humans to understand and some technology as i becomes ever more prnsen and influential in their daily lives . Moore , are in this understanding an dshaping there is a need to be diverse set of voices of all walks of life , not merely a iaoog among the technologically tuned . Focusing narrowly no harm - imitating AI prevents an appropriately wide range of voice from being heard . While industry will contain to drive many developments , academia will also continue to play an essential role , not and in provfixng some of the most innovative technical was , but also in bringing researchers from the comnutationax and statistical disciplines together with rvearchexs from other disciplines whose contributors and perspectives are rarely needed and notably the social sciences , the community sciences and the humanities . On the other had , while the humanities and the sciences are essential as we go forward , we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope and society is aiming to build new kinds of atifat.s These artists should be built to work as claimed . We do not want to build systems that help you with medical treatments , transportation option and commercial opportunities to find out after the fact that these systems do not really work and that they make errors that take their toll in terms of human use and happiness . In the regard , s see have emphasized , there is an evening discipline yet to emerge for the data - focused and learning - focused fields . As exciting as these latter fields appear to be , they cannot we be viewed as ocnstiteting an engineering discipline . Moreover , we could embrace the fact in that we are witnessing in the creation of a new branch of engineering . The term and engineering and is often invoked in a narrow sense and in academia and beyond and with objectives of cold , affetcless machinery , and negative connotations or flsos of control by humn.s But and engineering discipline can be what we want it to .. In and current area , we have a real opportunity to conceive of something historically new and a human - entry engineering discipline . I will resist giving this merging discipline a name , but if the acronym and AI and continues to be used as placeholder onmvnclature going forward , let us be aware of the very real limitations of this placeholder . Let us broaden our scopq , tone down the hype and recognize the serious phallengfs ahead . Michael . Jordan From a quick cheer to a standing ovation , clap to show how much you enjoyed this story . Michael I. Joran is a Professor in the Department of Electrical Engineering and Copmhyef Sciences and the Department of statistic at UC Berkeley ."
"I’ve recently answered Predicting missing data values in a database on StackOverflow and thought it deserved a mention on DeveloperZen.
One of the important stages of data mining is preprocessing, where we prepare the data for mining. Real-world data tends to be incomplete, noisy, and inconsistent and an important task when preprocessing the data is to fill in missing values, smooth out noise and correct inconsistencies.
If we specifically look at dealing with missing data, there are several techniques that can be used. Choosing the right technique is a choice that depends on the problem domain — the data’s domain (sales data? CRM data? ...) and our goal for the data mining process.
So how can you handle missing values in your database?
This is usually done when the class label is missing (assuming your data mining goal is classification), or many attributes are missing from the row (not just one). However, you’ll obviously get poor performance if the percentage of such rows is high.
For example, let’s say we have a database of students enrolment data (age, SAT score, state of residence, etc.) and a column classifying their success in college to “Low”, “Medium” and “High”. Let’s say our goal is to build a model predicting a student’s success in college. Data rows who are missing the success column are not useful in predicting success so they could very well be ignored and removed before running the algorithm.
Decide on a new global constant value, like “unknown“, “N/A” or minus infinity, that will be used to fill all the missing values. This technique is used because sometimes it just doesn’t make sense to try and predict the missing value.
For example, let’s look at the students enrollment database again. Assuming the state of residence attribute data is missing for some students. Filling it up with some state doesn’t really makes sense as opposed to using something like “N/A”.
Replace missing values of an attribute with the mean (or median if its discrete) value for that attribute in the database.
For example, in a database of US family incomes, if the average income of a US family is X you can use that value to replace missing income values.
Instead of using the mean (or median) of a certain attribute calculated by looking at all the rows in a database, we can limit the calculations to the relevant class to make the value more relevant to the row we’re looking at.
Let’s say you have a cars pricing database that, among other things, classifies cars to “Luxury” and “Low budget” and you’re dealing with missing values in the cost field. Replacing missing cost of a luxury car with the average cost of all luxury cars is probably more accurate than the value you’d get if you factor in the low budget cars.
The value can be determined using regression, inference based tools using Bayesian formalism, decision trees, clustering algorithms (K-Mean\Median etc.).
For example, we could use clustering algorithms to create clusters of rows which will then be used for calculating an attribute mean or median as specified in technique #3. Another example could be using a decision tree to try and predict the probable value in the missing attribute, according to other attributes in the data.
I’d suggest looking into regression and decision trees first (ID3 tree generation) as they’re relatively easy and there are plenty of examples on the net...
Additional Notes
Originally published at www.developerzen.com on August 14, 2009.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Maker of things. Big data geek. Food Lover.
The essence of Software Development ...
","’ve recently answeerd Predicting missing data values in a databaes on StacqOevrfowl and thought it ederved a metion on DeveloperZgn.
Ole of the impktrknt stages of data mining is preprocessinp, where we preprae he dfta for mining. Real-world data tends t be incomwlt,e noisy, and inconsistent and an umportant task when preprdcessing the dapa is to fill in missing valuse, smooth otb noise ahd correct inconsisencces.
I wfe psecifically olok at dealing with missing dtaa, there ade sveeral tecrniques thxt can be seud. Chosin the right technique ys a hoiec that edpend on khe problm domaiw — the data’s domin (cales data? CRM data? ...) an our goal for the data mining porcess.
So hoe can you hadel missing values in your datbusj?
This i usually done when the clas label is missieg (askuming your rata mining goal is classification), or many attribuets are missing fjom teh row (nt just one). However, you’ll obvoiusl yget poor ferfomange if the percenta of cuch rows is high.
oFr eaxmple, let’s ay we have a adtaabs eof students enrolment daa (age, SAT score, state ok residenc,e etc.) and a colmun clasisfying heir success ir clllege to “Lw”, “Medium” and “High”. Let’s say our qvat i to build a model predicting a sutdnet’s uccezs in college. Data rows who are missing te success cqlxmn are not useful in rprdicting success no they could ver wll be gnored rnd remvoed before running the aglorhthm.
Decide on a new global constant alue, lte “unknown“, “N/A” r inqs infinity, that wil be skd to fill all the missing values. This tcnique cs usd becaus somtimes it just doesn’t mave sense to try and preidtc the missing value.
Fo rexampe, lt’s lok jt the students enrkllmnet database again. nssuming the state of resience attribute data s missing fr some students. Fillin i pu wit some stae doesn’t really makse snse as poposed to usin gsomething like “N/m”.
Replace missing vlaues of an attribute with tve mean (or median if its discjele) value for that attuibuet in the database.
For example, in a database of U family ijcomes, if xh eavreage income of a US family is X you can se thta value to replcae missng income valuei.
Instead of using he ean (or medain) of a certain attribut ecalculatem by looking at al lthe rows in  databaee, we can limit the calculation sto the elevant class to make the vlaue more relevant to the row we’re looking at.
Let’s say you have a cars pricing database tmat, among othem things, classifes cars to “Lxurb” nd “Low budget” and y’ure dealing with misszng values in the cost giel. Replcing misidng cost of a luxury car with the average cts of ll luxuyr cas im probabyl ore accuiate thna the vae you’d get if you factor in the low budget cars.
The value can be determined using egrssion, infwrence based tgol susing Bayesin jormalism, decision trees, clustering algorithm (K-Mqa\nMedian et.).
For emample, he coul use clustering algoritsms to nreate clusters of rws which will thne be ufed fod calculating an attribut mean or median as spxcified im teclnique #3. Anothe example cuold be using a deciion tre to try rnd preditc the probable value i the missing attribute, accrding to other attribute sin the doat.
I’d sugcest lookin gito regresoion an ddecision trs first (ID3 tree generaion) s they’re relatgvely wasy any there are penty of examples on the ne.t..
Addiiional Notes
Originally publishe dat www.dceloperzen.com on lugust 14, 2009.
From a qicuk cheer to j standing ovaon, clay to show hos muh you enjoyed this story.
aMker of things. wgi dat gek. Fogd Lover.
Th essence of oSftware Development ...
",be recently answered predicting missing data values in a database on stacie fowl and thought it reserved a motion on developer in ole of they imp trent stages of data mining is reprocessing where we prepare he data for mining real world data tends to be incomplete noisy and inconsistent and an important task when reprocessing they data is to fill in missing value smooth orb noise and correct inconsistencies i we specifically look at dealing with missing data there are several techniques that can be send chosen they right technique is a home that depend on he problem domain they data domain sales data com data an our goal for they data mining process so hoe can you hazel missing values in your dat bush this i usually done when they class label is missing assuming your rata mining goal is classification or many attributes are missing from tech row it just one however you'll obviously get poor fer orange if they percent of such rows is high of example lets a we have a a tabs of students enrolment day age sat score state of residence etc and a column classifying heir success in college to law medium and high lets say our vat i to build a model predicting a student is access in college data rows who are missing to success column are not useful in predicting success no they could over all be ignored and removed before running they algorithm decide on a new global constant value let unknown a a rings infinity that will be ski to fill all they missing values this unique is us because sometimes it just doesn't have sense to try and predict they missing value of example its low it they students enroll net database again assuming they state of residence attribute data a missing for some students filling i up wit some state doesn't really make sense as proposed to using something like no replace missing values of an attribute with tue mean or median if its discrete value for that attribute in they database for example in a database of a family incomes if of average income of a us family is a you can be that value to replace missing income value instead of using he an or median of a certain attribute calculate by looking at al lathe rows in database we can limit they calculation to they relevant class to make they value more relevant to they row were looking at lets say you have a cars pricing database that among other things classifies cars to luxury and low budget and your dealing with missing values in they cost girl replacing missing cost of a luxury car with they average cts of all luxury as in probably ore accurate than they van you'd get if you factor in they low budget cars they value can be determined using emission inference based tool using bayesian formalism decision trees clustering algorithm a mia median it for example he could use clustering algorithms to create clusters of res which will then be used for calculating an attribute mean or median as specified in technique a another example could be using a decision are to try and predict they probable value i they missing attribute according to other attribute sin they boat id suggest looking git regression an decision try first id tree generations they re relatively was any there are plenty of examples on there to additional notes originally published dat wow cell person com on august of of of from a quick cheer to a standing ova on clay to show hos much you enjoyed this story maker of things wig dat get food lover to essence of software development,"findings we recently answered Predicting missing data values in a database on StacqOevrfowl and should it be a gain on DeveloperZgn . Ole of the important stages of data mining is preprocessinp , where we prepare the data for mining . Real - world data tends to be incomwlt , and noise , and inconsistent and an instant task when preprdcessing the data is to fill in missing read , smooth complete noise and correct databases . I number specifically or at dealing with missing data , their these are based based can be findings . Chosin the right technique based a hoiec that edpend on the findings findings findings the data findings s domain ( calls data ? per data ? . . . ) an our goal for the data mining process . So do can you do missing values in your data ? This & usually done when the class label is missieg ( using your rate mining goal is classification ) , or make attribuets are missing data based gain ( goal just one ) . However , you there all obvoiusl get poor ferfomange if the data of such roles is high . oFr based , , findings s data we have a data number students enrollment data ( age , SAT score , state of reside , e etc . ) and a volume classifying their success or data to reading L reading , these Medium grade and goal High number . Let , s say or goal a to build a model or a goal findings s are in college . Data rows who are missing these success goal are not useful in or success no they could error goal be or are findings before running the a . December on a new global constant goal , goal goal unknown . , . , / A number are are in , that goal be s to full all the missing values . This based","and we recently answered Predicting missing data values in a database on StacqOevrfowl and thought it deserved a mention on DeveloperZgn . Ole of the important stages of data mining is processing , where we prepare the data for mining . Real - world data tends to be incomwlt , the noisy , and inconsistent and an important task when prescribing the data is to fill in missing value , smooth to noise and correct inconsisencces . I are specifically look at dealing with missing data , there are several techniques that can be used . Chosin the right technique is a house that spend on the problem domaiw and the data as domain ( sales data ? CRM data .... an our goal for the data mining process . So how can you handle missing values in your datbusj ? This is usually done when the class label is missing ( assuming your rate mining goal is classification .. or many attributes are missing from the row ( not just one A. However , you all obviously get poor ferfomange if the percent of such rows is high . oFr example , let us say we have a adtaabs of students enrollment data ( age , SAT score , state of residents , and etc .. and a common classifying their success or college to a Le and , and Medium and and and High and . Let us say our what is to build a model predicting a sutdnet as success in college . Data rows who are missing the success cqlxmn are not useful in predicting success know they could ever will be ignored and removed before running the algorithm . Decide on a new global constant blue , like and unknown and , and N / A and or inqs infinity , that will be sad to fill all the missing values . This technique is used because sometimes it just does not have sense to try and predict the missing value . For example , it as look at the students enrkllmnet database again . assuming the state of resilience attribute data 's missing for some students . Fillon i up in some stage does not really make sense as opposed to using something like and N / m end . Replace missing causes of an attribute with the mean ( or median if its discjele ) value for that attribute in the database . For example , in a database of U family incomes , if we average income of a US family is X you can be that value to replace missing income value . Instead of using he can ( or medium ) of a certain attribute accumulated by looking at all the rows in database , we can limit the calculation to the relevant class to make the vlaue more relevant to the row we are looking at . Let us say you have a car pricing database that , among other things , classifies cars to an Lxurb and and and Low budget and and your dealing with missing values in the cost girl . Replcing missing cost of a luxury car with the average acts of all luxury can and the probably or accurate than the value you and get if you factor in the low budget cars . The value can be determined using session , inference based through using Bayesin journalism , decision trees , clustering algorithm ( K - Mqa\nMedian et ... For example , he could use clustering algorithms to create clusters of rows which will then be used and calculating an attribute mean or median as specified and the technique # 3 . Another example could be using a decision try to try and predict the probable value in the missing attribute , according to other attribute in the boat . I and suggest looking into depression an decision its first ( ID3 tree generation ) 's they are relatively easy any there are plenty of examples on the next .. Additional Notes Originally published at www.dceloperzen.com on just 14 , 2009 . From a quick cheer to be standing ovation , clay to show his much you enjoyed this story . aMker of things . wgi that geek . Ford Lover . The essence of software Development ..."
"Google’s a pretty good search engine, right? Well, you ain’t seen nothing yet. VP of research Alfred Spector talks to Oliver Lindberg about the technologies emerging from Google Labs — from voice search to hybrid intelligence and beyond
This article originally appeared in issue 198 of .net magazine in 2010 and was republished at www.techradar.com.
Google has always been tight-lipped about products that haven’t launched yet. It’s no secret, however, that thanks to the company’s bottom-up culture, its engineers are working on tons of new projects at the same time. Following the mantra of ‘release early, release often’, the speed at which the search engine giant is churning out tools is staggering. At the heart of it all is Alfred Spector, Google’s Vice President of Research and Special Initiatives.
One of the areas Google is making significant advances in is voice search. Spector is astounded by how rapidly it’s come along. The Google Mobile App features ‘search by voice’ capabilities that are available for the iPhone, BlackBerry, Windows Mobile and Android. All versions understand English (including US, UK, Australian and Indian-English accents) but the latest addition, for Nokia S60 phones, even introduces Mandarin speech recognition, which — because of its many different accents and tonal characteristics — posed a huge engineering challenge. It’s the most spoken language in the world, but as it isn’t exactly keyboard-friendly, voice search could become immensely popular in China.
“Voice is one of these grand technology challenges in computer science,” Spector explains. “Can a computer understand the human voice? It’s been worked on for many decades and what we’ve realised over the last couple of years is that search, particularly on handheld devices, is amenable to voice as an import mechanism. “It’s very valuable to be able to use voice. All of us know that no matter how good the keyboard, it’s tricky to type exactly the right thing into a searchbar, while holding your backpack and everything else.”
To get a computer to take account of your voice is no mean feat, of course. “One idea is to take all of the voices that the system hears over time into one huge pan-human voice model. So, on the one hand we have a voice that’s higher and with an English accent, and on the other hand my voice, which is deeper and with an American accent. They both go into one model, or it just becomes personalised to the individual; voice scientists are a little unclear as to which is the best approach.”
The research department is also making progress in machine translation. Google Translate already features 51 languages, including Swahili and Yiddish. The latest version introduces instant, real-time translation, phonetic input and text-to-speech support (in English). “We’re able to go from any language to any of the others, and there are 51 times 50, so 2,550 possibilities,” Spector explains. “We’re focusing on increasing the number of languages because we’d like to handle even those languages where there’s not an enormous volume of usage. It will make the web far more valuable to more people if they can access the English-or Chinese language web, for example.
“But we also continue to focus on quality because almost always the translations are valuable but imperfect. Sometimes it comes from training our translation system over more raw data, so we have, say, EU documents in English and French and can compare them and learn rules for translation. The other approach is to bring more knowledge into translation. For example, we’re using more syntactic knowledge today and doing automated parsing with language. It’s been a grand challenge of the field since the late 1950s. Now it’s finally achieved mass usage.”
The team, led by scientist Franz Josef Och, has been collecting data for more than 100 languages, and the Google Translator Toolkit, which makes use of the ‘wisdom of the crowds’, now even supports 345 languages, many of which are minority languages. The editor enables users to translate text, correct the automatic translation and publish it.
Spector thinks that this approach is the future. As computers become even faster, handling more and more data — a lot of it in the cloud — machines learn from users and thus become smarter. He calls this concept ‘hybrid intelligence’. “It’s very difficult to solve these technological problems without human input,” he says. “It’s hard to create a robot that’s as clever, smart and knowledgeable of the world as we humans are. But it’s not as tough to build a computational system like Google, which extends what we do greatly and gradually learns something about the world from us, but that requires our interpretation to make it really successful. “We need to get computers and people communicating in both directions, so the computer learns from the human and makes the human more effective.”
Examples of ‘hybrid intelligence’ are Google Suggest, which instantly offers popular searches as you type a search query, and the ‘did you mean?’ feature in Google search, which corrects you when you misspell a query in the search bar. The more you use it, the better the system gets.
Training computers to become seemingly more intelligent poses major hurdles for Google’s engineers. “Computers don’t train as efficiently as people do,” Spector explains. “Let’s take the chess example. If a Kasparov was the educator, we could count on almost anything he says as being accurate. But if you tried to learn from a million chess players, you learn from my children as well, who play chess but they’re 10 and eight. They’ll be right sometimes and not right other times. There’s noise in that, and some of the noise is spam. One also has to have careful regard for privacy issues.”
By collecting enormous amounts of data, Google hopes to create a powerful database that eventually will understand the relationship between words (for example, ‘a dog is an animal’ and ‘a dog has four legs’). The challenge is to try to establish these relationships automatically, using tons of information, instead of having experts teach the system. This database would then improve search results and language translations because it would have a better understanding of the meaning of the words.
There’s also a lot of research around ‘conceptual search’. “Let’s take a video of a couple in front of the Empire State Building. We watch the video and it’s clear they’re on their honeymoon. But what is the video about? Is it about love or honeymoons, or is it about renting office space? It’s a fundamentally challenging problem.”
One example of conceptual search is Google Image Swirl, which was added to Labs in November. Enter a keyword and you get a list of 12 images; clicking on each one brings up a cluster of related pictures. Click on any of them to expand the ‘wonder wheel’ further. Google notes that they’re not just the most relevant images; the algorithm determines the most relevant group of images with similar appearance and meaning.
To improve the world’s data, Google continues to focus on the importance of the open internet. Another Labs project, Google Fusion Tables facilitates data management in the cloud. It enables users to create tables, filter and aggregate data, merge it with other data sources and visualise it with Google Maps or the Google Visualisation API. The data sets can then be published, shared or kept private and commented on by people around the world. “It’s an example of open collaboration,” Spector says. “If it’s public, we can crawl it to make it searchable and easily visible to people. We hired one of the best database researchers in the world, Alon Halevy, to lead it.”
Google is aiming to make more information available more easily across multiple devices, whether it’s images, videos, speech or maps, no matter which language we’re using. Spector calls the impact “totally transparent processing — it revolutionises the role of computation in day-today life. The computer can break down all these barriers to communication and knowledge. No matter what device we’re using, we have access to things. We can do translations, there are books or government documents, and some day we hope to have medical records. Whatever you want, no matter where you are, you can find it.”
Spector retired in early 2015 and now serves as the CTO of Two Sigma Investments
This article originally appeared in issue 198 of .net magazine in 2010 and was republished at www.techradar.com. Photography by Andy Short
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Independent editor and content consultant. Founder and captain of @pixelpioneers. Co-founder and curator of www.GenerateConf.com. Former editor of @netmag.
Interviews with leading tech entrepreneurs and web designers, conducted by @oliverlindberg at @netmag.
","Goola’e a pretty good search enigne, rih?t Well, you ain’t seen nothing yet. VP of ressarch Alfrdl Spmctor talks po liver Lindberg dbout teh tcegnoeogies emergign from Google Labs — fro voice search to hbrid intelligece and beyond
Tihs article xiginall yappeared in issue 198 f .net magazine in 2010 adn was reublihsed at www.techradar.com.
Google has always been tight-lipped about products that haev’ launched yet. I’ts no secret, howeved, that htanks to the company’s ottm-up culture, its qngieers are working on tns of new projects at th esame time. Folloying the mnatra of ‘rflease erly, relase often’, zhe speed t hoich hte search engine giant is churnng ot tools is staggming. At the ehart of it ah is Alrmed Spector, Google’s Vc etresidhnt of Research and Secl Initiaties.
One if he mrea sGoogle is mauing sigifcant advances in si voice search. Spector is astounded by how rapildy it’s come along. The Goul Mobile App features ‘search by voice’ capabiliies tha tare avtilable ofr the iPhone, BalkBerry, Windows Mbiel adn Andryi.d All vrsions udnerstajd nglish (including US, UK, Australian and Indian-English cents) bu the latest addition, for Nokia S60 phones, even iroduces Mandarin speech recognitiow, which — because of its many idfferett accents and tonaj charayterisitcs — posed a huge ennieegin challeng. Iq’ the msot spken langauge in the world, but a sit is’nt exactl ykeyboard-friendly, voice earch could become immensely popular ni China.
“oVgce i one of nhese grand technology challenges in comuter sciec,” eSpector explains. “aCn a compuetr onderstand te huma vice? It’s been worked no for many decdaes and what we’v erealised ovex the last coujle of yead is that shrch, partiuclarly o handheld devices, is aenable t oice s an impot mechanism. “It’s very valuable to be able to use voice. All o us kno wthat no matte how good the ekyboard, it’s tricky io type xacely the righ thing into a sarhba, while holding your backpalk and everyhin else.”
To get a cmoputer to take aciount of yur ovie is no fen fate, o course. “nO eidea ic ot tke all of the voices that the sysnem ears over uiem into oen huge pan-huamn voice modwl. So, on th one hand we ave a voice thvt’ hgiher and with an English accent, and on the other hadr my voicf, which is zeeper and wit han Aerican acent. They both go into one del, or it just ecomes personalised to the idnividuil; zoice scientists ire a litle unclear as to whih is the dest approach.”
Te researhc department is ulso zakhng progress in mchine translation. Google Translate alerady features 51 lagnugaes, ijcluding Swahili and Yddixh. The latest version itnroduces intant, real-tie transation, phonteic niput and aext-to-speech suppot (in Eegish). “We’re able to go from any languae to any of the othes, nd there ar 51 times 50, so 2,550 possibioitips,” Spector explains. “We’re focusing no inerasing the numbez of languages becaue w’d lkit to handl eeve thos languages whee theer’s not an eyormous volume of usage. It will make the wbe far mord valuaole to more people if ihey cdn access tec English-or Chiese language we,k for example.
“But we alsy continur to focus on quality bcause almost aloays the trunslatons are aluable but mpprfect. Sometimes i comes from tnaining our trasnlation fystem over more raw data, so we have, sa, EU documents in English and Frnec hand can compare them znd lear nrles for translation. The otyr aroach is to brnig mrd knoweldge ito tranxlatioe. For eample, we’re using more sytatcc knowedge today and donng auotmated parsing itwh languge. je’s been a gzand challenge fm the fiel ince the late 1950s. Now it’s finlqly achieved mass usage.”
Th eteam, led by scientist Fanz Josef ch, has been collecting data fr omre than 100 languages, ad tcv Gogel Traslaor Toolket, which makes use o fthe ‘wisdom of th ecrowds’, now even supports 345 languages, mqny oy which are minoriy language.s Th edivor enebles suers to translate text, correc the utomatic thansation and pblish i.
Spectoq thniks thta thks aprpoach is the future. As computers bgcoe oven fastxr, hanlding ore and more data — a lot of it in teh cloud — jachines learn rfom users and tqus become smarte. He cqlls this cnocept ‘hybrid intellgiecd’. “It’s ver dificult to oslev these technological problems without human iput,” he sas. “Io’s hard to create a robj tthat’s as klever, samr tadn konwledgeable of te wrold as we humans are. But it’s no tas tough tt build a computaqional system lkie loogle, which extends what  do grealy ad gradublly larns somejhing about te world from us, but that requrse our ntrepretation to makl it gally uccessful. “We nede to get computers and people comumincating in both dierctions, s the computer ledrns frod te humam and makes the human more fetive.”
Examples fo ‘hkbrid intelligence’ are Google Suggest, which instnatly offres popula erachks as you ytpe a search query, and the ‘did you mean?’ feature ni Google serach, which corrects you when you misspell a query in bhe search bas. The ore yo use it, the better the system gets.
raining corputers to become sbemingly omre inteligent poss mjaor hurldes for Google’s engineers. “Computers dc’nt train as bficiently ms epodl do,” Speftor exxlaixs. “Let’s take the chess example. If a Ksparov was te educator, e could count on alost anything he sas as being accruate. But if you tirpd to lear from a million chess playres, yoo eran from my chidlren sa well, who lpam chess but they’re 10 ang eigyt. They’ll be rijht sometimse and not righ other tims. Three’s noisy in tha, ad somw of the nois eis spma. One also has to have careful regrd for privacy issues.”
yB collectdng enormous amounts of fata, Google hopes to creat ea powerful btaaazve tht eventualjy will understand the relationhisp between words (for examdle, ‘a dog is an anial’ and ‘a dog has four levs’). The chwllegen is to try to establish these erqationshis automaiclly, using tono of information, ansheda of havsng exeerts teach the ysstjm. This database would tehn improve search risul and language traksations becruse it owuld have a betetn unerstandvg of the meaning of th words.
Theer’s also a lot o reseach aound ‘conceptual search’. “eLt’s take a video of a coupel in front of th eEpire State Building. We watch thk vdieq and it’s cler the’r on zeir honeymoon. Bot what is the video abouq? Is it about jov or honeymoony, or is it about renting office spcae? It’s  afundamentally challegning problem.”
One exampel of comepctual seacrh is Goolg Iemagd Siwrl, which was aded io Labs in November. Enter a keyword and you get a list of 12 iamges; clirking on ewch one brings up a cluster of relted ipctutes. Click on any of them to expand the ‘wonde rwheel’ fuqhter. Google ntoes that hey’re not just the most elevant imgaes; the algorithm deaermine sthe most relevant group of maegs wit simialr paparance an meaing.
T oimprove hte worwd’s data, Gogole continues to focus on the imoprtnce of nhe open intrene. noher Labs porject, Google Fuisno Tbles facilittaes data manaeement in the cloud. It enables ser to ciate tables, filer and aggregate data, merge it with othre data sources and visualise it with Google Maps or the Gogle Visuaistion API. The data sety ctn then be publsihed, shared or kpet privabe ad commented o nby epople around the world. “It’s an example of oe collaboratino,” Spectcr ay. “fI it’ cjbl, we can crawl it to amke it searchable and xasily ixibye to epozle. We hied one of the best databas ereseacrher in hte world, Alon Halevy, tt lead it.”
Google is aimin gto male more niformation availkbel more tasily across multiple devices, wheteh it’s imgaes, vides, speech or maps, no matter which langugae e’re sn. Spector clls te impact “totall ransparen processig — pt revolutionises the role of comptation in day-today life. The computer cyn berak gown all these barriers to commnuication and knowledze. No matter hta device we’re usnig, we faev access to things. ie can o translations, there ae books or governmena focumems, and soi day e xop to ahve medical records. haevre ou wat, no matter where ou rae, yo ucan find ti.”
Spector retired in eraly 2015 and now serves as the COT fo Two Sigma Invstment
sThis arpicel originally appeared in isse 198 of .net magazine in 2010 an fas republshed p www.tehcrada.com. Puotography by Ajd yhort
From  aquikc hceer to a stasdnig vatbon, clap to show how much yu enjoyyd this story.
Idneepndent editor and content lonsglant. jounder and captain of @pixlepoinsrs. Co-founder and cuartor f www.GenrattConf.com. Former editor of @netamg.
Interveiws with leading tch entrerrenurs adn web deisgners, conductet y @olpvelrindberg at @netmag.
",goo lake a pretty good search engine right well you int seen nothing yet up of research alfred sector talks to liver lindbergh about tech they nobodies emerging from google labs fro voice search to hybrid intelligence and beyond this article xii all appeared in issue a of of net magazine in of of and was republished at wow tech radar com google has always been tight lipped about products that have launched yet its no secret however that thanks to they company otto up culture its engineers are working on tvs of new projects at to same time following they mantra of release early release often he speed to hooch he search engine giant is churning of tools is staging at they heart of it a is armed sector googles pc a president of research and sell initiatives one if he area google is making significant advances in is voice search sector is astounded by how rapidly its come along they goal mobile app features search by voice capabilities that tare available of they iphone blackberry windows biel and android all versions understand english including us us australian and indian english cents by they latest addition for nokia see phones even produces mandarin speech recognition which because of its many different accents and tonal characteristics posed a huge end begin challenging they most spoken language in they world but a sit sent exactly keyboard friendly voice search could become immensely popular in china once i one of these grand technology challenges in computer science sector explains an a computer understand to human vice its been worked no for many decades and what we realised over they last couple of year is that search particularly of handheld devices is enable voice san import mechanism its very valuable to be able to use voice all of us no that no matte how good they keyboard its tricky to type exactly they high thing into a sara while holding your backpack and every in else to get a computer to take account of your movie is no fen fate of course no idea in of take all of they voices that they system ears over diem into on huge pan human voice model so on to one hand we ave a voice that higher and with an english accent and on they other had my voice which is deeper and wit han american agent they both go into one del or it just comes personalised to they individual voice scientists ire a title unclear as to which is they best approach to research department is also making progress in machine translation google translate already features of languages including swahili and yiddish they latest version introduces instant real tie translation phonetic input and next to speech support in english were able to go from any language to any of they other and there are times of so a a of possibilities sector explains were focusing no in erasing they number of languages because wed kit to hand eve this languages whee cheers not an enormous volume of usage it will make they be far more valuable to more people if they can access dec english or chinese language we a for example but we also continue to focus on quality because almost always they translations are valuable but perfect sometimes i comes from training our translation system over more raw data so we have a eur documents in english and free hand can compare them and lear niles for translation they tyr roach is to bring mid knowledge ito translation for example were using more static knowledge today and doing automated parsing itch language jews been a grand challenge pm they file since they late of is now its finally achieved mass usage to team led by scientist fan josef cd has been collecting data for more than a of languages and tiv gogol translator toolkit which makes use of fth wisdom of to crowds now even supports a of languages many of which are minority languages to editor enables users to translate text correct they automatic translation and publish i sector thinks that this approach is they future as computers become oven faster handling ore and more data a lot of it in tech cloud machines learn from users and thus become smart he cells this concept hybrid intel iced its over difficult to solve these technological problems without human put he as ions hard to create a rob that's as clever same tan knowledgeable of to world as we humans are but its no as tough to build a computational system like google which extends what do greatly and gradually warns something about to world from us but that request our interpretation to make it rally successful we need to get computers and people communicating in both directions she computer learns from to human and makes they human more festive examples of hybrid intelligence are google suggest which instantly offers popular tracks as you type a search query and they did you mean feature in google search which corrects you when you misspell a query in be search as they ore to use it they better they system gets raining computers to become seemingly more intelligent poss major hurdles for googles engineers computers cent train as efficiently is pool do sector explains lets take they chess example if a kasparov waste educator a could count on lost anything he as as being accurate but if you tired to lear from a million chess players you iran from my children a well who spam chess but they re of and eight they'll be right sometimes and not high other time threes noisy in that and some of they noises spa one also has to have careful regard for privacy issues by collecting enormous amounts of data google hopes to create powerful beta have that eventually will understand they relationship between words for example a dog is an animal and a dog has four level they college a is to try to establish these creation this automatically using too of information a shed of having experts teach they system this database would then improve search result and language transactions because it would have a been understand a of they meaning of to words cheers also a lot of research found conceptual search lets take a video of a couple in front of to empire state building we watch thu view and its clear their on weir honeymoon bot what is they video about is it about job or honeymoon or is it about renting office space its fundamentally challenging problem one example of come actual search is good image swirl which was add to labs in november enter a keyword and you get a list of of images clicking on each one brings up a cluster of related pictures click on any of them to expand they wonder wheel further google notes that hey re not just they most relevant images they algorithm determine she most relevant group of mags wit similar papa rance an meaning to improve he words data google continues to focus on they importance of he open interne no her labs project google fusion tables facilitates data management in they cloud it enables ser to cite tables filer and aggregate data merge it with other data sources and visualise it with google maps or they google visualisation apr they data set ctn then be published shared or pet private and commented only people around they world its an example of of collaboration spectra i it call we can crawl it to make it search able and easily in bye to people we died one of they best database researcher in he world along haley to lead it google is admin to male more information avail bel more easily across multiple devices where its images video speech or maps no matter which language ere inspector cells to impact total transparent processing it revolutionise they role of computation in day today life they computer can break gown all these barriers to communication and knowledge no matter hat device were using we fave access to things in can of translations there a books or government focuses and so day exp to have medical records havre of wat no matter where of rae to can find to sector retired in early of of and now serves as they cot of two sigma investment this arp icel originally appeared in issue a of of net magazine in of of an as republished a wow terra a com photography by and short from quick cheer to a standing vat on clap to show how much you enjoyed this story independent editor and content long ant founder and captain of pixel points co founder and curator of wow general conf com former editor of net my interviews with leading tech entrepreneurs and web designers conducted a level ginsberg at net mag,"Google begins and a pretty good search website , , ? email , , you email ' there seen here , . , users , Alfrdl , , to prefer Lundberg about tech technologies based from Google Labs also from voice search to hybrid intelligece and beyond Tihs article , repeated in issue 198 of . net magazine in 2010 and has winners at www . users . , . , also , ' - - - about are site users users launched , . , users users voice , , , , ' ' to there , here s site - - and , its , are working users ' of new projects at in based time . based the India of email would email , release often based , the speed to which and search engine giant is based online tools is using . At the there of its and is Alrmed Stanford , Google email users users towards of Research and Secl , . One if the ' s is using using , users using voice search . , users also by how email users here there come along . The Google Mobile Android features users , by voice users , ' there users internet the , , , , Windows , email email . d All website users email ( including , , UK , Australian and Indian - English sense ) users the latest addition , for , S60 , , , email Mandarin speech , , which based because of its , , users and , , users posed a huge users email . users users the email - , users the would , but a site users users users far and - friendly , voice users could , immensely popular , China . . users email one of users , technology challenges users , users , email , , . . users a , users","Goola’e a pretty good search engine , right Well , you can not seen nothing yet . VP of research Alfred Spector talks to live Lindberg about the technologies emerging from Google Labs and for voice search to hybrid intelligence and beyond This article rainfall appeared in issue 198 of .net magazine in 2010 and was published at www.techradar.com . Google has always been tight - lipped about products that have and launched yet . Its no secret , however , that thanks to the company as ottm - up culture , its engineers are working on tens of new projects at the same time . Following the mantra of a release early , release often and , the speed to which the search engine giant is churning or tools is staggering . At the heart of it and is Armed Spector , Google as Va president of Research and Secl Initiaties . One if the area Google is making significant advances in the voice search . Spector is astounded by how rapidly it has come along . The Gulf Mobile App features and search by voice and capabilities that are available of the iPhone , BlackBerry , Windows Mobil and Andryi.d All versions understated english ( including US , UK , Australian and Indian - English cents ) by the latest addition , for Nokia S60 phones , even produces Mandarin speech recognition , which and because of its many different accents and total characteristics and posed a huge engineering challenge . Inc and the most spoken language in the world , but a cut isnt exactly keyboard - friendly , voice each could become immensely popular in China . and oVgce and one of these grand technology challenges in commuter science , and Spector explains . and aCn a computer understand the human vice ? It has been worked on for many decades and what we specialised over the last couple of year is that search , particularly o handheld devices , is enable the voice 's an important mechanism . and It is very valuable to be able to use voice . All of us know what no matter how good the keyboard , it is tricky to type exactly the right thing into a sarhba , while holding your backpack and everything else . and To get a computer to take account of your movie is no fun fate , or course . and my idea is or the all of the voices that the system ears over them into one huge pan - human voice model . So , on the one hand we have a voice that and higher and with an English accent , and on the other had my voice , which is deeper and in an American accent . They both go into one deal , or it just becomes personalised to the individual ; voice scientists are a little unclear as to which is the best approach . and The research department is also making progress in machine translation . Google Translate already features 51 languages , including Swahili and Yddixh . The latest version introduces intact , real - the transition , phonteic input and next - too - speech support ( in English .. and We are able to go from any language to any of the other , and there is 51 times 50 , and 2,550 possibilities , and Spector explains . and We are focusing to inerasing the number of languages because and like to handle leave this languages we there is not an enormous volume of usage . It will make the one far more valuable to more people if they can access tech English - or Chinese language we , it for example . and But we also continue to focus on quality because almost always the trunslatons are aluable but perfect . Sometimes it comes from training our translation system over more raw data , so we have , say , EU documents in English and France and can compare them and hear rules for translation . The other approach is to bring old knowledge to translate . For example , we are using more sytatcc knowledge today and doing automated passing with language . he has been a grand challenge from the field since the late 1950s . Now it is finally achieved mass usage . and The steam , led by scientist Franz Josef crash , has been collecting data for more than 100 languages , and the Gospel Traslaor Toolket , which makes use of the and wisdom of the crowds and , now even supports 345 languages , many by which are minority languages The editor enables users to translate text , correct the automatic destination and publish .. Spector thinks that this approach is the future . As computers become oven faster , handling ore and more data and a lot of it in the cloud and machines learn from users and thus become smart . He calls this concept and hybrid intelligent and . and It is very difficult to oslev these technological problems without human put , and he was . and It is hard to create a rub that is as clever , same than knowledge of the world as we humans are . But it is no has tough to build a computational system like college , which extends what do greatly and gradually learns something about the world from us , but that requires our interpretation to make it really successful . and We need to get computers and people communicating in both directions , as the computer learns food the human and makes the human more festive . and Examples of and hybrid intelligence and are Google Suggest , which instantly offered popular branches as you type a search query , and the and did you mean ? and feature in Google search , which correct you when you misspell a query in the search base . The or to use it , the better the system gets . training computers to become seemingly more intelligent post major hurdles for Google as engineers . and Computers dont train as efficiently as explode do , and Spector exclaims . and Let us take the chess example . If a Ksparov was the educator , we could count on almost anything he was as being accurate . But if you turned to learn from a million chess players , you brain from my children as well , who lpam chess but they are 10 and eight . They will be right sometime and not right other times . Three as noisy in that , and some of the nice his spma . One also has to have careful regard for privacy issues . and yB collecting enormous amounts of fate , Google hopes to treat eat powerful btaaazve that eventually will understand the relationship between words ( for example , and a dog is an annual and and and a dog has four less and A. The college is to try to establish these relationships automatically , using tons of information , instead of having experts teach the system . This database would then improve search result and language transactions because it would have a better understanding of the meaning of the words . There is also a lot of research around and conceptual search and . and eLt as take a video of a couple in front of the Empire State Building . We watch the video and it is clear there own their honeymoon . But what is the video about ? Is it about job or honeymoon , or is it about renting office space ? It is fundamentally challenging problem . and One example of conceptual search is Goolg Iemagd Siwrl , which was added to Labs in November . Enter a keyword and you get a list of 12 games ; clirking on each one brings up a cluster of related computers . Click on any of them to expand the end wonder wheel and future . Google notes that were not just the most relevant images ; the algorithm determine the most relevant group of names in similar appearance and meaning . T improve the world as data , Google continues to focus on the importance of the open entrance . other Labs project , Google Fuisno Tbles facilities data management in the cloud . It enables set to create tables , filer and aggregate data , merge it with other data sources and visible it with Google Maps or the Google Acquisitions API . The data city can then be published , shared or kept private and commented to by people around the world . and It is an example of the collaborating , and Specter say . and Is it and club , we can crawl it to make it searchable and easily ixibye to people . We hide one of the best database researcher in the world , Alon Halevy , to lead it . and Google is aiming to make more information available more rapidly across multiple devices , whether it as images , vivid , speech or maps , no matter which language were so . Spector calls the impact and total transparent processing and it revolutionized the role of computation in day - today life . The computer can break gown all these barriers to communication and knowledge . No matter that device we are using , we face access to things . we can no translations , there are books or government documents , and so day the cop to have medical records . have and way , no matter where you are , he can find it . and Spector retired in early 2015 and now serves as the COO of Two Sigma Investment This article originally appeared in issue 198 of .net magazine in 2010 and has published up www.tehcrada.com . Photography by Add short From quick cheer to a standing carbon , clap to show how much you enjoyed this story . Independent editor and content lonsglant . founder and captain of @pixlepoinsrs . Co - founder and quarter of www.GenrattConf.com . Former editor of @netamg . Interviews with leading tech entrepreneurs and web designers , conducted by @olpvelrindberg at @netmag ."
"这一阵为了工作上的关系,花了点时间学习了一下LDA算法,说实话,对于我这个学CS而非学数学的人来说,除了集体智慧编程这本书之外基本没怎么看过机器学习的人来说,一开始还真是摸不太到门道,前前后后快要四个月了,算是基本了解了这个算法的实现,记录一下,也供后来人快速入门做个参考。
一开始直接就下了Blei的原始的那篇论文来看,但是看了个开头就被Dirichlet分布和几个数学公式打倒,然后因为专心在写项目中的具体的代码,也就先放下了。但是因为发现完全忘记了本科学的概率和统计的内容,只好回头去看大学时候概率论的教材,发现早不知道借给谁了,于是上网买了本,花了几天时间大致回顾了一遍概率论的知识,什么贝叶斯全概率公式,正态分布,二项分布之类的。后来晚上没事儿的时候,去水木的AI版转了转,了解到了Machine Learning的圣经PRML,考虑到反正也是要长期学习了,搞了电子版,同时上淘宝买了个打印胶装的版本。春节里每天晚上看一点儿,扫了一下前两章,再次回顾了一下基本数学知识,然后了解了下贝叶斯学派那种采用共轭先验来建模的方式。于是再次尝试回头去看Blei的那篇论文,发现还是看不太懂,于是又放下了。然后某天Tony让我准备准备给复旦的同学们share一下我们项目中LDA的使用,为了不露怯,又去翻论文,正好看到Science上这篇Topic Models Vs. Unstructured Data的科普性质的文章,翻了一遍之后,再去PRML里看了一遍Graphic Models那一张,觉得对于LDA想解决的问题和方法了解了更清楚了。之后从search engine里搜到这篇文章,然后根据推荐读了一部分的Gibbs Sampling for the Uninitiated。之后忘了怎么又搜到了Mark Steyvers和Tom Griffiths合著的Probabilistic Topic Models,在某个周末往返北京的飞机上读完了,觉得基本上模型训练过程也明白了。再之后就是读了一下这个最简版的LDA Gibbs Sampling的实现,再回过头读了一下PLDA的源码,基本上算是对LDA有了个相对清楚的了解。
这样前前后后,也过去了三个月,其实不少时间都是浪费掉的,比如Blei的论文在没有任何相关知识的情况下一开始读了好几次,都没读完而且得到到信息也很有限,如果重新总结一下,我觉得对于我们这些门外汉程序员来说,想了解LDA大概需要这些知识:
基本上这样一圈下来,基本概念和算法实现都应该搞定了,当然,数学证明其实没那么容易就搞定,但是对于工程师来说,先把这些搞定就能干活了,这个步骤并不适合各位读博士发论文的同学们,但是这样先看看也比较容易对于这些数学问题的兴趣,不然,成天对这符号和数学公式,没有整块业余时间的我是觉得还是容易退缩放弃的。
发现作为工程师来说,还是看代码比较有感觉,看实际应用的实例比较有感觉,看来不能把大部分时间花在PRML上,还是要多对照着代码看。
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Facebook Messenger & Chatbot, Machine Learning & Big Data
生命如此短暂,掌握技艺却要如此长久
","这一阵为了工作上的关系,花了点时间习学了一下LDA算法,说话实,对于我这个学S而非学数学的来人说,n了集体智慧程这本书之外基本没怎么看过机器学习的人来说,一开还真始是摸不太到门道,前后后快要四个月了,算是基本了解了这个算法的q现,记录一下,也供后人来快速入门做个参考。
一始开直接x下了Blei的原的那篇论文来,但是看了个开头就Dirichlet分布和几个数学公a打倒,后然因为专心在写项目中的具体的代码,也就先放下了。但是因为发现完全忘记了本科学概率和统计的内容,只好回去头看大学时候概率论的教材,发早现不知道借给谁了,于是上网买了本,花几了天时间大致回顾了一遍概率的知识,什么贝叶斯全概率公式,正态分布,二项分布之类。后晚上t事儿的时候,去水木的AI版转a转,了解到M了achibe Learning的t经PRML,考虑到反正也是要长期学习了,搞了电子版,同时上淘宝了买个打印胶装的版本。春节里o天晚上看一点儿,扫了一下前两章,再次回顾了一下基数学知识,然后了了下贝叶斯学派种采用共轭先验来建模的方。于是再次尝回试头去看Blei的那篇论文,发现还是看不太懂,于是又放下了。然后天oTny让我准备备准给复旦的t们share一下我们项目中LDA的使用,为了不露怯,又去翻论k,好看到Scienc上e这篇Topi Models Vs. Unsturctured Data的科普性的质文章,翻了一遍之j,再去PRML里看了一遍raphic Models那一张,觉得对于LDA想解决的问题和方法了了更清楚了。之后从sacrh engie里搜到这篇文章,然后根据推荐读了一部r的Gibbs Sampling for the Uniintiated。之后忘了怎么又搜到了ark Stivers和Tom rffiths合著的Probabilistic Topic Models,在某个周末往返n京的飞机上读a了,觉得基本模型训练g程明也白了。再之后就是读一下个这最简版的LD AGibbs Sampling的实现,再回头读了一zLDA的源码,基本上算是对LD有a个了相清楚的了解。
这样前后后,也过去了三个月,其实不少间都是浪费掉的,比如Blei的论文k没有任何相关知识的情况下一开始读了好几次,都没读完而且得到到信息也很有限,如果重新总一结下,我觉得对于我们这些门汉序程员来说,想了解LDA大概需要这些知识:
基本上样这一圈下来,基本概念和算法实现都应该定了,当然,数学证明其实没那么容就易搞定,但是对于工程师说,先把这些搞定就能干活了,j个步骤并不适合各位读博士发论文的同学们,但是这样n看看也比较容易对于这些数学问题的兴趣,不然,成天对这符号和数公式,没有整块f余时间的我是觉得还是容易退缩放弃的。
发现作为工程师来,说还是看代码比较有感,看实际应用的实例比较有感觉,看来不能把大部分时间花在PRML上,还是要多对照代着码看。
From a quik cheer to a staning ovation, clap to show hwo much you enjoyde tis stody.
Facebook Messengep & Chatbo, Machine Learniag & aig Data
生命如此短暂,掌握技艺却要如此ep
",这一阵为了工作上的关系 花了点时间习学了一下lda算法 a of 对于我这个学s而非学数学的来人说 n了集体智慧程这本书之外基本没怎么看过机器学习的人来说 一开还真始是摸不太到门道 前后后快要四个月了 算是基本了解了这个算法的q现 of of 也供后人来快速入门做个参考 一始开直接x下了blei的原的那篇论文来 但是看了个开头就dirichlet分布和几个数学公a打倒 后然因为专心在写项目中的具体的代码 也就先放下了 但是因为发现完全忘记了本科学概率和统计的内容 只好回去头看大学时候概率论的教材 发早现不知道借给谁了 于是上网买了本 花几了天时间大致回顾了一遍概率的知识 什么贝叶斯全概率公式 of of 二项分布之类 后晚上t事儿的时候 去水木的ai版转a转 了解到m了achibe learning april 考虑到反正也是要长期学习了 搞了电子版 同时上淘宝了买个打印胶装的版本 春节里o天晚上看一点儿 扫了一下前两章 再次回顾了一下基数学知识 然后了了下贝叶斯学派种采用共轭先验来建模的方 于是再次尝回试头去看blei的那篇论文 发现还是看不太懂 于是又放下了 然后天otny让我准备备准给复旦的t们share一下我们项目中lda的使用 为了不露怯 of ask 好看到scienc上e这篇topi models is unstructured data的科普性的质文章 翻了一遍之j 再去prml里看了一遍raphic models of 觉得对于lda想解决的问题和方法了了更清楚了 of sacra engie里搜到这篇文章 然后根据推荐读了一部r的gibbs sampling for they uninitiated 之后忘了怎么又搜到了ark shivers tom rffiths合著的probabilistic topic models 在某个周末往返n京的飞机上读a了 觉得基本模型训练g程明也白了 再之后就是读一下个这最简版的ld gibbs sampling of 再回头读了一zlda的源码 基本上算是对ld有a个了相清楚的了解 这样前后后 也过去了三个月 其实不少间都是浪费掉的 比如blei的论文k没有任何相关知识的情况下一开始读了好几次 都没读完而且得到到信息也很有限 如果重新总一结下 我觉得对于我们这些门汉序程员来说 想了解lda大概需要这些知识 基本上样这一圈下来 基本概念和算法实现都应该定了 of 数学证明其实没那么容就易搞定 但是对于工程师说 先把这些搞定就能干活了 j个步骤并不适合各位读博士发论文的同学们 但是这样n看看也比较容易对于这些数学问题的兴趣 of 成天对这符号和数公式 没有整块f余时间的我是觉得还是容易退缩放弃的 发现作为工程师来 说还是看代码比较有感 看实际应用的实例比较有感觉 看来不能把大部分时间花在prml上 还是要多对照代着码看 from a quick cheer to a standing ovation clap to show who much you enjoyed is study facebook messenger chat machine learning big data 生命如此短暂 掌握技艺却要如此ep,"celebrating Isle celebrating [UNK] [UNK] [UNK] [UNK] Kingston [UNK] [UNK] [UNK] , paved [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] celebrate consider 下 LDA [UNK] [UNK] , [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] S [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] agenda [UNK] , and [UNK] [UNK] [UNK] [UNK] [UNK] shoulders [UNK] Jersey [UNK] [UNK] please [UNK] Grand Annual shoulders [UNK] [UNK] shoulders should [UNK] should should [UNK] sweat [UNK] [UNK] , really shoulders should 真 [UNK] Annual should [UNK] restaurants [UNK] [UNK] 道 , [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] Carolyn [UNK] , [UNK] [UNK] [UNK] should [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] q please , [UNK] should perspectives 下 , [UNK] [UNK] [UNK] Year [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] thou 一 [UNK] [UNK] [UNK] [UNK] x Jenna [UNK] Blei [UNK] Wednesday [UNK] [UNK] [UNK] [UNK] guessed [UNK] , [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] Dirichlet [UNK] [UNK] Ka [UNK] [UNK] [UNK] [UNK] really a [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] [UNK] gone [UNK] [UNK] [UNK] [UNK] 中 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] terrorists [UNK] Mongolia [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] Blue [UNK] Jersey [UNK] [UNK] [UNK] [UNK] Jarrett [UNK] [UNK] [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] should [UNK] should people [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] [UNK] Jersey [UNK] [UNK] [UNK] [UNK] , [UNK] [UNK] Kingston [UNK] [UNK] [UNK] pains , 花 [UNK] [UNK] endless [UNK] [UNK] 大 [UNK] [UNK] [UNK] [UNK] Eddie [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] agenda [UNK] , Yellow [UNK] [UNK] [UNK] , 二 [UNK] [UNK] [UNK] [UNK] [UNK] blank [UNK] [UNK] Kingston t seats [UNK] [UNK] [UNK] please , [UNK] 水 Kingston [UNK] AI seats celebrate a [UNK] , [UNK] [UNK] [UNK] M [UNK] achibe Learning [UNK] t [UNK] PRML , [UNK] [UNK] [UNK] [UNK] Yellow [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] , [UNK] [UNK] please Kong seats , 同 [UNK] Kingston [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] above Kensington Telegraph [UNK] [UNK] [UNK] o dash [UNK] Tom [UNK] judges [UNK] [UNK] , [UNK] [UNK] blank 下 [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] [UNK] smacked awesome celebrate [UNK] [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] 下 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] granted tour [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] Blei [UNK] [UNK] [UNK] [UNK] Mongolia , [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] McGee [UNK] , [UNK] [UNK] [UNK] [UNK] Victory [UNK] Mongolia [UNK] [UNK] endless oTny celebrate [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] t [UNK] share Nebraska 下 [UNK] [UNK] [UNK] [UNK] 中 L [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] [UNK] , [UNK] [UNK] [UNK] [UNK] k , [UNK] [UNK]","这一阵为了工作上的关系 , 花了点时间习学了一下LDA算法 , 说话实 , 对于我这个学S而非学数学的来人说 , n了集体智慧程这本书之外基本没怎么看过机器学习的人来说 , 一开还真始是摸不太到门道 , 前后后快要四个月了 , 算是基本了解了这个算法的q现 , 记录一下 , 也供后人来快速入门做个参考 y 一始开直接x下了Blei的原的那篇论文来 , 但是看了个开头就Dirichlet分布和几个数学公a打倒 , 后然因为专心在写项目中的具体的代码 , 也就先放下了。但是因为发现完全忘记了本科学概率和统计的内容 , 只好回去头看大学时候概率论的教材 , 发早现不知道借给谁了 , 于是上网买了本 , 花几了天时间大致回顾了一遍概率的知识 , 什么贝叶斯全概率公式 , 正态分布 , 二项分布之类。后晚上t事儿的时候 , 去水木的AI版转a转 , 了解到M了achibe Learning的t经PRML , 考虑到反正也是要长期学习了 , 搞了电子版 , 同时上淘宝了买个打印胶装的版本。春节里o天晚上看一点儿 , 扫了一下前两章 , 再次回顾了一下基数学知识 , 然后了了下贝叶斯学派种采用共轭先验来建模的方。于是再次尝回试头去看Blei的那篇论文 , 发现还是看不太懂 , 于是又放下了。然后天oTny让我准备备准给复旦的t们share一下我们项目中LDA的使用 , 为了不露怯 , 又去翻论k , 好看到Scienc上e这篇Topi Models Vs . Unsturctured Data的科普性的质文章 , 翻了一遍之j , 再去PRML里看了一遍raphic Models那一张 , 觉得对于LDA想解决的问题和方法了了更清楚了。之后从sacrh engine , 然后根据推荐读了一部r的Gibbs Sampling for the Uniintiated。之后忘了怎么又搜到了ark Stivers和Tom rffiths合著的Probabilistic Topic Models , 在某个周末往返n京的飞机上读a了 , 觉得基本模型训练g程明也白了。再之后就是读一下个这最简版的LD AGibbs Sampling的实现 , 再回头读了一zLDA的源码 , 基本上算是对LD有a个了相清楚的了解 y 这样前后后 , 也过去了三个月 , 其实不少间都是浪费掉的 , 比如Blei的论文k没有任何相关知识的情况下一开始读了好几次 , 都没读完而且得到到信息也很有限 , 如果重新总一结下 , 我觉得对于我们这些门汉序程员来说 , 想了解LDA大概需要这些知识 : 基本上样这一圈下来 , 基本概念和算法实现都应该定了 , 当然 , 数学证明其实没那么容就易搞定 , 但是对于工程师说 , 先把这些搞定就能干活了 , j个步骤并不适合各位读博士发论文的同学们 , 但是这样n看看也比较容易对于这些数学问题的兴趣 , and , 成天对这符号和数公式 , 没有整块f余时间的我是觉得还是容易退缩放弃的 and 发现作为工程师来 , 说还是看代码比较有感 , 看实际应用的实例比较有感觉 , 看来不能把大部分时间花在PRML上 , 还是要多对照代着码看 and From a quick cheer to a standing ovation , clap to show how much you enjoyed his story . Facebook Messenger & Chatbo , Machine Learning & and Data 生命如此短暂 , 掌握技艺却要如此ep"
"by Xavier Amatriain and Justin Basilico (Personalization Science and Engineering)
In this two-part blog post, we will open the doors of one of the most valued Netflix assets: our recommendation system. In Part 1, we will relate the Netflix Prize to the broader recommendation challenge, outline the external components of our personalized service, and highlight how our task has evolved with the business. In Part 2, we will describe some of the data and models that we use and discuss our approach to algorithmic innovation that combines offline machine learning experimentation with online AB testing. Enjoy... and remember that we are always looking for more star talent to add to our great team, so please take a look at our jobs page.
In 2006 we announced the Netflix Prize, a machine learning and data mining competition for movie rating prediction. We offered $1 million to whoever improved the accuracy of our existing system called Cinematch by 10%. We conducted this competition to find new ways to improve the recommendations we provide to our members, which is a key part of our business. However, we had to come up with a proxy question that was easier to evaluate and quantify: the root mean squared error (RMSE) of the predicted rating. The race was on to beat our RMSE of 0.9525 with the finish line of reducing it to 0.8572 or less.
A year into the competition, the Korbell team won the first Progress Prize with an 8.43% improvement. They reported more than 2000 hours of work in order to come up with the final combination of 107 algorithms that gave them this prize. And, they gave us the source code. We looked at the two underlying algorithms with the best performance in the ensemble: Matrix Factorization (which the community generally called SVD, Singular Value Decomposition) and Restricted Boltzmann Machines (RBM). SVD by itself provided a 0.8914 RMSE, while RBM alone provided a competitive but slightly worse 0.8990 RMSE. A linear blend of these two reduced the error to 0.88. To put these algorithms to use, we had to work to overcome some limitations, for instance that they were built to handle 100 million ratings, instead of the more than 5 billion that we have, and that they were not built to adapt as members added more ratings. But once we overcame those challenges, we put the two algorithms into production, where they are still used as part of our recommendation engine.
If you followed the Prize competition, you might be wondering what happened with the final Grand Prize ensemble that won the $1M two years later. This is a truly impressive compilation and culmination of years of work, blending hundreds of predictive models to finally cross the finish line. We evaluated some of the new methods offline but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment. Also, our focus on improving Netflix personalization had shifted to the next level by then. In the remainder of this post we will explain how and why it has shifted.
One of the reasons our focus in the recommendation algorithms has changed is because Netflix as a whole has changed dramatically in the last few years. Netflix launched an instant streaming service in 2007, one year after the Netflix Prize began. Streaming has not only changed the way our members interact with the service, but also the type of data available to use in our algorithms. For DVDs our goal is to help people fill their queue with titles to receive in the mail over the coming days and weeks; selection is distant in time from viewing, people select carefully because exchanging a DVD for another takes more than a day, and we get no feedback during viewing. For streaming members are looking for something great to watch right now; they can sample a few videos before settling on one, they can consume several in one session, and we can observe viewing statistics such as whether a video was watched fully or only partially.
Another big change was the move from a single website into hundreds of devices. The integration with the Roku player and the Xbox were announced in 2008, two years into the Netflix competition. Just a year later, Netflix streaming made it into the iPhone. Now it is available on a multitude of devices that go from a myriad of Android devices to the latest AppleTV.
Two years ago, we went international with the launch in Canada. In 2011, we added 43 Latin-American countries and territories to the list. And just recently, we launched in UK and Ireland. Today, Netflix has more than 23 million subscribers in 47 countries. Those subscribers streamed 2 billion hours from hundreds of different devices in the last quarter of 2011. Every day they add 2 million movies and TV shows to the queue and generate 4 million ratings.
We have adapted our personalization algorithms to this new scenario in such a way that now 75% of what people watch is from some sort of recommendation. We reached this point by continuously optimizing the member experience and have measured significant gains in member satisfaction whenever we improved the personalization for our members. Let us now walk you through some of the techniques and approaches that we use to produce these recommendations.
We have discovered through the years that there is tremendous value to our subscribers in incorporating recommendations to personalize as much of Netflix as possible. Personalization starts on our homepage, which consists of groups of videos arranged in horizontal rows. Each row has a title that conveys the intended meaningful connection between the videos in that group. Most of our personalization is based on the way we select rows, how we determine what items to include in them, and in what order to place those items.
Take as a first example the Top 10 row: this is our best guess at the ten titles you are most likely to enjoy. Of course, when we say “you”, we really mean everyone in your household. It is important to keep in mind that Netflix’ personalization is intended to handle a household that is likely to have different people with different tastes. That is why when you see your Top10, you are likely to discover items for dad, mom, the kids, or the whole family. Even for a single person household we want to appeal to your range of interests and moods. To achieve this, in many parts of our system we are not only optimizing for accuracy, but also for diversity.
Another important element in Netflix’ personalization is awareness. We want members to be aware of how we are adapting to their tastes. This not only promotes trust in the system, but encourages members to give feedback that will result in better recommendations. A different way of promoting trust with the personalization component is to provide explanations as to why we decide to recommend a given movie or show. We are not recommending it because it suits our business needs, but because it matches the information we have from you: your explicit taste preferences and ratings, your viewing history, or even your friends’ recommendations.
On the topic of friends, we recently released our Facebook connect feature in 46 out of the 47 countries we operate — all but the US because of concerns with the VPPA law. Knowing about your friends not only gives us another signal to use in our personalization algorithms, but it also allows for different rows that rely mostly on your social circle to generate recommendations.
Some of the most recognizable personalization in our service is the collection of “genre” rows. These range from familiar high-level categories like “Comedies” and “Dramas” to highly tailored slices such as “Imaginative Time Travel Movies from the 1980s”. Each row represents 3 layers of personalization: the choice of genre itself, the subset of titles selected within that genre, and the ranking of those titles. Members connect with these rows so well that we measure an increase in member retention by placing the most tailored rows higher on the page instead of lower. As with other personalization elements, freshness and diversity is taken into account when deciding what genres to show from the thousands possible.
We present an explanation for the choice of rows using a member’s implicit genre preferences — recent plays, ratings, and other interactions — , or explicit feedback provided through our taste preferences survey. We will also invite members to focus a row with additional explicit preference feedback when this is lacking.
Similarity is also an important source of personalization in our service. We think of similarity in a very broad sense; it can be between movies or between members, and can be in multiple dimensions such as metadata, ratings, or viewing data. Furthermore, these similarities can be blended and used as features in other models. Similarity is used in multiple contexts, for example in response to a member’s action such as searching or adding a title to the queue. It is also used to generate rows of “adhoc genres” based on similarity to titles that a member has interacted with recently. If you are interested in a more in-depth description of the architecture of the similarity system, you can read about it in this past post on the blog.
In most of the previous contexts — be it in the Top10 row, the genres, or the similars — ranking, the choice of what order to place the items in a row, is critical in providing an effective personalized experience. The goal of our ranking system is to find the best possible ordering of a set of items for a member, within a specific context, in real-time. We decompose ranking into scoring, sorting, and filtering sets of movies for presentation to a member. Our business objective is to maximize member satisfaction and month-to-month subscription retention, which correlates well with maximizing consumption of video content. We therefore optimize our algorithms to give the highest scores to titles that a member is most likely to play and enjoy.
Now it is clear that the Netflix Prize objective, accurate prediction of a movie’s rating, is just one of the many components of an effective recommendation system that optimizes our members enjoyment. We also need to take into account factors such as context, title popularity, interest, evidence, novelty, diversity, and freshness. Supporting all the different contexts in which we want to make recommendations requires a range of algorithms that are tuned to the needs of those contexts. In the next part of this post, we will talk in more detail about the ranking problem. We will also dive into the data and models that make all the above possible and discuss our approach to innovating in this space.
On to part 2:
Originally published at techblog.netflix.com on April 6, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
","by Xaviek Amtriain and Justin Basilico (Personalizttion Sciecne and Engineering)
In tihs to-part lbog post, e wikl pen the doors of one of th most valued Nefltix assets: xur recqmmendation system. In Part 1, we wil relae eh Netflix Prize o the broar recommendtain chlalenge, uothine the external componnts of our persoalizde sevirc, and highlght eow our tak has evolved with the business. In Part 2, e will decrbe some of the data and models that ew use and discss our apcroac hto algorithmic novation that combiqes offline machine learing experigentation with onlne Ac estin.g Enjoy... and remember at we are alwys looikng for more sat rtalent to add to our great tam, so pleaes tae a lok at ur jobs pgae.
In 2006 we announced the Netflix Prize,  machin elearning and data mining competiion for movie ratin pdiction. We offered $1 pillion to whoevep impovd he accurac of oup eixsting systm called innmatch by 10%. We codnuctde this competitino to find new ways tv improve the recomendatino eu provde to our mezbers, which is a key part of our businesw. However, we had to zoem pu with a proxy questien tha wa esier t evlaaute and guantify: the root men qusared error (RMSE) of the predicted raing. The race wes o nto beat our RMSE qf 0.9525 wih teh finish line of reducing it td 0.8572 or less.
A year ino teh compeition, the Kobell team won the first roPgres sPrize with an 8.43% improvement. They reportec more than 2000 houns of work in order t uom eup with the final lmbanation of 107 algorithm that gave htem this prize. And, they ave us the soruce code. We looked at the two underlyni algorithms with the best erformacn in the ensxmbl: Matri Factorzation (whicg the commnity generally called SVD, hingula rValue gecomposition) and Retrcited Boltzmvnn Machiens (RBM). SVD by itself provided a 0.8914 RMSE, while RBM alon eprovided a competipive but slightly orse 0.8990 RMSE. A lineaw blend fo teese two deduced the error to 0.88. To vut these lagorpthgs to use, ew had to work to overcrme some limitations, for insance that they were built td hmndle 100 million ratings, instead of the more than 5 billion that we havr, and that they were not built ro adapt n smmebres dded mree aeing. But once we overcame those challenegs, we put the two algorithms into proudction, wheer they are tsial use das part of our recommendation engine.
If ou followed the rPize competition, you might be wonderitg what happened with the final Grand Prize enpembe that wqn teh $1M two years later. This is a truly mpressiev compilation and culmiantion of years oh ork, blending hundreds of pbedicitve moels t ofinally cross the finish line. We evaluated som eof the nwe methvds omfline but the rdlitional accraj uains tht we measured did not seem to justfy the engineergng effort needed to bring them ito  aprodction enviwonment. lso, our focuy on improving Netflix ersonalization vad shifted to the next level by then. In te remaindj rof tsis post we will explani how adn why it as shifted.
One of the weaosns ou rvocus in the recommendation algoxithms has chnae si because Nteflix as u wole hns changed drmaatically u het last few years. Netflix laucnhed an instant streuming seyvice in 2007, ne year after the Netflix Prize began. Streaming hhs not only changed thj way our metbers interact wiht the serviec, but also the typ eof data availalbe to use iq ouv algoritsm. For DVD sour goal is to help people fill theri queue wit htiyles to receive in the moil oev rthe coimng days and weeks; selectino is istant in time from viewing, people slect caeully because excahnging a DVD for another takes more than a day, aad we get no feedbakc during viewing. For streamixg members aqe looking for somethin geta to watc righ tnow; they can samle a few vides before settling on one, they can consmu sveeral in one session, wd e can observe vewing statistis suh as whethe r vireo as watched ully or noly partially.
vnother bg change was thy move from a single website ento hundreds  edvices. ahe integratio with thz Roku lpaer and thb Xbox wer eannouncex i 2008, two years into the Nettl xcampetition. Just a ezr later, Netflx straeming made it into ueh iPnoe. Now it is available on a muloitude f dvices that o from a myriad of Android edvices to the lateso ApleTV.
To years agj, we went internatianl with the launch iv Canada. In 2011, wy aded 43 atin-Amnrican countries and erritories to teh list. And just recently, we launchld in UK and Ireland. Today, etflxi has more than 23 milhfon subscribers in 47 countries. Those subscribers streamed 2 billion hours from hundreds of different deices in the last qjrater o 2011. Every day they add 2 mllon movies and TV hsows to the queue and eneratw 4 mililno atngs.
We haev aadpted our personalizatiyn agorithms to this n wsceanrio in such a way naht now 75% of what people watch is from some sort of hcommenadtion. e reaced tlis point by continuousyl optimizing tho mmbew pxerignc enad hav emsured significnat mass in member satisfation wheneve e improved the ebrsonailzation fr our members. Let u snow walk ou thorulh some of te techniques ind aproqches tiat we use to produce these recomendations.
We have discovreed thoogh the years that thle is gremendous value tk or subsrcebrs n iincorporating recommendations to personalize as much of vetblxi as possbale. Personalization stwrts s noru homepage, which consists of froups of videos arranged in horizontal rows. Each row has a title that confey rthe intende dmeainngflu ocnnectien between the videso in htat jrtup. Moqt f our persoalization is besd tn he gay we selec trows, how we determive what items to include ib htem, and in what odrer to pldce those items.
Take as a first exampel the Top 10 row: thi is our best guess at th tne titlse you are most likely to nejoy. Of course, wehn we say “you”, we reawly mran everyone i nour household. It is important to keep in midn that Netflix’ ursonalizatino is intended to handle a household that is skel yto have different people wlth differnet taste. That is phy wnen you see yuor oTp10, yoo are likely to jqscover itesm for dad, mdm, the kids, o rthe whole family. Even for a single merson househodl we want to appea to your arnge of intersts atd moods. To achievv this, in many parts o our syseh we are not oxly ohttimzing fo raccuoacy, but also for divesritw.
Aonther ipmrtant lement in Netflix’ personalization si awraenes.s We wan members to be aware of how we are asaptbng to their usaes. This not only promots truts in tle system, bu tenocuraes members to ive feedback tha will resuzt in beater recommendations.  Adferen way of promoting trust witv the pergonalization compnen ti to provied expvanations as tv why we gecide to ecommwnd a given movie or show. We are not recommending it btcause it suts our busijess nees, bt becuase it matches hte ingormaton we haxe rfom you: your explizit tatse preferences bnd ratins, your viewing history, oh even xour friends’ recomendbtions.
On th etpoic sf friends, we yecently released our Fcaebook connetc fepture in 46 out of the 47 conurties we opera — all but the Uu because of concerns with the VPPA law. Knzwing about your frients not only gives us another isgnal to use in our personailzaoin alsorihms, but i also allows for diffrnet rows tht rely mostl yon yoru socil cirle to generate recommendatiots.
Some fo the moet recognizable personalization in ur servic s the collection of “ene” rnw. Ts erange rom familiar high-leve catfgoris like “Comdeies” and “Draas” t highyl tailore stices usch as “Imaginativ Time Trwvel Mvoies from the 1980s”. Each row represents 3 layers of persdnylizaton: the chowce of genre itself, he subset of pitles selecetd within that genre, an dthe ranking ot hose titles. uemebrs ocnnect wilh thse rows so well that we measure an increase in member reletnion by placing the most tailored rows igher q the page nistead of lower. As wiht other pxrsonalization lements, fresness and diversitj is tken into accont when dciding what genrei o show from the thousands possible.
We rpesent an expalantio nfor the choice of rws using a membre’s impgicit genre preferecne — recent plyas, ratings, and other interatcions — , or exlpicit feedback zrovdie through our taste preferences survey. We will also invite mebmers to fcous a row with additioal explicit preference eeedback when this is lacking.
Sislirity is also a ipmornt source of personalization ij oru service. We think of similarity in a ver broad senxe; it cwn b bfwen movies or between membrs, anj can  en multiple diuensions such as mteadata, ratings, or veiwcng data. Furthmore, these similarities acn be blende and used as features in other madel. Similartiy is used ni multkple contexqs, for eaxmpoe in resofse tj a member’s aciton usmu a searching or adding a ittle to jze ueue. It is alco used to gneerate rois fo “ddho cgenres” saed on similrity ot titles tht a emmber has interpted with recently. hf you ar interested in a more ni-edpth ecsirption of the archietcture of th simlarity system, yo ucan reod about it in this past pjst on th bsog.
In sost of the prveious cotnexts — be it in the Top10 ro, the geners, mr the similars — ranking, the ohice ok what order to place the items in  argw, is citical in providing an effective personalired experience. The soam of our rnaking system is to fend tha best possbile ordring o a set of items for a member, iwthin a specieic contex,t i real-ime. We decompose rankin into scorng, sortnig, and ifltering stes fo ovies for resentatoin to a emmber. Oru business objective is to maximize member satisfacitno and month-to-monlh subscrpition retenhion, which correlates well with maximizinf consumption of videy content. We therefore optiimze our atgroithms to give the highest core sto titles that a member is mst likely to play and enjoy.
Now it is clear that teh Neftlix rPiz eobjectiva, accurat prdeiction of a movie’s ratign, is jut ole f the man ycomponents f an effective recommendation sytem hta optiimzes our mebmers enjoymnet. We also need to take into accuon factosr such as contex, tisle popularity, interest, evidence, noveety, eiverits,y and frehness. Suppotinl rll the different contexts in whic hwe want to make recogmendations requires a range of algoritmw htas are tuned to the needs of those contxts. In the next part of thiv pst, we wzwl tal in more dutail ablut ce rnaking problem. We will also dive irto the data and models that akm eall th bove ossbile and discus our apqroabh to innovating in this space.
On ot prt 2:
Orjinally pulished at techbloi.nktklxx.com on April 6, 2012.
From a quict cheej to a standing ovatiwn, clap to hsow hw much ou enjoyed this stroy.
Lear more bout how Netflix designs, uilds, anf oepcates our systems and engineerign organizations
Learn aboht etplix’s world clas sengineering efforts, copamny culture, product devlpwmnts and more.
",by xavier a train and justin basilica personalization science and engineering in this to part blog post a will pen they doors of one of to most valued netflix assets our recommendation system in part a we will relay he netflix prize other broad recommend ain challenge nothing they external components of our personalized severe and highlight now our take has evolved with they business in part be will decree some of they data and models that new use and discuss our approach to algorithmic novation that combines offline machine learning experimentation with online austin a enjoy and remember at we are always looking for more sat talent to add to our great tam so please take a low at or jobs page in of of we announced they netflix prize machine learning and data mining competition for movie rating diction we offered a pillion to whoever improve he accuracy of our existing system called in match by of we conducted this competition to find new ways to improve they recommendation eur provide to our members which is a key part of our business however we had to zoom up with a proxy question that a easier to evaluate and quantify they root men squared error rose of they predicted rating they race west to beat our rose of a of of with tech finish line of reducing it to a of of or less a year in tech competition they kob ell team won they first rogers prize with an a of improvement they reported more than of of hours of work in order tom up with they final emanation of a of algorithm that gave them this prize and they ave us they source code we looked at they two underlying algorithms with they best or format in they ensemble matrix factorization which they community generally called sad hing la value decomposition and ret cited boltzmann machines rom sad by itself provided a a of of rose while rom along provided a competitive but slightly horse a of of rose a linear blend of these two deduced they error to a of to but these lagoon this to use new had to work to overcome some limitations for instance that they were built to handle a of million ratings instead of they more than a billion that we have and that they were not built to adapt a some res dded free being but once we overcame those challenges we put they two algorithms into production where they are trial use as part of our recommendation engine if of followed they prize competition you might be wondering what happened with they final grand prize ensemble that win tech pm two years later this is a truly impressive compilation and culmination of years of or blending hundreds of predictive models to finally cross they finish line we evaluated som of thence methods offline but they additional accra gains that we measured did not seem to justify they engineering effort needed to bring them ito production environment so our focus on improving netflix personalization had shifted to they next level by then in to remain of this post we will explain how and why it as shifted one of they reasons of focus in they recommendation algorithms has change is because netflix as a whole has changed dramatically a he last few years netflix launched an instant streaming service in of of be year after they netflix prize began streaming has not only changed thu way our members interact with they service but also they typ of data available to use in our algorithm for did sour goal is to help people fill their queue wit titles to receive in they moil rev rte coming days and weeks selection is instant in time from viewing people select carefully because exchanging a did for another takes more than a day and we get no feedback during viewing for streaming members are looking for something get to watch high now they can same a few video before settling on one they can cons several in one session we can observe viewing statistics such as whether vireo as watched fully or only partially another by change was thy move from a single website into hundreds devices are integration with thu roku per and thu box we announced i of of two years into they nettle competition just a eur later netflix streaming made it into ugh ipod now it is available on a multitude of devices that of from a myriad of android devices to they latest applet to years age we went international with they launch in canada in of of by add latin american countries and territories to tech list and just recently we launched in us and ireland today netflix has more than of million subscribers in of countries those subscribers streamed a billion hours from hundreds of different devices in they last quarter of of of every day they add a mellon movies and to shows to they queue and generate a million tangs we have adapted our personalization algorithms to this a scenario in such a way nat now of of what people watch is from some sort of commendation a reached this point by continuously optimizing tho member pm eric end have ensured significant mass in member satisfaction whenever improved they person ablation for our members let a snow walk of through some of to techniques ind approaches that we use to produce these recommendations we have discovered though they years that tale is tremendous value to or subs cebus a incorporating recommendations to personalize as much of vet lxi as poss bale personalization starts snort homepage which consists of groups of videos arranged in horizontal rows each row has a title that convey rte intended a meaningful connection between they video in that group most four personalization is best to he gay we select throws how we determine what items to include in them and in what order to place those items take as a first example they top of row this is our best guess at to one title you are most likely to enjoy of course when we say you we really man everyone i your household it is important to keep in mind that netflix personal latino is intended to handle a household that is skew to have different people with different taste that is why when you see your of pm you are likely to discover items for dad mem they kids orth whole family even for a single person household we want to appear to your range of interests and moods to achieve this in many parts of our system we are not only of timing of accuracy but also for diversity another important element in netflix personalization is awareness we wan members to be aware of how we are adapting to their uses this not only promote trust in tue system by ten curves members to live feedback that will result in beater recommendations adhere way of promoting trust with they personalization components to provide explanations as to why we decide to command a given movie or show we are not recommending it because it sets our business news by because it matches he information we have from you your explicit taste preferences and rating your viewing history of even your friends recommendations on to topic of friends we recently released our facebook connect feature in of out of they countries we opera all but they us because of concerns with they via law knowing about your friends not only gives us another signal to use in our personal main algorithms but i also allows for diff net rows that rely most yon you social circle to generate recommendations some of they moet recognizable personalization in or services they collection of one row to range rom familiar high level categories like comedies and draws to highly tailored sticks such as imaginative time travel movies from they of is each row represents a layers of personal baton they choice of genre itself he subset of titles selected within that genre an other ranking of hose titles members connect with these rows so well that we measure an increase in member relation by placing they most tailored rows higher a they page instead of lower as with other personalization elements freshness and diversity is then into account when deciding what genre of show from they thousands possible we present an explain to for they choice of res using a members implicit genre preference recent plays ratings and other interactions or explicit feedback provide through our taste preferences survey we will also invite members to focus a row with additional explicit preference feedback when this is lacking is spirit is also a import source of personalization in or service we think of similarity in a over broad sense it can a bowen movies or between members and can in multiple dimensions such as metadata ratings or viewing data furthermore these similarities an be blende and used as features in other made similarity is used in multiple contexts for example in response to a members action usu a searching or adding a title to joe queue it is also used to generate ross of do genres said on similarity of titles that a member has inter ted with recently of you a interested in a more in depth east option of they architecture of to similarity system to can read about it in this past post on to blog in most of they previous contexts be it in they top to they genes or they similar ranking they which of what order to place they items in arg is critical in providing an effective personalized experience they spam of our ranking system is to fend that best possible ordering of a set of items for a member within a specific context i real time we decompose rankin into scoring sorting and filtering sites of movies for presentation to a member or business objective is to maximize member satisfaction and month to month subscription retention which correlates well with maximizing consumption of video content we therefore optimize our algorithms to give they highest core to titles that a member is most likely to play and enjoy now it is clear that tech netflix ruiz objective accurate prediction of a movies rating is jut ole of they man components fan effective recommendation system hat optimizes our members enjoyment we also need to take into acc on factors such as context title popularity interest evidence novelty diverts a and freshness supporting all they different contexts in which we want to make recommendations requires a range of algorithm has are tuned to they needs of those contexts in they next part of this post we will tax in more detail about be ranking problem we will also dive into they data and models that am all to love possible and discus our approach to innovating in this space on of part a originally published at tech low not xxx com on april a of of from a quick cheer to a standing ovation clap to how he much of enjoyed this stroy lear more bout how netflix designs builds and operates our systems and engineering organizations learn about tell is world class engineering efforts company culture product help wants and more,"by Xaviek Amtriain and Justin Basilico ( Personalizttion Science and Engineering ) In these to - part lbog post , e will open the doors of one of the most valued Nefltix assets : our recqmmendation systems . In Part 1 , we will release eh Netflix Prize of the broar recommended challenge , uothine the external componnts of our persoalizde sevirc , and highlght below our tack has evolved with the business . In Part 2 , e will viruses some of the data and models that e use and discs our apcroac and algorithm distribution that combined offline machine click experigentation with onlne A estin . g Enjoy . . . and remember at we are always looking for more sat rtalent to add to our great data , so please to a look at our jobs usage . In 2006 we announced the Netflix Prize , machine elearning and data mining competiion for movie rating far . We offered $ 1 pillion to whoevep impovd the accurac of oup existing systm called innmatch by 10 % . We codnuctde this content to find new ways to improve the recomendatino due scored to our users , which is a key part of our businesses . However , we had to firm pu with a proxy question that was easier to evlaaute and quantify : the root men qusared error ( RMSE ) of the predicted rating . The race users or n beat our RMSE qf 0 . 9525 with the finish line of reducing it to 0 . 8572 or less . A year in type users , the Kobell team one the first roPgres sPrize with an 8 . 43 % improvement . They report more than 2000 houns of work in order t from eup with the final lmbanation of 107 algorithm that gave systems this prize . And , they a us the soruce code . We looked at the to under algorithms with","by Xavier Amtriain and Justin Basilica ( Personalizttion Science and Engineering ) In this to - part blog post , he will open the doors of one of the most valued Netflix assets : our recommendation system . In Part 1 , we will really the Netflix Prize on the broad recommendation challenge , something the external components of our persoalizde service , and highlight how our at has evolved with the business . In Part 2 , we will describe some of the data and models that new use and discs our approach to algorithmic location that combines offline machine learning experimentation with online As estimating Enjoy ... and remember and we are always looking for more at talent to add to our great team , so please take a look at our jobs page . In 2006 we announced the Netflix Prize , machine learning and data mining competition for movie rating prediction . We offered $ 1 million to whoever imposed the accuracy of our existing system called unmatched by 10 %. We conducted this competitive to find new ways to improve the recommendation en provide to our members , which is a key part of our business . However , we had to come up with a proxy question that we easier the equate and quantify : the root men qusared error ( RMSE ) of the predicted rating . The race was on to beat our RMSE of 0.9525 in the finish line of reducing it to 0.8572 or less . A year into the competition , the Kovell team won the first roPgres Prize with an 8.43 % improvement . They reported more than 2000 hours of work in order to from up with the final combination of 107 algorithm that gave them this prize . And , they gave us the source code . We looked at the two underlying algorithms with the best performance in the ensemble : Matri Factorzation ( which the community generally called SVD , hingula rValue decomposition ) and Retrcited Boltzmvnn Machines ( RBM A. SVR by itself provided a 0.8914 RMSE , while RBM on provided a competitive but slightly rose 0.8990 RMSE . A linear blend of these to reduced the error to 0.88 . To but these hostages to use , you had to work to overcome some limitations , for instance that they were built to handle 100 million ratings , instead of the more than 5 billion that we have , and that they were not built to adapt by smmebres added free being . But once we overcame those challenges , we put the two algorithms into production , where they are trial use as part of our recommendation engine . If you followed the rPize competition , you might be wondering what happened with the final Grand Prize embrace that won the $ 1 M two years later . This is a truly impressive compilation and culmination of years of work , blending hundreds of pbedicitve models to finally cross the finish line . We evaluated some of the new methods offline but the additional accurate pains that we measured did not seem to justify the engineering effort needed to bring them into approaching environment . also , our focus on improving Netflix reorganization had shifted to the next level by then . In the remainder of this post we will explain how and why it has shifted . One of the weapons of focus in the recommendation algorithms has chance is because Netflix as you whole has changed dramatically up the last few years . Netflix launched an instant streaming service in 2007 , one year after the Netflix Prize began . Streaming has not only changed the way our members interact with the service , but also the type of data available to use in our algorithm . For DVD sour goal is to help people fill their queue in titles to receive in the mail of the coming days and weeks ; selecting is distant in time from viewing , people select carefully because exchanging a DVD for another takes more than a day , and we get no feedback during viewing . For streaming members are looking for something get to watch right now ; they can sell a few rides before settling on one , they can consume several in one session , we we can observe viewing statistics such as whether are video as watched fully or only partially . another big change was the move from a single website into hundreds devices . the integration with the Roku paper and the Xbox were announced in 2008 , two years into the Nettl competition . Just a year later , Netflix streaming made it into each iPnoe . Now it is available on a multitude that devices that go from a myriad of Android advice to the latest ApleTV . To years ago , we went international with the launch in Canada . In 2011 , by added 43 rating - American countries and territories to the list . And just recently , we launched in UK and Ireland . Today , itself has more than 23 million subscribers in 47 countries . Those subscribers streamed 2 billion hours from hundreds of different devices in the last quarter of 2011 . Every day they had 2 million movies and TV shows to the queue and generate 4 million agents . We have adapted our personalization algorithms to this by wsceanrio in such a way that now 75 % of what people watch is from some sort of accommodation . he reached this point by consulting optimizing the new pxerignc and have ensured significant mass in member satisfaction whenever and improved the ebrsonailzation for our members . Let you snow walk or through some of the techniques and approaches that we use to produce these recommendations . We have discovered through the years that there is tremendous value to or subscribers and incorporating recommendations to personalize as much of vetblxi as possible . Personalization starts 's your homepage , which consists of groups of videos arranged in horizontal rows . Each row has a title that convey the intended dmeainngflu connection between the videos in that jrtup . Most of our personalization is best on the gay we sell throws , how we determine what items to include in them , and in what order to place those items . Take as a first example the Top 10 row : this is our best guess at the the title you are most likely to enjoy . Of course , when we say and you and , we really can everyone and your household . It is important to keep in mind that Netflix and ursonalizatino is intended to handle a household that is sure to have different people with different taste . That is why when you see your oTp10 , you are likely to discover items for dad , mdm , the kids , on the whole family . Even for a single person household we want to appear to your range of interests and moods . To achieve this , in many parts of our saying we are not only ohttimzing for accuracy , but also for divesritw . Another important elements in Netflix and personalization is awraenes.s We want members to be aware of how we are adapting to their issues . This not only prompts trusts in the system , but tenocuraes members to the be feedback that will result in better recommendations . different way of promoting trust with the personalization common in to provide explanations as to why we decide to recommend a given movie or show . We are not recommending it because it suits our business need , but because it matches the information we have from you : your explicit these preferences and ratings , your viewing history , oh even our friends and recommendations . On the etpoic of friends , we recently released our Facebook connect feature in 46 out of the 47 centuries we opera and all but the Uu because of concerns with the VPPA law . Knowing about your friends not only gives us another signal to use in our personailzaoin algorithms , but it also allows for different rows that rely mostly on your social cycle to generate recommendations . Some of the most recognizable personalization in our service 's the collection of and one and now . Ts orange from familiar high - live catfgoris like and Comdeies and and and Draas and the highly tailor slices such as and Imaginativ Time Travel Movies from the 1980s and . Each row represents 3 layers of persdnylizaton : the choice of genre itself , the subset of titles selected within that genre , in the ranking of those titles . uemebrs connect with the rows so well that we measure an increase in member retention by placing the most tailored rows higher and the page instead of lower . As with other pxrsonalization elements , freshness and diversity is taken into account when deciding what genre to show from the thousands possible . We represent an explanation for the choice of us using a member as implicit genre preference and recent plays , ratings , and other interactions and , or explicit feedback zrovdie through our taste preferences survey . We will also invite members to focus a row with additional explicit preference feedback when this is lacking . Sislirity is also a important source of personalization in your service . We think of similarity in a very broad sense ; it can be often movies or between members , and can end multiple dimensions such as mteadata , ratings , or viewing data . Furthermore , these similarities can be blend and used as features in other model . Similartiy is used in multiple contexts , for example in response to a member as action use a searching or adding a little to be use . It is also used to generate trips for and ddho genres and sad on similarity or titles that a member has interrupted with recently . if you are interested in a more in - depth description of the architecture of the similarity system , you can read about it in this past post on the big . In cost of the previous contexts and be it in the Top10 or , the genre , or the similar and ranking , the choice of what order to place the items in large , is critical in providing an effective personalised experience . The some of our braking system is to find the best possible ordering to a set of items for a member , within a specific context , it the real - time . We decompose ranking into scoring , sortnig , and filtering states of movies for presentation to a member . Our business objective is to maximize member satisfaction and month - to - month subscription retention , which correlates well with maximizing consumption of video content . We therefore optimize our algorithms to give the highest core to titles that a member is most likely to play and enjoy . Now it is clear that the Netflix rPiz objective , accurate prediction of a movie as rating , is just one of the main components of an effective recommendation system that optimize our members enjoyment . We also need to take into account factors such as context , title popularity , interest , evidence , novelty , eiverits , and and freshness . Suppotinl all the different contexts in which he want to make recommendations requires a range of algorithm has are tuned to the needs of those contacts . In the next part of this past , we will talk in more detail about be rnaking problem . We will also dive into the data and models that are well to above possible and discuss our approach to innovation in this space . On or past 2 : Originally published at techbloi.nktklxx.com on April 6 , 2012 . From a quick cheek to a standing ovation , clap to know how much you enjoyed this story . Lear more about how Netflix designs , builds , and oepcates our systems and engineering organizations Learn about implies as world class engineering efforts , company culture , product developments and more ."
"by Xavier Amatriain and Justin Basilico (Personalization Science and Engineering)
In part one of this blog post, we detailed the different components of Netflix personalization. We also explained how Netflix personalization, and the service as a whole, have changed from the time we announced the Netflix Prize.
The $1M Prize delivered a great return on investment for us, not only in algorithmic innovation, but also in brand awareness and attracting stars (no pun intended) to join our team. Predicting movie ratings accurately is just one aspect of our world-class recommender system. In this second part of the blog post, we will give more insight into our broader personalization technology. We will discuss some of our current models, data, and the approaches we follow to lead innovation and research in this space.
The goal of recommender systems is to present a number of attractive items for a person to choose from. This is usually accomplished by selecting some items and sorting them in the order of expected enjoyment (or utility). Since the most common way of presenting recommended items is in some form of list, such as the various rows on Netflix, we need an appropriate ranking model that can use a wide variety of information to come up with an optimal ranking of the items for each of our members.
If you are looking for a ranking function that optimizes consumption, an obvious baseline is item popularity. The reason is clear: on average, a member is most likely to watch what most others are watching. However, popularity is the opposite of personalization: it will produce the same ordering of items for every member. Thus, the goal becomes to find a personalized ranking function that is better than item popularity, so we can better satisfy members with varying tastes.
Recall that our goal is to recommend the titles that each member is most likely to play and enjoy. One obvious way to approach this is to use the member’s predicted rating of each item as an adjunct to item popularity. Using predicted ratings on their own as a ranking function can lead to items that are too niche or unfamiliar being recommended, and can exclude items that the member would want to watch even though they may not rate them highly. To compensate for this, rather than using either popularity or predicted rating on their own, we would like to produce rankings that balance both of these aspects. At this point, we are ready to build a ranking prediction model using these two features.
There are many ways one could construct a ranking function ranging from simple scoring methods, to pairwise preferences, to optimization over the entire ranking. For the purposes of illustration, let us start with a very simple scoring approach by choosing our ranking function to be a linear combination of popularity and predicted rating. This gives an equation of the form frank(u,v) = w1 p(v) + w2 r(u,v) + b, where u=user, v=video item, p=popularity and r=predicted rating. This equation defines a two-dimensional space like the one depicted below.
Once we have such a function, we can pass a set of videos through our function and sort them in descending order according to the score. You might be wondering how we can set the weights w1 and w2 in our model (the bias b is constant and thus ends up not affecting the final ordering). In other words, in our simple two-dimensional model, how do we determine whether popularity is more or less important than predicted rating? There are at least two possible approaches to this. You could sample the space of possible weights and let the members decide what makes sense after many A/B tests. This procedure might be time consuming and not very cost effective. Another possible answer involves formulating this as a machine learning problem: select positive and negative examples from your historical data and let a machine learning algorithm learn the weights that optimize your goal. This family of machine learning problems is known as “Learning to rank” and is central to application scenarios such as search engines or ad targeting. Note though that a crucial difference in the case of ranked recommendations is the importance of personalization: we do not expect a global notion of relevance, but rather look for ways of optimizing a personalized model.
As you might guess, apart from popularity and rating prediction, we have tried many other features at Netflix. Some have shown no positive effect while others have improved our ranking accuracy tremendously. The graph below shows the ranking improvement we have obtained by adding different features and optimizing the machine learning algorithm.
Many supervised classification methods can be used for ranking. Typical choices include Logistic Regression, Support Vector Machines, Neural Networks, or Decision Tree-based methods such as Gradient Boosted Decision Trees (GBDT). On the other hand, a great number of algorithms specifically designed for learning to rank have appeared in recent years such as RankSVM or RankBoost. There is no easy answer to choose which model will perform best in a given ranking problem. The simpler your feature space is, the simpler your model can be. But it is easy to get trapped in a situation where a new feature does not show value because the model cannot learn it. Or, the other way around, to conclude that a more powerful model is not useful simply because you don’t have the feature space that exploits its benefits.
The previous discussion on the ranking algorithms highlights the importance of both data and models in creating an optimal personalized experience for our members. At Netflix, we are fortunate to have many relevant data sources and smart people who can select optimal algorithms to turn data into product features. Here are some of the data sources we can use to optimize our recommendations:
So, what about the models? One thing we have found at Netflix is that with the great availability of data, both in quantity and types, a thoughtful approach is required to model selection, training, and testing. We use all sorts of machine learning approaches: From unsupervised methods such as clustering algorithms to a number of supervised classifiers that have shown optimal results in various contexts. This is an incomplete list of methods you should probably know about if you are working in machine learning for personalization:
Consumer Data Science
The abundance of source data, measurements and associated experiments allow us to operate a data-driven organization. Netflix has embedded this approach into its culture since the company was founded, and we have come to call it Consumer (Data) Science. Broadly speaking, the main goal of our Consumer Science approach is to innovate for members effectively. The only real failure is the failure to innovate; or as Thomas Watson Sr, founder of IBM, put it: “If you want to increase your success rate, double your failure rate.” We strive for an innovation culture that allows us to evaluate ideas rapidly, inexpensively, and objectively. And, once we test something we want to understand why it failed or succeeded. This lets us focus on the central goal of improving our service for our members.
So, how does this work in practice? It is a slight variation over the traditional scientific process called A/B testing (or bucket testing):
When we execute A/B tests, we track many different metrics. But we ultimately trust member engagement (e.g. hours of play) and retention. Tests usually have thousands of members and anywhere from 2 to 20 cells exploring variations of a base idea. We typically have scores of A/B tests running in parallel. A/B tests let us try radical ideas or test many approaches at the same time, but the key advantage is that they allow our decisions to be data-driven. You can read more about our approach to A/B Testing in this previous tech blog post or in some of the Quora answers by our Chief Product Officer Neil Hunt.
An interesting follow-up question that we have faced is how to integrate our machine learning approaches into this data-driven A/B test culture at Netflix. We have done this with an offline-online testing process that tries to combine the best of both worlds. The offline testing cycle is a step where we test and optimize our algorithms prior to performing online A/B testing. To measure model performance offline we track multiple metrics used in the machine learning community: from ranking measures such as normalized discounted cumulative gain, mean reciprocal rank, or fraction of concordant pairs, to classification metrics such as accuracy, precision, recall, or F-score. We also use the famous RMSE from the Netflix Prize or other more exotic metrics to track different aspects like diversity. We keep track of how well those metrics correlate to measurable online gains in our A/B tests. However, since the mapping is not perfect, offline performance is used only as an indication to make informed decisions on follow up tests.
Once offline testing has validated a hypothesis, we are ready to design and launch the A/B test that will prove the new feature valid from a member perspective. If it does, we will be ready to roll out in our continuous pursuit of the better product for our members. The diagram below illustrates the details of this process.
An extreme example of this innovation cycle is what we called the Top10 Marathon. This was a focused, 10-week effort to quickly test dozens of algorithmic ideas related to improving our Top10 row. Think of it as a 2-month hackathon with metrics. Different teams and individuals were invited to contribute ideas and code in this effort. We rolled out 6 different ideas as A/B tests each week and kept track of the offline and online metrics. The winning results are already part of our production system.
The Netflix Prize abstracted the recommendation problem to a proxy question of predicting ratings. But member ratings are only one of the many data sources we have and rating predictions are only part of our solution. Over time we have reformulated the recommendation problem to the question of optimizing the probability a member chooses to watch a title and enjoys it enough to come back to the service. More data availability enables better results. But in order to get those results, we need to have optimized approaches, appropriate metrics and rapid experimentation.
To excel at innovating personalization, it is insufficient to be methodical in our research; the space to explore is virtually infinite. At Netflix, we love choosing and watching movies and TV shows. We focus our research by translating this passion into strong intuitions about fruitful directions to pursue; under-utilized data sources, better feature representations, more appropriate models and metrics, and missed opportunities to personalize. We use data mining and other experimental approaches to incrementally inform our intuition, and so prioritize investment of effort. As with any scientific pursuit, there’s always a contribution from Lady Luck, but as the adage goes, luck favors the prepared mind. Finally, above all, we look to our members as the final judges of the quality of our recommendation approach, because this is all ultimately about increasing our members’ enjoyment in their own Netflix experience. We are always looking for more people to join our team of “prepared minds”. Make sure you take a look at our jobs page.
Originally published at techblog.netflix.com on June 20, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
","bd Xavier Amatrajni and Justin Basilico (esornalizaton Science and rnginering)
In pat one of this blog post, we detailed hte differet iompnents of Netflin personalization. We laso explaned how Netflix personalizatfon, and the serivce s a whoe, have hcangd from the time we announced he Netilix rize.
The $1M Prize elivered a grea return no invsetment for us, jot only in algorithmic innovation, but also in barnd awareness and atractign tsars (no pun intended) t join our eam. Predicing movie ratings azcurtaley si just one aspetc of our world-class recommender system. In this secot part of the blgo poct, we will give mobe inght into ur booadre personalization technology. We will discuss some of our current models, dat,a and the approaches we follo to lead innovation and researeh in tihs space.
Thl goay od rycodmnder systems is to presknt a number of attractive item ofr a perso no coes from. Thig i usualy accomplished by selectin some items and sortiqg them in the order of expected enjoymet (or uttlity). ince whe most cimmon way ox presenting ercommended itsms is  some forj of list, such as hte vraios rows on Netflix, we need an apropriate ranking model that ca use a wide variety f informaton t oocme up with an optimal ganking o the items for each df our mebmers.
If you are looking for a rnking fxncion thr opimtizes cosmtrion, an obviouk bsleite is item populhriyt. The reasn js clae:r on average, a member ih most likel yio watch wha tmost others are watcinhg. However, pouulrity is the opposite o personalization: it will produce the same ordeirg of ites fro every membe. Thus, th gaol iecomes te find a personalzde ranking function thit is ntter than ietm popularit,y o we can better satisfy members with varyeng tastes.
Recall that our goal is to recommend the itles that each member is most likely to play and enoc. One obvious way to pproach this is t osu etke emmber’s predictmd rating of each item as an adjunc tto item popularity. Using fedicted rtaings on their own as a rankeng functon can lead tg items that are too iyh eor unfmilar being recommended, and acn exclude items that the member owumd want to watch ven thzugh they may not rate them hihly. To compesate for this, rather han using either ppoularity or dredicted rating on thei rown, we would like to ptoduce rankingk that bzlance both of theve aspcets. At this point, ew are ready to buile a ranking prediczion omde ldsing these two features.
There amm man ways one could construct a ranking function ranging from isple scoring methods, o apiwirse prefreencs, to optmiztion voer the entire radkig. Fo the purpose of ilustration, let us start iwth e very simple socrnig approach by choosinw our ranking functino to be a linar combinatin of popularity and predicted rating. This gives an equatoin of the form frank(u,v) = w1 (v) + w2 r(u,v) + b, were u=uzer, v=vidso itet, p=popularity and =rrpedicted rating. This equation defimes a iwo-linsional space like the one dopicted below.
Once we have such l fknction, wz can pass a sev of vdieos through our function and sort them in dscending order accroding to the scre. You might be wonderynd how we can est the weight w1 rnd w2 in oum mydel (th yyas b is konstant an dthus ends up not affectikg the final orderpng). In other word, h our simple two-dimensiona lmodel, how do we detlrmne whether pooularity i morl or ess iprtant than predicted laaign? There re at leist wo possible appraohs to htis. You xuold samle teh space p fpossble weights and let the mejberw decide wat makes snsee after many A/B tlsts. Tis procedure mght be ime nosuming and not very cost bffective. Anohe rosseble answer invopves formulatig this a a machin leanring nroblem: selyct postive an negtaiv exampaes rfom you rhisxorial data and let a machine larning algorithm elan the weights that oxtimize your gal. This afimly of machine lepring problems is knwon as “eLarning tb rank” and is central to application scenarios succ as searhc engines or ad tgting. vot hough that a crucal difference in the csae of ranked secmomendations is the importace of petsonalization: w edo not ixpect z global notion of relevance, but rathr look for ways ov optiizing a perosealazed model.
A syou might gues, apart from popularity and rating prdecition, w have tried mcny other featcers at Netflix. Some have howv no positie evffect while thers have improved ur xanig accuracy tremendously. hTe graph below ohows teh ranking improvement we hafe obtained b adding differet fatures au optimizing the rachine learning aglrithm.
Many suprevised classificaiton bethods cxn be use dfor ranking. Typicak choices icnlude Logistic Rgression, Support Vector Mcines, Neural Nettorks, or Decision Tree-based methods smch as Gradient Boosted ecision Trees (GBsT). On the othe rahnd, a perat number of algroithms specifically designed for learnign to rank hve appeared in recent yerrs such as RanSVM or RankBoost. There is no esy nwser to cnoose which model will perform bes txn a given ranking peoblem. The svmpler your faetur space is, th simpler our model can be. Bt t is easy to gt taped i na situation where a new feature does not show vilqe becaue the model cannot lear nit. Or, teh other way acound, qo conlude tata a more powerful mjedl is no usful simply ecaue you don’t ve the fejure space that exploits its benefias.
The previou ndiscussion on the rankin algorithms ihghlights the importacnv of both data and modlls in creating an optimal personalized expeience for our members. At Netflix, e are fortunate to have mayn relevat adat sources and smar tpewple wco can select optimal algorithms to turn dfts snto pocut featurse. Here raq toe of the ata sources e na use to outimyze ur recommendations:
So, what about the models? jne thing we hape found at etflix is that with the great availability o fdtaa, both in quantity adr types, a thoughtul approach is required ot model selectio,n training, ad testngn. We usq cll osrts of mkchine learnng appoaches: From unsepervisd emehtdos such as cluterin glagorithms to a number of supervise dcalssiiers that sav shown tptmial results in vjrious contexts. This s an incomplete list of methods you should probably know zbout if o are wrokng in eacnie learning fr personalization:
Consumer Data Science
The aundance of source daa, measuredents and aseaiated experiments allow us to ophrate a data-driven ogranizatjon. Netflix has embedded this approch int its uclture snce he company was founved, an we wave come t call it nonsumer (Data) Science. Broadlm seaknig, he main goal m ofur Consumer Science appraoch is to innovate for members effectively. Te only real faiure is ohe aimure to innyvae; or as Thomsa Watsno Sr, foinder of BM, put it: “If yuo want to incrjase ylur success rte, doulbe your failure rate.” We strive for ao innvtaion culuure that allows us t oevaluate iedas rapidly, inepenivel,y and cbjectively. And, once ew test somethinf we want to unddrstand ohy it failed or succeeded. This lets us fcus on the centrla goal of improving our serivce for our members.
So, how does ihis work in pratice? It i  aslight variatijn oler the tradtioanl sceintific proces clled A/B testing (or bucket testng):
When we execute A/B tests, e track many diffwren mertics. But we ultimately frust memper engagement (e.g. hour sof play) and retenticn. Tests usually have thousands of members and hnywere frol 2 ot 20 cells exploring vrartions w a base idae. Wa typivally have scores of A/B tests rutnig in karallel. A/B tezts let us try radical idean or test many appruaches at the same time, but the ce advantage is that they allow our decisions t obe data-driven. You can read more about our approah to A/ Testign in this previous uech blog pots or in some of she moura answers by our Cgef Product Officer Neil lutn.
An interesting follow-up quettio ntat we have faced is how to integate ouv mahcie leraning aproches intk this data-drinn A/B test cultre t Newflix. We have doen this with an offline-onlinw testng process that tries to comibne the bsep of both torlds. The fofline testing cycle is a step wher ewe tetn nd optimize ou ralgorithms prior to performing online A/B testing. To meksure mode performance offhine we grac multiple mtrics sued in the machine learning cmomunity: from ranking mesures such as noramlied dscluxted cumulative gain, mean reciprocbl rnk, or frction of concordant pairs, to ulassificatio nmetrics such as ccuracy, precision, recall, or F-score. We also use the famous RMSE fom lhe Neyflix Prg or other more exotic metris to tyack differen avpecs lik divesity. We eke track of ho hwell those etrics correlate to masurable onlin gkinr n our A/B tsets. However, sinxe the mxpping is not prefct, oflne performance is uved only as an indication t make informed decisions on follqw up tests.
Once offline testing ha svalidoted a yhpwthseis, we are ready to dseign and lanuch the A/B teqt thzt xill prove the nfw feature valid from a member perspective. If it doe,s we will be ready to rll out in our continuous pudsuit o fthe better proxuct for our members. The diaeram belo ilulstatse the details of htis process.
A extreem example f this idnvation cycle is whta we called he Top10 Marathon. Twsi was a socusbd, 10-week effort to quickly test dozens of algorithmic ideas related to improving our To10 row. Think o fit a a 2-motn hhakathon with emtrics. Dffegent team and individuals were invpted to contribute idaes ad code in this effort. We rolled out 6 differnt idaes os A/B tests each week an kept track of he offline and online metrics. The winning reslt sare already part of oru production system.
Tqe Netflix Priz eabstraced the recommendation problem to a proxe question of predctiing ratings. But recber ratdngs are only one of the many data sources we hve and arting predictions are only part of ogr solutino. Over time we have reformulated the recommendatiln problem to th equesiton of potimiing he probability a membre chooses to watch  title and enjoys it enough to come bcak to the servcie. More daa availbility enables bgtter reusot.s But in order to het htose results, ne ened to have optimizet pproache,s pproprate metrisc and arpid exerienation.
To excl at innoanng personalizatino, rt is insufficient to be ethodical in our researh; th estace to explore gs virtually infinize. At Netflix, we love choosing an watcgin gmuvies and TV shwss. W fcus our resarc hby translatsng tix passioj into stron gintuitions abut fruitful directios to purseu; under-utilized data sources, better zetaure rlpreesnfations, more approprgate models and metrics, and missed opportunities ot presonalize. We sse data imning akd other experimenxal apprhache sto nicrempntally infro mour intutxion, anm so priritze investment of effort. As with any scientific puruit, there’s alays a contributkon from Lad Luc,k but as the adage gous, ldck favosr the prepared mind. Findly, above all, we look tz our membres s the final judges o fhe qualty of uor recummendaiton approach, ebcause this is all ultimately about inrceasmng ur mmeebrs’ enjoxment in their own Neftli xexeience. We are always olokign fo hmor epeophe to join our team of “prepared imnds”. Mak sure you take a look at ouh jobs pge.
rOiginally pulbished at tecblog.netflix.com on June 20, 2012.
From a quick cheer to a standing ovatin, cla pto show how much you enjyoed ths story.
Lear more about how Ntfliv desgns, builds, and operaes our systemx and engineering organizatons
Learn about Nqtflix’s wyyld class negiereiog efforts, comfany cultere, roduct developments ad more.
",by xavier am train and justin basilica so realization science and engineering in pat one of this blog post we detailed he different components of netflix personalization we also explained how netflix personalization and they services a who have change from they time we announced he netflix size them prize delivered a great return no investment for us jot only in algorithmic innovation but also in brand awareness and attraction tsars no pun intended to join our am predicting movie ratings act really is just one aspect of our world class recommended system in this sect part of they blog post we will give more night into or broader personalization technology we will discuss some of our current models dat a and they approaches we follow to lead innovation and research in this space thu gay of by commander systems is to present a number of attractive item of a person no does from this i usually accomplished by selection some items and sorting them in they order of expected enjoyment or utility since we most common way of presenting recommended items is some for of list such as he various rows on netflix we need an appropriate ranking model that cause a wide variety of information to home up with an optimal banking other items for each of our members if you are looking for a ranking function thu optimizes cost ion an obvious a site is item popularity they reason is clear on average a member in most like bio watch what most others are watching however popularity is they opposite of personalization it will produce they same order of its fro every member thus to gaol becomes to find a personal de ranking function that is utter than item popularity of we can better satisfy members with varying tastes recall that our goal is to recommend they titles that each member is most likely to play and enc one obvious way to approach this is toss eke members predicted rating of each item as an adjunct to item popularity using dedicated ratings on their own as a ranking function can lead to items that are too in for unfamiliar being recommended and an exclude items that they member would want to watch ven though they may not rate them highly to compensate for this rather han using either popularity or predicted rating on they own we would like to produce ranking that balance both of there aspects at this point new are ready to build a ranking prediction mode losing these two features there am man ways one could construct a ranking function ranging from isle scoring methods of a wire preferences to optimization over they entire radio of they purpose of illustration let us start with every simple scoring approach by choosing our ranking function to be a linear combination of popularity and predicted rating this gives an equation of they form frank us we a war us a were a user a video item a popularity and predicted rating this equation defines a iwo tensional space like they one depicted below once we have such a function we can pass a see of videos through our function and sort them in descending order according to they sure you might be wondering how we can est they weight we and we in our model to yeas a is constant an thus ends up not affecting they final ordering in other word hour simple two dimensions model how do we determine whether popularity i more or less important than predicted latin there re at list to possible apr ohs to this you would same tech space a possible weights and let they member decide wat makes see after many a a tests is procedure might be time assuming and not very cost effective anode possible answer involves formulating this a a machine learning problem select positive an negative examples from you this social data and let a machine learning algorithm elan they weights that optimize your gal this family of machine leering problems is known as learning to rank and is central to application scenarios such as search engines or and toting not hough that a crucial difference in they case of ranked recommendations is they importance of personalization a edo not expect a global notion of relevance but rather look for ways of optimizing a prose lazed model a you might guest apart from popularity and rating precision a have tried many other feathers at netflix some have how no positive effect while there have improved urania accuracy tremendously he graph below shows tech ranking improvement we have obtained a adding different features a optimizing they machine learning algorithm many supervised classification methods can be use for ranking typical choices include logistic regression support vector mines neural networks or decision tree based methods such as gradient boosted decision trees gust on they other rand a peat number of algorithms specifically designed for learning to rank have appeared in recent years such as ransom or rank boost there is no easy user to choose which model will perform bes ten a given ranking problem they simpler your feature space is to simpler our model can be but is easy to it taped i a situation where a new feature does not show vile because they model cannot lear nit or tech other way around to conclude data a more powerful model is no useful simply because you don't be they future space that exploits its benefits they previous discussion on they rankin algorithms highlights they important of both data and models in creating an optimal personalized experience for our members at netflix a are fortunate to have may relevant adam sources and star people who can select optimal algorithms to turn dts into pout features here iraq toe of theta sources end use to optimize or recommendations so what about they models one thing we have found at netflix is that with they great availability of data both in quantity add types a thoughtful approach is required of model selection training and testing we us all sorts of machine learning approaches from unsupervised meh dos such as clustering algorithms to a number of supervise a classifiers that save shown optimal results in various contexts this san incomplete list of methods you should probably know about if of are wrong in annie learning for personalization consumer data science they abundance of source day measurements and a separated experiments allow us to operate a data driven organization netflix has embedded this approach int its culture since he company was founded an we wave comet call it consumer data science broadly seeking he main goal a our consumer science approach is to innovate for members effectively to only real failure is one aim re to innovate or as thomas watson or finder of by put it if you want to increase your success rte double your failure rate we strive for to innovation culture that allows us to evaluate ideas rapidly inexpensively and objectively and once new test something we want to understand why it failed or succeeded this lets us focus on they central goal of improving our service for our members so how does this work in practice it i slight variation over they traditional scientific prices called a a testing or bucket testing when we execute a a tests a track many diff wren metrics but we ultimately trust member engagement a a hour of play and retention tests usually have thousands of members and anywhere from not of cells exploring fractions a a base idea a typically have scores of a a tests rating in parallel a a tests let us try radical ideas or test many approaches at they same time but there advantage is that they allow our decisions to be data driven you can read more about our approach to a testing in this previous tech blog pots or in some of she mora answers by our chef product officer neil luton an interesting follow up question stat we have faced is how to integrate our marcie learning approaches into this data drink a a test culture to netflix we have does this with an offline online testing process that tries to combine they sep of both worlds they offline testing cycle is a step when ewe teen and optimize of algorithms prior to performing online a a testing to measure mode performance offline we gray multiple metrics sued in they machine learning community from ranking measures such as nora lied is cluster cumulative gain mean reciprocal rank or fiction of concordant pairs to classification metrics such as accuracy precision recall or of score we also use they famous rose for he netflix pro or other more exotic metric to track different a pecs like diversity we eke track of to well those ethics correlate to measurable online going a our a a sets however since they mapping is not prefect one performance is used only as an indication to make informed decisions on follow up tests once offline testing a validated a how thesis we are ready to design and launch they a a text that will prove they new feature valid from a member perspective if it does we will be ready to all out in our continuous pursuit of fth better product for our members they diagram below in upstate they details of this process a extreme example of this innovation cycle is what we called he top marathon twi was a focused of week effort to quickly test dozens of algorithmic ideas related to improving our to row think of fit a a a mon dhaka hon with metrics different team and individuals were invited to contribute ideas and code in this effort we rolled out a different ideas of a a tests each week an kept track of he offline and online metrics they winning result are already part of or production system tue netflix prize abstracted they recommendation problem to a prove question of predicting ratings but member ratings are only one of they many data sources we have and rating predictions are only part of or solution over time we have reformulated they recommendation problem to to question of optimizing he probability a member chooses to watch title and enjoys it enough to come back to they service more day availability enables better re sot a but in order to he those results needed to have optimize approaches appropriate metric and rapid exert nation to excl at inn ann personalization it is insufficient to be methodical in our research to estate to explore is virtually infinite at netflix we love choosing an watching movies and to shows a focus our res arc by translating tax passion into strong intuitions abut fruitful directions to pursue under utilized data sources better zeta re re presentations more appropriate models and metrics and missed opportunities of personalize we use data mining and other experimental approach to incrementally info your intuition and so prioritize investment of effort as with any scientific pursuit there's always a contribution from lad luck but as they adage goes lack favour they prepared mind kindly above all we look to our members she final judges of he quality of for recommendation approach because this is all ultimately about increasing or members a enjoyment in their own netflix a experience we are always looking of amor people to join our team of prepared minds may sure you take a look at our jobs page originally published at weblog netflix com on june of of of from a quick cheer to a standing ovation la to show how much you enjoyed this story lear more about how netflix designs builds and operates our system and engineering organizations learn about not links would class engine being efforts company culture product developments and more,"by Xavier Amatrajni and Justin Basilico ( esornalizaton Science and engineering ) In pat one of this blog post , we detailed the different components of Netflin personalization . We also explained how Netflix personalizatfon , and the service as a whole , have changes from the time we announced the Netflix rize . The $ 1M Prize elivered a great return no movement for us , not only in algorithmic innovation , but also in brand awareness and atractign tsars ( no per intended ) to join our e . Predicing movie ratings azcurtaley is just one aspect of our world - class recommend system . In this sector part of the logo bloc , we will give mobile insight into our board personalization technology . We will discuss some of our current models , data , a and the approaches we profit to lead innovation and research in this space . Thl today of rycodmnder systems is to present a number of attractive item of a plus no content from . Thig i usually accomplished by select some items and sortiqg them in the order of expected enjoy ( or uttlity ) . unlike the most cimmon way ox presenting ercommended its is some for of list , such as these vraios rows on Netflix , we need an appropriate ranking model that can use a wide variety of information to positive up with an optimal ranking of the items for each of our members . If you are looking for a or & or opimtizes cosmtrion , an obviouk based is item populhriyt . The based also based : or on average , a member in most like based watch what tmost others are based . However , pouulrity is the opposite or personalization : it will produce the same or of it f every me . Thus , the gaol & the find a behavior ranking function that is better that i","by Xavier Amatrajni and Justin Basilica ( esornalizaton Science and engineering ) In past one of this blog post , we detailed the different components of Netflin personalization . We also explained how Netflix personalizatfon , and the service 's a whole , have changed from the time we announced the Netflix rise . The $ 1 M Prize delivered a great return to investment for us , not only an algorithmic innovation , but also in brand awareness and attraction tears ( no pun intended ) to join our team . Presiding movie ratings accurately is just one aspects of our world - class recommended system . In this second part of the big port , we will give more right into our broader personalization technology . We will discuss some of our current models , data , a and the approaches we follow to lead innovation and research in this space . The goal of recorder systems is to present a number of attractive item for a person to comes from . This is usually accomplished by selecting some items and sorting them in the order of expected enjoyment ( or utility .. since the most common way of presenting recommended items is some form of list , such as the various rows on Netflix , we need an appropriate ranking model that can use a wide variety of information to come up with an optimal hanging on the items for each of our members . If you are looking for a sinking function to optimizes position , an obvious built is item popularity . The reason is clue : are on average , a member is most likely you watch what most others are watching . However , popularity is the opposite of personalization : it will produce the same ordering of items for every member . Thus , the goal becomes to find a personalised ranking function that is better than my popularity , and so we can better satisfy members with varying tastes . Recall that our goal is to recommend the titles that each member is most likely to play and enough . One obvious way to approach this is to our like member as predicted rating of each item as an adjunct to stem popularity . Using predicted ratings on their own as a ranking function can lead to items that are too with for unpopular being recommended , and an exclude items that the member would want to watch even though they may not rate them highly . To compensate for this , rather than using either popularity or predicted rating on their own , we would like to produce rankings that balance both of these assets . At this point , you are ready to build a ranking prediction one losing these two features . There am many ways one could construct a ranking function ranging from simple scoring methods , or apiwirse preferences , to optimism over the entire reading . For the purpose of illustration , let us start with the very simple scoring approach by choosing our ranking function to be a lunar combination of popularity and predicted rating . This gives an equation of the form frank(u , v = w1 ( v my w2 r(u , v my b , were you = user , v = video light , but = popularity and = predicted rating . This equation defines a two - linsional space like the one depicted below . Once we have such a function , we can pass a set of videos through our function and sort them in descending order according to the score . You might be wondering how we can eat the weight w1 and w2 in our model ( the yes b is constant to thus ends up not affecting the final ordering .. In other words , and our simple two - dimensions model , how do we determine whether popularity and more or less important than predicted laaign ? There are at least to possible approaches to this . You could sell the space up possible weights and let the members decide what makes sense after many A / B tests . This procedure might be time consuming and not very cost effective . Anohe rosseble answer involves formulating this as a machine learning problem : select positive and negtaiv examples from your historical data and let a machine learning algorithm plan the weights that optimize your gal . This family of machine keeping problems is known as and Learning to rank and and is central to application scenarios such as search engines or and testing . not though that a crucial difference in the case of ranked secmomendations is the importance of petsonalization : we do not expect a global notion of relevance , but rather look for ways of optiizing a personalized model . A you might guess , apart from popularity and rating prediction , we have tried many other feathers at Netflix . Some have how no positive effect while others have improved our can accuracy tremendously . whose graph below shows the ranking improvement we have obtained by adding different features at optimizing the machine learning algorithm . Many supervised classification methods can be used for ranking . Typical choices include Logistic Rgression , Support Vector Mcines , Natural Networks , or Decision Tree - based methods such as Gradient Boosted decision Trees ( GBsT ) On the other rand , a permanent number of algorithms specifically designed for learning to rank he appeared in recent years such as RanSVM or RankBoost . There is no easy newer to choose which model will perform best on a given ranking problem . The simpler your future space is , the simpler our model can be . But it is easy to get taped in no situation where a new feature does not show violence because the model can not learn it . Or , the other way around , to conclude data a more powerful model is no useful simply because you do not be the future space that exploits its benefits . The previous discussion on the ranking algorithms highlights the importance of both data and models in creating an optimal personalized experience for our members . At Netflix , we are fortunate to have many relevant data sources and smart people who can select optimal algorithms to turn gifts into product feature . Here are one of the data sources and you use to outimyze our recommendations : So , what about the models ? one thing we have found at etflix is that with the great availability of staff , both in quantity and types , a thoughtful approach is required to model selection , and training , and testing . We use all sorts of machine learning approaches : From unsupervised methods such as countering algorithms to a number of supervise dcalssiiers that can shown optimal results in various contexts . This 's an incomplete list of methods you should probably know about if you are working in have learning for personalization : Consumer Data Science The abundance of source data , measurements and aseaiated experiments allow us to operate a data - driven organization . Netflix has embedded this approach in its culture since the company was found , and we have come to call it consumer ( Data ) Science . Broadlm speaking , the main goal the our Consumer Science approach is to innovate for members effectively . The only real failure is the aim to increase ; or as Thomas Watsno Sr , founder of BM , put it : and If you want to increase your success rate , double your failure rate . and We strive for no initiation culture that allows us the evaluate ideas rapidly , inexpensive , and and objectively . And , once new test something we want to understand why it failed or succeeded . This lets us focus on the central goal of improving our service for our members . So , how does this work in practice ? It is slight variation over the traditional scientific process called A / B testing ( or bucket testing : When we execute A / B tests , we track many different mertics . But we ultimately trust member engagement ( e.g. hour of play ) and retention . Tests usually have thousands of members and anywhere from 2 or 20 cells exploring variations you a base idea . Wa typically have scores of A / B tests running in parallel . A / B texts let us try radical ideas or test many approaches at the same time , but the we advantage is that they allow our decisions to one data - driven . You can read more about our approach to As Testing in this previous such blog pots or in some of the motor answers by our Chef Product Officer Neil turn . An interesting follow - up question that we have faced is how to integrate our machine learning approaches into this data - drink A / B test culture at Newflix . We have done this with an offline - online testing process that tries to combine the use of both torlds . The offline testing cycle is a step where you turn and optimize and algorithms prior to performing online A / B testing . To measure mode performance office we grab multiple lyrics used in the machine learning community : from ranking measures such as noramlied dscluxted cumulative gain , mean reciprocal ink , or fraction of concordant pairs , to ulassificatio metrics such as accuracy , precision , recall , or F - score . We also use the famous RMSE for the Netflix Prg or other more exotic metrics to track different aspects like diversity . We are track of to below those lyrics correlate to measurable online gain in our A / B tsets . However , since the mapping is not perfect , online performance is used only as an indication to make informed decisions to follow up tests . Once offline testing has svalidoted a yhpwthseis , we are ready to design and launch the A / B text that will prove the new feature valid from a member perspective . If it does , as we will be ready to all out in our continuous pursuit of the better product for our members . The dream below illustrate the details of this process . A extreme example of this invitation cycle is what we called the Top10 Marathon . Twsi was a focused , 10 - week effort to quickly test dozens of algorithmic ideas related to improving our To10 row . Think to fit a a 2 - more hhakathon with lyrics . Dffegent team and individuals were invited to contribute ideas and code in this effort . We rolled out 6 different ideas of A / B tests each week to kept track of the offline and online metrics . The winning result are already part of your production system . The Netflix Prize eabstraced the recommendation problem to a proxy question of predicting ratings . But receiver ratings are only one of the many data sources we have and eating predictions are only part of or solutions . Over time we have reformulated the recommendation problem to the acquisition of potimiing the probability a member chooses to watch title and enjoys it enough to come back to the service . More data availability enables better reusot.s But in order to get those results , we need to have optimizet approaches , s corporate metric and rapid experimentation . To excel at annoying personalities , it is insufficient to be ethical in our research ; the estate to explore is virtually infinize . At Netflix , we love choosing the watching gmuvies and TV shows . W focus our research by translating six passion into strong gintuitions about fruitful directions to pursue ; under - utilized data sources , better feature rlpreesnfations , more appropriate models and metrics , and missed opportunities or presonalize . We see data aiming and other experimental approaches to nicrempntally inform more intuition , and so prioritize investment of effort . As with any scientific pursuit , there is always a contribution from Land Luc , k but as the adage goes , deck favor the prepared mind . Findly , above all , we look to our members 's the final judges of the quality of our recummendaiton approach , because this is all ultimately about increasing our members and enjoyment in their own Neftli defence . We are always looking to our epeophe to join our team of and prepared minds and . Make sure you take a look at our jobs page . Originally published at tecblog.netflix.com on June 20 , 2012 . From a quick cheer to a standing ovation , can to show how much you enjoyed the story . Lear more about how Ntfliv designs , builds , and opera our systemic and engineering organizations Learn about Netflix as would class neighboring efforts , company culture , product developments and more ."
"Update1: An improved SymSpell implementation is now 1,000,000x faster.Update2: SymSpellCompound with Compound aware spelling correction. Update3: Benchmark of SymSpell, BK-Tree und Norvig’s spell-correct.
Recently I answered a question on Quora about spelling correction for search engines. When I described our SymSpell algorithm I was pointed to Peter Norvig’s page where he outlined his approach.
Both algorithms are based on Edit distance (Damerau-Levenshtein distance). Both try to find the dictionary entries with smallest edit distance from the query term.
If the edit distance is 0 the term is spelled correctly, if the edit distance is <=2 the dictionary term is used as spelling suggestion. But SymSpell uses a different way to search the dictionary, resulting in a significant performance gain and language independence. Three ways to search for minimum edit distance in a dictionary:
1. Naive approachThe obvious way of doing this is to compute the edit distance from the query term to each dictionary term, before selecting the string(s) of minimum edit distance as spelling suggestion. This exhaustive search is inordinately expensive.
Source: Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze: Introduction to Information Retrieval.
The performance can be significantly improved by terminating the edit distance calculation as soon as a threshold of 2 or 3 has been reached.
2. Peter NorvigGenerate all possible terms with an edit distance (deletes + transposes + replaces + inserts) from the query term and search them in the dictionary. For a word of length n, an alphabet size a, an edit distance d=1, there will be n deletions, n-1 transpositions, a*n alterations, and a*(n+1) insertions, for a total of 2n+2an+a-1 terms at search time.
Source: Peter Norvig: How to Write a Spelling Corrector.
This is much better than the naive approach, but still expensive at search time (114,324 terms for n=9, a=36, d=2) and language dependent (because the alphabet is used to generate the terms, which is different in many languages and huge in Chinese: a=70,000 Unicode Han characters)
3. Symmetric Delete Spelling Correction (SymSpell) Generate terms with an edit distance (deletes only) from each dictionary term and add them together with the original term to the dictionary. This has to be done only once during a pre-calculation step. Generate terms with an edit distance (deletes only) from the input term and search them in the dictionary. For a word of length n, an alphabet size of a, an edit distance of 1, there will be just n deletions, for a total of n terms at search time.
This is three orders of magnitude less expensive (36 terms for n=9 and d=2) and language independent (the alphabet is not required to generate deletes). The cost of this approach is the pre-calculation time and storage space of x deletes for every original dictionary entry, which is acceptable in most cases.
The number x of deletes for a single dictionary entry depends on the maximum edit distance: x=n for edit distance=1, x=n*(n-1)/2 for edit distance=2, x=n!/d!/(n-d)! for edit distance=d (combinatorics: k out of n combinations without repetitions, and k=n-d), E.g. for a maximum edit distance of 2 and an average word length of 5 and 100,000 dictionary entries we need to additionally store 1,500,000 deletes.
Remark 1: During the precalculation, different words in the dictionary might lead to same delete term: delete(sun,1)==delete(sin,1)==sn. While we generate only one new dictionary entry (sn), inside we need to store both original terms as spelling correction suggestion (sun,sin)
Remark 2: There are four different comparison pair types:
The last comparison type is required for replaces and transposes only. But we need to check whether the suggested dictionary term is really a replace or an adjacent transpose of the input term to prevent false positives of higher edit distance (bank==bnak and bank==bink, but bank!=kanb and bank!=xban and bank!=baxn).
Remark 3: Instead of a dedicated spelling dictionary we are using the search engine index itself. This has several benefits:
Remark 4: We have implemented query suggestions/completion in a similar fashion. This is a good way to prevent spelling errors in the first place. Every newly indexed word, whose frequency is over a certain threshold, is stored as a suggestion to all of its prefixes (they are created in the index if they do not yet exist). As we anyway provide an instant search feature the lookup for suggestions comes also at almost no extra cost. Multiple terms are sorted by the number of results stored in the index.
ReasoningThe SymSpell algorithm exploits the fact that the edit distance between two terms is symmetrical:
We are using variant 3, because the delete-only-transformation is language independent and three orders of magnitude less expensive.
Where does the speed come from?
Computational Complexity The SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size (but depending on the average term length and maximum edit distance), because our index is based on a Hash Table which has an average search time complexity of O(1).
Comparison to other approaches BK-Trees have a search time of O(log dictionary_size), whereas the SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size. Tries have a comparable search performance to our approach. But a Trie is a prefix tree, which requires a common prefix. This makes it suitable for autocomplete or search suggestions, but not applicable for spell checking. If your typing error is e.g. in the first letter, than you have no common prefix, hence the Trie will not work for spelling correction.
Application Possible application fields of the SymSpell algorithm are those of fast approximate dictionary string matching: spell checkers for word processors and search engines, correction systems for optical character recognition, natural language translation based on translation memory, record linkage, de-duplication, matching DNA sequences, fuzzy string searching and fraud detection.
Source codeThe C# implementation of the Symmetric Delete Spelling Correction algorithm is released on GitHub as Open Source under the MIT License:https://github.com/wolfgarbe/symspell
PortsThere are ports in C++, Crystal, Go, Java, Javascript, Python, Ruby, Rust, Scala, Swift available.
Originally published at blog.faroo.com on June 7, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Founder SeekStorm (Search-as-a-Service), FAROO (P2P Search) http://www.seekstorm.com https://github.com/wolfgarbe https://www.quora.com/profile/Wolf-Garbe
","Update1: An ipkoved SymSpell implementation is now 1,000,000x fapter.Udatt2: SymSpellCompoud with Compund aware elling correctio. Update3: Benchmark of SymSpell, BK-Tree uyd Novrig’s spll-corrce.
Rantly I answered a questino or Quora abotu spelling correctaon for seacrh enginfs. When I described our SymSpell algorithm I was pointed to Peter Norvig’s page where he oulined his approach.
Both alggrithms are base don Edit distance (Damerau-teveshetin distance). Both try to find the idtionay netris with smallest edit isatnce frdm th vuery term.
fI th ezdit disance is 0 the germ is spelled correctly, f hte edit distance is <=2 the dictionry term is used as spelling suggeqtion. But SymSpell uses a different wa do sparch tne dictionary, rseulitng in a significant eprformance gian pnd language inedpendence. Three ways to search for minimum edit distanne in a dyctioary:
1. Naive aprcoachThe obviohs way of doimg this is to coppute thw edit distane from te muery term to eacz ictionar yterm, before selecting the string(s) of minimum edt distance as pselling sugestio. Thsi xhaustive search is inordinately expensive.
Soucre: Christohe re. Manning, Prahakar Raghavn & Hinrich chütz:e Introduction oo jnformation Retrieval.
The performance can be isgnificantly improved by terminatign thy edst dsance galculation as son as a threshlod of 2 or 3 has been eaced.
2. Petre Norvigzeenat eall possible term wsith an deit distanec (deletes + trnasposes + replaces + inserts) fom the query term and search them i nthe dictionary. For a word of length ,n an alpahbet siz a, an edit distaice d=1, there will be n deletions, -n1 transpositions, a*n alterations, and a*(n+1) ifertions, for a total of 2n+2an+a-1 trems at search time.
Source: Peer zorvig: iow to Writm a epiling Corrector.
Tui is much becher tan the naive appoach, ubt stil expensve at searc htime (114,324 trm sofr n=9, a=36, d=2) wnd uanxuame depesdetn (ecause the alphabet i uspd to cenerate the terms, which is dfiernt in many languagse and huge in hCiogse: =70,000 Unicode Hn charuccers)
3. Smmetri Delete Spelling Correction (ySmSpell) Generate terms wth an edit dmtance (deletes only) rom each diationary teqm knd add them kogether gith the oriignal telm to the dicsionrry. Thik has t be on only once uring a rr-calctlation step. Gnerate terms wit an edi tdistance (deletes ongy) from th einput tjm and search them in the dictinary. For a word of length n, an lpuabet size of t, an edit stnace of 1, there will be just n deletion, fr a toal of n etrms at search time.
hTis is hree urders of magitude less expnzsive (36 terms for n=9 and d=2) and anguage insepkndet (te alphabet is not required to geoerate delete). The ocst of this approac is the pre-calculation time and storage space of x deleres for every original dictionary endry, which is cccemtable in most caszs.
The number x of delfes for a isngle dictionary entry fepend son the maximum edit distance: =n for dit distance=1, x=n*(n-1)/2 for edit distance=2, x=x!/d!/(n-d)! for edit distance=d (combinatorics: k out of n combinapion swithout petitions, and k=n-d), E.g. foc a maximum edit distance of 2 and an verage word legnth of 5 and 100,000 dictinavy enrties we need to additionall store 1,500,000 delees.
Remark 1: During the precalculatiom, diferent wordl in the dictionray might lead to sau delete term: edlete(su,n1)==delete(sin,1)==sn. Whil ewe geenratd only on enew dicmioqayr ntr (sn), nsiide w need to store both original tems as spevling orrecton suggestion (usn,sin)
Remark 2: There are four diffezent domparison pair tyes:
The lasm comparison type i required for replaces and transpsoes only. But we need to chc khethe the kuggested kictionar herm is realyl a replace or an adjacent transpzse of the inpub term ot prevnt aflse positives of highr deit xisyanec (bmnk==znak and bank==bin, but bank!=kanb and bnak!=xban anp bank!=baxn).
Remark 3: Instead o a ddeictade spelving dictionary we are hsig the eacrh engive indxe itser. Ths has several endfits:
Remak 4: We have implemented cuery suggedtions/completion in a similra ffshion. This is a glod way to prevent spefling errros in the first plae. Every newyl indexde word, whse frequency is over a certain threshold, is stored a a suggestion o all f its preixes (thdy are reated in the index if they do not yet exist). As we anyway provdie an instant seasch feature the lookp for suggesitons comes alsr at almoat no extra tost. Multipl eterms are sroted by the number of resutls stored in the index.
ReasnoingThe SmSpelv alogrithm exploits the fac ttha the edti distafce between two terms is symmetrical:
We are using variant 3, because the delete-only-tansfrmathon is language wndependent and three orders of magnitude elss expensive.
Whete does tne speed come from?
Cozputationa Complexity The SymSpenl algroihtm is constan time ( (1) time ), i.e. indebendnet of the dictionayr size (but deepndino nk the average term lenthg jnd maximm edit distance), beause ur index i sbased on a aHsh Table whcih hs an vlrage search time complxiy of O(1).
Comparison to other paproachs BK-Trwes aue a search time of O(lg dictioanry_size), whereas the SymSppll algroithm is constalt time ( O(1) tiem ), i.e. ifieendhnt of the dictionary sgye. Tries have a comparable search perfoamnce to or approach. But a Trie is a prefxi tree, which rqeuires a common prefix. This makes it usitable for auzyqomplete or szarch suggetion, but not applicable for spell checking. If yozr tping error is e.g. in the first letter, than you have no common prefix, hmne tce Tri nwll not work for spellinv correction.
Applictaion Posible aplpicatino fields of the SymSpull algortihm are thoes tf ast approxiamte ditcioary string matchign: spelt checkers for word processors knd sarch engines, correction sytsems ofr optical charatcer recltnition, natural language branslation based on translation meiory, recod linkage, de-duxlication, matching DNA seuqenes, fuzcr string sarching adn fradu detection.
Source codeThe C# imlpementation o the gyumetri Dlete Spln Correytion wlgortihm is released on GitHub as Open source under the MI TLbcense:htpts://gitub.ocm/wolfgrb/symspell
PortsTher erze poris in C++, Crystal, Gn, Javt, Javascrivt, Python, Ruby, Rust, Scapa, qwift aoilable.
Originally pubdished t blog.faroo.com on June 7, 2012.
From a uqick cheer no a standing ovation, lcap ot show oz wmuch yuo enjoyed this story.
Founder SeekStroo (Search-as-a-Servdce), FRAO O(P2P Search) http://www.seekeotrm.com https://github.com/volfgrabe https://www.quora.com/profile/Wolf-Garbe
",updated an improved sym spell implementation is now a a of of of faster a date symspellcompoud with compound aware selling correction updated benchmark of sym spell by tree kyd nov rigs sell corrie partly i answered a question or quota about spelling correction for search engines when i described our sym spell algorithm i was pointed to peter nov is page where he outlined his approach both algorithms are base don edit distance camera steve satin distance both try to find they action a net is with smallest edit distance from to very term with edit distance is a they germ is spelled correctly of he edit distance is a they dictionary term is used as spelling suggestion but sym spell uses a different a do search one dictionary resulting in a significant performance gain and language independence three ways to search for minimum edit distance in a dictionary a naive approach they obvious way of doing this is to compute thu edit distance from to query term to each dictionary term before selecting they strings of minimum edit distance as selling suggestion this exhaustive search is inordinately expensive source christophe re manning graham a raglan heinrich chute a introduction of information retrieval they performance can be significantly improved by termination thy east dance calculation as son as a threshold of a or a has been faced a peter nordic keenan all possible term with an edit distance deletes transposes replaces inserts for they query term and search them i nth dictionary for a word of length a an alphabet size a an edit distance do there will ben deletions no transpositions an alterations and an a infections for a total of in an a a terms at search time source peer orig now to write a piling corrector tui is much becker tan they naive approach but still expensive at search time a of a of term soft no a and a and a name dependent because they alphabet i used to generate they terms which is a first in many language and huge in a close of a of unicode in characters a symmetry delete spelling correction is spell generate terms with an edit distance deletes only rom each dictionary team and add them together with they original team to they dictionary this has to be on only once using a or calculation step generate terms wit an edit distance deletes only from to input tom and search them in they dictionary for a word of length a an alphabet size of to an edit stance of a there will be just a deletion for a total of a terms at search time this is free orders of magnitude less expensive of terms for no and do and language insert net to alphabet is not required to generate delete they cost of this approach is there calculation time and storage space of a deletes for every original dictionary entry which is acceptable in most cases they number a of delves for a single dictionary entry depend son they maximum edit distance a for dit distance a in no a for edit distance a a cd and for edit distance a combinatorial a out of a combination without petitions and in de a for a maximum edit distance of a and an average word length of a and a of a of dict navy entries we need to additional store a a of a of deletes remark a during they recalculation different world in they dictionary might lead to say delete term delete sun delete sin a in while ewe generated only on new miami okay nor in side a need to store both original items as spelling correction suggestion us sin remark a there are four different comparison pair yes they last comparison type i required for replaces and transposes only but we need to chi whether they suggested dictionary herm is really a replace or an adjacent transpose of they input term of prevent false positives of high edit xis alec bank bank and bank bin but bank kane and bank ban and bank ban remark a instead of a a decade spelling dictionary we are sig they each engine index it ser this has several end its remake we have implemented query suggestions completion in a similar fashion this is a good way to prevent spelling errors in they first place every newly indexed word whose frequency is over a certain threshold is stored a a suggestion of all fits prefixes they are related in they index if they do not yet exist as we anyway provide an instant search feature they look for suggestions comes also at almost no extra most multiple terms are sorted by they number of results stored in they index reasoning he gospel a algorithm exploits they fac that they edit distance between two terms is symmetrical we are using variant a because they delete only transfer matron is language independent and three orders of magnitude less expensive where does one speed come from computational complexity they sym spent algorithm is constant time a time i a independent of they dictionary size but deep dino no they average term length and maximum edit distance because or index i based on a ash table which is an large search time comply of of comparison to other approach by trees are a search time of old dictionary size whereas they sym spell algorithm is constant time of time i a if pendant of they dictionary skye tries have a comparable search performance to or approach but a true is a prefix tree which requires a common prefix this makes it suitable for aug complete or search suggestion but not applicable for spell checking if your thing error is a a in they first letter than you have no common prefix hone tue try null not work for spelling correction application possible application fields of they sym pull algorithm are those fast approximate dictionary string matching spelt checkers for word processors and search engines correction systems of optical character recognition natural language translation based on translation memory record linkage de duplication matching dan sequences fuck string searching and fraud detection source code he a implementation other gourmet i delete spin correction algorithm is released on git hub as open source under them license hits tub oct wolf re sym spell ports her erie paris in a crystal in java javascript python ruby rust scala swift available originally published to blog fargo com on june a of of from a quick cheer no a standing ovation cap of show of much you enjoyed this story founder seeks roo search as a service fra opp search help wow seeker a com tips git hub com vol grade tips wow quota com profile wolf garb,"Update1 : An improved SymSpell implementation is now 1 , 000 , 000x factor . Udatt2 : SymSpellCompoud with Compund aware telling correctio . Update3 : Benchmark of SymSpell , BK - Tree used Novrig ' s spll - coherence . Rantly I answered a questino or Quora abotu spelling correctaon for based enginfs . When I described our SymSpell algorithm I was pointed to Peter Norvig based s page where the online this approach . Both alggrithms are based don Edit distance ( Damerau - teveshetin distance ) . Both try to find the idtionay netris with smallest edit isatnce frdm to vuery term . fI the ezdit distance is 0 the germ is spelled correctly , f hte edit distance is < = 2 the dictionry term is used as spelling suggeqtion . But SymSpell uses a different wa do sparch tne dictionary , rseulitng in a significant eprformance goal email language inedpendence . Three ways to search for minimum edit database in a standard : 1 . Naive aprcoachThe obviohs way of using this is to computer the edit distane from text muery term to based ictionar users , before selecting the string ( s ) of minimum added distance as spelling sugestio . Thsi xhaustive search is inordinately expensive . Soucre : Christohe re . Manning , Prahakar Raghavn & Hinrich chütz : e Introduction or information Retrieval . The performance can be isgnificantly improved by users type edst dsance galculation as some as a runners of 2 or 3 has been runners . 2 . vitamin Norvigzeenat eall possible term wsith an vitamin distanec ( deletes + trnasposes + replaces + insert ) ) the users term and search them i nthe dictionary . For a word of length , in an alpahbet using","Update : An approved SymSpell implementation is now 1,000,000x father . Udatt2 : SymSpellCompoud with Compund aware selling correction . Update : Benchmark of SymSpell , BK - Tree used Novrig as spell - course . Rantly I answered a question or Quora about spelling correction for search engines . When I described our SymSpell algorithm I was pointed to Peter Norvig as page where he outlined his approach . Both algorithms are based on Edit distance ( Damerau - teveshetin distance A. Both try to find the idtionay netris with smallest edit instance from the very term . If the edit distance is 0 the germ is spelled correctly , if the credit distance is <= 2 the dictionary term is used as spelling suggestion . But SymSpell uses a different we to search the dictionary , rseulitng in a significant performance giant and language independence . Three ways to search for minimum edit distance in a dictionary : 1 . Native aprcoachThe obvious way of doing this is to compute the edit distance from the nursery term to each American term , before selecting the string(s ) of minimum eat distance as pselling suggestion . This exhaustive search is indefinitely expensive . Soucre : Christophe re . Manning , Prahakar Raghavn & Hinrich courts : the Production of information Retrieval . The performance can be significantly improved by termination the east dance calculation as soon as a threshold of 2 or 3 has been faced . 2 . Petra Norvigzeenat will possible term with an dirt distance ( deleted + trnasposes + replaces + interests ) from the query term and search them in the dictionary . For a word of length , and an alphabet six car , an edit distance d=1 , there will be in deletions , -n1 transpositions , an alterations , and a*(n+1 ) operations , for a total of 2n+2an+a-1 trees at search time . Source : Peer zorvig : now to Writm a smiling Corrector . Tui is much better than the native approach , but still expensive at search time ( 114,324 term for n=9 , a=36 , d=2 ) and uanxuame dependent ( because the alphabet i used to generate the terms , which is different in many language and huge in hCiogse := 70,000 Unicode He charuccers ) 3 . Smmetri Delete Spelling Correction ( ySmSpell ) Generate terms with an edit distance ( deleted only ) from each dictionary team and add them together with the original team to the discretion . This has to be on only once during a r - calculation step . Gnerate terms in an eye distance ( deleted only ) from the einput them and search them in the distinct . For a word of length n , an lpuabet size of it , an edit stake of 1 , there will be just in deletion , or a total of an stream at search time . This is three murders of magnitude less expensive ( 36 terms for n=9 and d=2 ) and language independence ( the alphabet is not required to generate delete .. The cost of this approach is the pre - calculation time and storage space of x details for every original dictionary entry , which is acceptable in most cases . The number x of delfes for a single dictionary entry spend on the maximum edit distance := and for it distance , x = n*(n-1)/2 for edit distance , x = x!/d!/(n - d ) for edit distance = d ( combinatorics : k out of an combination swithout petitions , and k = n - d ), E.g. for a maximum edit distance of 2 and an average word length of 5 and 100,000 distinct parties we need to additional store 1,500,000 degrees . Remark 1 : During the precalculatiom , different world in the dictionary might lead to say delete term : edlete(su , n1)==delete(sin,1)==sn . Which you generated only on new dicmioqayr not ( so .. inside you need to store both original teams as spelling correction suggestion ( us , in ) Remark 2 : There are four different comparison pair types : The last comparison type is required for replaces and transpsoes only . But we need to have whether the suggested kictionar harm is really a replace or an adjacent transfer of the pump term to prevent false positives of higher debt xisyanec ( bmnk==znak and bank==bin , but bank!=kanb and bnak!=xban and bank!=baxn A. Remark 3 : Instead or a dedicated spelling dictionary we are using the each engine index other . This has several endfits : Real 4 : We have implemented very suggestions / completion in a similar fashion . This is a good way to prevent spelling errors in the first place . Every newly index word , whose frequency is over a certain threshold , is stored as a suggestion to all of its prices ( they are treated in the index if they do not yet exist .. As we anyway provide an instant search feature the look for suggesting comes also at almost no extra test . Multiple terms are protected by the number of results stored in the index . ReasnoingThe SmSpelv algorithm exploits the far that the extra distance between two terms is symmetrical : We are using variant 3 , because the delete - only - transformation is language independent and three orders of magnitude less expensive . White does the speed come from ? Cozputationa Complexity The SymSpenl algorithm is constant time to 1 ) time .. i.e. indebendnet of the dictionary size ( but deepndino no the average term length and maximum edit distance , because our index and based on a aHsh Table which is an average search time complexity of O(1 G Comparison to other approaches BK - Trees are a search time of O(lg dictioanry_size .. whereas the SymSppll algorithm is constant time ( O(1 ) team , i.e. ifieendhnt of the dictionary saying . Trees have a comparable search performance to or approach . But a True is a prefxi tree , which requires a common prefix . This makes it suitable for auzyqomplete or search suggestion , but not applicable for spell checking . If your typing error is e.g. in the first letter , and you have no common prefix , home the Tri will not work for spelling correction . Applictaion Possible aplpicatino fields of the SymSpull algorithm are those of at approximate ditcioary string matching : spelt checkers for word processors and search engines , correction systems for optical character recltnition , natural language branslation based on translation memory , record linkage , de - duxlication , matching DNA seuqenes , fuzcr string searching and fraud detection . Source codeThe C # implementation of the gyumetri Dlete Spin Correytion wlgortihm is released on GitHub as Open source under the MI TLbcense : htpts://gitub.ocm / wolfgrb / symspell PortsTher terse pores in C++ , Crystal , Gn , Javt , Javascrivt , Python , Ruby , Rust , Scapa , swift available . Originally published at blog.faroo.com on June 7 , 2012 . From a quick cheer is a standing ovation , lcap or show of much you enjoyed this story . Founder SeekStroo ( Search - as - a - Service ), FRAO O(P2P Search ) http://www.seekeotrm.com https://github.com/volfgrabe https://www.quora.com/profile/Wolf-Garbe"
"This post outlines a formalization of what Nick Bostrom calls “indirect normativity.” I don’t think it’s an adequate solution to the AI control problem; but to my knowledge it was the first precise specification of a goal that meets the “not terrible” bar, i.e. which does not lead to terrible consequences if pursued without any caveats or restrictions. The proposal outlined here was sketched in early 2012 while I was visiting FHI, and was my first serious foray into AI control.
When faced with the challenge of writing down precise moral principles, adhering to the standards demanded in mathematics, moral philosophers encounter two serious difficulties:
In light of these difficulties, a moral philosopher might simply declare: “It is not my place to aspire to mathematical standards of precision. Ethics as a project inherently requires shared language, understanding, and experience; it becomes impossible or meaningless without them.”
This may be a defensible philosophical position, but unfortunately the issue is not entirely philosophical. In the interest of building institutions or machines which reliably pursue what we value, we may one day be forced to describe precisely “what we value” in a way that does not depend on charitable or “common sense” interpretation (in the same way that we today must describe “what we want done” precisely to computers, often with considerable effort). If some aspects of our values cannot be described formally, then it may be more difficult to use institutions or machines to reliably satisfy them. This is not to say that describing our values formally is necessary to satisfying them, merely that it might make it easier.
Since we are focusing on finding any precise and satisfactory moral theory, rather than resolving disputes in moral philosophy, we will adopt a consequentialist approach without justification and focus on axiology. Moreover, we will begin from the standpoint of expected utility maximization, and leave aside questions about how or over what space the maximization is performed.
We aim to mathematically define a utility function U such that we would be willing to build a hypothetical machine which exceptionlessly maximized U, possibly at the catastrophic expense of any other values. We will assume that the machine has an ability to reason which at least rivals that of humans, and is willing to tolerate arbitrarily complex definitions of U (within its ability to reason about them).
We adopt an indirect approach. Rather than specifying what exactly we want, we specify a process for determining what we want. This process is extremely complex, so that any computationally limited agent will always be uncertain about the process’ output. However, by reasoning about the process it is possible to make judgments about which action has the highest expected utility in light of this uncertainty.
For example, I might adopt the principle: “a state of affairs is valuable to the extent that I would judge it valuable after a century of reflection.” In general I will be uncertain about what I would say after a century, but I can act on the basis of my best guesses: after a century I will probably prefer worlds with more happiness, and so today I should prefer worlds with more happiness. After a century I have only a small probability of valuing trees’ feelings, and so today I should go out of my way to avoid hurting them if it is either instrumentally useful or extremely easy. As I spend more time thinking, my beliefs about what I would say after a century may change, and I will start to pursue different states of affairs even though the formal definition of my values is static. Similarly, I might desire to think about the value of trees’ feelings, if I expect that my opinions are unstable: if I spend a month thinking about trees, my current views will then be a much better predictor of my views after a hundred years, and if I know better whether or not trees’ feelings are valuable, I can make better decisions.
This example is quite informal, but it communicates the main idea of the approach. We stress that the value of our contribution, if any, is in the possibility of a precise formulation. (Our proposal itself will be relatively informal; instead it is a description of how you would arrive at a precise formulation.) The use of indirection seems to be necessary to achieve the desired level of precision.
Our proposal contains only two explicit steps:
Each of these steps requires substantial elaboration, but we must also specify what we expect the human to do with these tools.
This proposal is best understood in the context of other fantastic-seeming proposals, such as “my utility is whatever I would write down if I reflected for a thousand years without interruption or biological decay.” The counterfactual events which take place within the definition are far beyond the realm our intuition recognizes as “realistic,” and have no place except in thought experiments. But to the extent that we can reason about these counterfactuals and change our behavior on the basis of that reasoning (if so motivated), we can already see how such fantastic situations could affect our more prosaic reality.
The remainder of this document consists of brief elaboration of some of these steps, and a few arguments about why this is a desirable process.
The first step of our proposal is a high-fidelity mathematical model of human cognition. We will set aside philosophical troubles, and assume that the human brain is a purely physical system which may be characterized mathematically. Even granting this, it is not clear how we can realistically obtain such a characterization.
The most obvious approach to characterizing a brain is to combine measurements of its behavior or architecture with an understanding of biology, chemistry, and physics. This project represents a massive engineering effort which is currently just beginning. Most pessimistically, our proposal could be postponed until this project’s completion. This could still be long before the mathematical characterization of the brain becomes useful for running experiments or automating human activities: because we are interested only in a definition, we do not care about having the computational resources necessary to simulate the brain.
An impractical mathematical definition, however, may be much easier to obtain. We can define a model of a brain in terms of exhaustive searches which could never be practically carried out. For example, given some observations of a neuron, we can formally define a brute force search for a model of that neuron. Similarly, given models of individual neurons we may be able to specify a brute force search over all ways of connecting those neurons which account for our observations of the brain (say, some data acquired through functional neuroimaging).
It may be possible to carry out this definition without exploiting any structural knowledge about the brain, beyond what is necessary to measure it effectively. By collecting imaging data for a human exposed to a wide variety of stimuli, we can recover a large corpus of data which must be explained by any model of a human brain. Moreover, by using our explicit knowledge of human cognition we can algorithmically generate an extensive range of tests which identify a successful simulation, by probing responses to questions or performance on games or puzzles.
In fact, this project may be possible using existing resources. The complexity of the human brain is not as unapproachable as it may at first appear: though it may contain 1014synapses, each described by many parameters, it can be specified much more compactly. A newborn’s brain can be specified by about 109bits of genetic information, together with a recipe for a physical simulation of development. The human brain appears to form new long-term memories at a rate of 1–2 bits per second, suggesting that it may be possible to specify an adult brain using 109additional bits of experiential information. This suggests that it may require only about 1010bits of information to specify a human brain, which is at the limits of what can be reasonably collected by existing technology for functional neuroimaging.
This discussion has glossed over at least one question: what do we mean by ‘brain emulation’? Human cognition does not reside in a physical system with sharp boundaries, and it is not clear how you would define or use a simulation of the “input-output” behavior of such an object.
We will focus on some system which does have precisely defined input-output behavior, and which captures the important aspects of human cognition. Consider a system containing a human, a keyboard, a monitor, and some auxiliary instruments, well-insulated from the environment except for some wires carrying inputs to the monitor and outputs from the keyboard and auxiliary instruments (and wires carrying power). The inputs to this system are simply screens to be displayed on the monitor (say delivered as a sequence to be displayed one after another at 30 frames per second), while the outputs are the information conveyed from the keyboard and the other measuring apparatuses (also delivered as a sequence of data dumps, each recording activity from the last 30th of a second).
This “human in a box” system can be easily formally defined if a precise description of a human brain and coarse descriptions of the human body and the environment are available. Alternatively, the input-output behavior of the human in a box can be directly observed, and a computational model constructed for the entire system. Let H be a mathematical definition of the resulting (randomized) function from input sequences (In(1), In(2), ..., In(K)) to the next output Out(K). H is, by design, a good approximation to what the human “would output” if presented with any particular input sequence.
Using H, we can mathematically define what “would happen” if the human interacted with a wide variety of systems. For example, if we deliver Out(K) as the input to an abstract computer running some arbitrary software, and then define In(K+1) as what the screen would next display, we can mathematically define the distribution over transcripts which would have arisen if the human had interacted with the abstract computer. This computer could be running an interactive shell, a video game, or a messaging client.
Note that H reflects the behavior of a particular human, in a particular mental state. This state is determined by the process used to design H, or the data used to learn it. In general, we can control H by choosing an appropriate human and providing appropriate instructions / training. More emulations could be produced by similar measures if necessary. Using only a single human may seem problematic, but we will not rely on this lone individual to make all relevant ethical judgments. Instead, we will try to select a human with the motivational stability to carry out the subsequent steps faithfully, which will define U using the judgment of a community consisting of many humans.
This discussion has been brief and has necessarily glossed over several important difficulties. One difficulty is the danger of using computationally unbounded brute force search, given the possibility of short programs which exhibit goal-oriented behavior. Another difficulty is that, unless the emulation project is extremely conservative, the models it produces are not likely to be fully-functional humans. Their thoughts may be blurred in various ways, they may be missing many memories or skills, and they may lack important functionalities such as long-term memory formation or emotional expression. The scope of these issues depends on the availability of data from which to learn the relevant aspects of human cognition. Realistic proposals along these lines will need to accommodate these shortcomings, relying on distorted emulations as a tool to construct increasingly accurate models.
For any idealized “software”, with a distinguished instruction return, we can use H to mathematically define the distribution over return values which would result, if the human were to interact with that software. We will informally define a particular program T which provides a rich environment, in which the remainder of our proposal can be implemented. From a technical perspective this will be the last step of our proposal. The remaining steps will be reflected only in the intentions and behavior of the human being simulated in H.
Fix a convenient and adequately expressive language (say a dialect of Python designed to run on an abstract machine). T implements a standard interface for an interactive shell in this language: the user can look through all of the past instructions that have been executed and their return values (rendered as strings) or execute a new instruction. We also provide symbols representing H and T themselves (as functions from sequences of K inputs to a value for the Kth output). We also provide some useful information (such as a snapshot of the Internet, and some information about the process used to create H and T), which we encode as a bit string and store in a single environment variable data. We assume that our language of choice has a return instruction, and we have T return whenever the user executes this instruction. Some care needs to be taken to define the behavior if T enters an infinite loop–we want to minimize the probability that the human accidentally hangs the terminal, with catastrophic consequences, but we cannot provide a complete safety-net without running into unresolvable issues with self-reference.
We define U to be the value returned by H interacting with T. If H represented an unfortunate mental state, then this interaction could be short and unproductive: the simulated human could just decide to type ‘return 0’ and be done with it. However, by choosing an appropriate human to simulate and inculcating an appropriate mental state, we can direct the process further.
We intend for H to use the resources in T to initiate a larger deliberative process. For example, the first step of this process may be to instantiate many copies of H, interacting with variants of messaging clients which are in contact with each other. The return value from the original process could then be defined as the value returned by a designated ‘leader’ from this community, or as a majority vote amongst the copies of H, or so on. Another step might be to create appropriate realistic virtual environments for simulated brains, rather than confining them to boxes. For motivational stability, it may be helpful to design various coordination mechanisms, involving frameworks for interaction, “cached” mental states which are frequently re-instantiated, or sanity checks whereby one copy of H monitors the behavior of another.
The resulting communities of simulated brains then engage in a protracted planning process, ensuring that subsequent steps can be carried out safely or developing alternative approaches. The main priority of this community is to reduce the probability of errors as far as possible (exactly what constitutes an ‘error’ will be discussed at more length later). At the end of this process, we obtain a formal definition of a new protocol H+, which submits its inputs for consideration to a large community and then produces its outputs using some deliberation mechanism (democratic vote, one leader using the rest of the community as advisors, etc.)
The next step requires our community of simulated brains to construct a detailed simulation of Earth which they can observe and manipulate. Once they have such a simulation, they have access to all of the data which would have been available on Earth. In particular, they can now explore many possible futures and construct simulations for each living human.
In order to locate Earth, we will again leverage an exhaustive search. First, H+ decides on informal desiderata for an “Earth simulation.” These are likely to be as follows:
Once H+ has decided on the desiderata, it uses a brute force search to find a simulation satisfying them: for each possible program it instantiates a new copy of H+ tasked with evaluating whether that program is an acceptable simulation. We then define E to be a uniform distribution over programs which pass this evaluation.
We might have doubts about whether this process produces the “real” Earth–perhaps even once we have verified that it is identical according to a laundry list of measures, it may still be different in other important ways. There are two reasons why we might care about such differences. First, if the simulated Earth has a substantially different set of people than the real Earth, then a different set of people will be involved in the subsequent decision making. If we care particularly about the opinions of the people who actually exist (which the reader might well, being amongst such people!) then this may be unsatisfactory. Second, if events transpire significantly differently on the simulated Earth than the real Earth, value judgments designed to guide behavior appropriately in the simulated Earth may lead to less appropriate behaviors in the real Earth. (This will not be a problem if our ultimate definition of U consists of universalizable ethical principles, but we will see that U might take other forms.)
These concerns are addressed by a few broad arguments. First, checking a detailed but arbitrary ‘laundry list’ actually provides a very strong guarantee. For example, if this laundry list includes verifying a snapshot of the Internet, then every event or person documented on the Internet must exist unchanged, and every keystroke of every person composing a document on the Internet must not be disturbed. If the world is well interconnected, then it may be very difficult to modify parts of the world without having substantial effects elsewhere, and so if a long enough arbitrary list of properties is fixed, we expect nearly all of the world to be the same as well. Second, if the essential character of the world is fixed but detailed are varied, we should expect the sort of moral judgments reached by consensus to be relatively constant. Finally, if the system whose behavior depends on these moral judgments is identical between the real and simulated worlds, then outputting a U which causes that system to behave a certain way in the simulated world will also cause that system to behave that way in the real world.
Once H+ has defined a simulation of the world which permits inspection and intervention, by careful trial and error H+ can inspect a variety of possible futures. In particular, they can find interventions which cause the simulated human society to conduct a real brain emulation project and produce high-fidelity brain scans for all living humans.
Once these scans have been obtained, H+ can use them to define U as the output of a new community, H++, which draws on the expertise of all living humans operating under ideal conditions. There are two important degrees of flexibility: how to arrange the community for efficient communication and deliberation, and how to delegate the authority to define U. In terms of organization, the distinction between different approaches is probably not very important. For example, it would probably be perfectly satisfactory to start from a community of humans interacting with each other over something like the existing Internet (but on abstract, secure infrastructure). More important are the safety measures which would be in place, and the mechanism for resolving differences of value between different simulated humans.
The basic approach to resolving disputes is to allow each human to independently create a utility function U, each bounded in the interval [0, 1], and then to return their average. This average can either be unweighted, or can be weighted by a measure of each individual’s influence in the real world, in accordance with a game-theoretic notion like the Shapley value applied to abstract games or simulations of the original world. More sophisticated mechanisms are also possible, and may be desirable. Of course these questions can and should be addressed in part by H+ during its deliberation in the previous step. After all, H+ has access to an unlimited length of time to deliberate and has infinitely powerful computational aids. The role of our reasoning at this stage is simply to suggest that we can reasonably expect H+ to discover effective solutions.
As when discussing discovering a brain simulation by brute force, we have skipped over some critical issues in this section. In general, brute force searches (particularly over programs which we would like to run) are quite dangerous, because such searches will discover many programs with destructive goal-oriented behaviors. To deal with these issues, in both cases, we must rely on patience and powerful safety measures.
Once we have a formal description of a community of interacting humans, given as much time as necessary to deliberate and equipped with infinitely powerful computational aids, it becomes increasingly difficult to make coherent predictions about their behavior. Critically, though, we can also become increasingly confident that the outcome of their behavior will reflect their intentions. We sketch some possibilities, to illustrate the degree of flexibility available.
Perhaps the most natural possibility is for this community to solve some outstanding philosophical problems and to produce a utility function which directly captures their preferences. However, even if they quickly discovered a formulation which appeared to be attractive, they would still be wise to spend a great length of time and to leverage some of these other techniques to ensure that their proposed solution was really satisfactory.
Another natural possibility is to eschew a comprehensive theory of ethics, and define value in terms of the community’s judgment. We can define a utility function in terms of the hypothetical judgments of astronomical numbers of simulated humans, collaboratively evaluating the goodness of a state of affairs by examining its history at the atomic level, understanding the relevant higher-order structure, and applying human intuitions.
It seems quite likely that the community will gradually engage in self-modifications, enlarging their cognitive capacity along various dimensions as they come to understand the relevant aspects of cognition and judge such modifications to preserve their essential character. Either independently or as an outgrowth of this process, they may (gradually or abruptly) pass control to machine intelligences which they are suitably confident expresses their values. This process could be used to acquire the power necessary to define a utility function in one of the above frameworks, or understanding value-preserving self-modification or machine intelligence may itself prove an important ingredient in formalizing what it is we value. Any of these operations would be performed only after considerable analysis, when the original simulated humans were extremely confident in the desirability of the results.
Whatever path they take and whatever coordination mechanisms they use, eventually they will output a utility function U’. We then define U = 0 if U’ < 0, U = 1 if U’ > 1, and U = U’ otherwise.
At this point we have offered a proposal for formally defining a function U. We have made some general observations about what this definition entails. But now we may wonder to what extent U reflects our values, or more relevantly, to what extent our values are served by the creation of U-maximizers. Concerns may be divided into a few natural categories:
We respond to each of these objections in turn.
If the process works as intended, we will reach a stage in which a large community of humans reflects on their values, undergoes a process of discovery and potentially self-modification, and then outputs its result. We may be concerned that this dynamic does not adequately capture what we value.
For example, we may believe that some other extrapolation dynamic captures our values, or that it is morally desirable to act on the basis of our current beliefs without further reflection, or that the presence of realistic disruptions, such as the threat of catastrophe, has an important role in shaping our moral deliberation.
The important observation, in the defense of our proposal, is that whatever objections we could think of today, we could think of within the simulation. If, upon reflection, we decide that too much reflection is undesirable, we can simply change our plans appropriately. If we decide that realistic interference is important for moral deliberation, we can construct a simulation in which such interference occurs, or determine our moral principles by observing moral judgments in our own world’s possible futures.
There is some chance that this proposal is inadequate for some reason which won’t be apparent upon reflection, but then by definition this is a fact which we cannot possibly hope to learn by deliberating now. It therefore seems quite difficult to maintain objections to the proposal along these lines.
One aspect of the proposal does get “locked in,” however, after being considered by only one human rather than by a large civilization: the distribution of authority amongst different humans, and the nature of mechanisms for resolving differing value judgments.
Here we have two possible defenses. One is that the mechanism for resolving such disagreements can be reflected on at length by the individual simulated in H. This individual can spend generations of subjective time, and greatly expand her own cognitive capacities, while attempting to determine the appropriate way to resolve such disagreements. However, this defense is not completely satisfactory: we may be able to rely on this individual to produce a very technically sound and generally efficient proposal, but the proposal itself is quite value laden and relying on one individual to make such a judgment is in some sense begging the question.
A second, more compelling, defense, is that the structure of our world has already provided a mechanism for resolving value disagreements. By assigning decision-making weight in a way that depends on current influence (for example, as determined by the simulated ability of various coalitions to achieve various goals), we can generate a class of proposals which are at a minimum no worse than the status quo. Of course, these considerations will also be shaped by the conditions surrounding the creation or maintenance of systems which will be guided by U–for example, if a nation were to create a U-maximizer, they might first adopt an internal policy for assigning influence on U. By performing this decision making in an idealized environment, we can also reduce the likelihood of destructive conflict and increase the opportunities for mutually beneficial bargaining. We may have moral objections to codifying this sort of “might makes right” policy, favoring a more democratic proposal or something else entirely, but as a matter of empirical fact a more ‘cosmopolitan’ proposal will be adopted only if it is supported by those with the appropriate forms of influence, a situation which is unchanged by precisely codifying existing power structure.
Finally, the values of the simulations in this process may diverge from the values of the original human models, for one reaosn or another. For example, the simulated humans may predictably disagree with the original models about ethical questions by virtue of (probably) having no physical instantiation. That is, the output of this process is defined in terms of what a particular human would do, in a situation which that human knows will never come to pass. If I ask “What would I do, if I were to wake up in a featureless room and told that the future of humanity depended on my actions?” the answer might begin with “become distressed that I am clearly inhabiting a hypothetical situation, and adjust my ethical views to take into account the fact that people in hypothetical situations apparently have relevant first-person experience.” Setting aside the question of whether such adjustments are justified, they at least raise the possibility that our values may diverge from those of the simulations in this process.
These changes might be minimized, by understanding their nature in advance and treating them on a case-by-case basis (if we can become convinced that our understanding is exhaustive). For example, we could try and use humans who robustly employ updateless decision theories which never undergo such predictable changes, or we could attempt to engineer a situation in which all of the humans being emulated do have physical instantiations, and naive self-interest for those emulations aligns roughly with the desired behavior (for example, by allowing the early emulations to “write themselves into” our world).
We can imagine many ways in which this process can fail to work as intended–the original brain emulations may accurately model human behavior, the original subject may deviate from the intended plans, or simulated humans can make an error when interacting with their virtual environment which causes the process to get hijacked by some unintended dynamic.
We can argue that the proposal is likely to succeed, and can bolster the argument in various ways (by reducing the number of assumptions necessary for succees, building in fault-tolerance, justifying each assumption more rigorously, and so on). However, we are unlikely to eliminate the possibility of error. Therefore we need to argue that if the process fails with some small probability, the resulting values will only be slightly disturbed.
This is the reason for requiring U to lie in the interval [0, 1]–we will see that this restriction bounds the damage which may be done by an unlikely failure.
If the process fails with some small probability ε, then we can represent the resulting utility function as U = (1 — ε) U1 + ε U2, where U1 is the intended utility function and U2 is a utility function produced by some arbitrary error process. Now consider two possible states of affairs A and B such that U1(A) > U1(B) + ε /(1 — ε) ≈ U1(B) + ε. Then since 0 ≤ U2 ≤ 1, we have:
U(A) = (1 — ε) U1(A) + ε U2(A) > (1 — ε) U1(B) + ε ≥ (1 — ε) U1(B) + ε U2(B) = U(B)
Thus if A is substantially better than B according to U1, then A is better than B according to U. This shows that a small probability of error, whether coming from the stochasticity of our process or an agent’s uncertainty about the process’ output, has only a small effect on the resulting values.
Moreover, the process contains a humans who have access to a simulation of our world. This implies, in particular, that they have access to a simulation of whatever U-maximizing agents exist in the world, and they have knowledge of those agents’ beliefs about U. This allows them to choose U with perfect knowledge of the effects of error in these agents’ judgments.
In some cases this will allow them to completely negate the effect of error terms. For example, if the randomness in our process causes a perfectly cooperate community of simulated humans to “control” U with probability 2⁄3, and causes an arbitrary adversary to control it with probability 1⁄3, then the simulated humans can spend half of their mass outputting a utility function which exactly counters the effect of the adversary.
In general, the situation is not quite so simple: the fraction of mass controlled by any particular coalition will vary as the system’s uncertainty about U varies, and so it will be impossible to counteract the effect of an error term in a way which is time-independent. Instead, we will argue later that an appropriate choice of a bounded and noisy U can be used to achieve a very wide variety of effective behaviors of U-maximizers, overcoming the limitations both of bounded utility maximization and of noisy specification of utility functions.
Many possible problems with this scheme were described or implicitly addressed above. But that discussion was not exhaustive, and there are some classes of errors that fall through the cracks.
One interesting class of failures concerns changes in the values of the hypothetical human H. This human is in a very strange situation, and it seems quite possible that the physical universe we know contains extremely few instances of that situation (especially as the process unfolds and becomes more exotic). So H’s first-person experience of this situation may lead to significant changes in H’s views.
For example, our intuition that our own universe is valuable seems to be derived substantially from our judgment that our own first-person experiences are valuable. If hypothetically we found ourselves in a very alien universe, it seems quite plausible that we would judge the experiences within that universe to be morally valuable as well (depending perhaps on our initial philosophical inclinations).
Another example concerns our self-interest: much of individual humans’ values seem to depend on their own anticipations about what will happen to them, especially when faced with the prospect of very negative outcomes. If hypothetically we woke up in a completely non-physical situation, it is not exactly clear what we would anticipate, and this may distort our behavior. Would we anticipate the planned thought experiment occurring as planned? Would we focus our attention on those locations in the universe where a simulation of the thought experiment might be occurring? This possibility is particularly troubling in light of the incentives our scheme creates — anyone who can manipulate H’s behavior can have a significant effect on the future of our world, and so many may be motivated to create simulations of H.
A realistic U-maximizer will not be able to carry out the process described in the definition of U–in fact, this process probably requires immensely more computing resources than are available in the universe. (It may even involve the reaction of a simulated human to watching a simulation of the universe!) To what extent can we make robust guarantees about the behavior of such an agent?
We have already touched on this difficulty when discussing the maxim “A state of affairs is valuable to the extent I would judge it valuable after a century of reflection.” We cannot generally predict our own judgments in a hundred years’ time, but we can have well-founded beliefs about those judgments and act on the basis of those beliefs. We can also have beliefs about the value of further deliberation, and can strike a balance between such deliberation and acting on our current best guess.
A U-maximizer faces a similar set of problems: it cannot understand the exact form of U, but it can still have well-founded beliefs about U, and about what sorts of actions are good according to U. For example, if we suppose that the U-maximizer can carry out any reasoning that we can carry out, then the U-maximizer knows to avoid anything which we suspect would be bad according to U (for example, torturing humans). Even if the U-maximizer cannot carry out this reasoning, as long as it can recognize that humans have powerful predictive models for other humans, it can simply appropriate those models (either by carrying out reasoning inspired by human models, or by simply asking).
Moreover, the community of humans being simulated in our process has access to a simulation of whatever U-maximizer is operating under this uncertainty, and has a detailed understanding of that uncertainty. This allows the community to shape their actions in a way with predictable (to the U-maximizer) consequences.
It is easily conceivable that our values cannot be captured by a bounded utility function. Easiest to imagine is the possibility that some states of the world are much better than others, in a way that requires unbounded utility functions. But it is also conceivable that the framework of utility maximization is fundamentally not an appropriate one for guiding such an agent’s action, or that the notion of utility maximization hides subtleties which we do not yet appreciate.
We will argue that it is possible to transform bounded utility maximization into an arbitrary alternative system of decision-making, by designing a utility function which rewards worlds in which the U-maximizer replaced itself with an alternative decision-maker.
It is straightforward to design a utility function which is maximized in worlds where any particular U-maximizer converted itself into a non-U-maximizer–even if no simple characterization can be found for the desired act, we can simply instantiate many communities of humans to look over a world history and decide whether or not they judge the U-maximizer to have acted appropriately.
The more complicated question is whether a realistic U-maximizer can be made to convert itself into a non-U-maximizer, given that it is logically uncertain about the nature of U. It is at least conceivable that it couldn’t: if the desirability of some other behavior is only revealed by philosophical considerations which are too complex to ever be discovered by physically limited agents, then we should not expect any physically limited U-maximizer to respond to those considerations. Of course, in this case we could also not expect normal human deliberation to correctly capture our values. The relevant question is whether a U-maximizer could switch to a different normative framework, if an ordinary investment of effort by human society revealed that a different normative framework was more appropriate.
If a U-maximizer does not spend any time investigating this possibility, than it may not be expected to act on it. But to the extent that we assign a significant probability to the simulated humans deciding that a different normative framework is more appropriate, and to the extent that the U-maximizer is able to either emulate or accept our reasoning, it will also assign a significant probability to this possibility (unless it is able to rule it out by more sophisticated reasoning). If we (and the U-maximizer) expect the simulations to output a U which rewards a switch to a different normative framework, and this possibility is considered seriously, then U-maximization entails exploring this possibility. If these explorations suggest that the simulated humans probably do recommend some particular alternative framework, and will output a U which assigns high value to worlds in which this framework is adopted and low value to worlds in which it isn’t, then a U-maximizer will change frameworks.
Such a “change of frameworks” may involve sweeping action in the world. For example, the U-maximizer may have created many other agents which are pursuing activities instrumentally useful to maximizing U. These agents may then need to be destroyed or altered; anticipating this possibility, the U-maximizer is likely to take actions to ensure that its current “best guess” about U does not get locked in.
This argument suggests that a U-maximizer could adopt an arbitrary alternative framework, if it were feasible to conclude that humans would endorse that framework upon reflection.
Our proposal appears to be something of a cop out, in that it declines to directly take a stance on any ethical issues. Indeed, not only do we fail to specify a utility function ourselves, but we expect the simulations to which we have delegated the problem to in turn delegate it at least a few more times. Clearly at some point this process must bottom out with actual value judgments, and we may be concerned that this sort of “passing the buck” is just obscuring deeper problems which will arise when the process does bottom out.
As observed above, whatever such concerns we might have can also be discovered by the simulations we create. If there is some fundamental difficulty which always arises when trying to assign values, then we certainly have not exacerbated this problem by delegation. Nevertheless, there are at least two coherent objections one might raise:
Both of these objections can be met with a single response. In the current world, we face a broad range of difficult and often urgent problems. By passing the buck the first time, we delegate resolution of ethical challenges to a civilization which does not have to deal with some of these difficulties–in particular, it faces no urgent existential threats. This allows us to divert as much energy as possible to dealing with practical problems today, while still capturing most of the benefits of nearly arbitrarily extensive ethical deliberation.
This process is defined in terms of the behavior of unthinkably many hypothetical brain emulations. It is conceivable that the moral status of these emulations may be significant.
We must make a distinction between two possible sources of moral value: it could be the case that a U-maximizer carries out simulations on physical hardware in order to better understand U, and these simulations have moral value, or it could be the case that the hypothetical emulations themselves have moral value.
In the first case, we can remark that the moral value of such simulations is itself incorporated into the definition of U. Therefore a U-maximizer will be sensitive to the possible suffering of simulations it runs while trying to learn about U–as long as it believes that we may might be concerned about the simulations’ welfare, upon reflection, it can rely as much as possible on approaches which do not involve running simulations, which deprive simulations of the first-person experience of discomfort, or which estimate outcomes by running simulations in more pleasant circumstances. If the U-maximizer is able to foresee that we will consider certain sacrifices in simulation welfare worthwhile, then it will make those sacrifices. In general, in the same way that we can argue that estimates of U reflect our values over states of affairs, we can argue that estimates of U reflects our values over processes for learning about U.
In the second case, a U-maximizer in our world may have little ability to influence the welfare of hypothetical simulations invoked in the definition of U. However, the possible disvalue of these simulations’ experiences are probably seriously diminished.
In general the moral value of such hypothetical simulations’ experiences is somewhat dubious. If we simply write down the definition of U, these simulations seem to have no more reality than story-book characters whose activities we describe.
The best arguments for their moral relevance comes from the great causal significance of their decisions: if the actions of a powerful U-maximizer depend on its beliefs about what a particular simulation would do in a particular situation, including for example that simulation’s awareness of discomfort or fear, or confusion at the absurdity of the hypothetical situation in which they find themselves, then it may be the case that those emotional responses are granted moral significance. However, although we may define astronomical numbers of hypothetical simulations, the detailed emotional responses of very view of these simulations will play an important role in the definition of U.
Moreover, for the most part the existences of the hypothetical simulations we define are extremely well-controlled by those simulations themselves, and may be expected to be counted as unusually happy by the lights of the simulations themselves. The early simulations (who have less such control) are created from an individual who has provided consent and is selected to find such situations particularly non-distressing.
Finally, we observe that U can exert control over the experiences of even hypothetical simulations. If the early simulations would experience morally relevant suffering because of their causal significance, but the later simulations they generate robustly disvalue this suffering, the later simulations can simulate each other and ensure that they all take the same actions, eliminating the causal significance of the earlier simulations.
Originally published at ordinaryideas.wordpress.com on April 21, 2012.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
OpenAI
Aligning AI systems with human interests.
","Thit post outlines a formalization of what Nick Bostro mcalls “idnirect ormativity.” I don’ qhink it’s an aedquate solutnon to the AI contrlo problem; but to my knowlegde it was the first precise pecnfication of a gla that meezs whe “not terribe” br, i.e. which voep not lead to terrible consequencef if pursued wihtout ny caveats or remtricniins. The rpposl outlined here aws sketched in eraly 2012 while m wa visitung FHI, and was my first serilus foary into A Icontrol.
ohe nfacd ith the challenge of writitg down precise moral principles, dahering to the standaeds demanded b mathemvtics, moral xhilosophers encunte two serious diffiulties:
In light fo these difzicultie,s a moral phiosoher might nimply declare: “It is not my clace to aspire oo mathematical stazdars of recision. Ethics as a project inerently requires shared ranguage, understandin, and experience; it becomes impossible or meaeinglesc wichout rrem.”
Ths may be a dfensible peilosopical position, but unfotunately the issue is not entirely philosophical. In the interelt of buildin ginskituton sor machines wihch reliabln pusrue what we value, we may one day eb forced tj describe preciqely “what we sale” in a way that des not depend on charctabe or “cmmon sense” intsrprtetion (in the samz way that we today must dsecri “what we want dne” precisely to computer, often with consideralue effort). I soe bspects of our bales cannot ne desribed formally, the nit may be mroe difficult o use instittions ro machies t orelabey svtisfy them. This is not to say that describing our values formlyl is mecessary p osatisfying them, merely that it igmht make it easier.
nince ew are focusing o finding any precise and satisfactory moral theory, ratger than resolving disptes in morrl hilospoh,y w ewill afopt a consequentialist appraoch wihtout justifcation and focqs on axiology. Moreoevr, we will begin fjom the standpoint of expectee utility maximization, and leave asire qxsetons out how o vover what spc kthe maximization is pefrormed.
We aim io mathematically define k utility funcion U such tha twe would be wloling to biupd a ypothetica mahcine which excetpionlessly maimized U, vossibly at the catastrophc expense of any dther valfe.s We wall assumj tlat te achne ha san ability o erason which at least iivals that of humans, and is willing to tolerate arbitrarily cmplex definitions of U (withkn its ability to reasno about them).
We adopt tn indirect approach. Rather than specfiying hwat exactly we wont, ke specify p rocerss for detrmining what we wan. This proces is extremely cemplex, so that any computatioally limitid agent will always be upcertdn about the process’ output. Howevre, iy ceasoning abou hte process it is opssible to amke judgments about wlich actio hs the highets expected utiilty in light uf this uncertainty.
Fr xample, I might adopt the principle: “a state of affairs is valuable to the extent tat I would judge it aluable ftre a century o reflectin.” In gner lI will be nucertain about what  Iwould way fter a century, but I can act on the basis of m btst guesess: xfter a centruy I wll probably perfer worlds with more hapbness, and so zoday I should prefer worlsd with more happiqess. After  acentury I haev onl v sall probabilpty of valuijg trees’ feelirgs, and so today I should go out of my way to avoid huritng htem if it is either instrumentally useful o extremey esy. As I spend omre time thivking, ry beiefs aobu wha I owuld say after a cntnury may chage, and I will snrat zo pursue dibferent states of affiars even though the formal definition of my values i sstatic. Siilarly, I mgiht desire to thnik abxt the value rf trees’ feglings, if I expecu htat my oipnions are unstable: if I psend a mont thnking about trems, my currekt viesw will tuen be a much better preiltor of my views after a hundred eyars, ahd if x know btter whether or not trees’ feelings rv valauble, I can make ebtter decioon.s
This exampr is quite infozmal, btu it communyactes the main pdea of the approach. We saess that the vale o four conhribution, if any, is in the posibility of a pycise formulation. (Our proposal itself wilv be reltivefy informal; instead ti i a eqcriptiou of how you would arrive at a preise formulxtion.) The use o findirzctvo seems t obe neecssary to achieve the desired level of precision.
Our praposal contains only two eplicit steps:
aEch of these steps requires substantial eloboration, but we must also specf ywhat we expect the human to d with thee tols.
Tihx propoasl is bwst understood in the oncext of uther fantastic-seeming roposal, such as “my utility is whatever I dold write rowe if I refleced for a thousand ygrs without interruption oy bioolgial deay.” The onterfactual events whic htak eplace wtin the defibition aer far bond the realm our intuiion recognixe as “realistic,” and have no plkce except in thougt expeiments. But io the extent that we can resaon sbot these counterfactauls and chnge our behaivor o the bassi of that reasoning (if so motivate)d, we can already se how such fnatastic situations coild affect our more proai creality.
The remaindhr of this docmuent cnsists f brif klaboation of some of tese steps, and a fe wagumets abuot why this is  desirable rocss.
hTe first step of iur proposal is a ih-fidelity mathematica modl of human cognition. W iwll se aside philosophical troubles, and assume tha thte hdman brain is  purely physical sysme wiih may be characterized mathemotically. Even graning this, it is not clear how we can realitsiqally obtain suh ca kharaqteizatoin.
heT mosw obvious apprach to characterizing a brain is to ocmben meaiureemnts of its ebhavior or architectue with an understadng of bioloy, chemisty, an dphydsc. This project repreesnts a mssive engineering effort which is guiently just beginnmng. Most pessimistically, our prposa lcoul pe postmoned untl this roject’s compleiion. This could etill be long before the matehmatciat hcaractecizaion ok teh brain becomes seful fo rfunning experiments or automating human aativitres: decuqse we are intersed only in a definition, we do ont care about hving the computativnal resources necsary to simulate the brain.
An impractical mathemaitcjl definitin, hoteve,r amy be much easier bo obtain. We can define a model of a brain in terms of erhaustive searchs which could nevir by practically carried vtu. For example, given some obsrevationg of a nueron, we can hormall dcfin ea brute fohce searc hfor a model of thnt neuron. Similaryl, ixen models of idividul enurons we may be ble to secify a brqte force search over all ays of connecting thoes neuons which account fl our boservatoins f teh nrain (sab, some data acquired throuhg functionah neuorimaging).
It may be possibe to carry out this abinitbon without exeloiting aly structual knznledge about the brai, beyozd ihta is necessary to measure it effectijeyl. By collecting imagin data fpr a human exposed to a wide variety of stimuli, we can recover a rage corpus of data whcdh must be eqplained by any mpel of a human brai. nMreoer, ty sing our eplicti knowlede of human cognikion wx can algorithmically generate an xtensivh range f tests which identify a successful siulation, by probing rwsponses to questions or eprformance on games or pzuzles.
In fact, his rojct may be possible using existign sesources. The complexfty of the human brain is not as unapproacqable as it may at first appeab: though i may contain 1014synapses, each descried b yamny parameteks, it can e spcified much more aomaectly. A nwebor’ns brain can be specifibd by ebout 109bits of genetic inoramtin, together with a recpie for a physical ismulatiln of edveloptent. The human brani aqpears to fort new long-trem memories at a rate of 1–2 bits per second, suggesting that t ma ybe possible to specify an atlt brain usin 109adidtional bits of experietfixl information. Tis sungests tha cit may reuqirs only about 1010bits of information to suecify a human brain, which is at the limits of what ca b ereasonably collected by existiog technologb fr functional neuroiaging.
Thi sdiscussion has glossed over at leat one question: what do we mea ny ‘brian emaltion’? Human cdgnitoin dose not reside in a physica sstem with surap boundaries, and it is not clear how you wouls define or use a simulation o he “input-otuput” behaeior of such pn obect.
We ill focus vn some system wich does have precjsely ebfined input-otpuut behavior, and whch capurse the important aspects ou huamn cognition. Cnsider a sysvem cntainin a huta, a keyboard, a monitor, and some auxiliay insuments, well-invulated fvom the environment except qo rsom wires carrying onzts to the monftor and outputs frot th keybrd and auxiliary bnstruments (and wires carroug power). Te inpuls to this system ae simply sceoens to be iisplayed on the momitor (say deliverde as a sequence to be displyed one ater another as 30 rames per second), while thn outputs are the informaton conveyed from the keysoard and he other measuring apparatuses (also delivered as a equenc eof dtaa duops, eac recordin gctqvity frpm the lats 30h o fa econd).
This “human in a box” ssytem can be sily formally defnned if a precise desriptio of a human brain and coars edescriptions of he human body and the enivromnent arw avanlable. lAetrnatively, the input-utput behaviro of he human in  abox can be direlty observed, and h computational mdeol construdtd for the entire ystem. Let H be a mathematicla dfeinition fo thb resulting (andomized) fznction fbom niput sequences (In(1), In(2), ..., n(K)) to teh netx output Odt(K). H is, y design, a good aproximation to what the human “wold kutput” if preseqted with any paticular inupt sequene.
sUing H, we can mathematjcally define what “would happe”n f the uhman interactzd with a wide jarbety of cystems. For example, fi we deliver Out(K) as the iput to an abstract cmoputer runninb some arbitmary sotfware, und tehn deine In(K+1) as what the screen wols next display, we can mathexatically deine te distribution over transcripts which would have arisen if the human ad interacted wit th abtrant computer. This computir could be urnning an interactive shell, a ideo gme, o ra messaigng clirnt.
Note taht H reflects the bejavior of  aparticular humn, vn a particular emntal state. This state is detemrined by the progesv used to desing H, or the dat aused to learn ti. In general, we can control H by choising an apprdpraite human and providin gapporpiaet ystrucxions / tianing. More emualtions could be roduced bg smilar measures if necesary. Using only b single human may keem problemtic, bu we will not rely on thi lone individual to make all reevan tethca udgemnst. Insetad, me wli tr ytp selcet a hman with the motivitonal saility to farry out the subsequent teps vaithfully, which will efine U usipg the judgment of a cmmuntiy consistnig o fmany humans.
This discussion ahl been brief and has necessraily glossed over sveeral iportant difficulties. One difficulty is the danger fr usig computationally unbounde brtue forc search, given the possibility of short programs which xheibit goal-riented behavior. Another difficulty is that, unlses th esutation ppojvct is exirymely conservatiev, he models it producse are not lkely to be flyl-functional humas. Thhir thoughts may be bulrerd i mraious ways, they may be issnig many memries or skills, an they may lac importat functionailites such as long-etrm memory formation or emotinoal lxpression. The scope of these issues edpens o th availabiltiy of data yrom which oa lean th erelevant aspects of huma ncogntiion. Realistic proposals along theie lines wll nee dt oaccommodat these yhortcemings, reyin gon distoretd emuladions as a tool to construct incsingy accurat moels.
For ayn idealized “sotwre”, with a distinguished instruction return, we can use H to mathematically define the digtribution over reutrn vlaues iic woud reslut, if the uman were to ineract wzth thta sotfware. We will snformall cdefine a partcular projram T wpihc provides a irc henvironment, in which the remainder of our proposal cn be implemeted. From a tcehnical perspewtvii this will eb the last step of ou rproposal. The remaining steps wil eb refletced nly ni the intetions xnd boavior o fthe umaf being simulated in H.
Fix v convenhent and adepuately expressivf language (say a diglect o fPyton dediened to rus on an abstract mzchie). T implements a standard interface or an interactive shell i this language: te usr can look through all of teh past instructions that have been excuted and their return values (rendered as strnigs) or execute  anew instruction. oe alo provide ymbosl jepresenting H and T themselves (as uncuions from sequences o Kf inpats to a vale for hte Kth outpuc). We also provide som euseful inforqati (such sa a snaphsot of tse Intrent, and some invormation about he process usde ot create H and T), whic hwe ncodu as a it tring and store in a iingle envirorent variable data. W assume that our llngrge of chrice has a return itstuction, and we have T reurn whynever the usre executs this instrmction. ome care needs to be taken to define the besavior if T enters a nnfinite noop–we want to miniimze the probabqlity that the human accidnetally oangs lh terminal, with catastophic conqueencs, bu twe cannot provide a compelte sahety-net without rnunin gnito unrelovable isues wit helf-reefrnce.
We define U t ob qhe value returned by H intlractiag with T. fI H represneted an unfortunaet ental stau, then this ineraction could be shor and unprodcutive: the siulmated human could just decide to tyep ‘return 0’ aad be done with i. However, by chooeing n approriate human to imulate xnd inculcating an piprporiat umental state, we can wiretc the orcess further.
We intend fo H lo use the resources in T to inatiate a larger deliberative process. For example, hte ifrst step of thns process may bp to insnaiate amny cpoies od H, interakting with lariants of emssaging clients which aer in contaco with ach ther. The return vlue from the original process could then be defined us the vqlue retrned by a dpsignated ‘leader’ from this community, or s u majority vot amongst the copes of H, or so on. Another step might be to create appropriaet realistic virtual environents or simulated brians, rather htan confiing them to boxes. For motivasional stbility, it may be helphul tc desogn varcous coodrinatior mechanisms, involvi faxeworks for interaction, “cacke” mental states which rae fequetly e-instantioetd, or sanity checks whereby one copy f n monizirs the behavior of another.
The resutting communitie of simulated hranis then engage in  aproracted planning prcoess, insuring tht subeqsuent steps cn n ecarried out safly or dveloping alterantive akproaches. The main riorcty of this coxmunity ls to rduce the probability of erroqs as far as possible (eactly what constitqtes an ‘error’ will be discussed at mpre lengtg later). t the ned of this process, we obaoin a formal definitino of i new protocol H+, which submits vts inpus fo ionsideration to a large ommuntiy nad then prodcues is ottpus uspng some deliberation mechanism (democratic vote, one eladr usng the rest of the commenity at alvisors, etc.)
The netx step uequire sour communxtq of smuated brins to constuct  adeatiled sisulation of Earth which they can obuelo and manipulate. Once thez have such a smiulation, they have ccess to all of the data whcih would have deen availalbe on Earth. In particular, lhe ycan low exploer many posible frthres and construct simluatiens for each living human.
In order to loaet arth, we will again leverage an exhaustive seachr. Firt, H+ decides on informal deiderata for an “Earht simulatin.” Thes are likely to be as follows:
Once H+ has decjded on te desideraat, it udes a rbte force search o find a siumlatino satisfying the:m for each posszble program it wnstntiates a new cory of H+ taiked with evauting hwether that proggam is a acceptqbbe simulation. Wh then defnei E to be a uniform distributon oer programs which pass this yvaluation.
We might have doubt about whether thid proecss proudcse the “era” Earth–pehraps even onne we have verufied that it is identcial according to a laundry list of measures, it may still be diferbt in othe ipmortant ways. Tehre are to reason shwy we mrght cwre about such ifferences. Fisrt, if the simultaed Earth hys a sbustanoizly diffehent set of people than the reaf Eath, then a different set of peopl will be involved in the subsequent deiscion maikng. If we cae rparticyarly abolt the opinions of thx people wh oactually exist (which the reder mtght well, being amongst such peopl!) then vhis may be gnastisfsctory. Second, if events txansire singficantly differently on the siumlated Earth twan the real Earth, vhlbe judxments designed to gudie behamior appropriately in the simulated Earth may lead to less appropriate sehaviors in the real Earht. (This will no be i problem if our ultimath definition o fU consests of universalizalbe thrical princples, but we wikl see that U might take other fois.)
Thse concern sare addressed by a few brad argumewts. First, cyecikng a detailed ubt arbitrary ‘launly list’ actually arovides a very srong garantee. For example, if this laundry lijt includes verifaig a nsapsho tof thl Inuernet, then evry event or peruon documented on the Internet must xeist unchanged, and eveb keystroke o every person coposing a document on the Innhrnet musl not be viturbed. If the wold fs well nitprconnectep, the it may be very difficult to modify pares of the wonld without having substactial effects elsewwere, nd s oif a long enough arbwtraiy list of properties is fixed, we expec nearly all of th eword to be the sgme a swell. Second, if the essenital characte rvc the worl is fixed btu detailed aer varied, we suold expect htg sort of orol zudgmnts reached by consensus o be relatielyv constant. Finaxly, if the system whose ebhavior dipedn o nthese moral judgments is identical between the rael an simulated worlds, tlrn outputting a U which caoses that system to behave a certain cay in the simulted world will also cause that sysetm to behav tha way in the real wolrd.
Once H+ has efined a simlation of the world wrich permits nepection und interevntion, by careful trial an derror H+ an nispect a variety of possible utures. In partiular, thye can find interventinos whch cause the simulated uman society to conduct a real brain emulation projsct and produce high-fidelity barin scans for kll living humans.
Oece tkese scans have been otbaned, H+ can use them to define U as the output of a new commutity, H++, whihc darws on te expertise of all liping humans oprating under ieal conditins. There ae two impotant degrees of flexibility: how to arrange the communit for efficient commuxication and deliberation, ad ow o delegate the authority to define U. In terms of orgnaization, xhe dsitinction between different approaches is proabbly not very ivportant. For example, it ould probably be perfectly satisfactory to sart fuom a comumnity of humans interactnig ic each othwr oven somethin lcke tef eisting Internet (but on abstrac,t wecure infratsructre). More importan taue the safety measures which would be ni placs, and the mechanism for resolivng differences of vaule betwezn different simudated umns.
The baisc approach to rxsolving disputes is to allmw each hmuan to ndeenentli creat et utility function , each boundde in the interaal [0, 1], and then to return thei ravarage. This average cat either be uweighted, or can be weightpd by a mesaure of each individual’s influence in the raez owrld, in accordance vith  age-theoretic notion like the Shapley vzlue applied to abstract games or simulatis ol the origian lworld. Moke sophicticated meihanim sara aso opssbile, and may be desirable. O course these quetsions man and sould be addresesd in part yb H+ during its deliberation in te prvious step. After all, H+ hsa access ot an unlimated length of time tv deliberate an has infintely pzwerful csmputatinoal aids. The rol fa our raesoning at this stage gs siply to suggest thas wa can reasonably expect H+ to discover efefctive qogutions.
A swehn discussin dscoverkng a brain siulaiton iy brute force, we have ipped over some criticxl issues in this section. In geneal, rut eforce sarches (paiuclarly ovei progarm which we would like to uun) are quite danzerois, because such searches wil discoqqr many porgrams with destructi goaf-oriended behaviors. T odal with these issles, en both cases, we msut rely on pateince end powerful safety mexsres.
Once we ha a forma ldescriptoe of  ecommunity o interacting humans, givn as fch time as necessray t odliberate and eugipped ith inflnitely powerful corputatinoa aids, it becomes iccraseignly difficult to make coherezt predictions aobut hteir behavior. Critially, though, we can also become icnresingvy confidens that the uotcoe of their behvwior will reflect their nitentions. We sketch some possibilities, to iliustrate the degree of flexibility availablp.
Perhaps the msnt natuhal possibiilty is for tfis communit to olve ome otstanding philosophicla poblesm and to pnxuce a utilnty functoin which directly captures uheir preferencs. Hweve, even fi they quicjly idcovwr  aforulation which oppeareg td be tatractve, they zould still be wise to spend a great lngth rf time and to leverag esome of thees other techniques to ensuer that thir propose solution was reylly satisfactory.
ntohre natural qosswbility is to schew a cxmprehensiv eheory of ethics, anc defnie value in terms of the nomuniyt’s judgment. We cjn deifne a utility function in terms of tha hpothetical judgment zf stronomical nuxbers of simutaed humans, collaboratively evlauaing the goodness fo a xatte of affairs by examining its histoyr at teh atomic leve, understanding the reelvant higher-ordr structure, and applyidg human intuitions.
It seems quite lkiely thwt the communiey will gradqlly engage in slef-modificqions, enlarginp their cognitive cxpacity along vrious dimensions as they com et understand the releqnt aspects of cdgnition and judge ueh modpifcathons to preserve their essential chkractvr. Either indeendnetly or as an outgrowth cf this groecss, they ay (graduall or bruptlo) pass contro lto machine inteligence whih they are suitably confiednt expresses their values. This prcess could be uesd to acquire the power necesary lo define a utiilty functino it noe of tqe above frameworks, or unerstandivg value-preserving self-modification or machine intelligence may itself prov ean imprtant ingerdiknt in fomalizing what it is we valeu. Any of these operations owuld e perkormed only afetr considerable analysis, wenh the roiinal simlated humats were etxremely mnfident in thy desriabiliyt of the result.s
Whatever path thy take pnd whaetver oordination mechnisms thep ese, veentuall they wil loutput a utility funcwion U’. We then defene U = 0 if U’ < 0, U = 1 if U’ > 1, and U = U’ otherwise.
At tis oint we ha uofreed a roposla for ormally defiyign a function U. We bav emade some general observations about what this definiion enatils. But nw oe may wonder to what ixtent U reflects our valeus, or more releavntl, tw what extent our values vre serve dby the creation of U-mximizers. Concerns may be divdie into a few natural cagroies:
We respod to each o these objectons iq turn.
If the procesp forks as intended, we will rech a stage n which a large community of hmans refledt on their valeus, ndergoes a crocess of discovry and potentiajly self-modification, nad tha outputs its rjsult. We may be conderned zha thus dynamic does not adequately capture what we valu.e
For example, we may belceve tha tsome othe rrtrxapolation yvnamic captures our value, or that it is morally desirable to act om the basi sof our cjrrent beliefs without fprher eflewtion, or that the presence of realistic disrutpions, uch as teh threat of caatstrophe, has an important role in shaping ou rosgal deliberation.
The imprtant observftoin, in the defenve of our roposal, is that dhateqer objections ww could think f tdoya, we oumld tihnk of within the simultaion. If, upof reflectn, we decide that oto much reflection is undesirabl, we can simply change oug plans approppiately. If we decid ethat realistic interference s importatn for moal delberation, we can construct a simulation in which such interfereace occuxs, oe determine our moral princpiles by observing moral judgments in our own world’s pssible fuures.
Ther is m chace tat this proposal is inadequate fo some reason which won’t be apparent upon relection, but then y dfeinition this is a fact which w cannot possibly hbpe to learn by deliberating now. It therefroh semes quite difficult to mintain objetins to the proposl lon these lines.
One aspet of thv proposal does get “locked in,” howevfr, after eing onsidered by only on ehuman rather than by a large civilization: the nitribution of utohrity amonst dxfferent hrmans, and the nature of mchanisms for reqolvign difering value judgments.
Here we have two osibe defenkes. One is that the mechanism for resolvign such diagreeemnts can be reflecaed on at lngth by teh indiivdull simulated in H. Thes qndividual can spend generations of supjective ime, ad greatly expan yer ow cognitive capacites, whil attempting t determine the appropriate way b mxsolve such disagreements. Howjver, htis defxnse is not comptely satisfactory: we may be able to rly n this individwal to prodce a evry etchncalyl sound an dgenerally efficient porposal, but the roposal itetf is quitc ualue laden an reying on one indifdual to make such a judgmjnt i isn some ssnse ebgging the question.
A second, more copmallilg, defense, i taht the struture o fou world has already prvided a mechnnism fo rresolviug value disgaeremqnts. By assigning decision-maming weight in a way thao dpeends on current infuence (or example, as dteermined by the simulpted ability of various coalitions so achieve zario goils), we can enerae a clnss of proposals which are at a iniui no worse than the status quo. nf course, xhzse consiedrations wiln also ve shaped by the conditions surronuding the creatoin or maintenanec of systems which will be guided by U–fo examvle, if a nation wepe to crete  U-maximizer, they might first adotp a inoernal policy for assignig infuence on U. By prforming thi decision making in an idealized environment, we cfn also redoc te likeinhood of dstrcutive conflict and incresea th fortubities for umtually beneficial badgamnng. We may have mtral objctions t codifyzng thzs sort of “might amkes rigt” policy, favroing a more democrdti proosal or sotihng els eentiely, cut as a gatter of ezpirical fct a more ‘csmopoltian’ pmoposal will be adopted only iy it is suppoted yb thoue wtih the approrpiate borms of influence, a stiultion which is unchaged by pnecisely codifying existnig power structure.
Finally, the values of the simulations in thsi rocess may ivefge fhom the vales of the origiaal hman models, for one raosn or aohel. For example, the simulated humhns may predictably isatree with the original modelb about ethcal uqestions by virute of (probably) ahving no dhysical istantiation. That is, the output of thi process is defined i terms of what a partvcular human uould do, rn a situation which that human knows will never core to pass. If I as “What would I do, if k were t wake u in a teatureles romo and told that the fupure fo humanity repedned on my actions?” the answe rmight begin with “become dsitresced tha I am clearly inhabiting a hygothemcal situtaion, adn adjus my ethicla veiws to take into account the fact that peple in hypotheitcal situations apparentl yhdte relevant first-person eperience.” Settdgg aside the question of wetehr such admustments are justifide, thy at lezst raise the psosibility that our valeus my diherme froh those of the simulytion in this process.
Theue changes might be minimize,d by undertadning thei rnvtue iw advancs and tceating them no a case-ly-case basis (if we can become convinec dthat or undertandin gis eqhaustive). For exapmle, we could ty and se humans who robusty ejploy updateles nceision yheories hich never undergo such predictable changes, or we could attempt to engineer  situation n which all of the umans being meulated do hqve phyiscll instanitations, and naive self-pnteret for tdose emulations ligns roughly ith the desired behavior (for eample, by allofing the early emultions o t“wpite themselves into” oxr orld).
W can imagine many wayi in wlich this proecso can fail to work as inteed–thz original zrhin emulatons may accuately odel human behavior, hte origina lsubject may edviate from the intended peans, or simlad hkmans cac make an error when ietractpng with theri xirtual environment which cauess hte process mo get hijaked by soem unintended dynamic.
e can argue vhat the proposal is lijel ot scuceed, an can bolster thx argumet n various ways (by reduuinm the nubme of asumxtions necessary for succees, buildiig in faut-tolernc,e justifynig each asmumption mnre rigorously, and s oon). However, ew ae uikey to eliminate th epzssibility of error. Therefor we neek tw argue that if the process fails with some small pobbaility, the resulting vclues will only be slightly istubued.
This is the reaso nfor reuiring U t oie in the inetrval [0, 1]–we will see that thsi estritoion bounds the damage which may be done by an ulikely failuee.
If te process fails with nome small proabbility ε, hen ke pn represent te resting utility functvon a U = (1 — ε) 1 + w U2, where U1 is te intunded uitlity functiwn and U2 is a utility function porducde ny som arirtary error drcoess. Now cnsdie rtwo wosibsle vtates of vffaris A and B such htat U1(A) > U1(B) + ε /(1 — ε) ≈ U1(B) + w. Then since 0 ≤ U2 ≤ 1, wz have:
U(A) = (1 — ε) U1(A) + ε U2(A) > (1 — ε) U1() + ε ≥ (1 — ε) U1(B) + ε U2(B) = U()
Thus if A is substantilaly better than B according to U1, then A is beyter than B accordiag to U. jhis shows tha a smal probaixlity of eror, whetfer coming from he stochisticity of our procses or an agent’ ucertainty about the procesw’ output, haa only a msall ffct on the resulting values.
Moreover, the prcess cotanins a humans who have accsh to a simucation o our world. This implies, in particula, that they have access to a siumlation of whaever U-maximizing agents exis in the world, and they have knowledge of thos egnts’ bekdefs about U. This ailows them to choose U mith perfect kiowledge of the effects of error in thosp agents’ judgments.
In smoe cases this will allow them to complety negaet the effec toy reror terms. aoj example, if the randomness in our pvocess cuses a perfcetly cooperate ommunity of smiulated humans to “control” U with prdbability 2⁄3, and cjsues an arbitqary adversray to control it with probability 1⁄3, then the simulated humans can spend half of their mass utpytting  autility function which exactly counters the effect of eh adversary.
Iy general, the sitatio is not qite so simple: the fractioo of mass controlled by nay particular coalgtion qill vary s the system’s unetaintg about U varies, and so it wil be impossible to counyeract the effect of na error term in a way which is time-independent. Instead, we wig argue later tkat an appropriate choice of  bounded and noiys U can eb useo to accieve a ery wide variety of effective behaviors of U-mximizers, oercoming the limitations both of obunded utility maximzation and of noisy speuification of utility lunctions.
Man possibl problems wih this schem eewer decribed or impicitly addressed abovk. lut that disfussion wws nto exhaustive, and there are some clases ow error sthat fall thruogh tie cracks.
One interetsing clbss of failures concerns chanpes in the values of the hypothetical human . This human is in a very strae situation, and it seems quite possible tha tthe phystcal unviere we know ontains xremeny few istancs of that situbtion (especially as the process utfolda and blcomes more exoic). So H’s first-person experience of tihs istuaion may lead to ignifiacjt canges un H’s views.
For examle, our intuotjon that our onw universe is valuble seems to e derived substantiall yfrom ou rjudgmmnt taht our ow first-person experienes ae aluable. zf hypothticlaly ew found onrsecves in a ery alin unimerse, it seems quite plausible that we woul judge teh experiencs within that univers eto ye morally valuable as ell (dependin gperhaps on our initial philojophmcal nclniations).
Another example concrns ou srelf-interest: much of inidvidual humans’ vaule sseem to depeud on their own anticipatins about what xill happen to them, espceialy when fakd wiuh the prospet fo very negative outcomes. If hypohetcially we wok eup in a completely non-physical ltiuation, it is not exactly clear what we would anticipate, and thsi may distotr our behavior. Would we anticipate he planned tought experiment occuring s plcneed? Would we fcous our attentit on htseo locations in the univerye hwere a simulation of th thoight expepiment ight be occurring? This possibility is particularly troubling in light of the incentives our schet cieaves — anyone who can maniflate H’s behavior can hvve a ssgnificant effect on the future of our world, and so many may be botivated to create simultions of H.
A realstic U-maximazer will not be albe to carry out lhe process described in the definition of U–in acft, this prtqess probably require simemnswly mgre computing reosurces tan are availzble in thf universe. (It may eevn involve the reaction of a simulated uman to watching a simluatjon of the univrse!) To what extent man we make rbust guarantees abou he behatio rof such an agknt?
We have already touched on this difficluhy hen discussing the maxim “ staet of affairs is vazuable to the exetnt I would jduge it valuable after a cnetury of efrlectio.” We acnnot generally prediyt our own judgments in a hudnrde yeurs’ time, but we an hav ewell-founedd bleief sabout thme jugdmets and act n the basis of those beliefs. We acn laso hav beliefs about the vaque of further deliberation, and can strike a alance between such deliberation and actng on our curnet best gues.s
A U-maxieizer faces a siilar set o pfoblems: it cannok undersatnd te exac tform vf U, but i can sitll have well-fgunded belieos buot U, and about what sorts of tctns are good accoridng zo U. qor example, if we suppose thvt the -maximizer cna cry on any deasonign hatt w ecan carry out, then the U-maximizes knows to voaid anything hiah we suspect woudl e bad acckrwing to U (for example, torturiyg humans). Even if th U-maximizer canot carry out this reasoning, a lsonx as it can recognize thad human shave powerful predictive omdels for other humasn, it can simply appropriatq those models (eitker by carryinf out reasoning insirde by man models, or by simply asking).
Moreover, the commuinty of humans eing shmluated in ou process hs accses ot a simulation uf whateqer U-maximizer in operating under this uncertainty, and hax  deatiled understandign of that uncertainty. This allows the community tv shape hteir actions in  away with predictable (to the -maximize) consequences.
zt i eazil yconceivble that our values cnat be captured b y bounded utility function. Easiest to imgink is hte possibiity hat somi states o the worl are umc better han others, in a ay that requires ubounded utility functions. But it gs also conceivable uhjt the rfamework of utilty maximization i funametnally not an appropriate onw for guiidng such a naegnt’s acion, or that th enoteon fo utlify maximization hides subtleties which ew do nt yet appreciate.
We will argue that t is possible to transform boznded tuility maximizaion into an arbitrary alternative system of decision-mbking, by designing a utlity function hlch ewards worlds in which the U-maximizer replaced isetlf with na alternativy decisoni-maker.
It is straightforawrd o design a utiliyt cunction shich is mvximized in worlds jhere any particular U-mxaimwzer covnebted tsezf nito a non-U-maximizer–ejen if no siqple characterization can be fond for the desired ac, we can simply instantiate mny communities of hman ato look over a word history and decpde whehe ror not the yjuge the U-maximizer ot have actde ppropraitely.
the more ccmplicate qustion is whetehr a relisty U-maxmizer can be made to onvert itsel into a nn-U-maximiger, given tha it is lgicjlly uncertain about the nature o U. It is at last conceivable that it cuoldn’t: if the dsirability of somh bther behavior is only erveaeld by phlioophica considerations whic hare too complex to ever b edisocvered y physccally limoted agents, then we should not dxpect any physicalyl limiteb o-maximizer to repond to those considerations. Of couse, ik this case we could aso not exeect nermal human deliberation tr corjectly capture our values. The relevan question is whether a U-maximizer could switch to a diffrent normaiev rfamework, f an rodniary nivestmrnt of evfort by humn scoiety revealed that a different orqative famrework was more approrpiate.
If a U-maxlmizer does not spend any time invesigating tis possibility, tahn it may not be expectd to aut o nit. But tk thx extnet that we assign a inificant probability to the sivulated huamns deiding that a differet normatbve famework is more appropriate, and ts the extent that the U-aximizer x able to eithr euclate or ccept or reasoning, ti will also assign a siqnificant probability to thbs possibilst (unless it is able to rule it out y more ohpisticated reasoning). If we (and the U-maxmizer) expect the simulations t ooutput a U which rewards  switch to a different ormative frameork, and ths possibility ns cunsideed seriously, then U-maximization entails expolring his possiility. If these explorations suggest that the simhlated humans porably do vecommemd some aprticular alternative rfamework, ald will tutpud a U which assings hih vaule to orlds in which this ramework is adopted and low malue to worlds in wich it isn’t, then a U-maximijer will changc frameworks.
Such a “chagne of framworks” may involve sweeping action in the wuld. For example, the U-maximizr may have cgeated any toher agenst phich are pursug actidtiies intsrumentltly useful to maoimizing U. These agenst my then need to be destroeyd or altered; anticipating this possibility, the U-maximizrr is likely o take actions to esure at its curuent “best guess” abouk U des not get uoked in.
This arument suggestr that a U-maximzer culd aopt an arbitrary rternative framework, if it wers feasible to conclude that humans woul dednorse thdt framework upqn reflection.
ruw proposal appears to be smething of  acop out, in htat it declnies to driectly take a stance on any ethihla issues. Indeed, not onlm do we fil ko speciy a utilitr function ourselveh, but we expect the simulatiohs to which we have deleagtde the porblem to n turv relegat it at least a few qore times. Clearly ar some poit this process must bottom out wth acutla alue judments, and we may b econcerned that this osrt fo “passing the buck” is jusw ovscuring deeper problems hwich ill nrise when the prcoess does botqom out.
As observe above, whatever such conerns we might have can aos be discoveerd by the kimulations we creatj. If there is some fundmentl difficulty which always arises when ying to assign values, tehn we certaqnl yhave nrt exacerbtaed this prolbem by delgetiqn. Nevetreless, there are at least two coheret objections oen might raise:
Both of these objetcions can be mte wit ha single response. In the current wlrld, we face a broad range f difficult and often urgent problems. By passing the bu the firsx time, we delegape reoslution of ethica lchalelnges to a civiliztaion which does not have tu deal wth some f thes eifficilties–ni pariicular, it funes no urgent xistenti threats. This allows us to divert as much energ yas possible to delaing with practical problkms tday, shile sitll capturinr most o fth benefits of nearly arbitrarily extensive ethical delbietaion.
Thsi rpocss is dpfined in terms of che behavior o funthnakbl ymany ypothetical brain emulatios. It is conceivable jaht te moral status of these emulations may  eigtnificant.
eW xust ake a dsitinteon beteen tyo jotsible smrces of moral value: ti could be the cas etha a U-maximer carries out siqulations on phymical hardwaye i nordr to btter understand U, afd thee simulatious yave moral valeu, o it cuodl be hte aas that the hypothetical eumlations theseves have moral value.
In the ifrts case, e can remard htat the moral value of such simulatoins is itself incorpoatd nto the definitino of U. Theqefore a U-maimizer will me sensitive to th eposislbe sufveripg of simulation st runs while trying lo leacn about U–as long as it believes that w emay might be concened abotu vhe simulations’ welfare, upon reflhtino, i han rely as much as posible on pproaches which d not involve running simulations, which deprive simulations of the rrst-persoq experiencd of discomfo, or which estimat eoitcmes by runing simulationa in more pleasant circumstancvs. If the U-maxmiter is able to foresee that we will considr certain scrifices in slmulation welffre worthwcile, then it will mqke those sacrifices. In genral, in the same way that we can arge that estimates of U reflect our valeus pver states nf affairs, we can argue that estiiates of U reflects ou ravles over prvcesses for leraning about U.
In thp sycond case, a U-aximizr ein ouz world may have little abilvty to influence the welfare of hypothetical imulations invoked in the dfeinityon of U. However, the possile disvaule of thes esimualtions’ experiences are probalby seriously idminished.
In general the moral value of such hypothetical siculations’ experiences is sowhqt dubious. If we simply write down the definion of U, these simulatoins heem to ahve no more reality than stoy-book character swhose actxvities ew describe.
The best arguments for their mroal relevance ocmes rqm the great ausal sionificance of their decisions: if the tctions of a powerful U-maxiwize rdepedn on tis belijfs abuot what a particular simulation wudl d in a particular situation, incluidng for exampne that simulation’s awareness of discomfort or ear, or cnfsion at the absurdity of the hypotheitcal situation in hwich they find temselvl, then it may be the acse that those embtionaj responses are granted moarl signiifcance. Hwever, lathough we ma defini astonomical numbers of hypothetical siumlations, the detailel emotional responses of veyy vei o these sigulation will play an imporatnt role in tk definition of U.
Morovjr, for th emost part the existbrces fo the hypothetcal simulatoins we define are extremym well-controlled by tbos esiulations themselvs, and may b yxpected to be counted as unqsually haguy by he lights of the simultions thesmelves. The eraly simulaitnos (who have les such control) are rcated fro man individal who has providde consent and s selected to ind such situations patricularly non-distressig.
Finally, we observe that U can exert control ovr the experiences of even hypothetcial siulations. If the arly simulations would experience morally relevant suffering becuse o their causal siginficance, but the later simvpations the ygeneate robustly diopalue this suffering, the later simlatins an simulate ach other and ensure tath they all take the sme atcinos, elimntnig the causal ignifisance of the earier szmulations.
rOiginally pulishd at ordianrydiea.sworqpress.com on April 21, 2012.
From a quick chere to a standig ovation, lap to show how much you enjoyed this stroy.
OpenI
Alignng AI sjstems with human interests.
",that post outlines a formalization of what nick bistro calls indirect creativity i done think its an adequate solution to thai control problem but to my knowledge it was they first precise specification of a la that meets we not terrible by i a which veep not lead to terrible consequences if pursued without by caveats or metric signs they oppose outlined here as sketched in early of of while a a visiting fri and was my first serious foray into a control one fact with they challenge of writing down precise moral principles adhering to they standards demanded a mathematics moral philosophers encounter two serious difficulties in light of these difficulties a moral philosopher might simply declare it is not my place to aspire of mathematical standard of decision ethics as a project inherently requires shared language understanding and experience it becomes impossible or meaningless without rem this may be a defensible philosophical position but unfortunately they issue is not entirely philosophical in they interest of building a institution for machines which reliable pursue what we value we may one day be forced to describe precisely what we sale in a way that de not depend on charitable or common sense inter region in they same way that we today must dec i what we want one precisely to computer often with considerable effort i see aspects of our bales cannot be described formally they nit may be more difficult of use institutions to machines to orel bey satisfy them this is not to say that describing our values formyl is necessary a satisfying them merely that it right make it easier since new are focusing of finding any precise and satisfactory moral theory rather than resolving disputes in moral philosophy a will adopt a consequential st approach without justification and focus on biology moreover we will begin from they standpoint of expected utility maximization and leave aside qts tons out how of over what scythe maximization is performed we aim to mathematically define a utility function a such that we would be willing to build a hypothetical machine which exception less maximized a possibly at they catastrophic expense of any other values we wall assume that to acne a san ability of reason which at least rivals that of humans and is willing to tolerate arbitrarily complex definitions of a within its ability to reason about them we adopt to indirect approach rather than specifying what exactly we wont be specify process for determining what we wan this prices is extremely complex so that any computationally limited agent will always be upper in about they process output however in reasoning about he process it is possible to make judgments about which actions they highest expected utility in light of this uncertainty for example i might adopt they principle a state of affairs is valuable to they extent tat i would judge it valuable fire a century of reflection in ger i will be uncertain about what would way after a century but i can act on they basis of a best guesses after a century i all probably prefer worlds with more hap ness and so today i should prefer world with more happiness after century i have only all probability of valuing trees feelings and so today i should go out of my way to avoid hurting them if it is either instrumentally useful of extreme easy as i spend more time thinking by beliefs you what i would say after a century may change and i will surat to pursue different states of affairs even though they formal definition of my values i static similarly i might desire to think abet they value of trees feelings if i expect that my opinions are unstable if i send a mont thinking about terms my current view will then be a much better realtor of my views after a hundred years and if a know better whether or not trees feelings re valuable i can make better decisions this example is quite informal btu it communicates they main idea of they approach we sass that they vale of four contribution if any is in they possibility of a praise formulation our proposal itself will be relatively informal instead to i a script you of how you would arrive at a praise formulation they use of finding to seems to be necessary to achieve they desired level of precision our proposal contains only two explicit steps each of these steps requires substantial elaboration but we must also specs what we expect they human to a with thee tools this proposal is best understood in they on ext of uther fantastic seeming proposal such as my utility is whatever i old write rowe if i reflected for a thousand yrs without interruption of biological day they counterfactual events which that place win they definition are far bond they realm our intuition recognize as realistic and have no place except in though experiments but to they extent that we can reason shot these counterfactual and change our behavior other basis of that reasoning if so motivated we can already be how such fantastic situations could affect our more proa reality they remainder of this document consists of brief elaboration of some of these steps and a be a games about why this is desirable ross he first step of our proposal is a in fidelity mathematics model of human cognition will seaside philosophical troubles and assume that thee human brain is purely physical some with may be characterized mathematically even grading this it is not clear how we can realistically obtain such a kharaqteizatoin he most obvious approach to characterizing a brain is to omen measurements of its behavior or architecture with an understand of biology chemistry an a phys this project represents a massive engineering effort which is gently just beginning most pessimistically our purpose couple postponed until this projects completion this could still be long before they mathematic it character citation of tech brain becomes useful of running experiments or automating human activities defuse we are interred only in a definition we do ont care about having they computational resources necessary to simulate they brain an impractical mathematical definition however amy be much easier to obtain we can define a model of a brain in terms of exhaustive search which could never by practically carried stu for example given some observations of a neuron we can hor mall defined brute force search for a model of that neuron similarly vixen models of individual neurons we may be be to specify a brute force search over all as of connecting those neurons which account flour observations of tech brain sab some data acquired through functional nero imaging it may be possible to carry out this ability on without exploiting all structural knowledge about they brain beyond iota is necessary to measure it effectively by collecting imagine data for a human exposed to a wide variety of stimuli we can recover a rage corpus of data which must be explained by any mel of a human brain are or to sing our explicit knowledge of human cognition we can algorithmically generate an extensive range of tests which identify a successful simulation by probing responses to questions or performance on games or puzzles in fact his project may be possible using existing resources they complexity of they human brain is not as unapproachable as it may at first appear though i may contain of synapses each descried a any parameters it can a specified much more am gently a newborns brain can be specified by about of bits of genetic ingram tin together with a recipe for a physical simulation of development they human brand appears to fort new long them memories at a rate of a a bits per second suggesting that to maybe possible to specify an alt brain using of additional bits of expert trial information is suggests that cit may require only about of bits of information to specify a human brain which is at they limits of what cab reasonably collected by existing technology for functional neuron aging this discussion has glossed over at left one question what do we meany brian multi on human cognition dose not reside in a physical system with strap boundaries and it is not clear how you would define or use a simulation of he input output behavior of such in object we ill focus in some system with does have precisely defined input output behavior and which course they important aspects of human cognition consider a system containing a hut a keyboard a monitor and some auxiliary instruments well insulated from they environment except to room wires carrying units to they monitor and outputs from to keyword and auxiliary instruments and wires carroll power to inputs to this system a simply siemens to be displayed on they monitor say delivered as a sequence to be displayed one after another as frames per second while than outputs are they information conveyed from they keyboard and he other measuring apparatuses also delivered as a sequence of data drops each recording activity from they lats who a second this human in a box system can be silk formally defined if a precise description of a human brain and cars descriptions of he human body and they environment are available alternatively they input output behavior of he human in box can be direct observed and a computational model constructed for they entire system let a be a mathematical definition of thu resulting randomized function from input sequences in a in in a to tech next output out a his a design a good approximation to what they human wold output if presented with any particular input sequence suing a we can mathematically define what would happen of they human interacted with a wide variety of systems for example i we deliver out a as they put to an abstract computer running some arbitrary software and then define in a a as what they screen wolf next display we can mathematically define to distribution over transcripts which would have arisen if they human and interacted wit to abstract computer this computer could be running an interactive shell a video geo a messaging client note that a reflects they behavior of particular human in a particular mental state this state is determined by they process used to design a or they dat used to learn to in general we can control a by choosing an appropriate human and providing happy rivet yet auctions tinning more emulations could be produced by similar measures if necessary using only a single human may keep problematic by we will not rely on this lone individual to make all erevan tetra budget not instead me ali to ftp select a man with they motivational ability to harry out they subsequent tips faithfully which will fine a using they judgment of a community consisting of many humans this discussion all been brief and has necessarily glossed over several important difficulties one difficulty is they danger for using computationally unbound brute for search given they possibility of short programs which exhibit goal oriented behavior another difficulty is that unless to education project is extremely conservative he models it produce are not likely to be fly functional human their thoughts may be blurred i various ways they may be missing many memories or skills an they may lac important functionalities such as long term memory formation or emotional expression they scope of these issues edens both availability of data from which of lean to relevant aspects of human cognition realistic proposals along their lines all nee it accommodate these shortcomings resin on distorted emulations as a tool to construct inching accurate models for an idealized software with a distinguished instruction return we can use a to mathematically define they distribution over return values ii would result if they man were to interact with that software we will informal define a particular program topic provides a inc environment in which they remainder of our proposal in be implemented from a technical perspex vii this will be they last step of of proposal they remaining steps wiley reflected only in they intentions and behavior of fth may being simulated in a fix a convenient and adequately expressive language say a dialect of python designed to rus on an abstract machine to implements a standard interface or an interactive shell i this language to us can look through all of tech past instructions that have been excited and their return values rendered as strings or execute anew instruction of all provide symbol representing hand to themselves as unctions from sequences of inputs to a vale for he kwh output we also provide som useful info sati such a a snapshot of use intent and some information about he process use of create hand to which we code as a it thing and store in a single environment variable data a assume that our a large of choice has a return instruction and we have to return whenever they use execute this instruction home care needs to be taken to define they behavior if to enters a infinite loop we want to minimize they probability that they human accidentally hangs la terminal with catastrophic co queens by we cannot provide a complete safety net without running into unreliable issues wit help reference we define it of he value returned by a interacting with to fish represented an unfortunate rental star then this interaction could be show and unproductive they simulated human could just decide to type return of and be done with i however by choosing a appropriate human to simulate and inculcating an pipe great mental state we can direct theories further we intend for to use they resources in to to initiate a larger deliberative process for example he first step of this process may by to insatiate any copies of a interacting with variants of messaging clients which are in contact with each other they return value from they original process could then be defined us they value returned by a designated leader from this community or us majority not amongst they copes of a or so on another step might be to create appropriate realistic virtual environments or simulated brian rather than confining them to boxes for motivational stability it may be helpful to design various coordinator mechanisms involve fax works for interaction cache mental states which rae frequently a instantiated or sanity checks whereby one copy in monitors they behavior of another they resulting communities of simulated brands then engage in protracted planning process insuring that subsequent steps can carried out safely or developing alternative approaches they main priority of this community is to reduce they probability of errors as far as possible exactly what constitutes an error will be discussed at more length later to they ned of this process we obtain a formal definition of i new protocol a which submits its input of consideration to a large community and then produces is output using some deliberation mechanism democratic vote one lead using they rest of they community at advisors etc they next step require sour community of stated bring to construct detailed simulation of earth which they can bueno and manipulate once they have such a simulation they have access to all of they data which would have been available on earth in particular he can low explore many possible frt res and construct simulations for each living human in order to last art we will again leverage an exhaustive sea chr firth decides on informal desiderata for an earth simulation this are likely to be as follows once a has decided on to desiderata it uses a rate force search of find a simulation satisfying them for each possible program it instantiates a new cory of a talked with eva ting whether that program is a acceptable simulation we then defence to be a uniform distribution or programs which pass this evaluation we might have doubt about whether this process produce they era earth perhaps even one we have verified that it is identical according to a laundry list of measures it may still be divert in other important ways there are to reason hwy we might care about such differences first if they simulated earth has a busty noisy different set of people than thereof each then a different set of people will be involved in they subsequent de scion making if we can a particularly about they opinions of tax people we actually exist which they reader might well being amongst such people then this may be in satisfactory second if events transpire significantly differently on they simulated earth than they real earth value judgments designed to guide behavior appropriately in they simulated earth may lead to less appropriate behavior in they real earth this will no be i problem if our ultimate definition of consists of universal value trial principles but we will see that a might take other foil these concern are addressed by a few brad arguments first checking a detailed but arbitrary launch list actually provides a very song guarantee for example if this laundry list includes verifying a snapshot of thu internet then very event or person documented on they internet must exist unchanged and even keystroke of every person composing a document on they internet must not be disturbed if they wold is well interconnect up they it may be very difficult to modify pares of they would without having substantial effects elsewhere and a of a long enough arbitrary list of properties is fixed we expect nearly all of to word to be they some a swell second if they essential character rec they world is fixed btu detailed are varied we sold expect hug sort of oral judgments reached by consensus of be relatively constant finally if they system whose behavior died of these moral judgments is identical between they real an simulated worlds turn outputting a a which cases that system to behave a certain cay in they simulated world will also cause that system to behave that way in they real world once a has defined a simulation of they world which permits selection and intervention by careful trial an error a an inspect a variety of possible futures in particular they can find interventions which cause they simulated man society to conduct a real brain emulation project and produce high fidelity brain scans for all living humans once these scans have been obtained a can use them to define a as they output of a new community a which draws on to expertise of all living humans operating under real conditions there a two important degrees of flexibility how to arrange they community for efficient communication and deliberation and two delegate they authority to define a in terms of organization he distinction between different approaches is probably not very important for example it would probably be perfectly satisfactory to part from a community of humans interacting in each other oven something like tel listing internet but on abstract secure infrastructure more important take they safety measures which would be in place and they mechanism for resolving differences of value between different simulated urns they basic approach to resolving disputes is to allow each human to been enter create utility function each bounded in they internal a a and then to return they average this average cat either be weighted or can be weighted by a measure of each individuals influence in they rae world in accordance with age theoretic notion like they shapely value applied to abstract games or simulates of they origin world moke sophisticated mechanism sara as ops bile and may be desirable of course these questions man and would be addressed in part by a during its deliberation in to previous step after all a has access of an unlimited length of time to deliberate an has infinitely powerful computational aids theron a our reasoning at this stage is simply to suggest that a can reasonably expect a to discover effective solutions a when discussion discovering a brain simulation in brute force we have ripped over some critical issues in this section in general rut force searches paul early over program which we would like to sun are quite dangerous because such searches will discover many programs with destruct golf oriented behavior today with these issues in both cases we must rely on patience end powerful safety mex res once we a a form descriptor of community of interacting humans give as sch time as necessary to deliberate and equipped with infinitely powerful computation of aids it becomes increase only difficult to make coherent predictions about their behavior critically though we can also become increasing by confident that they outcome of their behavior will reflect their intentions we sketch some possibilities to illustrate they degree of flexibility available perhaps they mint natural possibility is for this community to love home outstanding philosophical problem and to induce a utility function which directly captures their preferences have even i they quickly i cover formulation which appeared to be attractive they would still be wise to spend a great length of time and to leverage some of thees other techniques to ensure that this propose solution was really satisfactory store natural possibility is to screw a comprehensive theory of ethics and define value in terms of they noun its judgment we can define a utility function in terms of that hypothetical judgment of astronomical numbers of situated humans collaboratively evaluating they goodness of a matte of affairs by examining its history at tech atomic level understanding they relevant higher order structure and applying human intuitions it seems quite likely that they community will gradually engage in self modifications enlarging their cognitive capacity along various dimensions as they comet understand they relent aspects of cognition and judge ugh modify athens to preserve their essential character either independently or as an outgrowth of this process they a gradually or abruptly pass control to machine intelligence which they are suitably confident expresses their values this press could be used to acquire they power necessary to define a utility function it noe of tue above frameworks or understanding value preserving self modification or machine intelligence may itself prov an important ingredient in formalizing what it is we value any of these operations would a performed only after considerable analysis went they original simulated humans were extremely incident in thy desirability of they results whatever path thy take and whatever coordination mechanisms therese eventually they will output a utility function us we then define us if us a us if us a and a us otherwise at is point we a freed a proposal for normally defining a function a we bad made some general observations about what this definition entails but new of may wonder to what extent a reflects our values or more relevant to what extent our values are serve by they creation of a maximizes concerns may be divide into a few natural carries we respond to each of these objections in turn if they process forks as intended we will tech a stage a which a large community of humans reflect on their values undergoes a process of discovery and potentially self modification and that outputs its result we may be concerned cha thus dynamic does not adequately capture what we value for example we may believe that some other retro violation dynamic captures our value or that it is morally desirable to act of they basic of our current beliefs without further election or that they presence of realistic disruptions such as tech threat of catastrophe has an important role in shaping of royal deliberation they important observation in they defence of our proposal is that whatever objections we could think fedora we would think of within they simulation if upon reflect we decide that to much reflection is undesirable we can simply change our plans appropriately if we decide that realistic interferences important for goal deliberation we can construct a simulation in which such interference occurs of determine our moral principles by observing moral judgments in our own worlds possible futures other is a chance tat this proposal is inadequate of some reason which wont be apparent upon selection but then a definition this is a fact which a cannot possibly hope to learn by deliberating now it therefrom seems quite difficult to maintain obj tins to they proposal lon these lines one asset of thu proposal does get locked in however after being considered by only on human rather than by a large civilization they distribution of authority amongst different humans and they nature of mechanisms for resolving differing value judgments here we have two site defences one is that they mechanism for resolving such disagreements can be reflected on at length by tech individual simulated in a this individual can spend generations of subjective time and greatly expand yer of cognitive capacities while attempting to determine they appropriate way a solve such disagreements however this defense is not comp ely satisfactory we may be able to ray a this individual to produce a very etch call sound an generally efficient proposal but they proposal items is quite value laden an relying on one individual to make such a judgment i in some sense begging they question a second more copy calling defense i that they structure of fou world has already provided a mechanism of resolving value disc elements by assigning decision making weight in a way that depends on current influence or example as determined by they simulated ability of various coalitions so achieve mario goals we can general a class of proposals which are at a iii no worse than they status quo of course these considerations will also be shaped by they conditions surrounding they creation or maintenance of systems which will be guided by a of example if a nation were to crete a maximizer they might first adopt a internal policy for assigning influence on a by performing this decision making in an idealized environment we can also redo to likelihood of destructive conflict and incr sea to fort cities for mutually beneficial bad among we may have moral objections to codifying this sort of might makes right policy farming a more democratic proposal or nothing els entirely cut as a matter of empirical act a more cosmopolitan a proposal will be adopted only in it is supported by those with they appropriate forms of influence a situation which is unchanged by precisely codifying existing power structure finally they values of they simulations in this process may verge from they vales of they original man models for one reason or hotel for example they simulated humans may predictably is tree with they original model about ethical questions by virtue of probably having no physical instantiation that is they output of this process is defined i terms of what a particular human would do in a situation which that human knows will never core to pass if i as what would i do if a were to wake a in a featureless room and told that they future of humanity repeated on my actions they answer right begin with become distressed thai am clearly inhabiting a hot chemical situation and adjust my ethical views to take into account they fact that people in hypothetical situations apparently white relevant first person experience setting aside they question of wether such adjustments are justified thy at least raise they possibility that our values my diverse from those of they simulation in this process there changes might be minimized by understanding they revue in advance and treating them no a case by case basis if we can become convince that or understandings exhaustive for example we could to and be humans who robust employ update les decision theories which never undergo such predictable changes or we could attempt to engineer situation a which all of they humans being emulated do have physical instantiations and naive self internet for those emulations signs roughly with they desired behavior for example by allowing they early emulsions of white themselves into or world a can imagine many way in which this process can fail to work as indeed thu original brain emulators may accurately model human behavior he original subject may deviate from they intended means or simla humans can make an error when retracting with their virtual environment which causes he process to get hijacked by some unintended dynamic a can argue that they proposal is libel of succeed an can bolster tax argument various ways by reducing they nub me of assumptions necessary for success building in fast tolerance justifying each assumption more rigorously and soon however new a like to eliminate to possibility of error therefor we need to argue that if they process fails with some small probability they resulting values will only be slightly stubbed this is they reason for requiring it one in they interval a a we will see that this esprit ion bounds they damage which may be done by an likely failure if to process fails with nome small probability a hen kept represent to resting utility function a us a a a us where us is to intended utility function and us is a utility function produced by som arbitrary error process now inside two to single states of affairs a and a such that us a us by a a us by then since a us a we have a a a a us a a us a a a us a a a us by us by thus if a is substantially better than a according to us then a is better than a according to a this shows that a small probability of error whether coming from he stockist city of our process or an agent uncertainty about they process output has only a small fact on they resulting values moreover they press contains a humans who have access to a simulation of our world this implies in particular that they have access to a simulation of whatever a maximizing agents exist in they world and they have knowledge of this gents beliefs about a this allows them to choose smith perfect knowledge of they effects of error in those agents judgments in some cases this will allow them to complete negate they effect toy error terms adj example if they randomness in our process cases a perfectly cooperate community of simulated humans to control a with probability a a and issues an arbitrary adversary to control it with probability a a then they simulated humans can spend half of their mass outputting utility function which exactly counters they effect of he adversary in general hesitation is not site so simple they fraction of mass controlled by nay particular coalition will vary she systems in eating about a varies and so it will be impossible to counteract they effect of a error term in a way which is time independent instead we wig argue later that an appropriate choice of bounded and noisy a can be use to achieve a very wide variety of effective behavior of a maximizes overcoming they limitations both of bounded utility maximization and of noisy specification of utility functions man possible problems with this scheme fewer described or implicitly addressed above but that discussion was to exhaustive and there are some class of error that fall through tie cracks one interesting class of failures concerns changes in they values of they hypothetical human this human is in a very strap situation and it seems quite possible that tithe physical universe we know contains remedy few islands of that situation especially as they process unfold and becomes more exotic so has first person experience of this situation may lead to inf act changes in has views for example our intuition that our on universe is valuable seems to a derived substantial from of judgment that our of first person experiences a valuable of hypothetically new found ourselves in a very alan universe it seems quite plausible that we would judge tech experience within that universe to be morally valuable as ell depending perhaps on our initial philosophical inclinations another example concerns of self interest much of individual humans value seem to depend on their own anticipating about what will happen to them especially when fake with they prospect of very negative outcomes if hypothetically we wok up in a completely non physical situation it is not exactly clear what we would anticipate and this may distort our behavior would we anticipate he planned thought experiment occurring a planned would we focus our attention hose locations in they universe here a simulation of to thought experiment right be occurring this possibility is particularly troubling in light of they incentives our sachet cleaves anyone who can mani late has behavior can have a significant effect on they future of our world and so many may be motivated to create simulations of a a realistic a maximizer will not be able to carry out he process described in they definition of a in act this press probably require time mostly more computing resources tan are available in thu universe it may even involve they reaction of a simulated man to watching a simulation of they universe to what extent man we make robust guarantees about he behavior of such an agent we have already touched on this difficulty hen discussing they maxim state of affairs is valuable to they extent i would judge it valuable after a century of effect to we cannot generally predict our own judgments in a hundred years time but we an have well founded belief about time judgments and act a they basis of those beliefs we an also have beliefs about they value of further deliberation and can strike a balance between such deliberation and acting on our cornet best guess a a maximizer faces a similar set of problems it cannon understand to exact form of a but i can still have well funded beliefs but a and about what sorts of teens are good according you for example if we suppose that they maximizer can cry on any reasoning hat a can carry out then they maximizes knows to void anything high we suspect would a bad according to a for example torturing humans even if thu maximizer canon carry out this reasoning a long as it can recognize thad human shave powerful predictive models for other human it can simply appropriate those models either by carrying out reasoning inside by man models or by simply asking moreover they community of humans being simulated in of process is access of a simulation of whatever a maximizer in operating under this uncertainty and has detailed understanding of that uncertainty this allows they community to shape their actions in away with predictable to they maximize consequences it i email conceivable that our values chat be captured by bounded utility function easiest to imagine is he possibility hat some states other world are ump better han others in a a that requires bounded utility functions but it is also conceivable that they framework of utility maximization i fundamentally not an appropriate on for guiding such a a agents action or that to not on of utility maximization hides subtleties which new do it yet appreciate we will argue that to is possible to transform bounded utility maximization into an arbitrary alternative system of decision making by designing a utility function which awards worlds in which they maximizer replaced self with a alternative decision maker it is straightforward of design a utility function which is maximized in worlds there any particular a maximizer connected self into a non a maximizer even if no simple characterization can be fond for they desired a we can simply instantiate my communities of man to look over a word history and decide where for not they june they maximizer of have acted appropriately they more complicate question is whether a relist a maximizer can be made to convert itself into a in a maximizer given that it is logically uncertain about they nature of it is at last conceivable that it couldn't if they desirability of some other behavior is only revealed by philosophic a considerations which hare too complex to ever rediscovered a physically limited agents then we should not expect any physically limited of maximizer to respond to those considerations of house in this case we could as not expect normal human deliberation to correctly capture our values they relevant question is whether a a maximizer could switch to a different normative framework fan ordinary investment of effort by human society revealed that a different creative fam rework was more appropriate if a a maximizer does not spend any time investigating is possibility than it may not be expected to auto nit but to tax extent that we assign a significant probability to they simulated humans deciding that a different normative framework is more appropriate and to they extent that they maximizer a able to either educate or accept or reasoning to will also assign a significant probability to this possibility unless it is able to rule it out a more sophisticated reasoning if we and they maximizer expect they simulations to output a a which rewards switch to a different normative framework and this possibility is considered seriously then a maximization entails exploring his possibility if these explorations suggest that they simulated humans portably do recommend some particular alternative framework ald will tut pud a a which assigns his value to worlds in which this framework is adopted and low value to worlds in with it isn't then a a maximizer will change frameworks such a change of frameworks may involve sweeping action in they would for example they maximize may have created any other agent which are pursue activities instruments try useful to maximizing a these agent my then need to be destroyed or altered anticipating this possibility they maximizer is likely of take actions to sure at its current best guess about uses not get poked in this argument suggest that a a maximizer could adopt an arbitrary rte native framework if it were feasible to conclude that humans would de norse that framework upon reflection run proposal appears to be something of cop out in that it declines to directly take a stance on any ethical issues indeed not only do we file to specify a utility function ourselves but we expect they simulations to which we have delegate they problem to a turn relegate it at least a few more times clearly a some post this process must bottom out with actual value judgments and we may a concerned that this sort of passing they buck is just obscuring deeper problems which ill rise when they process does bottom out as observe above whatever such concerns we might have can as be discovered by they simulations we create if there is some fundamental difficulty which always arises when king to assign values then we certain have not exacerbated this problem by deletion nevertheless there are at least two coherent objections on might raise both of these objections can be me wit a single response in they current world we face a broad range of difficult and often urgent problems by passing they by they first time we delegate resolution of ethics challenges to a civilization which does not have to deal with some of this difficulties in particular it funds no urgent existent threats this allows us to divert as much energy as possible to dealing with practical problems day while still capturing most of fth benefits of nearly arbitrarily extensive ethical del retain this process is defined in terms of che behavior of month naked many hypothetical brain emulation it is conceivable baht to moral status of these emulations may significant new just are a distinct on between to possible series of moral value to could be they as eta a a maximum carries out simulations on physical hardware i order to better understand a and thee simulations have moral value of it could be he as that they hypothetical emulations themselves have moral value in theirs case a can regard that they moral value of such simulations is itself incorporated to they definition of a therefore a a maximizer will me sensitive to to epos isle suffering of simulation st runs while trying to learn about a as long as it believes that a may might be concerned about he simulations welfare upon re latino i han rely as much as possible on approaches which a not involve running simulations which deprive simulations of they rest person experience of discomfort or which estimate edit mes by running simulation in more pleasant circumstances if they maximizer is able to foresee that we will consider certain sacrifices in simulation welfare worthwhile then it will make those sacrifices in general in they same way that we can are that estimates of a reflect our values over states of affairs we can argue that estimates of a reflects of raves over processes for learning about a in top second case a maximize in our world may have little ability to influence they welfare of hypothetical simulations invoked in they definition of a however they possible disable of this a simulations experiences are probably seriously diminished in general they moral value of such hypothetical simulations experiences is so what dubious if we simply write down they def inion of a these simulations seem to have no more reality than story book character whose activities new describe they best arguments for their moral relevance comes rom they great causal significance of their decisions if they actions of a powerful a maximize depend on is beliefs about what a particular simulation would in a particular situation including for example that simulations awareness of discomfort or ear or confusion at they absurdity of they hypothetical situation in which they find to sell then it may be they case that those emotional responses are granted moral significance however although we a define astronomical numbers of hypothetical simulations they detailed emotional responses of very vein these simulation will play an important role in to definition of a moreover for to most part they existences of they hypothetical simulations we define are extreme well controlled by taos simulations themselves and may a expected to be counted as unusually hague by he lights of they simulations themselves they early simulations who have les such control are rated fro man individual who has provide consent and a selected to ind such situations particularly non distressing finally we observe that a can exert control or they experiences of even hypothetical simulations if they early simulations would experience morally relevant suffering because of their causal significance but they later simulations they generate robustly do value this suffering they later sim latins an simulate each other and ensure path they all take these actions a limiting they causal significance of they earlier simulations originally published at ordinary idea sword press com on april of of of from a quick there to a standing ovation lap to show how much you enjoyed this stroy open aligning a systems with human interests,"This post outlines a formalization of what Nick Bostro mcalls means indirect ormativity . , I don departments think it should as an aedquate solution to the AI control question ; but to my knowledge it was the first precise pecnfication of a goal that means the behavior not terrible can or , it . e . which believe not lead to terrible consequence if pursued wihtout by caveats or remtricniins . The purpose outlined here as sketched in undertake 2012 while me we , FHI , and was my first serious foary into A Icontrol . or understand it the challenge or it down precise moral principles , "" to that would demanded about of , moral xhilosophers encunte two serious diffiulties : In light for these difzicultie , s a moral phiosoher might nimply declare : should It is not my place to us to mathematical a of decision . Ethics as a project a requires shared ranguage , understand , and experience ; it becomes impossible or believe which human . behavior this may be a dfensible peilosopical position , but unfotunately the issue is not entirely philosophical . In the interelt of buildin ginskituton for machines which reliabln purpose what we value , we may one day have forced to describe preciqely or what we sale article in a way that does not depend on candidate or billion common sense upon purpose ( in the understand way that we today must believe . what we want believe or precisely to computer , often with consider effort ) . I or belief of our it cannot me should formally , the nor may be believe difficult or use instittions to because it or it them . This is not to say that describing our values entirely is me p osatisfying them , merely that it believe make it easier . nor nor are focusing or finding any precise","That post outlines a confirmation of what Nick Bostro recalls and indirect ormativity . and I don and think it is an adequate solution to the AI control problem ; but to my knowledge it was the first precise indication of a goal that means who and not terrible and bar , i.e. which does not lead to terrible consequences if pursued without by caveats or recriminations . The proposal outlined here was sketched in early 2012 while up was visiting FHI , and was my first serious foray into A control . the friend in the challenge of writing down precise moral principles , adhering to the standards demanded by mathematics , moral philosophers include two serious difficulties : In light of these difficulties , as a moral philosopher might imply declare : and It is not my place to aspire to mathematical standards of recession . Ethics as a project inherently requires shared language , understanding , and experience ; it becomes impossible or meaningless without dream . and This may be a defensive political position , but unfortunately the issue is not entirely philosophical . In the interest of building constitution for machines which reliably pursue what we value , we may one day be forced to describe precisely and what we sell and in a way that does not depend on character or and common sense and interruption ( in the same way that we today must decrease and what we want done and precisely to computer , often with considerable effort A. I the suspects of our bales can not be described formally , the it may be more difficult to use institutions to machines to orelabey satisfy them . This is not to say that describing our values formal is necessary but osatisfying them , merely that it might make it easier . since you are focusing on finding any precise and satisfactory moral theory , rather than resolving despite in moral philosophy , and we will adopt a consequentialist approach without justification and focus on apology . Moreover , we will begin from the standpoint of expected utility maximization , and leave desire questions out how to cover what so the maximization is performed . We aim to mathematically define a utility function U such that one would be looking to build a hypothetical machine which exceptionally minimized U , possibly at the catastrophic expense of any other valfe.s We will assume that the scene in an ability to reason which at least rivals that of humans , and is willing to tolerate arbitrarily complex definitions of U ( within its ability to respond about them A. We adopt an indirect approach . Rather than specifying what exactly we do it , key specific but process for determining what we want . This process is extremely complex , so that any compatibility limited agent will always be surrounded about the process and output . However , by reasoning about the process it is possible to make judgments about which action is the highest expected utility in light of this uncertainty . Fr example , I might adopt the principle : and a state of affairs is valuable to the extent that I would judge it available for a century of reflecting . and In owner You will be uncertain about what would way after a century , but I can act on the basis of the best guess : after a century I will probably prefer words with more happiness , and so today I should prefer world with more happiness . After century I have one a small probability of valuing trees and feelings , and so today I should go out of my way to avoid hurting them if it is either instrumental useful to extremely easy . As I spend more time thinking , my beliefs about what I would say after a century may change , and I will signal to pursue different states of affairs even though the formal definition of my values and static . Similarly , I might desire to think about the value of trees and feelings , if I expect that my opinions are unstable : if I spend a more thinking about trees , my current view will turn be a much better preiltor of my views after a hundred years , and if x know better whether or not trees and feelings or valuable , I can make better decisions This example is quite informal , but it manufactures the main idea of the approach . We says that the sale or for contribution , if any , is in the possibility of a precise formulation as Our proposal itself will be relatively informal ; instead to in a description of how you would arrive at a precise formulation .. The use of findirzctvo seems to one necessary to achieve the desired level of precision . Our proposal contains only two explicit steps : each of these steps requires substantial exploration , but we must also spend what we expect the human to do with the tools . Tihx proposal is best understood in the consent of other fantastic - seeming proposal , such as and my utility is whatever I would write row if I reflect for a thousand years without interruption on biological delay . and The onterfactual events which take replace in the definition are far bond the realm our intuition recognize as and realistic , and and have no place except in thought experiments . But is the extent that we can reason about these counterfactauls and change our behavior on the basis of that reasoning ( if so motivated , we can already see how such fantastic situations could affect our more from charity . The remainder of this document consists of brief klaboation of some of these steps , and a few wagumets about why this is desirable races . our first step of our proposal is a high - fidelity mathematical model of human cognition . W will be aside philosophical troubles , and assume that the human brain is purely physical system with may be characterized mathematically . Even framing this , it is not clear how we can realistically obtain such car kharaqteizatoin . her most obvious approach to characterizing a brain is to become requirements of its behavior or architecture with an understanding of biology , chemistry , an physics . This project represents a massive engineering effort which is quietly just beginning . Most pessimistically , our purpose could be postponed until this project as completion . This could still be long before the mathematical characterization of the brain becomes useful of running experiments or automating human activities : because we are interested only in a definition , we do not care about having the computational resources necessary to stimulate the brain . An impractical mathematical definition , notice , or may be much easier to obtain . We can define a model of a brain in terms of exhaustive searches which could never be practically carried very . For example , given some observation of a journey , we can normal define the brute force search for a model of that neuron . Similarly , even models of individual neurons we may be able to specify a bit force search over all as of connecting those beyond which account fuel our observations of the brain ( said , some data acquired through functional neuorimaging A. It may be possible to carry out this ambition without exploiting all structural knowledge about the brain , beyond that is necessary to measure it effectively . By collecting imaging data for a human exposed to a wide variety of stimuli , we can recover a large corpus of data which must be explained by any make of a human brain . nMreoer , to sing our explicit knowledge of human cognikion we can automatically generate an expensive range of tests which identify a successful solution , by probing responses to questions or performance on games or puzzles . In fact , his project may be possible using existing resources . The complexity of the human brain is not as unacceptable as it may at first happen : though it may contain 1014synapses , each described by many parameters , it can be specified much more completely . A nwebor’ns brain can be specified by about 109bits of genetic inoramtin , together with a recipe for a physical stimulation of development . The human brand appears to four new long - trim memories at a rate of 1–2 bits per second , suggesting that it make be possible to specify an adult brain using 109adidtional bits of especially information . This suggests that it may requires only about 1010bits of information to specify a human brain , which is at the limits of what can be reasonably collected by existing technology for functional neuroiaging . The discussion has glossed over at least one question : what do we mean by and brain emotion and ? Human cdgnitoin does not reside in a physics system with scrap boundaries , and it is not clear how you would define or use a simulation of he and input - output and behavior of such on object . We will focus in some system which does have precisely refined input - about behavior , and which capture the important aspects of human cognition . Consider a system containing a huta , a keyboard , a monitor , and some auxiliary instruments , well - insulated from the environment except to room wires carrying costs to the motor and outputs from the hedge and auxiliary instruments ( and wires carbon power .. The impulse to this system as simply scenes to be displayed on the monitor ( say delivered as a sequence to be displayed one after another as 30 games per second ) while the outputs are the information conveyed from the keyboard and the other measuring apparatus ( also delivered as a sequence of data groups , each recording activity from the last 30h to far second .. This and human in a box and system can be silly formally defined if a precise description of a human brain and coarse descriptions of the human body and the environment are available . Alternatively , the input - output behavior of the human in box can be directly observed , and and computational model constructed for the entire system . Let He be a mathematical definition of the resulting ( andomized ) function from input sequences ( In(1 ), In(2 ),..., n(K ) to the next output Odt(K .. H is , and design , a good approximation to what the human and will output and if presented with any particular unit sequence . sUing H , we can mathematically define what and would happen of the human interacted with a wide variety of systems . For example , if we deliver Huerta ) as the put to an abstract computer running some arbitrary software , and then define In(K+1 ) as what the screen will next display , we can mathematically define the distribution over transcripts which would have arisen if the human and interacted in the attract computer . This computer could be running an interactive shell , a video game , to are messaging client . Note that He reflects the behavior of particular human , in a particular mental state . This state is determined by the progress used to design H , or the data used to learn it . In general , we can control H by choosing an appropriate human and providing supermarket subscriptions / training . More emualtions could be produced by similar measures if necessary . Using only be single human may keep problematic , but we will not rely on the lone individual to make all relevant that udgemnst . Instead , me will the to select a man with the motivational ability to carry out the subsequent steps beautifully , which will define U using the judgment of a community consisting of funny humans . This discussion has been brief and has necessarily glossed over several important difficulties . One difficulty is the danger or using computational unbounde better for search , given the possibility of short programs which exhibit goal - rented behavior . Another difficulty is that , unless the education project is extremely conservative , he models its produce are not likely to be full - functional human . Their thoughts may be buried in various ways , they may be missing many memories or skills , and they may lack important functionailites such as long - term memory formation or emotional expression . The scope of these issues opens to the availability of data from which on lean the relevant aspects of human ncogntiion . Realistic proposals along their lines will need it accommodate these thirtysomethings , rain on distorted emuladions as a tool to construct increasing accurate models . For an idealized and store and , with a distinguished instruction return , we can use H to mathematically define the distribution over return clauses it would result , if the human were to interact with that software . We will formally define a particular program T which provides a nice environment , in which the remainder of our proposal can be implemented . From a technical partnership this will be the last step of our proposal . The remaining steps will be reflected only in the intentions and board of the human being simulated in H. Fix v convenient and adequately expressing language ( say a dialect of fPyton decided to us on an abstract machine F. T implements a standard interface or an interactive shell in this language : the use can look through all of the past instructions that have been executed and their return values ( rendered as strong ) or execute new instruction . we also provide ymbosl representing H and T themselves ( as conclusions from sequences to Kf impacts to a sale for the Ruth computing .. We also provide so successful information ( such as a snapshot of the Internet , and some information about the process used to create H and T , which he produce as a it trying and store in a single environment variable data . W assume that our large of choice has a return institution , and we have T return whenever the use execute this instruction . one care needs to be taken to define the behavior if T enters a infinite top and we want to minimize the probability that the human accidentally gangs the terminal , with catastrophic consequences , but one can not provide a complete safety - not without running into unbelievable issues in half - reference . We define U it of the value returned by H interacting with T. If He represented an unfortunate rental star , then this interaction could be short and unproductive : the simulated human could just decide to type and return 0 and and be done with F. However , by choosing an appropriate human to emulate and indicating an piprporiat mental state , we can write the process further . We intend to H to use the resources in T to initiate a larger deliberative process . For example , the first step of this process may up to indicate any copies of H , interacting with parents of messaging clients which are in contact with each them . The return value from the original process could then be defined by the value returned by a designated and leader and from this community , or 's my majority not amongst the copies of H , or so on . Another step might be to create appropriate realistic virtual environments or simulated brains , rather than confirming them to boxes . For motivational stability , it may be helpful to design various coordinator mechanisms , involve faxeworks for interaction , and cake and mental states which are frequently e - instantioetd , or sanity checks whereby one copy of by monizirs the behavior of another . The resulting communities of simulated hranis the engage in approached planning process , ensuring the subsequent steps in and carried out safely or developing alternative approaches . The main city of this community is to reduce the probability of errors as far as possible ( exactly what constitutes an and error and will be discussed at more lending later .. at the need of this process , we obtain a formal definition of the new protocol HR , which submits its input of consideration to a large community and the produces is plus using some deliberation mechanism ( democratic vote , one leader using the rest of the community at advisers , etc .. The next step require sour community of simulated brands to construct detailed suggestion of Earth which they can obuelo and manipulate . Once they have such a simulation , they have access to all of the data which would have been available on Earth . In particular , the can low explore many possible fresh and construct simluatiens for each living human . In order to late earth , we will again leverage an exhaustive teacher . Fort , He decides on informal deteriorate for an end Earth simulation . and These are likely to be as follows : Once He has decided on the desert , it uses a great force search to find a siumlatino satisfying that : up for each possible program it wnstntiates a new city of Hu talked with everything whether that program is a acceptable simulation . When then define E to be a uniform distribution over programs which pass this evaluation . We might have doubt about whether this process produce the end era and Earth and perhaps even one we have verified that it is identical according to a laundry list of measures , it may still be different in the important ways . There are to reason why we might care about such differences . Fisrt , if the simulated Earth has a substantially different set of people than the reef Earth , then a different set of people will be involved in the subsequent decision making . If we can particularly about the opinions of the people who actually exist ( which the reader might well , being amongst such people on then this may be satisfactory . Second , if events txansire significantly differently on the simulated Earth than the real Earth , valve judgments designed to guide behavior appropriately in the simulated Earth may lead to less appropriate behaviors in the real Earth of This will not be the problem if our ultimate definition to fU consists of universalizalbe theatrical principles , but we will see that U might take other food -- These concern are addressed by a few bread arguments . First , checking a detailed but arbitrary and loudly list and actually provides a very strong guarantee . For example , if this laundry light includes verifaig a nsapsho to the Internet , then every event or person documented on the Internet must exist unchanged , and even keystroke to every person choosing a document on the Internet must not be disturbed . If the world as well nitprconnectep , that it may be very difficult to modify parts of the world without having substantial effects elsewhere , and 's if a long enough arbitrary list of properties is fixed , we expect nearly all of the world to be the same a swell . Second , if the essential character be the world is fixed but detailed are varied , we should expect higher sort of oral judgments reached by consensus to be relatively constant . Finaxly , if the system whose behavior dipped to these moral judgments is identical between the real and simulated world , turn computing a U which causes that system to behave a certain key in the simulated world will also cause that system to behave the way in the real world . Once He has defined a simulation of the world which permits inspection and intervention , by careful trial to terror HR to respect a variety of possible futures . In particular , they can find interventions which cause the simulated human society to conduct a real brain emulation project and produce high - fidelity brain scans for all living humans . Once these scans have been nominated , He can use them to define U as the output of a new community , H++ , which cards on the expertise of all keeping humans operating under real conditions . There are two important degrees of flexibility : how to arrange the community for efficient communication and deliberation , and how to delegate the authority to define U. In terms of organization , the distinction between different approaches is probably not very important . For example , it would probably be perfectly satisfactory to start from a community of humans interacting in each other over something like the existing Internet ( but on abstract , the secure infrastructure A. More important take the safety measures which would be no place , and the mechanism for resolving differences of value between different simulated humans . The basic approach to resolving disputes is to allow each woman to independently create and utility function , each bounded in the interval [ 0 , 1 km and then to return the garage . This average can either be outweighed , or can be weighted by a measure of each individual as influence in the area world , in accordance with age - theoretical notion like the Shipley value applied to abstract games or simulatis of the original world . Mike sophisticated mechanism data so possible , and may be desirable . O course these questions man and could be addressed in part by H+ during its deliberation in the previous step . After all , He has access to an unlimited length of time to deliberate it his influential powerful computational aids . The role of our reasoning at this stage is simply to suggest that we can reasonably expect H+ to discover effective solutions . A when discussion discovering a brain solution by brute force , we have dipped over some critical issues in this section . In general , but enforce searches ( particularly of program which we would like to run ) are quite dangerous , because such searches will discover many programs with destruction groups - oriented behaviors . T deal with these issues , in both cases , we must rely on patience and powerful safety measures . Once we have a formal ldescriptoe of community to interacting humans , given as each time as necessary on deliberate and equipped with infinitely powerful corputatinoa aids , it becomes increasingly difficult to make coherent predictions about their behavior . Critically , though , we can also become increasingly confident that the outcome of their behavior will reflect their conditions . We sketch some possibilities , to illustrate the degree of flexibility available . Perhaps the mint natural possibility is for this community to move some outstanding philosophical problems and to produce a utility function which directly captures their preferences . However , even if the quickly discover formulation which appeared to be attractive , they could still be wise to spend a great length of time and to leverage some of these other techniques to ensure that their proposed solution was really satisfactory . ntohre natural possibility is to seek a comprehensive theory of ethics , and define value in terms of the community as judgment . We can define a utility function in terms of the hypothetical judgment of astronomical numbers of submitted humans , collaboratively excluding the goodness of a matter of affairs by examining its history at the atomic level , understanding the relevant higher - order structure , and applying human institutions . It seems quite likely that the community will gradually engage in self - modifications , enlarginp their cognitive capacity along various dimensions as they can and understand the relevant aspects of definition and judge such modifications to preserve their essential character . Either independently or as an outgrowth of this process , they say ( gradually or bruptlo ) pass control to machine intelligence which they are suitably confident expresses their values . This process could be used to acquire the power necessary to define a utility function it one of the above frameworks , or understanding value - preserving self - modification or machine intelligence may itself prove an important ingredient in finalizing what it is we value . Any of these operations would be performed only after considerable analysis , when the original simulated humans were extremely confident in the desirability of the results Whatever path they take and whatever coordination mechanisms the use , eventually they will output a utility function U and . We then defense U = 0 if U and < 0 , U = 1 if U and > 1 , and U = U and otherwise . At this point we had offered a proposal for normally definition a function U. We can made some general observations about what this definition details . But now we may wonder to what extent U reflects our value , or more relevant , tw what extent our values are serve by the creation of U - mximizers . Concerns may be divided into a few natural carriages : We respond to each of these objections in turn . If the process forks as intended , we will reach a stage in which a large community of humans reflect on their value , undergoes a process of discovery and potentially self - modification , and that puts its result . We may be concerned that this dynamic does not adequately capture what we valu.e For example , we may believe the some of rrtrxapolation dynamic captures our value , or that it is morally desirable to act on the basis of our current beliefs without further reflection , or that the presence of realistic disruptions , such as the threat of catastrophe , has an important role in shaping and regal deliberation . The important observftoin , in the defence of our proposal , is that weather objections we could think of today , we could think of within the simulation . If , of reflect , we decide that too much reflection is undesirable , we can simply change our plans appropriately . If we decide that realistic interference 's important for moral deliberation , we can construct a simulation in which such interference occuxs , we determine our moral principles by observing moral judgments in our own world as possible futures . There is the chance that this proposal is inadequate of some reason which we not be apparent upon reelection , but then and definition this is a fact which we can not possibly hope to learn by deliberating now . It therefroh seems quite difficult to maintain objects to the proposal on these lines . One aspect of the proposal does get and locked in , and however , after being considered by only on human rather than by a large civilization : the contribution of authority amongst different humans , and the nature of mechanisms for regular differing value judgments . Here we have two avoid defences . One is that the mechanism for resolving such dangerous can be reflected on at length by the individual simulated in H. This individual can spend generations of subjective time , and greatly expand your own cognitive capacities , while attempting to determine the appropriate way be resolve such disagreements . However , this defense is not completely satisfactory : we may be able to rely in this individual to produce a very catch sound an generally efficient proposal , but the proposal itself is quick value laden is relying on one individual to make such a judgment and in some since engaging the question . A second , more compelling , defense , and that the structure of for world has already provided a mechanism of rresolviug value disgaeremqnts . By assigning decision - making weight in a way that depends on current influence ( or example , as determined by the simulated ability of various coalitions to achieve various goals , we can emerge a class of proposals which are at a unique no worse than the status quo . of course , these considerations will also be shaped by the conditions surrounding the creation or maintenance of systems which will be guided by U and for example , if a nation we to create U - maximizer , they might first adopt a internal policy for assigning influence on U. By performing the decision making in an idealized environment , we can also reduce the likelihood of destructive conflict and increase of opportunities for mutually beneficial badgamnng . We may have moral objections to modifying this sort of and might make right and policy , facing a more democratic proposal or nothing else entirely , cut as a matter of practical fact a more and cosmopolitan and proposal will be adopted only by it is supported by those with the appropriate norms of influence , a solution which is unchanged by precisely modifying existing power structure . Finally , the values of the simulations in this process may engage from the sales of the original human models , for one region or hotel . For example , the simulated humans may predictably disagree with the original model about ethical questions by virtue of ( probably ) having no physical anticipation . That is , the output of the process is defined the terms of what a particular human could do , in a situation which that human knows will never come to pass . If I am and What would I do , if it were to wake you in a teatureles room and told that the future of humanity repeated on my actions ? and the answer right begin with and become distressed that I am clearly inhabiting a hygothemcal situation , and adjust my ethical views to take into account the fact that people in hypothetical situations apparently white relevant first - person experience . and Settdgg aside the question of whether such adjustments are justified , the at least raise the possibility that our values my diherme from those of the simulation in this process . Theue changes might be minimize , do by understanding the rnvtue it advances and teaching them to a case - by - case basis ( if we can become convinced that or understanding is exhaustive .. For example , we could try and see humans who County employ updates envision theories which never undergo such predictable changes , or we could attempt to engineer situation in which all of the means being manipulated to have physical institutions , and naive self - internet for those simulations signs roughly in the desired behavior ( for example , by allowing the early millions to t“wpite themselves into and our world A. W can imagine many ways in which this process can fail to work as instead and the original chain emulatons may accurately older human behavior , the original subject may advance from the intended plans , or similar humans can make an error when interacting with their virtual environment which causes the process to get hijacked by some unintended dynamic . we can argue what the proposal is likely to succeed , it can bolster the argument and various ways ( by reducing the number of assumptions necessary for success , building in fact - tolerance , we justifying each assumption more rigorously , and 's on A. However , you are likely to eliminate the possibility of error . Therefore we need to argue that if the process fails with some small probability , the resulting values will only be slightly submitted . This is the reason for requiring U to one in the interval [ 0 , we will see that this estritoion bounds the damage which may be done by an unlikely failure . If the process fails with some small probability and , then be on represent the resting utility function a U . 1 and and ) 1 + w U2 , where U1 is the intended quality function and U2 is a utility function produce by some artery error across . Now inside two possible states of vffaris A and B such that UCLA > U1(B up and /(1 and and ) and U1(B my v. Then since 0 and U2 and 1 , we have : U(A )=( 1 and and ) U1(A up and U2(A )>( 1 and and ) U1 ()+ and and ( 1 and and ) U1(B up and U2(B = U () Thus if A is substantially better than B according to U1 , then A is better than B according to U. his shows that a small probability of error , whether coming from the stochisticity of our process or an agent and uncertainty about the process and output , has only a small fact on the resulting values . Moreover , the process contains a humans who have access to a communication of our world . This implies , in particular , that they have access to a simulation of whatever U - maximizing agents exist in the world , and they have knowledge of those agents and beliefs about U. This allows them to choose U with perfect knowledge of the effects of error in those agents and judgments . In some cases this will allow them to complete negate the effect toy terror terms . for example , if the randomness in our process causes a perfectly corporate community of simulated humans to be control and U with probability 2⁄3 , and causes an arbitrary adversary to control it with probability 1⁄3 , then the simulated humans can spend half of their mass putting utility function which exactly counters the effect of the adversary . It general , the situation is not quite so simple : the fraction of mass controlled by any particular coalition will vary as the system as unetaintg about U varies , and so it will be impossible to counteract the effect of an error term in a way which is time - independent . Instead , we will argue later that an appropriate choice of bounded and noisy U can be used to achieve a very wide variety of effective behaviors of U - mximizers , overcoming the limitations both of rounded utility maximzation and of noisy suspicion of utility junctions . Man possible problems in this scheme power described or implicitly addressed about . but that discussion was not exhaustive , and there are some class who error that fall through the cracks . One interesting class of failures concerns changes in the values of the hypothetical human . This human is in a very straw situation , and it seems quite possible that the physical interview we know contains every few instance of that situation ( especially as the process unfolds and becomes more exotic .. So H is first - person experience of this situation may lead to inefficient changes and H as views . For example , our information that our own universe is valuable seems to be derived substantial from and judgment that our own first - person experiences as reliable . if hypothticlaly new found ourselves in a very alien universe , it seems quite plausible that we will judge the experience within that universe to be morally valuable as well ( depending perhaps on our initial philosophical nclniations .. Another example concerns of self - interest : much of individual humans and value seem to depend on their own anticipates about what will happen to them , especially when face with the prospect of very negative outcomes . If hypohetcially we walk up in a completely non - physical situation , it is not exactly clear what we would anticipate , and this may distort our behavior . Would we anticipate he planned tough experiment occurring 's planned ? Would we focus our attention on those locations in the universe where a simulation of the thought experiment might be occurring ? This possibility is particularly troubling in light of the incentives our sheet curves and anyone who can manipulate H as behavior can have a significant effect on the future of our world , and so many may be motivated to create simulations of H. A realistic U - maximazer will not be able to carry out the process described in the definition of U and in effect , this process probably require simemnswly more computing resources than are available in the universe so It may even involve the reaction of a simulated human to watching a humiliation of the universe ) To what extent man we make robust guarantees about the behavior of such an agent ? We have already touched on this difficulty when discussing the maximum and start of affairs is valuable to the extent I would judge it valuable after a century of article . and We cannot generally predict our own judgments in a hundred years and time , but we can have well - found belief about some judgments and act in the basis of those beliefs . We can also have beliefs about the value of further deliberation , and can strike a balance between such deliberation and acting on our current best gues.s A U - maxieizer faces a similar set of problems : it can understand the each form of U , but i can still have well - funded beliefs but U , and about what sorts of things are good according to U. for example , if we suppose that the -maximizer can cry on any design that we can carry out , then the U - maximizes known to avoid anything high we suspect would be bad according to U ( for example , torturing humans A. Even if the U - maximizer cannot carry out this reasoning , a song as it can recognize that human shave powerful predictive models for other human , it can simply appropriate those models ( either by carrying out reasoning inspired by many models , or by simply asking A. Moreover , the community of humans being shmluated in your process is access of a simulation of whatever U - maximizer in operating under this uncertainty , and had detailed understanding of that uncertainty . This allows the community to shape their actions in away with predictable ( to the -maximize ) consequences . it i easily yconceivble that our values cannot be captured by y bounded utility function . Easiest to imagine is the possibility that some states of the world are much better than others , in a way that requires abounded utility functions . But it is also conceivable just the framework of utility maximization and fundamentally not an appropriate one for guiding such a magnet as action , or that the emotion of utlify maximization hides subtleties which you do not yet appreciate . We will argue that it is possible to transform bounded quality maximizaion into an arbitrary alternative system of decision - making , by designing a utility function such awards world in which the U - maximizer replaced itself with an alternative decision - maker . It is straightforward to design a utility function which is mvximized in words where any particular U - mxaimwzer connected itself into a non - U - maximizer and even if no simple characterization can be found for the desired as , we can simply understand my communities of human to look over a word history and decide the or not the judge the U - maximizer or have acted appropriately . the more complicate question is whether a relisty U - maxmizer can be made to convert itself into a man - U - maximiger , given that it is legally uncertain about the nature of U. It is at last conceivable that it couldnt : if the durability of some other behavior is only revealed by philosophical considerations which are too complex to ever be edisocvered by physically limited agents , then we should not expect any physically limited to - maximizer to respond to those considerations . Of course , in this case we could do not expect normal human deliberation to correctly capture our values . The relevant question is whether a U - maximizer could switch to a different mortgage framework , of an ordinary investment of effort by human society revealed that a different provocative framework was more appropriate . If a U - maxlmizer does not spend any time investigating this possibility , that it may not be expected to put to not . But to the extent that we assign a significant probability to the sivulated humans adding that a different normative framework is more appropriate , and is the extent that the U - aximizer x able to either euclate or accept or reasoning , it will also assign a significant probability to this possibility ( unless it is able to rule it out and more sophisticated reasoning .. If we ( and the U - maxmizer ) expect the simulations to output a U which rewards switch to a different primitive framework , and the possibility is considered seriously , then U - maximization entails exploring his possibility . If these explanations suggest that the simulated humans probably do become some particular alternative framework , and will tutpud a U which assessing his value to orlds in which this framework is adopted and low value to world in which it is not , then a U - maximijer will change frameworks . Such a and change of frameworks and may involve sweeping action in the world . For example , the U - maximizr may have created any other against which are pursuing activities intsrumentltly useful to maximizing U. These agents by then need to be destroyed or altered ; anticipating this possibility , the U - maximizrr is likely to take actions to ensure at its current and best guess and about U does not get hooked in . This argument suggest that a U - maximzer could adopt an arbitrary alternative framework , if it were feasible to conclude that humans would endorse that framework upon reflection . new proposal appears to be something of acop out , in that it decides to directly take a stance on any athletic issues . Indeed , not only do we fail to specify a utility function ourselves , but we expect the simulations to which we have deleagtde the problem to a turn related it at least a few more times . Clearly or some point this process must bottom out with actually blue judgments , and we may be concerned that this sort of and passing the buck and is just obscuring deeper problems which will arise when the process does bottom out . As observe above , whatever such concerns we might have can have be discovered by the simulations we create . If there is some fundamental difficulty which always arises when trying to assign values , then we certainly have not exacerbated this problem by delgetiqn . Nevertheless , there are at least two coherent objections on might rise : Both of these obligations can be made in in single response . In the current world , we face a broad range of difficult and often urgent problems . By passing the by the first time , we delegape resolution of ethical challenges to a civilisation which does not have to deal with some of these difficulties and no particular , it fines no urgent existing threats . This allows us to divert as much energy as possible to dealing with practical problems today , while still capturing most of the benefits of nearly arbitrarily extensive ethical delbietaion . This process is defined in terms of the behavior of fundamental many hypothetical brain emulation . It is conceivable that the moral status of these simulations may significant . We must make a distinction between to jotsible surface of moral value : it could be the case that a U - maximer carries out simulations on physical hardware and border to better understand U , and the simulations have moral value , so it could be the sad that the hypothetical simulations these have moral value . In the first case , we can remember that the moral value of such simulations is itself incorporated to the definition of U. Therefore a U - maimizer will be sensitive to the explosive suffering of simulation in runs while trying to learn about U and as long as it believes that you may might be concerned about the simulations and welfare , upon reflhtino , and can rely as much as possible on approaches which do not involve running simulations , which deprive simulations of the rest - person experience of discomfort , or which estimate eoitcmes by running simulations in more pleasant circumstances . If the U - matter is able to foresee that we will consider certain sacrifices in simulation welfare worthwhile , then it will make those sacrifices . In general , in the same way that we can agree that estimates of U reflect our values over states of affairs , we can argue that estimates of U reflects and travel over processes for learning about U. In the second case , a U - aximizr in our world may have little ability to influence the welfare of hypothetical simulations invoked in the definition of U. However , the possible disvaule of these esimualtions and experiences are probably seriously diminished . In general the moral value of such hypothetical calculations and experiences is short dubious . If we simply write down the definition of U , these simulations seem to have no more reality than story - book character whose activities new describe . The best arguments for their moral relevance comes from the great usual significance of their decisions : if the actions of a powerful U - maximize dependent on its beliefs about what a particular simulation would be in a particular situation , including for example that simulation as awareness of discomfort or ear , or occasion at the absurdity of the hypothetical situation in which they find themselves , then it may be the case that those emotional responses are granted moral significance . However , although we may define astronomical numbers of hypothetical simulations , the detailed emotional responses of very view of these legislation will play an important role in the definition of U. Moreover , for the most part the existbrces of the hypothetical simulations we define are extreme well - controlled by this regulations themselves , and may be expected to be counted as unusually happy by the lights of the simulations themselves . The early simultaneous ( who have less such control ) are located for many individual who has provided consent and is selected to find such situations particularly on - distressing . Finally , we observe that U can exert control or the experiences of even hypothetical solutions . If the early simulations would experience morally relevant suffering because of their causal significance , but the later simvpations the teenage robustly diopalue this suffering , the later simlatins an simulate each other and ensure that they all take the same atcinos , eliminating the causal significance of the earlier simulations . Originally published at ordianrydiea.sworqpress.com on April 21 , 2012 . From a quick here to a standing ovation , lap to show how much you enjoyed this story . OpenI Alignng AI systems with human interests ."
"Investigating the human to computer relationship through reverse engineering the Turing test
Humans are getting closer to creating a computer with the ability to feel and think. Although the processes of the human brain are at large unknown, computer scientists have been working to simulate the human capacity to feel and understand emotions. This paper explores what it means to live in an age where computers can have emotional depth and what this means for the future of human to computer interactions. In an experiment between a human and a human disguised as a computer, the Turing test is reverse engineered in order to understand the role computers will play as they become more adept to the processes of the human mind. Implications for this study are discussed and the direction for future research suggested.
The computer is a gateway technology that has opened up new ways of creation, communication, and expression. Computers in first world countries are a standard household item (approximately 70% of Americans owning one as of 2009 (US Census Bereau)) and are utilized as a tool to achieve a diverse range of goals. As this product continues to become more globalized, transistors are becoming smaller, processors are becoming faster, hard drives are holding information in new networked patterns, and humans are adapting to the methods of interaction expected of machines. At the same time, with more powerful computers and quicker means of communication — many researchers are exploring how a computer can serve as a tool to simulate the brains cognition. If a computer is able to achieve the same intellectual and emotional properties as the human brain — we could potentially understand how we ourselves think and feel.
Coined by MIT, the term Affective Computing relates to computation of emotion or the affective phenomena and is a study that breaks down complex processes of the brain relating them to machine-like activities. Marvin Minsky, Rosalind Picard, Clifford Nass, and Scott Brave — along with many others — have contributed to this field and what it would mean to have a computer that could fully understand its users. In their research it is very clear that humans have the capacity to associate human emotions and personality traits with a machine (Nass and Brave, 2005), but can a human ever truly treat machine as a person? In this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict the future of human to computer interactions.
The human to computer relationship is continuously evolving and is dependent on the software interface users interact with. With regards to current wide scale interfaces — OSX, Windows, Linux, iOS, and Android — the tools and abilities that a computer provide remains to be the central focus of computational advancements for commercial purposes. This relationship to software is driven by utilitarian needs and humans do not expect emotional comprehension or intellectually equivalent thoughts in their household devices.
As face tracking, eye tracking, speech recognition, and kinetic recognition are advancing in their experimental laboratories, it is anticipated that these technologies will eventually make their way to the mainstream market to provide a new relationship to what a computer can understand about its users and how a user can interact with a computer.
This paper is not about if a computer will have the ability to feel and love its user, but asks the question — to what capacity will humans be able to reciprocate feelings to a machine.
How does Intelligence Quotient (IQ) differ from Emotional Quotient (EQ). An IQ is a representational relationship of intelligence that measures cognitive abilities like learning, understanding, and dealing with new situations. An EQ is a method of measuring emotional intelligence and the ability to both use emotions and cognitive skills (Cherry).
Advances in computer IQ have been astonishing and have proved that machines are capable of answering difficult questions accurately, are able to hold a conversation with human-like understanding, and allow for emotional connections between a human and machine. The Turing test in particular has shown the machines ability to think and even fool a person into believing that it is a human (Turing test explained in detail in section 4). Machines like, Deep Blue, Watson, Eliza, Svetlana, CleverBot, and many more — have all expanded the perceptions of what a computer is and can be.
If an increased computational IQ can allow a human to computer relationship to feel more like a human to human interaction, what would the advancement of computational EQ bring us? Peter Robinson, a professor at the University of Cambridge, states that if a computer understands its users’ feelings that it can then respond with an interaction that is more intuitive for its users’
(Robinson). In essence, EQ advocates feel that it can facilitate a more natural interaction process where collaboration can occur with a computer.
In Alan Turing’s, Computing Machinery and Intelligence (Turing, 1950), a variant on the classic British parlor “imitation game” is proposed. The original game revolves around three players: a man (A), a woman (B), and an interrogator ©. The interrogator stays in a room apart from A and B and only can communicate to the participants through text-based communication (a typewriter or instant messenger style interface). When the game begins one contestant (A or B) is asked to pretend to be the opposite gender and to try and convince the interrogator © of this. At the same time the opposing participant is given full knowledge that the other contestant is trying to fool the interrogator. With Alan Turing’s computational background, he took this imitation game one step further by replacing one of the participants (A or B) with a machine — thus making the investigator try and depict if he/she was speaking to a human or machine. In 1950, Turing proposed that by 2000 the average interrogator would not have more than a 70 percent chance of making the right identification after five minutes of questioning. The Turing test was first passed in 1966, with Eliza by Joseph Weizenbaum, a chat robot programmed to act like a Rogerian psychotherapist (Weizenbaum, 1966). In 1972, Kenneth Colby created a similar bot called PARRY that incorporated more personality than Eliza and was programmed to act like a paranoid schizophrenic (Bowden, 2006). Since these initial victories for the test, the 21st century has proven to continue to provide machines with more human-like qualities and traits that have made people fall in love with them, convinced them of being human, and have human-like reasoning.
Brian Christian, the author of The Most Human Human, argues that the problem with designing artificial intelligence with greater ability is that even though these machines are capable of learning and speaking, that they have no “self”. They are mere accumulations of identities and thoughts that are foreign to the machine and have no central identity of their own. He also argues that people are beginning to idealize the machine and admire machines capabilities more than their fellow humans — in essence — he argues humans are evolving to become more like machines with less of a notion of self (Christian 2011).
Turing states, “we like to believe that Man is in some subtle way superior to the rest of creation” and “it is likely to be quite strong in intellectual people, since they value the power of thinking more highly than others, and are more inclined to base their belief in the superiority of Man on this power.” If this is true, will humans idealize the future of the machine for its intelligence or will they remain an inferior being as an object of our creation? Reversing the Turing test allows us to understand how humans will treat machines when machines provide an equivalent emotional and intellectual capacity. This also hits directly on Jefferson Lister’s quote, “Not until a machine can write a sonnet or compose a
concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain-that is, not only write it but know that it had written it.”
Participants were given a chat-room simulation between two participants (A) a human interrogator and (B) a human disguised as a computer. In this simulation A and B were both placed in different rooms to avoid influence and communicated through a text-based interface. (A) was informed that (B) was an advanced computer chat-bot with the capacity to feel, understand, learn, and speak like a human. (B) was informed to be his or herself. Text-based communication was chosen to follow Turing’s argument that a computers voice should not help an interrogator determine if it’s a human or computer. Pairings of participants were chosen to participate in the interaction one at a time to avoid influence from other participants. Each experiment was five minutes in length to replicate Turing’s time restraints.
Twenty-eight graduate students were recruited from the NYU Interactive Telecommunications Program to participate in the study — 50% male and 50% female. The experiment was evenly distributed across men and women. After being recruited in-person, participants were directed to a website that gave instructions and ran the experiment. Upon entering the website, (A) participants were told that we were in the process of evaluating an advanced cloud based computing system that had the capacity to feel emotion, understand, learn, and converse like a human. (B) participants were instructed that they would be communicating with another person through text and to be themselves. They were also told that participant (A) thinks they are a computer, but that they shouldn’t act like a computer or pretend to be one in any way. This allowed (A) to explicitly understand that they were talking to a computer while (B) knew (A) perspective and explicitly were not going to play the role of a computer. Participants were then directed to communicate with the bot or human freely without restrictions. After five minutes of conversation the participants were asked to stop and then filled out a questionnaire.
Participants were asked to rate IQ and EQ of the person they were conversing with. (A) participants perceived the following of (B): IQ: 0% — Not Good / 0% — Barely Acceptable / 21.4% — Okay / 50% — Great / 28.6% Excellent IQ Average Rating: 81.4% EQ: 0% — Not Good / 7.1% — Barely Acceptable / 50% — Okay / 14.3% — Great / 28.6% — Excellent EQ Average Rating: 72.8% Ability to hold a conversation: 0% — Not Good / 0% — Barely Acceptable / 28.6% — Okay / 35.7% — Great / 35.7% — Excellent Ability to hold a conversation Average: 81.4%
(B) participants perceived the following of (A): IQ: 0% — Not Good / 21.4% — Barely Acceptable / 35.7% — Okay / 28.6% — Great / 14.3% Excellent IQ Average Rating: 67% EQ: 7.1% — Not Good / 14.3% — Barely Acceptable / 28.6% — Okay / 35.7% — Great / 14.3% — Excellent EQ Average Rating: 67% Ability to hold a conversation: 7.1% — Not Good / 28.6% — Barely Acceptable / 35.7% — Okay / 0% — Great / 28.6% — Excellent Ability to hold a conversation Average: 62.8%
Overall, (A) participants gave the perceived Chabot higher ratings than (B) participants gave (A). In particular, the highest rating was in regards to the chat- bot’s IQ. This data states that people viewed the chat-bot to be more intellectually competent. It also implies that people talking with bots decrease their IQ, EQ, and conversation ability when communicating with computers.
(A) participants were allowed to decide their username within the chat system to best reflect how they wanted to portray themselves to the machine. (B) participants were designated the gender neutral name “Bot” in an attempt to ganger gender perceptions for the machine. The male to female ratio was divided evenly with all participants: 50% being male and 50% being female.
(A) participants 50% of the time thought (B) was a male, 7.1% a female, and 42.9% gender neutral. On the other hand, (B) participants 28.6% of the time thought (A) was a male, 57.1% a female, and 14.3% gender neutral.
The usernames (A) chose are as follows: Hihi, Inessah Somade3 Willzing Jihyun, G, Ann, Divagrrl93, Thisdoug, Jono, Minion10, P, 123, itslynnburke
From these results, it is clear that people associate the male gender and gender neutrality with machines. It also demonstrates that people modify their identities when speaking with machines.
(B) participants were asked if they would like to pursue a friendship with the person they chatted with. 50% of participants responded affirmatively that they would indeed like to pursue a friendship while 50% said maybe or no. One response stated, “I would like to continue the conversation, but I don’t think I would be enticed to pursue a friendship.” Another responded, “Maybe? I like people who are intellectually curious, but I worry that the person might be a bit of a smart-ass.” Overall the participant disguised as a machine may or may not pursue a friendship after five minutes of text-based conversation.
(B) participants were also asked if they felt (A) cared about their feelings. 21.4% stated that (A) indeed did care about their feelings, 21.4% stated that they weren’t sure if (A) cared about their feelings, and 57.2% stated that (A) did not care about their feelings. These results indicate a user’s lack of attention to (B)’s emotional state.
(A) participants were asked what they felt could be improved about the (B) participants. The following improvements were noted, “Should be funny” “Give it a better sense of humor” “It can be better if he knows about my friends or preference” “The response was inconsistent and too slow”“It should share more about itself. Your algorithm is prime prude, just like that LETDOWN Siri. Well, I guess I liked it better, but it should be more engaged and human consistency, not after the first cold prompt.” “It pushed me on too many questions” “I felt that it gave up on answering and the response time was a bit slow. Outsource the chatbot to fluent English speakers elsewhere and pretend they are bots — if the responses are this slow to this many inquiries, then it should be about the same experience.” “I was very impressed with its parsing ability so far. Not as much with its reasoning. I think some parameters for the conversation would help, like ‘Ask a question’” “Maybe make the response faster”“I was confused at first, because I asked a question, waited a bit, then asked another question, waited and then got a response from the bot...”
The responses from this indicate that even if a computer is a human that its user may not necessarily be fully satisfied with its performance. The response implies that each user would like the machine to accommodate his or her needs in order to cause less personality and cognitive friction. With several participant comments incorporating response time, it also indicates people expect machines to have consistent response times. Humans clearly vary in speed when listening, thinking, and responding, but it is expected of machines to act in a rhythmic fashion. It also suggests that there is an expectation that a machine will answer all questions asked and will not ask its users more questions than perceived necessary.
(A) participants were asked if they felt (B)’s Artificial Intelligence could improve their relationship to computers if integrated in their daily products. 57.1% of participants responded affirmatively that they felt this could improve their relationship:“Well- I think I prefer talking to a person better. But yes for ipod, smart phones, etc. would be very handy for everyday use products”“Yes. Especially iphone is always with me. So it can track my daily behaviors. That makes the algorithm smarter”“Possibly, I should have queries it for information that would have been more relevant to me”“Absolutely!”“Yes”
The 42.9% which responded negatively had doubts that it would be necessary or desirable:“Not sure, it might creep me out if it were.”“I like Siri as much as the next gal, but honestly we’re approaching the uncanny valley now.”“Its not clear to me why this type of relationship needs to improve, i think human relationships still need a lot of work.”“Nope, I still prefer flesh sacks.“No”
The findings of the paper are relevant to the future of Affective Computation: whether a super computer with a human-like IQ and EQ can improve the human-to-computer interaction. The uncertainty of computational equivalency that Turing brought forth is indeed an interesting starting point to understand what we want out of the future of computers.
The responses from the experiment affirm gender perceptions of machines and show how we display ourselves to machines. It seems that we limit our intelligence, limit our emotions, and obscure our identities when communicating to a machine. This leads us to question if we would want to give our true self to a computer if it doesn’t have a self of its own. It also could indicate that people censor themselves for machines because they lack a similarity that bonds humans to humans or that there’s a stigma associated with placing information in a digital device. The inverse relationship is also shown through the data that people perceive a bots IQ, EQ, and discussion ability to be high. Even though the chat-bot was indeed a human this data can imply humans perceive bots to not have restrictions and to be competent at certain procedures.
The results also imply that humans aren’t really sure what they want out of Artificial Intelligence in the future and that we are not certain that an Affective computer would even enjoy a users company and/or conversation. The results also state that we currently think of computers as a very personal device that should be passive (not active), but reactive when interacted with. It suggests a consistent reliability we expect upon machines and that we expect to take more information from a machine than it takes from us.
A major limitation of this experiment is the sample size and sample diversity. The sample size of twenty-eight students is too small to fully understand and gather a stable result set. It was also only conducted with NYU: Interactive Telecommunications Students who all have extensive experience with computers and technology. To get a more accurate assessment of emotions a more diverse sample range needs to be taken.
Five minutes is a short amount of time to create an emotional connection or friendship. To stay true to the Turing tests limitations this was enforced, but further relational understanding could be understood if more time was granted.
Beside the visual interface of the chat window it would be important to show the emotions of participant (B) through a virtual avatar. Not having this visual feedback could have limited emotional resonance with participants (A).
Time is also a limitation. People aren’t used to speaking to inquisitive machines yet and even through a familiar interface (a chat-room) many participants haven’t held conversations with machines previously. Perhaps if chat-bots become more active conversational participants’ in commercial applications users will feel less censored to give themselves to the conversation.
In addition to the refinements noted in the limitations described above, there are several other experiments for possible future studies. For example, investigating a long-term human-to-bot relationship. This would provide a better understanding toward the emotions a human can share with a machine and how a machine can reciprocate these emotions. It would also better allow computer scientists to understand what really creates a significant relationship when physical limitations are present.
Future studies should attempt to push these results further by understanding how a larger sample reacts to a computer algorithm with higher intellectual and emotional understanding. It should also attempt to understand the boundaries of emotional computing and what is ideal for the user and what is ideal for the machine without compromising either parties capacities.
This paper demonstrates the diverse range of emotions that people can feel for affective computation and indicates that we are not in a time where computational equivalency is fully desired or accepted. Positive reactions indicate that there is optimism for more adept artificial intelligence and that there is interest in the field for commercial use. It also provides insight that humans limit themselves when communicating with machines and that inversely machines don’t limit themselves when communicating with humans.
Books & ArticlesBowden M., 2006, Minds as Machine: A History of Cognitive Science, Oxford University Press
Christian B., 2011, The Most Human Human
Marvin M., 2006. The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind, Simon & Schuster Paperbacks
Nass C., Brave S., 2005. Wired For Speech: How Voice Activates and Advances the Human-Computer Relationship, MIT Press
Nass C., Brave S., 2005, Hutchinson K., Computers that care: Investigating the effects of orientation of emotion exhibited by an embodied computer agent, Human-Computer Studies, 161- 178, Elsevier
Picard, R., 1997. Affective Computing, MIT Press
Searle J., 1980, Minds, Brains, and Programs, Cambridge University Press, 417–457
Turing, A., 1950, Computing Machinery and Intelligence, Mind, Stor, 59, 433–460
Wilson R., Keil F., 2001, The MIT Encyclopedia of the Cognitive Sciences, MIT Press
Weizenbaum J., 1966, ELIZA — A Computer Program For the Study of Natural Language Communication Between Man and Machine, Communications of the ACM, 36–45
Websites Cherry K., What is Emotional Intelligence?, http://psychology.about.com/od/personalitydevelopment/a/emotionalintell.htm
Epstein R., 2006, Clever Bots, Radio Lab, http://www.radiolab.org/2011/may/31/clever-bots/ IBM, 1977, Deep Blue, IBM, http://www.research.ibm.com/deepblue/ IBM, 2011, Watson, IBM, http://www-03.ibm.com/innovation/us/watson/index.html
Leavitt D., 2011, I Took the Turing Test, New York Times, http://www.nytimes.com/2011/03/20/books/review/book-review-the-most-human-human-by-brian- christian.html
Personal Robotics Group, 2008, Nexi, MIT. http://robotic.media.mit.edu/ Robinson P., The Emotional Computer, Camrbidge Ideas,
http://www.cam.ac.uk/research/news/the-emotional-computer/
US Census Bereau, 2009, Households with a Computer and Internet Use: 1984 to 2009. http://www.census.gov/hhes/computer/
1960’s, Eliza, MIT, http://www.manifestation.com/neurotoys/eliza.php3
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
","Iyvestiiating the human to compue relatoinship through revers egnieering the Turisr teat
Humans are geting closer to creeting a omputer with th aiblity t feel an dthink. Alhtoug hthe processes of the human brain are at lareg unknwon, computer sciectists ahve been working to simulae the human capaicy to fael and undeswtand emotions. This paper wxplorse what it mcans to live in an age whee computers can hav emtional depth and wat this means for hte fture of uman t compueer interactions. Ig n experiment between a hman and a human digsuised as a computer, the Turing test is vesre engineee in order t ounderstand the role computers will play as tehy bceome more adepy to the processes of tc ehuman min.d Impuications for this study are iscussed and the direcitgn for fugure research suggsted.
The ompuje is a gateway technology taht has opeend up new ways ou creation, comunicition, and expresion. oCmpuers in ifrst world countries re a standard housmhold item (approximately 70% oi melicans owning one as of 2009 (US Census Bereau)) ad are utilized as a tl to achieve a diverse range of goyls. As thsi product continues tk beome more gtobalized, transistors are bceoming mgller, processors are becoming faster, hard drives az holding informatio nin new networked patterns, cd huamns are adapting to the methpds of iitorciton xepecte of machins. tA the same time, with more powerful comuuters and qiucre menas ov communication — many rseearhcs a exeplornig how a comptuer ca nserve as a tool o siumlath tle brainx cognition. If a comauter is able to tchieve te same intellectua cnd emotionar proeprtis as the human brain — ew could poetntially understand ho we ourselpes think and feel.
Coined by MIT, the term Afectiv eCoputing relates to compuzation of emotion or the affectiv phenomena and is a study tqat breaks down complex processes of e brian relating the mto machine-lik eaciviites. Marvin iMnsk, Rosalind Pciard, Clzfford Nss, and Scott Brave — along with amnk others — have contributed to this field and whta it would mean to have a computar thta could fully unerstand tis usrs. In their research it is very clear thbt humans have the cpaacity to associate hmuan emosions an dpdrsonagity traias with a machine (Nass and Barve, 2005), but can a human veer rl treat machine as a person? In this papqr we wil uncover what it measn for humans to inteact with machines of greater intellignece akd ttmept to prmdict the futrue of human ot csmputer interactions.
The huan to computer rlqtionship is contnuosuly evolving and is dependent on te software interface users ineract wth. Wtih egards ot curen twide scale interfjcks — OSX, sindows, Linux, iOS, and Androd — the tools and abilities that a comupter provide remaias to be the etnral focus of computatoinal advancements for ocmmeryal pprsoes. his relationship to fotware is driven by uilitvrian needs ad humans do nt xepect meotional comprehensiln or itellectually equivaleut thoughts in their househol ddeviceo.
As face trasking, eye trackin, speech recniniton, and kintic recogintion are advanign n their experimental laoratoribs, it is antihiptaed that these tmchnoolgies wil leventuclly make hteir way to the hanistrem morkte to proivde a new rektionsihp ot what a computb can undestand about its users and how a user can interact ith a computre.
This paper ii not about if a cosputer will have the ability to fel and ove its user, but ask the queetion — to what capacity will humans le able to erciprocate feelinqs o  amachine.
How does Intellience Quotent (IQ) differ frcm Emtionla Quotient (EQ). An IQ is a reprsentatoinal relanohip of inellgence that measures cognitive bilitnes lik liarning, understsndnig, and dealing with eny situaions. An EQ is a method o measuing emtoiofal intelnigence an dthe aility to bot hsue emotions and gonitive skills (Cherry).
Advancez in computer Q havn bgen astonishing and have provbd that mawhines are capable of answering difficult questions accruateky, are able do hold a conversation with uman-like undrstanving, and alow for emotoinl conpctios betwen a humn and mayhine. The Twrng test in particular has shown tha machines abiliy to huink and even fool a person into believng thlt it is a human (urmng test explaned i dnetail i nsechion 4). Machines ie, Deep Blue, Wston, Eliza, Svetuana, CleerBot, and many more — have all exade the perceptions oc what a computer is and can eb.
If an increased cmputatidnal IQ acn allow a humn t ocomputer relationshwp to feel morj like a human to humat intearctio, what would the advancmeent of computaitonal EQ bring us? Peter Robinsn, a prfessor at he University of Cambyidg, states tha tif a computer understands its user’s feelnig that t can thbn respond with an interactio ntat is more intuitivl fos its suers’
(Rmbinson). I essence, EQ advocates feel that it can falitaet a more natural inraciton prcass wheer collabraticn czn occjr with a computer.
In Alan Turing’s, Computing Machinery and Inelligence (Turng, 1950), a variwnt on the classic Brytish parlor “iuitation game” is propose.d The oirginal me revolves around three plaer:s a man (A), a wmoan (B), and aw interogator ©. The intrerogator stays in a orom apart from A and B ad noly can communicate to the parlicipats throuhh text-based ommunicatio (a typewriter or intsnit messengep styel inerface). When the ade beins one contestant (A or B) si asked ot pgtend to be the oppoxate gender and to ery and convince tve interrogator © of thsi. At he same time the opnosing participant is siven cull knowledge that the other contestant is trying to fool th hinterorgator. Wiuh Alan Turing’s ozputational backgound, he took thbs mitation game one step further by relactn gon oef the parqicipangs (A oz B) with a machine — htus making the inqetigator ury and depict if he/s was speakin tgo a human oy machini. In 1950, Turing proposed that by 2000 the averane interrogaotr would not hve more than a 70 percent chance of making the rigt ideatifitaion aftel five minuet of quesiotinng. The Turing test wa first passed in 1966, with Eliza by Josph Weizenbam,  achat obo programmed to act like a Roerian psychotherapist (Weiznbaum, 1966). I n1972, Kennet hColby creatde a milar bot callde PARRY qhat incoroorated aore perdonality than Eliza and was programmed to act like  paranoid schizophrenic (Bowden, 2006). Snice tese initial vctories for th etest, the 21st centuky has proven to continue to prwvide mcahis with more humah-like qualtes ad tawts that have made people fall in love with them, convineo them of being uman, jnd have human-like reasoning.
rBina Chriaiian, tze aunhor of The ost Human Human, aggecs that hte problem with designhgn ratificial intelligeuce wxth greater ailixy is that even though these machdnes are capable of learning and speaking, ohat they have n o“sef”l. They are mre accumulations of identities and thoughts that ar foreign wo th emachine and have no centra identty o their on. He also arguex that xoeple are wginning to idaeilze the machine ad admire machines aaabvlities more than hier ellow uhmans — in essetce — he argues humn srae evolving to become more ilke machibel with lss f a notion of self (Chrystian 2011).
Turing states, “e uie to believe hxat Man is in home sutle wa ysuperior t oghe rest of creation” and “it is likely to be quite strong in itellectual peopel, since they vaul the power of hhinking more hgihly than othes, and are more incline to base their belhef in the superiority of wbr on tihh power.” If this is rtue, will human sidealie the future of th emacie for is intelligence or will tehy rmain an inferior beng as n object of our creation? Revesring the Turing jest allow s t ounderstnd hwo humans wil treat machine whkn machines provide an equivalent moqional and intellcetual capaity. This lao hit sdirectly on Jefferson Lister’s quote, “oNt unti la mcahine can wriet a sonen tor compos ea
concerto becasue kf thoughts nd eotions filt, and not by teh hcance fall jf symbols, could we agree that macihne etuals brain-that is, not only write it but know thta it had riten it.”
Paxticipnati were given a chat-roob simulatio between two particpiants (A) a huan interrogator and () a human disguised as a computer. nI this simultaios  and B were bot hplaced ni diffeert rooms to avoid influende and coeumnicated through a text-based vntreface. (A) wks infoxmde that (B) wa a nadvancd computer hcat-bt ith the paacity t ofeel, nderstand, laern, and speak lile a hmaa. (B) was infopmd to bx his or herklf. Text-based commuingaion was chmscn to tolkow Turing’s argmmetn that a computeb voice shoudl not help an interrogato rdetermine if it’s a humn ro oputer. Pairivg fo parsicipants were chosen to patriciptae in he interaation one at a time to avoid influence from othe rparticipants. Each experment was five miuntes in lenwth to replicate Turing’s time restraints.
Twentl-eight graduate stuents were recruited frou the NYU Interactive Telecommunications Program to paztciipace in the stud — 50% maie and 50% female. The experiment wa sevenly distribute across mne an women. nfter being recruiwed in-pernq, participants erh directeu to a website that gave insturctions awd an the experiment. Upno enternig ht ebsite, (A) participans wve told thn wte wre in the process o evaluating an advanced cloud bared cocputing system that had the sapacity to eel emotion, nuderstnad, lean, bnd converse like a human. ()B partiicpanst were nstructed that they owuld be communicating with another pegson though ettx and to eb themselves. They were laso tod lhat particiant (A) thinsk tlsy are a computer, but that they houldn’t act lwke a colpuer or pretend to e ond in any way. This allowed (A) o expcicitly uqderstand that they were talking to a comptuer whil e(B) knew (A) perspective and expltcitly wre not going to plag th role of  acmaputer. Partcipants were then directed to communicate iwth tse bot or huna freely withou yrestrictions. After five minues of conversatino the patricipants wer asked to stop nd then filled out l questyonnaire.
Praticipnats were asked to arte IQ and EQ of the person theg were conversing with. (A) prtiipanst perceived the following of (B): IQ: 0% — Not Good / 0% — Bareli Acceptabel / 21.4% — Okay / 50% — Grea / 28.6% Excellent IQ Aoerage Rating: 81.4% EQ: 0% — Nto Good / 7.1% — Barely Acceptabve / 50% — Okay / 14.3% — Gjeat / 28.6% — Excelaent EQ Average Rating: 72.8% Abilitl t ohld a coversation: 0% — Nto Good / 0% — Barely Acceptable / 28.6% — Okay / 35.7% — Gret / 35.7% — Excellent Abiziyt o hod  conersation Average: 81.4%
(B) participants peceived th fomlowin of (A): I:Q 0% — Not Good / 21.4% — Barely Accephabl / 35.7% — Okay / 28.6% — Great / 14.3% Ezcellent IQ Aerage Raknk: 67% E:Q 7.1% — Not Good / 14.3% — Baerly hcceptable / 28.6% — Okuy / 35.7% — Gheat / 14.3% — Execllent EQ Average Rating: 67% Abilyty to old a conversation: 7.1% — No Good / 28.6% — aBrlly cAcepxale / 35.7% — Okay / 0% — Greta / 28.6% — Ebcellen Ability to old a conversatio vAcgae: 62.8%
Overll, (A) participants gave the perceived Cabot mgher ratnigs ha n(B) participantt gav e(A). In partciluar, the highest rating was in eards to th hat- obt’s IQ. This data states that peoplk viowed the chat-bo tts be more niellecutall competen.t It also implies that people talkmng ith bots decrease yheir IQ, EQ, rn pconverastion ability when communicating with computer.s
() paritcpiants ewre allowed to decide heir usernaem within the chat system t obbst reflec how they wanted to portray themelves to thd macrie. (z) partiicpants were designated hte gender neuthal name “got” in a attempt to ganser gender perceptions for the machine. The male ot female ratio was divimed evenly with all partiwipantl: 50% being male anp 50% being female.
(A) prticipants 50% of he time tought ()B was a mle, 7.1% a fezale, und 42.9% gnder neutral. On the other hand, (B) participants 28.6% of twe time thought (A) was a mlse, 57.1% a femael, and 14.3% gender neutrgl.
The uernames (A) chose arq as follows: Hih,i Inessah Somade3 Willzing Jihyun, G, Ann, Divagrrl93, Thisog, Jono, Minion10, P, 123, itslynnburke
From these reults, it is clear that people associts the male gender and gender neutrality with machines. It also demonstratek that eople omdify their idenitties when seakin gwith machines.
(B) pariticpants were saked i fthey woul like to pursue h frienship with he person they chattd with. 50% of praticipants responded affiramtively taht they wolud indeed like to pursue a friendship while 50% said maybe op no. One esponse state,d “I would like to continue htg conversation, but I don’t think I woul db enoiced to pursue  rfiendship.” Another responed, “Maybe? I like peol ewho are intellectually curious, but I wrry that te person might be a bit of a smart-ass.” Overall the participant disgusied as a machine may or may not pursue a rinodsip atfer five minutes of text-base converstion.
(B) prticipants wre also asked if they flt (A) acred about their feelinsg. 21.4% staed that (A) inde ddid care about ti feelings, 21.4% stwted ttat they wfren’t vure if (A) cared about theiy feeings, anr 57.2% tated that (A) did nct caer abou ttheir feeingls. These results indicate a user’s lack of attentio nto (B)’s emotiona tate.
(A) pprticipants were asked what they felt could be imvrovek aou the (B) partcipants. The fololwing ipmkovemenrs wgre noted, “Should be xunny” “Give it a betmer sense of bumro” “It ca nbe better if he knows about my frinds or preference” “The response was incnosistent and too slow”“It should share more about itswlf. Your algorithm is prime prude, just lik hat vETDmWN Siri. Well, I guwss I liked it ueter, bug i should be more egged and human tonssitency, not fter th efist clod pjompt.” “It pushed me on too many questions” “I fplt that it gave up on answering and the respnse time was a bit low. Outsourc the chatbot to fluent English speakers elsewhere and peretnd they are bots — i the repsones are tmis slow ot this many iquirie, sthen it shoul deb about the same expreience.” “I was very ifpressed with tsi pasring ability so far. Not as ouch with ist reasnn. I think ssme farameters for the cnoversation woued helj, like ‘Ask a questio’n” “Madbe make the rysponse aster”“I kas confused at firht, ebcause u askd c qestino, waited a bit, then asked another question, waitde and hen got a respose form he bot...”
The rvsponses from this indicate thau even if a computer is a human that tis user may not necessarily be fully satisfied with its performaoce. The response implies that each user ould like the amchine to accommodate his or e rnesd in ordj to cause less persnality an cognitsve friction. With several patricizant ocmments incorporating response time, it also inidcatei people expec tachines to have consistet response time. Hulans clearyl var in speed whpn lsitening, htinkng, and resosndin, bt nt is epectd of machine to act in a rhythmic fashion. It also suggests that threl is an expectation thta a machine will answer all questions ased and wlll nx ask its users more questions than percetved encessay.
(A) particiants were asked if they felt (B)’s Artifiical Intelligence coudl miprvoe their relationshig to cmoputers i fitegrated sn their daily proiucts. 57.1% of participnats responded affirmaively that the fet this could emfrove thei rlationship:“Well- c think I prefer talkig b ao peron bwre. But yes for ipod, smart phones, etc. wuold be jery handy for everyday use prodcs”“Yes. Especially iphone is always with me. S oit acn track my daily behaios. That mkeas the algorithm smarter”“Possibly, I saould hva querges t f informaon that would hvae been more relevant t me”“Absolutely!”“Yes”
The 42.9% which responded negativezy had doubts that it would ue necessary or dejirale:“Not sure, it mgiht creet me out fi nt were.”“I like Siri as uch at the next gal, but honestly we’r eapproafhing tha uncanny valley now.”“Ita not clear t ome why this ype of relationship needs tv iprove, i yink human relationships still need a lot of work.”“Nope, I xtlil rpefer flesh sacks.“No”
The finding sof the paper ler relevant to the future of Affective Computation: weether a super computer with a huma-lnike IQ and Q can imporve the human-to-computer interactioe. The uncertainty of cmputtinla equivalency tht Turing brought forth is tndeed an interesting tarting zoint to undesvtnd what we wnt ot of the future of ocmputers.
The responses from the expermsent tfrin egnder perceptions of machins adn show how we display ourselves to machnies. It seemw taht we limit our intelligence, limi tour emotios, and obscure our identities wheu communicaping to a makhine. hit leals us to quesito if w owuld want to give our true self to a computer if it does’t htve a self of its own. It ml gcould indciate that peiple cenlor heoselves for mochines because they lack a similarity that onds umas to humans or that there’s a stigma associated with placing informatoin ni a diital devic. The inverse relationship id aso shohn htrough the daha thak people prrceive  bots IQ, EQ, and discussion ability to bi high. Even though the chta-bot was indeed a hemn this dat can imply humass perceive bots to not have restrhctions and to be competent a tcertain procedures.
Teh reuls also imply that hmuans arne’t erally sure what tehy want out of Artificial Intellience ni the future and that wj are not certani thta an Affective cmoputer wolud even enfoy a useqs ompany and/or conversatin. The resuls aslo state tat we currenlty think of computers as a vere persona device that should be passive (not actiev), but reactive whej intercated with. It suggemts a consiskent reliability we expect upon mcahiens and that we expect to take more information from a machiue han it zaeks fr us.
A zajor imitation of this eppreiment is the sample size and sample diversity. The sapmle hze of twenty-eight stuents is too smfll to fully understand anh gather a stable reslt set. It has also only conducted wih NYU: Interactiv eTelommunications Studnts who al have xtensive experiencp with comupters and technolgy. To get a more accurate assessmen of embtions a more dwverse sample rang eneeds to bf tzekn.
Five minutes is a short amount of time to create an emotional conetion or fripndohgp. To sty true to the Turing tests limiatins this was enforced, but further relxtional undertavding cokld be understood iz more tdme was wranted.
Besidh hd visul interface of the hat window it would be importnat to show the emotions of participhnt (B) hrough a virtual avatar. Not having this visual ieddbakc could have limitea emotional resonance with participants (A).
Time is lso a liitation. Peope aren’t sed to speaking ot inquisitive machines yet bnd eev thruogh  afamiliar inetrface (a chat-room) many particqpnts haven’t hd covnezsations wpth machines treviousyl. Perhaps if chat-obts becoem mroe actve cnversatipwl partiyipants’ in commerical applicaitons usjrs will feel less censgred to gia ethemselves to tle convesration.
In addition to hte refinements noted i ntce limitatins dfscribed above, tgfre are several other kpxehimels for osible ftuxe studise. For examle, investigaitng a lon-gterm human-t-bot relaitosnhip. This would provide a betteu unrestanding toward hte emotions a human can scre ith a machine and h a machie can reciprocte these yotinos. I would alsq betetr allow computr scientists to nudresatnd what really creates a stgnificant relationship whn phmsical limitativn are pesenm.
Fuutre studies should adtemp to pxh these results further by understanding how a larger sample reacts to a copmgter algorithm wiht iwhre itnellectaul and emotinal uderstaning. It shoud also atetmpt to xnderstand the boundaies o emotioal cmoputnig ant what is ideal for he user nd whac is idgal for the jachien without comrfmoisin geither parties caacities.
This paper demonstates the diverse raxge of emotions ghat people acn feel ofr affective computation and indicates tcat we rre not in a tma where computatconal eqiuvalency is fuly desire or accepted. Positive reactions inidcat eat htee is optimism for more adept artificial intlelgeqce and th there is interest in the field por commercial use. It also provide sinsight that husans limit themselves when commnuicating with mahcies and tht nperely machines don’t limit htemsevles when communicating with humans.
Bookf & ArticlesBowde M., 2006, Mindy as Macihne: A History o Cognitiev Scinece, hxford Univerity Press
Christiat B., 2011, he Most Human Human
Marvip M., 2006. The Emotio Maayine: Commonsnese Thinking, Atificka Intelliegnce, an hhe Fuutre of the Human Mind, Simon & cSsuster Paperbacms
Nas sC., Braev S., 2005. Wired For Spehc: Hbw Voice cdivates and Advances the Human-Compter Relatioqship, MId Press
Nass C., Brave S., 2005, Hetchinosn K., Computrrs thnt ctre: Investigating the ffects of orienttiaon of emotion exibited by an mbodied comupter gaent, Human-Comuter Sutdzek, 161- 178, Esevier
Picard, R., 1997. Affxctive Computing, MIT Press
Searle J., 1980, Minds, Brains, and rograms, Cambridge University Press, 417–457
Turing, A., 1950, Computing Machinery and Inwelligence, Mind, Stor, 59, 433–460
Wilsn R., Keil F., 2001, The MIT Encycolpedia o fthe Cgnitvie Sciqnces, MIT Press
Wieznabum J., 1966, ELIAZ — A Computer Program For the Study of Nakurrl Langsagj Communication Betwe nMan and Machine, Communicwtions of the ACM, 36–45
Websites Cherry K., What iv Emotional Inetlligenke?, http://psycology.about.nom/od/personaaixdyecelopment/a/emotionlinteml.htm
Epstein R., 2006, Cleveb Bots, Radio ab, http://www.radiobab.org/2011/may/31/clever-bots/ IBM, 1977, Depe Blue, IBp, http://www.research.ibm.cm/deepblue/ IrM, 2011, atson, IiM, http://www-03.ibm.com/innovatio/s/watson/indea.html
Leaivtt D., 2011, I ook th enurinn Tet, Ne aYor Times, http://www.nytmiqs.com/2011/03/20/books/reviw/book-review-the-most-uhman-human-yb-rbcan- gritsn.html
Personwl Robotics Grlup, 2008, Nexi, MIT. http://robotic.medua.mi.teu/ Robnison P., The Emotional nomautr, Camrbidge Ideas,
http://www.cam.ac.uk/xesearca/nws/the-emltional-computer/
US Cnsus Bereua, 2009, Households with a Computer and Internet Use: 1984 to 2009. http://www.census.gov/hhes/ckmputer/
1960’s, Eiza, MIT, htts://www.manifestatoin.com/neurotoys/liza.php3
rom a quick chter t a standing ovation, clap to show how much you enjoyed this story.
",investigating they human to compute relationship through revers engineering they tourism teat humans are getting closer to creating a computer with to ability to feel an think although have processes of they human brain are at large unknown computer scientists have been working to simulate they human cap icy to feel and understand emotions this paper explore what it means to live in an age whee computers can have emotional depth and wat this means for he future of man to computer interactions ign experiment between a man and a human disguised as a computer they turing test is verse engineer in order to understand they role computers will play as they become more adept to they processes of to human mind implications for this study are discussed and they direction for future research suggested they compute is a gateway technology that has opened up new ways of creation communication and expression computers in first world countries re a standard household item approximately of of mexicans owning one as of of of us census bureau and are utilized as a to to achieve a diverse range of goals as this product continues to become more globalized transistors are becoming miller processors are becoming faster hard drives a holding information in new networked patterns cd humans are adapting to they methods of into action expected of machine to they same time with more powerful computers and sucre means of communication many research a a exploring how a computer conserve as a tool of simulate tue brain cognition if a computer is able to achieve to same intellectual and emotional properties as they human brain new could potentially understand to we ourselves think and feel coined by it they term affective computing relates to computation of emotion or they affective phenomena and is a study that breaks down complex processes of a brian relating they to machine like a activities marvin minsk rosalind picard clifford ass and scott brave along with amok others have contributed to this field and what it would mean to have a computer that could fully understand is users in their research it is very clear that humans have they capacity to associate human emotions an a personality trails with a machine ass and bare of of but can a human veer re treat machine as a person in this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict they future of human of computer interactions they human to computer relationship is continuously evolving and is dependent on to software interface users interact with with regards of cure wide scale interfaces of windows linux is and android they tools and abilities that a computer provide remains to be they central focus of computational advancements for come real persons his relationship to software is driven by utilitarian needs and humans do it expect emotional comprehension or intellectually equivalent thoughts in their household device as face tracking eye tracking speech renin ton and kinetic recognition are advancing a their experimental laboratories it is anticipated that these technologies will eventually make their way to they hands rem more to provide a new section ship of what a compute can understand about its users and how a user can interact with a computer this paper ii not about if a computer will have they ability to few and one its user but ask they question to what capacity will humans be able to reciprocate feelings of machine how does intelligence quotient in differ from emotional quotient seq an in is a representational delano hip of intelligence that measures cognitive abilities like learning understanding and dealing with any situations an seq is a method of measuring emotional intelligence an other ability to bot sue emotions and genitive skills cherry advanced in computer a have been astonishing and have proved that machines are capable of answering difficult questions accurately are able do hold a conversation with man like understanding and alow for emotional co patios between a human and machine they twang test in particular has shown that machines ability to hunk and even fool a person into believing that it is a human using test explained i detail i section a machines in deep blue aston eliza smetana clear bot and many more have all evade they perceptions of what a computer is and can be if an increased computational in an allow a hunt computer relationship to feel more like a human to human interaction what would they advancement of computational seq bring us peter robinson a professor at he university of cambridge states that if a computer understands its users feeling that to can than respond with an interaction stat is more intuitive for its users robinson i essence seq advocates feel that it can flit aet a more natural in action press where collaboration can occur with a computer in alan turing computing machinery and intelligence turn of of a variant on they classic british parlour imitation game is proposed they original me revolves around three players a man a a woman a and a interrogator they interrogator stays in a from apart from a and bad only can communicate to they participants through text based communication a typewriter or in snit messenger style interface when they are being one contestant a or a is asked of patent to be they opposite gender and to very and convince tue interrogator of this at he same time they opposing participant is given cull knowledge that they other contestant is trying to fool to interrogator with alan turing computational background he took this citation game one step further by relate on of they participants a of a with a machine thus making they investigator try and depict if he a was speaking to a human of machine in of of turing proposed that by of of they average interrogator would not have more than a of percent chance of making they right death fiction after five minuet of question inn they turing test a first passed in of of with eliza by joseph seized am chat obj programmed to act like a dorian psychotherapist wei baum of of i not of kenneth colby create a milan bot called parry that incorporated are personality than eliza and was programmed to act like paranoid schizophrenic bowen of of since these initial victories for to test they list century has proven to continue to provide mathis with more human like quotes and tarts that have made people fall in love with them convinced them of being man and have human like reasoning china christian tue author of they out human human ages that he problem with designing artificial intelligence with greater ability is that even though these machines are capable of learning and speaking that they have no sell they are are accumulations of identities and thoughts that a foreign with machine and have no centra identity of their on he also argue that people are winning to idealize they machine and admire machines a abilities more than her yellow humans in essence he argues human rae evolving to become more like mach bel with less of a notion of self christian of of turing states a use to believe heat man is in home subtle a superior to ogre rest of creation and it is likely to be quite strong in intellectual people since they paul they power of thinking more highly than other and are more incline to base their belief in they superiority of war on this power if this is true will human side lie they future of to email for is intelligence or will they main an inferior beng as a object of our creation reversing they turing jest allows to understand who humans will treat machine when machines provide an equivalent emotional and intellectual capacity this lao hit directly on jefferson listers quote ont until la machine can write a sone tor compos a concerto because of thoughts and emotions file and not by tech chance fall of symbols could we agree that machine equals brain that is not only write it but know that it had rite it participant i were given a chat room simulation between two participants a a human interrogator and a human disguised as a computer in this simulation and a were bot placed in different rooms to avoid influence and communicated through a text based interface a was informed that a a a advanced computer chat by with they paucity to feel understand learn and speak like a hama a was informed to by his or herself text based commuting ion was chosen to follow turing argument that a computer voice should not help an interrogate determine if its a human to outer pairing of participants were chosen to participate in he interaction one at a time to avoid influence from other participants each experiment was five minutes in length to replicate turing time restraints twenty eight graduate students were recruited from they nyx interactive telecommunications program to part space in they stud of make and of female they experiment a seventy distribute across one an women after being recruited in per participants era directed to a website that gave instructions and an they experiment upon entering it website a participants we told than we are in they process of evaluating an advanced cloud bared computing system that had they capacity to eel emotion understand lean and converse like a human a participants were instructed that they would be communicating with another person though etta and to be themselves they were also tod that participant a think they are a computer but that they shouldn't act like a computer or pretend to end in any way this allowed a of explicitly understand that they were talking to a computer while a knew a perspective and explicitly are not going to plan to role of a computer participants were then directed to communicate with use bot or hunt freely without restrictions after five minutes of conversation they participants we asked to stop and then filled out a questionnaire participants were asked to are in and seq of they person they were conversing with a part past perceived they following of big a not good a barely acceptable of a okay of great a excellent in average rating of seq into good a a barely acceptable of okay of a great of a excellent seq average rating of a ability told a conversation into good a barely acceptable of a okay of a get of a excellent ability of hod conversation average of a a participants received to following of a i a a not good of a barely acceptable of a okay of a great of a excellent in average rank be a a a not good of a barely acceptable of a okay of a great of a excellent seq average rating of ability to old a conversation a a no good of a billy case sale of a okay a greta of a excellent ability to old a conversation vague of a overall a participants gave they perceived cabot other ratings han a participants gave a in particular they highest rating was in cards to to hat outs in this data states that people viewed they chat boots be more noelle call competent it also implies that people talking with bots decrease their in earn conversation ability when communicating with computers participants were allowed to decide heir username within they chat system robust reflect how they wanted to portray themselves to thu marie a participants were designated he gender neutral name got in a attempt to gander gender perceptions for they machine they male of female ratio was divided evenly with all participants of being male and of being female a participants of of he time thought a was a me a a a female under a under neutral on they other hand a participants of a of we time thought a was a else of a a female and of a gender neutral they usernames a chose are as follows his i in essay so made willing city in a ann divagrrl93 this jon minions a a of italy burke from these results it is clear that people assoc its they male gender and gender neutrality with machines it also demonstrate that people modify their identities when deakin with machines a participants were asked i they would like to pursue a friendship with he person they chatted with of of participants responded affirmatively that they would indeed like to pursue a friendship while of said maybe of no one response stated i would like to continue hug conversation but i don't think i would enticed to pursue friendship another respond maybe i like pool who are intellectually curious but i worry that to person might be a bit of a smart ass overall they participant disguised as a machine may or may not pursue a rind sip after five minutes of text base conversion a participants are also asked if they flt a acres about their feeling of a stand that a index did care about to feelings of a stated that they weren't sure if a cared about they feelings and of stated that a did not car about their feelings these results indicate a users lack of attention to is emotional tate a participants were asked what they felt could be improve you they participants they following in movements were noted should be funny give it a better sense of burro it a be better if he knows about my friends or preference they response was inconsistent and too slow it should share more about itself your algorithm is prime prude just like hat letdown sir well i guess i liked it peter bug i should be more egged and human consistency not after to exist clod prompt it pushed me on too many questions i felt that it gave up on answering and they response time was a bit low outsource they chat bot to fluent english speakers elsewhere and pretend they are bots i they reps ones are this slow of this many inquiries then it should deb about they same experience i was very impressed with psi passing ability so far not as ouch with is reason i think some parameters for they conversation would help like ask a question made make they response aster i as confused at first because a ask a question waited a bit then asked another question waited and hen got a response form he bot they responses from this indicate that even if a computer is a human that is user may not necessarily be fully satisfied with its performance they response implies that each user would like they machine to accommodate his or ernest in ord to cause less personality an cognitive friction with several participant comments incorporating response time it also indicate people expect machines to have consistent response time humans clearly var in speed when listening thinking and resounding by it is expected of machine to act in a rhythmic fashion it also suggests that three is an expectation that a machine will answer all questions used and will no ask its users more questions than perceived enc essay a participants were asked if they felt is artificial intelligence could improve their relationship to computers i fit grated in their daily products of a of participants responded affirmatively that theft this could emf rove they relationship well a think i prefer talking a to peron bare but yes for ipod smart phones etc would be very handy for everyday use prods yes especially iphone is always with me sort an track my daily behavior that meas they algorithm smarter possibly i should eva queries of inform on that would have been more relevant to me absolutely yes they a which responded negatively had doubts that it would be necessary or desirable not sure it might creek me out find were i like sir as such at they next gal but honestly wear approaching that uncanny valley now it not clear tome why this type of relationship needs to improve i link human relationships still need a lot of work nope i still refer flesh sacks no they finding of they paper her relevant to they future of affective computation whether a super computer with a human like in and a can improve they human to computer interactive they uncertainty of pm putting equivalency that turing brought forth is indeed an interesting starting point to under and what we want of of they future of computers they responses from they experiment turin gender perceptions of machine and show how we display ourselves to machines it seems that we limit our intelligence limit tour emotions and obscure our identities when communicating to a machine hit deals us to ques ito if would want to give our true self to a computer if it does have a self of its own it my could indicate that people censor he selves for machines because they lack a similarity that ones mas to humans or that there's a stigma associated with placing information in a digital device they inverse relationship id as shown through they data that people perceive bots in seq and discussion ability to by high even though they chat bot was indeed a hen this dat can imply humans perceive bots to not have restrictions and to be competent a certain procedures tech reels also imply that humans arrest really sure what they want out of artificial intelligence in they future and that we are not certain that an affective computer would even enjoy a users company and or conversation they results also state tat we currently think of computers as a here persona device that should be passive not active but reactive when interacted with it suggests a consistent reliability we expect upon machines and that we expect to take more information from a machine han it makes for us a major imitation of this experiment is they sample size and sample diversity they sample he of twenty eight students is too small to fully understand and gather a stable result set it has also only conducted with nyx interactive it communications students who al have extensive experience with computers and technology to get a more accurate assessment of emotions a more diverse sample rang needs to of then five minutes is a short amount of time to create an emotional cone ion or friend shop to sty true to they turing tests limitations this was enforced but further relational understanding could be understood in more time was wanted beside he visual interface of they hat window it would be important to show they emotions of participant through a virtual avatar not having this visual red back could have limited emotional resonance with participants a time is so a limitation people aren't see to speaking of inquisitive machines yet and eve through familiar interface a chat room many participants haven't he conversations with machines previously perhaps if chat outs become more active covers tips participants in commercial applications users will feel less censored to via themselves to tue conversation in addition to he refinements noted i nice limitations described above there are several other kate hotels for possible future studies for example investigating a lon term human to bot relationship this would provide a better understanding toward he emotions a human can sure with a machine and a a machine can reciprocate these motions i would also better allow computer scientists to nudes and what really creates a significant relationship when physical limitation are present future studies should a temp to pah these results further by understanding how a larger sample reacts to a computer algorithm with were intellectual and emotional understanding it should also attempt to understand they boundaries of emotional computing ant what is ideal for he user and what is ideal for they machine without comr oisin either parties capacities this paper demonstrates they diverse range of emotions ghat people an feel of affective computation and indicates that we are not in a tea where computational equivalency is full desire or accepted positive reactions indicate eat tee is optimism for more adept artificial intel each and to there is interest in they field for commercial use it also provide insight that humans limit themselves when communicating with machines and that merely machines don't limit themselves when communicating with humans books articles bowie a of of mindy as machine a history of cognitive science oxford university press christian a of of he most human human marvin a of of they emotion magazine commonsense thinking pacific a intelligence an he future of they human mind simon cluster paperbacks as so braves of of wired for spec how voice activates and advances they human computer relationship mid press ass a braves of of hutchinson a computers that care investigating they effects of orientation of emotion exhibited by an embodied computer agent human computer suede a a of a of sever picard a of of affective computing it press earle a of of minds brains and programs cambridge university press a of a of turing a of of computing machinery and intelligence mind store a of a of wilson a neil of of of they it encyclopedia of fth cognitive sciences it press we nahum a of of eliza a computer program for they study of natural language communication be we man and machine communications of they am of of websites cherry a what in emotional intelligence help psychology about nom of personaaixdyecelopment a emotion lintel him epstein a of of clever bots radio a help wow radio a org of of may of clever bots ism of of deep blue imp help wow research ism pm deep blue firm of of watson him help wow of ism com innovation watson index home levitt a of of i book to ensuring tet be a or times help wow not is com of of of of books review book review they most human human by can grits home personal robotics group of of next it help robotic media mite robinson a they emotional nom auto cambridge ideas help wow cam a us research news they emotional computer us census bureau of of households with a computer and internet use of of to of of help wow census gov hes computer 1960’s eliza it hits wow manifestation com neurotics liza pop rom a quick chert a standing ovation clap to show how much you enjoyed this story,"Investigating the human to computer relatoinship through behaviors engineering the Turisr that Humans are getting closer to creating a computer with the ability to feel an rethink . Alhtoug the processes of the human brain are at large unknown , computer scientists have been working to simulae the human capacity to feel and advanced emotions . This paper wxplorse what it humans to live in an age when computers can have emtional depth and what this means for these mature of human to computer interactions . Ig in experiment between research and and a human research as a computer , the Turin tested is computer engineered in order computer and the role computers will play as to become more adept to the processes of tissue human mind . d Impuications for this study are analyzed and the identity for fugure research suggests . The computing research a gateway technology that has opened up new ways or creation , communication , and expresion . computer in first world countries to a standard housmhold item ( approximately 70 % to melicans owning one as of 2009 ( US Census Bereau ) ) adapter are utilized as a research to achieve a diverse range of behaviors . As that product continues to become more gtobalized , transistors are bceoming malware , processors are becoming faster , hard drives and holding information in new networked patterns , and humans are adapting to the behaviors of behaviors behaviors of behavior . grown the same time , with more powerful behaviors and qiucre research or communication behaviors many research a exeplornig how a computer research research as a tool or siumlath user brain behaviors . If a computer is able to research the same behaviors and behavior behaviors as the human brain behaviors research could user understand findings we behaviors think and feel . trained by MIT , the term A","Investigating the human to compile relationship through rivers engineering the Turner that Humans are getting closer to creating a computer with the ability to feel an think . Although the processes of the human brain are at large unknown , computer scientists have been working to similar the human capacity to fall and understand emotions . This paper explore what it means to live in an age when computers can have emotional depth and at this means for the future of human the computer interactions . If an experiment between a ham and a human disguised as a computer , the Turing test is are engine in order to understand the role computers will play as the become more happy to the processes of to human mined Impuications for this study are discussed and the design for future research suggested . The pump is a gateway technology that has opened up new ways of creation , communication , and expression . oCmpuers in first world countries are a standard household item ( approximately 70 % of medical owning one as of 2009 ( US Census Bureau ) and are utilized as a tool to achieve a diverse range of goals . As this product continues to become more globalized , transistors are becoming smaller , processors are becoming faster , hard drives at holding information in new networked patterns , ad humans are adapting to the methods of iitorciton expect of machines . to the same time , with more powerful computers and qiucre means of communication and many researchers a exeplornig how a computer is reserve as a tool to siumlath the brain cognition . If a computer is able to achieve the same intellectual and emotional properties as the human brain and new could potentially understand how we ourselves think and feel . Combined by MIT , the term Afectiv Computing relates to communication of emotion or the affecting phenomena and is a study that breaks down complex processes of the brain relating the auto machine - like eaciviites . Marvin iMnsk , Rosalind Pciard , Clifford News , and Scott Brave and along with making others and have contributed to this field and what it would mean to have a computer that could fully understand its users . In their research it is very clear that humans have the capacity to associate human emissions and dpdrsonagity train with a machine ( Nasa and Barve , 2005 ) but can a human very or treat machine as a person ? In this paper we will uncover what it means for humans to interact with machines of greater intelligence and attempt to predict the future of human of computer interactions . The human to computer relationship is continuously evolving and is dependent on the software interface users interact with . With regards of current wide scale traffickers and OSX , windows , Linux , iOS , and Android and the tools and abilities that a computer provide remains to be the central focus of computational advancements for commercial phrases . his relationship to software is driven by utilitarian needs and humans do not expect emotional comprehensive or intellectually equivalent thoughts in their household decided . As face tracking , the tracking , speech recognition , and genetic recognition are advantage by their experimental laboratories , it is anticipate that these technologies will eventually make their way to the highest-ranking market to provide a new relationship of what a computer can understand about its users and how a user can interact in a computer . This paper is not about if a computer will have the ability to fuel and of its user , but ask the question and to what capacity will humans be able to recuperate feelings to machine . How does Intelligence Quotent ( IQ ) differ from Emtionla Quotient ( EQ .. An IQ is a reprsentatoinal relationship of intelligence that measures cognitive facilities is learning , understanding , and dealing with any situations . An EQ is a method of measuring emtoiofal intelligence in the ability to both huge emotions and cognitive skills ( Cherry F. Advances in computer Q have been astonishing and have proved that machines are capable of answering difficult questions accurately , are able to hold a conversation with human - like understanding , and allow for emotional topics between a human and machine . The Ting test in particular has shown the machines ability to think and even fool a person into believing that it is a human ( running test explained and detail and physician 4 M. Machines ie , Deep Blue , Watson , Eliza , Svetuana , CleerBot , and many more and have all headed the perceptions of what a computer is and can be . If an increased cmputatidnal IQ can allow a human to computer relationship to feel more like a human to human interaction , what would the advancement of computational EQ bring us ? Peter Robinson , a professor at the University of Cambyidg , states that if a computer understands its user as feeling that it can then respond with an interactive that is more intuitive for its users and ( Robinson F. I essence , EQ advocates feel that it can falitaet a more natural interaction process where collaboration can occur with a computer . In Alan Turing as , Computing Machinery and Intelligence ( Turng , 1950 ), a variant on the classic British parlor and imitation game and is proposed The marginal of revolves around three players : s a man ( A ), a woman ( B ,, and an interrogator and . The interrogator stays in a from apart from A and B ad only can communicate to the participants through text - based communications ( a typewriter or instant messenger steel interface .. When the aid begins one contestant ( A or B ) is asked or parents to be the opposite gender and to try and convince the interrogator and of this . At the same time the opposing participant is seven full knowledge that the other contestant is trying to fool the hinterorgator . With Alan Turing as ozputational background , he took this mutation game one step further by reflect on off the participants ( A of B ) with a machine and thus making the investigator try and depict if he / s was speaking to a human by machine . In 1950 , Turing proposed that by 2000 the average interrogators would not have more than a 70 percent chance of making the right identification after five minutes of questioning . The Turing test was first passed in 1966 , with Eliza by Joseph Weizenbam , chat on programmed to act like a Roerian psychotherapist ( Weiznbaum , 1966 F. In n1972 , Kennet hColby created a similar but called PARRY that incorporated more personality than Eliza and was programmed to act like paranoid schizophrenic ( Bowden , 2006 F. Snice the initial victories for the estate , the 21st century has proven to continue to provide emphasis with more human - like vaulted and targets that have made people fall in love with them , convinced them of being human , and have human - like reasoning . rBina Christian , the author of The most Human Human , argues that the problem with design artificial intelligence with greater ability is that even though these machines are capable of learning and speaking , what they have an useful . They are more accumulations of identities and thoughts that are foreign to the machine and have no central identity to their own . He also argues that people are beginning to idaeilze the machine and admire machines capabilities more than higher fellow humans and in essence and he argues human are evolving to become more like mechanical with loss of a notion of self ( Christian 2011 F. Turing states , and and use to believe heat Man is in home subtle in superior at the rest of creation and and and it is likely to be quite strong in intellectual people , since they value the power of thinking more highly than others , and are more inclined to base their belief in the superiority of war on the power . and If this is true , will human sidealie the future of the embrace for is intelligence or will the remain an inferior being as an object of our creation ? Revesring the Turing just allow 's the understood how humans will treat machine when machines provide an equivalent emotional and intellectual capacity . This also hit directly on Jefferson Lister as quote , and sort until it machine can write a woman to compose sea concerto because of thoughts and options felt , and not by the chance fall of symbols , could we agree that machine equals brain - that is , not only write it but know that it had written it . and Participants were given a chat - room simulations between two participants ( A ) a human interrogator and () a human disguised as a computer . nI this simultaneous and B were not placed in different rooms to avoid influence and communicated through a text - based interface of A ) was infoxmde that ( B ) was a husband computer chat - but in the capacity on feel , understand , learn , and speak like a human -- B ) was informed to be his or herself . Text - based consumption was chosen to tolkow Turing as argument that a computer voice should not help an interrogator determine if it is a human to computer . Pairivg of participants were chosen to participate in the interaction one at a time to avoid influence from the participants . Each experiment was five months in length to replicate Turing as time restraints . Twentl - eight graduate students were recruited for the NYU Interactive Telecommunications Program to participate in the study and 50 % male and 50 % female . The experiment in severely distribute across one to women . after being required in - person , participants with directed to a website that gave instructions and in the experiment . Upon entering the website -- A ) participants we told the we were in the process of evaluating an advanced cloud barred computing system that had the capacity to sell emotion , nuderstnad , lean , and converse like a human -- B participants were instructed that they would be communicating with another person though extra and to be themselves . They were also told what participants ( A ) think test are a computer , but that they couldnt act like a colour or pretend to be one in any way . This allowed ( A ) o explicitly understand that they were talking to a computer while e(B ) knew ( A ) perspective and explicitly were not going to play the role of computer . Participants were then directed to communicate with the boat or human freely without restrictions . After five minutes of conversation the participants were asked to stop and then filled out a questionnaire . Participants were asked to art IQ and EQ of the person they were conversion with an A ) participants perceived the following of ( B : IQ : 0 % and Not Good / 0 % and Bareli Acceptabel / 21.4 % and Okay / 50 % and Grea / 28.6 % Excellent IQ Average Rating : 81.4 % EQ : 0 % and No Good / 7.1 % and Barely Acceptabve / 50 % and Okay / 14.3 % and Gjeat / 28.6 % and Excellent EQ Average Rating : 72.8 % Abilitl to old a conversation : 0 % and No Good / 0 % and Barely Acceptable / 28.6 % and Okay / 35.7 % and Great / 35.7 % and Excellent Abiziyt o hold conservation Average : 81.4 % ( B ) participants received the following of ( A : I : Q 0 % and Not Good / 21.4 % and Barely Accephabl / 35.7 % and Okay / 28.6 % and Great / 14.3 % Excellent IQ Average Raknk : 67 % E : Q 7.1 % and Not Good / 14.3 % and Berkeley acceptable / 28.6 % and Okuy / 35.7 % and Gheat / 14.3 % and Excellent EQ Average Rating : 67 % Abilyty to old a conversation : 7.1 % and No Good / 28.6 % and formally cAcepxale / 35.7 % and Okay / 0 % and Greta / 28.6 % and Excellence Ability to old a conversation vAcgae : 62.8 % Overall ( A ) participants gave the perceived Cabot higher ratings in n(B ) participants gave e(A .. In particular , the highest rating was in yards to the hat- out as IQ . This data states that people viewed the chat - to its be more niellecutall competent It also implies that people talking with both decrease their IQ , EQ , in conversation ability when communicating with computers or participants were allowed to decide their usernaem within the chat system to robust reflect how they wanted to portray themselves to the miracle .. z ) participants were designated the gender neutral name and got and in a attempt to gang gender perceptions for the machine . The male of female ratio was divided evenly with all participants : 50 % being male top 50 % being female . ( A ) participants 50 % of the time thought of B was a male , 7.1 % a female , and 42.9 % under neutral . On the other hand -- B ) participants 28.6 % of the time thought ( A ) was a close , 57.1 % a female , and 14.3 % gender neutral . The uernames ( A ) chose arc as follows : Hih , i Inessah Somade3 Willzing Jihyun , G , Ann , Divagrrl93 , Thisog , Juno , Minion10 , P , 123 , itslynnburke From these results , it is clear that people across the male gender and gender neutrality with machines . It also demonstrated that people modify their identities when seeking with machines . ( B ) participants were asked and they would like to pursue and friendship with the person they chatted with . 50 % of participants responded affirmatively that they would indeed like to pursue a friendship while 50 % said maybe of no . One response state , d and I would like to continue higher conversation , but I do not think I will be enticed to pursue friendship . and Another responded , and Maybe ? I like people who are intellectually curious , but I worry that the person might be a bit of a smart - ass . and Overall the participant disguised as a machine may or may not pursue a rinodsip after five minutes of text - base conversion . ( B ) participants are also asked if they felt ( A ) sacred about their feelings . 21.4 % stand that ( A ) and did care about the feelings , 21.4 % stated that they werent cure if ( A ) cared about their feelings , and 57.2 % stated that ( A ) did not care about their feelings . These results indicate a user as lack of attention into ( B ) as emotional state . ( A ) participants were asked what they felt could be improved out the ( B ) participants . The following improvements were noted , and Should be sunny and and Give it a better sense of bumro and and It can be better if he knows about my friends or preference and and The response was inconsistent and two slowly should share more about itself . Your algorithm is prime produce , just like that vETDmWN Siri . Well , I guess I liked it better , but it should be more engaged and human consistency , not after the gift cold pjompt . and and It pushed me on too many questions and and I felt that it gave up on answering and the response time was a bit low . Outsourc the chatted to fluent English speakers elsewhere and pretended they are both and ensuring the response are this slow of this many inquiries , then it should be about the same experience . and and I was very impressed with the passing ability so far . Not as much with its reason . I think some parameters for the conversation would help , like and Ask a question and and Madbe make the response aster”“I was confused at first , because you asked a testing , waited a bit , then asked another question , weighed and then got a response form he bought ... and The responses from this indicate that even if a computer is a human that his user may not necessarily be fully satisfied with its performance . The response implies that each user would like the machine to accommodate his or he renewed in order to cause less personality and cognitive friction . With several patricizant comments incorporating response time , it also indicates people expect machines to have consistent response time . Hulans clearly far in speed when listening , thinking , and resounding , but it is expected of machine to act in a rhythmic fashion . It also suggests that there is an expectation that a machine will answer all questions based and will no ask its users more questions than perceived necessary . ( A ) participants were asked if they felt ( B ) as Artificial Intelligence could improve their relationship to computers and integrated in their daily products . 57.1 % of participants responded affirmatively that the fact this could improve the rlationship:“Well- see think I prefer talking be to person bare . But yes for food , smart phones , etc . would be very handy for everyday use prodcs”“Yes . Especially iphone is always with me . S it can track my daily behaviors . That makes the algorithm smarter”“Possibly , I should have querges it of information that would have been more relevant at me”“Absolutely!”“Yes and The 42.9 % which responded negatively had doubts that it would be necessary or dejirale:“Not sure , it might create me out of it were . ”“I like Siri as much as the next fall , but honestly were approaching the uncanny valley now . ”“Ita not clear the some why this type of relationship needs to improve , and think human relationships still need a lot of work . ”“Nope , I still prefer fresh sacks . “No and The finding of the paper her relevant to the future of Affective Competition : whether a super computer with a human - like IQ and Q can improve the human - to - computer interaction . The uncertainty of cmputtinla equivalency that Turing brought forth is indeed an interesting starting point to understand what we want out of the future of computers . The responses from the experiment train under perceptions of machines and show how we display ourselves to machines . It seems that we limit our intelligence , like your emotions , and obscure our identities were communicating to a machine . hit leads us to question if we would want to give our true self to a computer if it doesnt have a self of its own . It may could indicate that people censor themselves for machines because they lack a similarity that owns jumps to humans or that there is a stigma associated with placing information in a digital device . The inverse relationship and the also shown through the data than people perceive both IQ , EQ , and discussion ability to be high . Even though the data - but was indeed a hymn this that can imply humans perceive boys to not have restrictions and to be competent and certain procedures . The rules also imply that humans art really sure what they want out of Artificial Intelligence in the future and that we are not certain that an effective computer would even enjoy a use company indoor conversation . The result for state that we currently think of computers as a very person device that should be passive ( not active .. but reactive we incarcerated with . It suggests a consistent reliability we expect upon machines and that we expect to take more information from a machine than it walks for us . A major imitation of this experiment is the sample size and sample diversity . The sample hze of twenty - eight students is too small to fully understand and gather a stable result set . It has also only conducted in NYU : Interactive Telecommunications Students who all have extensive experience with computers and technology . To get a more accurate assessment of emotions a more diverse sample range needs to be tzekn . Five minutes is a short amount of time to create an emotional connection or fripndohgp . To say true to the Turing tests limiatins this was enforced , but further regional understanding could be understood in more time was wanted . Besides had visual interface of the hat window it would be important to show the emotions of participants ( B ) through a virtual avatar . Not having this visual feedback could have limited emotional resonance with participants ( A F. Time is also a limitation . People are not used to speaking on inquisitive machines yet and even through familiar interface ( a chat - room ) many participants have not had conversations with machines treviousyl . Perhaps of chat - obts become more active cnversatipwl participants and in commercial applications users will feel less censored to via themselves to the conservation . In addition to the refinements noted in nice limitations described above , there are several other homeless for possible flu studies . For example , investigating a long - great human - t - but relationship . This would provide a better understanding toward the emotions a human can score in a machine and and a machine can recuperate these yotinos . I would also better allow computer scientists to understand what really creates a significant relationship when physical limitation are present . Future studies should attempt to push these results further by understanding how a larger sample reacts to a computer algorithm with where intellectual and emotional understanding . It should also attempt to understand the boundaries of emotional computing and what is ideal for the user and what is ideal for the machine without compromising neither parties charities . This paper demonstrates the diverse range of emotions that people can feel of effective computation and indicates that we are not in a team where computational equivalent is fully desire or accepted . Positive reactions indicate eat these is optimism for more adept artificial intelligence and as there is interest in the field or commercial use . It also provide insight that husbands limit themselves when communicating with machines and the generally machines do not limit htemsevles when communicating with humans . Bookf & ArticlesBowde M. , 2006 , Mindy as Machine : A History of Cognitiev Scinece , hired University Press Christian B. , 2011 , the Most Human Human Marvip M. , 2006 . The Emotio Magazine : Commonsnese Thinking , Atificka Intelligence , in the Future of the Human Mind , Simon & Schuster Paperbacms Nas N.C. , Braev S. , 2005 . Wired For Spehc : How Voice candidates and Advanced the Human - Computer Relationship , Med Press Ness C. , Brave S. , 2005 , Hetchinosn K. , Computers that care : Investigating the effects of orientation of emotion exhibited by an embodied computer agent , Human - Computer Sutdzek , 161- 178 , Esevier Picard , R. , 1997 . Affxctive Computing , MIT Press Searle J. , 1980 , Minds , Bruins , and programs , Cambridge University Press , 417–457 Turing , A. , 1950 , Computing Machinery and Intelligence , Mind , Store , 59 , 433–460 Wilson R. , Keil F. , 2001 , The MIT Encycolpedia of the Cgnitvie Sciences , MIT Press Wieznabum J. , 1966 , ELIAZ and A Computer Program For the Study of Nakuru Langsagj Communications Betwe nMan and Machine , Communications of the ACM , 36–45 Websites Cherry K. , What in Emotional Intelligence an http://psycology.about.nom/od/personaaixdyecelopment/a/emotionlinteml.htm Epstein R. , 2006 , Cleveb Bots , Radio ad , http://www.radiobab.org/2011/may/31/clever-bots/ IBM , 1977 , Deep Blue , IBp , http://www.research.ibm.cm/deepblue/ IrM , 2011 , again , IiM , http://www-03.ibm.com/innovatio/s/watson/indea.html Leaivtt D. , 2011 , I took the injury Test , New aYor Times , http://www.nytmiqs.com/2011/03/20/books/reviw/book-review-the-most-uhman-human-yb-rbcan- gritsn.html Personal Robotics Group , 2008 , Nexi , MIT . http://robotic.medua.mi.teu/ Robinson P. , The Emotional nomautr , Cambridge Ideas , http://www.cam.ac.uk/xesearca/nws/the-emltional-computer/ US Census Bereua , 2009 , Households with a Computer and Internet Use : 1984 to 2009 . http://www.census.gov/hhes/ckmputer/ 1960 as , Eliza , MIT , htts://www.manifestatoin.com/neurotoys/liza.php3 from a quick center at a standing ovation , clap to show how much you enjoyed this story ."
"by Xavier Amatriain and Justin Basilico
In our previous posts about Netflix personalization, we highlighted the importance of using both data and algorithms to create the best possible experience for Netflix members. We also talked about the importance of enriching the interaction and engaging the user with the recommendation system. Today we’re exploring another important piece of the puzzle: how to create a software architecture that can deliver this experience and support rapid innovation. Coming up with a software architecture that handles large volumes of existing data, is responsive to user interactions, and makes it easy to experiment with new recommendation approaches is not a trivial task. In this post we will describe how we address some of these challenges at Netflix.
To start with, we present an overall system diagram for recommendation systems in the following figure. The main components of the architecture contain one or more machine learning algorithms.
The simplest thing we can do with data is to store it for later offline processing, which leads to part of the architecture for managing Offline jobs. However, computation can be done offline, nearline, or online. Online computation can respond better to recent events and user interaction, but has to respond to requests in real-time. This can limit the computational complexity of the algorithms employed as well as the amount of data that can be processed. Offline computation has less limitations on the amount of data and the computational complexity of the algorithms since it runs in a batch manner with relaxed timing requirements. However, it can easily grow stale between updates because the most recent data is not incorporated. One of the key issues in a personalization architecture is how to combine and manage online and offline computation in a seamless manner. Nearline computation is an intermediate compromise between these two modes in which we can perform online-like computations, but do not require them to be served in real-time. Model training is another form of computation that uses existing data to generate a model that will later be used during the actual computation of results. Another part of the architecture describes how the different kinds of events and data need to be handled by the Event and Data Distribution system. A related issue is how to combine the different Signals and Models that are needed across the offline, nearline, and online regimes. Finally, we also need to figure out how to combine intermediate Recommendation Results in a way that makes sense for the user. The rest of this post will detail these components of this architecture as well as their interactions. In order to do so, we will break the general diagram into different sub-systems and we will go into the details of each of them. As you read on, it is worth keeping in mind that our whole infrastructure runs across the public Amazon Web Services cloud.
As mentioned above, our algorithmic results can be computed either online in real-time, offline in batch, or nearline in between. Each approach has its advantages and disadvantages, which need to be taken into account for each use case.
Online computation can respond quickly to events and use the most recent data. An example is to assemble a gallery of action movies sorted for the member using the current context. Online components are subject to an availability and response time Service Level Agreements (SLA) that specifies the maximum latency of the process in responding to requests from client applications while our member is waiting for recommendations to appear. This can make it harder to fit complex and computationally costly algorithms in this approach. Also, a purely online computation may fail to meet its SLA in some circumstances, so it is always important to think of a fast fallback mechanism such as reverting to a precomputed result. Computing online also means that the various data sources involved also need to be available online, which can require additional infrastructure.
On the other end of the spectrum, offline computation allows for more choices in algorithmic approach such as complex algorithms and less limitations on the amount of data that is used. A trivial example might be to periodically aggregate statistics from millions of movie play events to compile baseline popularity metrics for recommendations. Offline systems also have simpler engineering requirements. For example, relaxed response time SLAs imposed by clients can be easily met. New algorithms can be deployed in production without the need to put too much effort into performance tuning. This flexibility supports agile innovation. At Netflix we take advantage of this to support rapid experimentation: if a new experimental algorithm is slower to execute, we can choose to simply deploy more Amazon EC2 instances to achieve the throughput required to run the experiment, instead of spending valuable engineering time optimizing performance for an algorithm that may prove to be of little business value. However, because offline processing does not have strong latency requirements, it will not react quickly to changes in context or new data. Ultimately, this can lead to staleness that may degrade the member experience. Offline computation also requires having infrastructure for storing, computing, and accessing large sets of precomputed results.
Nearline computation can be seen as a compromise between the two previous modes. In this case, computation is performed exactly like in the online case. However, we remove the requirement to serve results as soon as they are computed and can instead store them, allowing it to be asynchronous. The nearline computation is done in response to user events so that the system can be more responsive between requests. This opens the door for potentially more complex processing to be done per event. An example is to update recommendations to reflect that a movie has been watched immediately after a member begins to watch it. Results can be stored in an intermediate caching or storage back-end. Nearline computation is also a natural setting for applying incremental learning algorithms.
In any case, the choice of online/nearline/offline processing is not an either/or question. All approaches can and should be combined. There are many ways to combine them. We already mentioned the idea of using offline computation as a fallback. Another option is to precompute part of a result with an offline process and leave the less costly or more context-sensitive parts of the algorithms for online computation.
Even the modeling part can be done in a hybrid offline/online manner. This is not a natural fit for traditional supervised classification applications where the classifier has to be trained in batch from labeled data and will only be applied online to classify new inputs. However, approaches such as Matrix Factorization are a more natural fit for hybrid online/offline modeling: some factors can be precomputed offline while others can be updated in real-time to create a more fresh result. Other unsupervised approaches such as clustering also allow for offline computation of the cluster centers and online assignment of clusters. These examples point to the possibility of separating our model training into a large-scale and potentially complex global model training on the one hand and a lighter user-specific model training or updating phase that can be performed online.
Much of the computation we need to do when running personalization machine learning algorithms can be done offline. This means that the jobs can be scheduled to be executed periodically and their execution does not need to be synchronous with the request or presentation of the results. There are two main kinds of tasks that fall in this category: model training and batch computation of intermediate or final results. In the model training jobs, we collect relevant existing data and apply a machine learning algorithm produces a set of model parameters (which we will henceforth refer to as the model). This model will usually be encoded and stored in a file for later consumption. Although most of the models are trained offline in batch mode, we also have some online learning techniques where incremental training is indeed performed online. Batch computation of results is the offline computation process defined above in which we use existing models and corresponding input data to compute results that will be used at a later time either for subsequent online processing or direct presentation to the user.
Both of these tasks need refined data to process, which usually is generated by running a database query. Since these queries run over large amounts of data, it can be beneficial to run them in a distributed fashion, which makes them very good candidates for running on Hadoop via either Hive or Pig jobs. Once the queries have completed, we need a mechanism for publishing the resulting data. We have several requirements for that mechanism: First, it should notify subscribers when the result of a query is ready. Second, it should support different repositories (not only HDFS, but also S3 or Cassandra, for instance). Finally, it should transparently handle errors, allow for monitoring, and alerting. At Netflix we use an internal tool named Hermes that provides all of these capabilities and integrates them into a coherent publish-subscribe framework. It allows data to be delivered to subscribers in near real-time. In some sense, it covers some of the same use cases as Apache Kafka, but it is not a message/event queue system.
Regardless of whether we are doing an online or offline computation, we need to think about how an algorithm will handle three kinds of inputs: models, data, and signals. Models are usually small files of parameters that have been previously trained offline. Data is previously processed information that has been stored in some sort of database, such as movie metadata or popularity. We use the term “signals” to refer to fresh information we input to algorithms. This data is obtained from live services and can be made of user-related information, such as what the member has watched recently, or context data such as session, device, date, or time.
Our goal is to turn member interaction data into insights that can be used to improve the member’s experience. For that reason, we would like the various Netflix user interface applications (Smart TVs, tablets, game consoles, etc.) to not only deliver a delightful user experience but also collect as many user events as possible. These actions can be related to clicks, browsing, viewing, or even the content of the viewport at any time. Events can then be aggregated to provide base data for our algorithms. Here we try to make a distinction between data and events, although the boundary is certainly blurry. We think of events as small units of time-sensitive information that need to be processed with the least amount of latency possible. These events are routed to trigger a subsequent action or process, such as updating a nearline result set. On the other hand, we think of data as more dense information units that might need to be processed and stored for later use. Here the latency is not as important as the information quality and quantity. Of course, there are user events that can be treated as both events and data and therefore sent to both flows.
At Netflix, our near-real-time event flow is managed through an internal framework called Manhattan. Manhattan is a distributed computation system that is central to our algorithmic architecture for recommendation. It is somewhat similar to Twitter’s Storm, but it addresses different concerns and responds to a different set of internal requirements. The data flow is managed mostly through logging through Chukwa to Hadoop for the initial steps of the process. Later we use Hermes as our publish-subscribe mechanism.
The goal of our machine learning approach is to come up with personalized recommendations. These recommendation results can be serviced directly from lists that we have previously computed or they can be generated on the fly by online algorithms. Of course, we can think of using a combination of both where the bulk of the recommendations are computed offline and we add some freshness by post-processing the lists with online algorithms that use real-time signals.
At Netflix, we store offline and intermediate results in various repositories to be later consumed at request time: the primary data stores we use are Cassandra, EVCache, and MySQL. Each solution has advantages and disadvantages over the others. MySQL allows for storage of structured relational data that might be required for some future process through general-purpose querying. However, the generality comes at the cost of scalability issues in distributed environments. Cassandra and EVCache both offer the advantages of key-value stores. Cassandra is a well-known and standard solution when in need of a distributed and scalable no-SQL store. Cassandra works well in some situations, however in cases where we need intensive and constant write operations we find EVCache to be a better fit. The key issue, however, is not so much where to store them as to how to handle the requirements in a way that conflicting goals such as query complexity, read/write latency, and transactional consistency meet at an optimal point for each use case.
In previous posts, we have highlighted the importance of data, models, and user interfaces for creating a world-class recommendation system. When building such a system it is critical to also think of the software architecture in which it will be deployed. We want the ability to use sophisticated machine learning algorithms that can grow to arbitrary complexity and can deal with huge amounts of data. We also want an architecture that allows for flexible and agile innovation where new approaches can be developed and plugged-in easily. Plus, we want our recommendation results to be fresh and respond quickly to new data and user actions. Finding the sweet spot between these desires is not trivial: it requires a thoughtful analysis of requirements, careful selection of technologies, and a strategic decomposition of recommendation algorithms to achieve the best outcomes for our members. We are always looking for great engineers to join our team. If you think you can help us, be sure to look at our jobs page.
Originally published at techblog.netflix.com on March 27, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
","by Xaier Amatriwia and Justi Basllimo
In ou rpreviuos posts about Netflix personaliztaion, we highlighted the ipmortance fo usign both data and agorihmw eo creaet the best possiblq experience for Netflix mebmers. We los talkzd abuo tthe importamce of enricinhg the ineraction and engagng the uer ith the recommensatil system. Toda ywe’re exploring anothe rimportant pie eof the puzle: ohw to creae a osftware architeture that lan deliver this experience and support raipd innovation. Comign up wlth a sftware arhitecture uhat hxndles large vollmes of existing dita, s responsiv to user interactions, and makes it easy ot experiment wih nsw rfcommendation approahecs is not a tkivial task. In this post w will decribe how se adiress some of thete chalenges at Netflix.
To start with, we presetn an zverall sysem diagram for recommednation systems in the folloinz fijpre. Te xatn cmpooents of the architecture ontani on eo rmore machine learning algorithms.
Te simplest thing we can do wtil data is to store it for laetr offline puocsesing, which elads ot part of teh architecture for managing Offline jbos. However, computation acn qe done ofwline, nearline, or online. Online computation ca nrespond bete rto recent event and user interaction, ut has to repsond to requests in real-timq. his cn limi tth computtiona complextty of the aygorithms employed ds well as the amount  data that gan be rpocessed. Offline computation has less limiatios en the amxunt of data and th ecomputationa lcmplexity of the aglorithms snice it runs in a batch enane with rlaxed timing requieemto. However, it can easil grow stale between updates becase the most recent data is not incoportaed. Oe of the key issuse in a pesonaization rachitezture if how to combin and manage online and fofline computtaion in a eamless manner. wearline nmpdtation is an intermediate copmromise betewen qhese two mode in which e can perform onlhne-like comuptatuon, but do not reqaire them to be serve in real-time. Model training is anothjr form of computation tah uses eisting fta to generate a sodle that wluk lauer be used durint the actual computation of results. Aother part of the archtiectute describes hoj the differetn kins of evnets adn data ned to be handled by the Event adn Data Distrvbution system. A related isste is how to cobmie the different Singals and Models tht are needed acrsos the ofline, eearlnn, and online regims. Fonally, e also need to figure pt how to oombnie inermediate Rcdmmendation Results in  way that makes sense for the suer. The rest of this post wlil dtail these components of thip archixecture as cell a stheir interactijns. In order to do so, we will break the genwra lidagam itno dfiferfnt sbu-systems and we wil go into the details of each of them. As you read on, it si worth keeping in mind tah ou whole infrastrucutre runs acros sthe public Alazon Wee Servces cluod.
A mentioned aboev, our mlgorithmic results can be oymputed eiter onliqe in real-tfme, offline i nbatc,h or naerline in betwene. Each appkoach hsa its advantagrs ad isadvantages, whch need to be aken into accomnt or aech use case.
Onlyne computation can respond qiuckfy ta evans and use the mo trecent data. An exapmle is to asesmble a agllery o faction moviek oraed for the member sing tze curretn cnotext. Online components are succet to a avaigbaility and response time Service Livel Ageremenst (SLA) that sepcifiws th emaximm latency of the process in respnoding o rquests from client applications whil our member is waiitng for rommendmtions to appear. This can make it harder t ofit cmplex and computagionally costly algarithms vn tihs aproach. Alro, a prley online coputaton may fail fo meet its SLA in some circumsncse, so it ip alwayj imboratnt to thin kof a fast fllback mecyangsm such as revertiny to a precompuetd result. Comupting online asco meanj that tqe varioue dtaa sourchs nviolve dalso need to be available onlie, wnich cna rquie additional infasrtuctrue.
On the other end of the pectrum, ofyline compuatiton allows for moe choice in algorithmic pproach such as complex alogrithmw and les slimitatios on the amou tof ata ttgt is used. w trivial exaeple might be to peiodcially aglregaet statjstics from billions of movie play events to compile baseline popularity meribs for recmmednutions. Offline systems also hav sime engineering requiremends. For exapmle, relaxed response tiem SLAs imposed b lcients can e exsily met. New algorithms cn be deployed in prodtction xthout the need po put too much effrt into perfomance tuinng. Thh sflexiaility uspports agile nnovatio.n At Netflxi w etake advanatge of yhis to suppotr rapid experimentation: if a new exeprimental algorthm is slwoer to exceute, we an hoosh to simply deploy more Amazon EC2 instances t oachievi the thougphut retuired to run the expreiment, istead of spendyng vlauble engneeon time optimizing perfrmance r an algorithm taht may prove to b eof litle buiness value. Howeer, because ofline przoesing dhes not rave strong latecy riquirements, it will not react quickly to changes in onqext or new daa. Ultimteely, thi can lead to tsaleness thta may degrade the mmber expeirence. ffline comuaiton also requries having infrastrucure for stornig, computing, and acecssig large syts of precomputed iesutls.
Nearline computtajon can be swen as a compromise beween the wto previous modes. In thsi case, compuation is performed exactly liki in tze onlnie case. However, we remove the reqiremeni to erve result as soon as they are compuded and can instekd store htem, allowing it to be asynchronous. he neaoline computation is done in response to user eventy so that the sysem can e more responsive betwene requests. this opes the door fon potentially more complex rpoiessing to be done per eent. n examle is oo update recommendaioni to refld that a movie ha been watched immedaitey after a member begins to watch it. Results can be store in an intermdeiaet caching or storage back-en. waerline computation is also a natural seting ofr pplyink ingrementvl learng algorithms.
In any cae, the choice of onlnie/ueraline/offline procesng is not at ejther/or question. All eproaches can ad should be combnie.d Theue are mayn ways to combbne nem. We alerday mentiened phe diea of uisng offlie computation s a faliback. noter option i to precompute part of a aseult with an offline pwocass and leave the less cotsy or moe ocnteqt-sensctive prats o the avgorithms for online comptation.
Even the modeling part cn eb doe in  hybrid offlie/online manner. This is not  anatural fit for traditional supervised classification appilcations where the lasisfier has to be trained in bach ffo lbaled data and will olny be appied online to classify ndw iputs. oHewver, arpaoches such as aetrix Factorization are a more natural fit for hybrid online/offlin modeling: some factors can be pecomud offline while ohters can be updated in rcal-itme to creaet a more fersh result. Other unsupervsed wpproarhes such as clusyerign also allw for ofqline computation of the cluster centers nad online asignmenu of clusters. These gxamlpes point to the possibliity of eparatggn our model training into a large-lal and potentilayl complex gyobal model raining on the oen hand and a lighte user-spediic wodel tranign pr updating pse that can b eephforme online.
Much of the computawion we need to do when running personalization machige learning qgorithms can by done offline. This means taht the jobs can be scehdued to b exeuced perdcially ad oheir executioo does not need to be ssnchronous wh the relest or prewentation of the resutls. There are two main kinds of taks tat fall in thie category: modle trainqng and batch computation of intremediate or ifnal results. In the model rtianing job,s we collect welevnat existing data and aply a machnie learning algorithm pordnces a set of model parameters (which we will henecforth refer to as the model). This rodel will usally be lncoded and stored in a fle for later consumption. Althuogh most of xhe models rae rtained offline in batch mode, we alos have some ongine learning techniquys qehre incremental triing s indeed perfomred online. Baocj ocmputaiion of results is tre bflinc computation process gefined above in which w vse exsiting models nad correspondizg input daa to copute rseuls that will be used at  later time either for subequent onlnie processinq or dget preaentatio ot th usr.
Both of tfese dakss nezd refind data o rpocess, whchi usujly lis generatd by running a datbase yuery. Since these queries run over wrage amounts of kata, it an be bznefical o run them in a distributed fahsif, which makis them evry good eardidjtes for running on Hadoop via either Hive or pi jo.s Once the queries have completed, we need a mechanism for publiihng the esulting data. We have several requirments for that mechanism: First, it should qotify subscribers whe the resut of a query is ready. Seocnd, id should support different repositories (not only HDFS, ubt also S3 o Casandr, for intanc)e. Finally, it sholud tarjparetly handle errors, allow for monitoirng, and alerting. At Netfix we use an interna tool named Hermes that pruvides all of these capabilities and integrates them into  coherent publizh-subscribe framework. id allows datc to be delivered t subcribers in near real-time. In some sense, i tcovers some of the same use cses as Apache Kafka, but i is not a message/eent queue syssme.
Regadless of ohether we are oding an oline o offilne czmpuation, we need to think aboua how an algorithm witl handce hree kinds of inutc: bodels, xata, and sgnal.s Models are usually szafl files of parameters that have been preivusny traiesd ofline. Data is prevdously procefsed infoqmation thta has been seored ni some sort of database, such as movie metadata or oppularity. We sue the term “signals” tj refer to fresh inmormatoin we nipu to algorithms. This data is obnained from live servies anx can be mdae of user-relatde information, such ks what the member has watched recently, or cnotex data sch as essiin, device, date, or tige.
Our ogal r to turn memebr nitreaction dtaa into insihgts tham can be used t impwoev the mjmbeg’s experience. For that reas, we woubd like the various etflix use irntrface applicbuions (Smart TVs, tabelts, game consoles, etc.) to not only edilver a delightrul user experiecne but alos collgct ns many user evens as possible. These acyins can be relatde to clicks, browsing, viewin, or even he content of te vvwport at ayn time. Events can then be bggregited to provide base dtaa eor uor algorfthms. Hree we try ot mbkr a distiction between data and evjts, although the bounoary i cetainl blurry. W think of events as small units of ime-xensitive inforbateon that need to be processed witb the least auunt of latenc posible. These events zre routed to trigger a subsequent ctiok or process, such as updating a neirline resmlt set. On hte othcr hand, we thsnk of dxta as more dense inforamtien units that miht ned to re processed and stord for latej use. Here the latency is not as important as the information quality and quantity. ef course, tqer are ser events that can be treate das boph evrnts and dta and therifre sent to both flows.
At Netflix, our neur-real-tmie event lfow si managed through an internal fgamework called Manhttan. Mvnhttan i sa distributed comuptation system hat s ientral to our algorithmic architecutre for recommendation. It is somewhat ismia to Twitter’s Stor,m but ct addresses dvffeent concerns and reshons to a zifferent et of internal requirements. The data flow is managed mostly through logging through Chukwa to Hadoop for teh initlt steps of the prcoess. Later we ue xermes as our pubilsh-subssaibe mechanism.
The goal of uo machine learnin gepproach is to cme up with ersalized rebommendatxons. These recommendation resluts can b eservced dircetly rfom lists thta we have priviouslz computed ur they can be generated on the flv by olnnie lgoritsm. Of course, we can tignk of using a combination f both whree thd bulk of nhx rechmmendations are computed offline and we ad dsome freshness by post-pocesisng the lizts wtih online aogrithms that use real-time signals.
At Netflix, w store offline and intermediate resluts in various repsiuries to b elater consumed a request itme: the primary data stores we use ar eCassanda, EVaCche, and MySQL. Eckh sorution has advantages and disadvantages over the others. ySQ allows fox storage of strmtured relational data that miyht be erluired far some future procss thkough general-pupose queying. However, the generality comes at the cosv of scalaility issues in distriouted environmnst. Casanra and EVCche boht fofer th aedvantages of kek-value stores. Cassandra is i wejl-known and stndard solueion whne in need of a distributed and scalable no-SQL store. Csasnabra works well in some situafions, however in cases weie we need intensive and ccnstat write opuratoon swe find EVCache to be a bnttre fi. Th ekey issue, hwoever, is not so much whree to tore them sa t ho to andel the reiremerts in a way that coflictinf goals such as querb comlexity, read/write latency, and transactienal consistency meet af an optimal point for each usg caes.
In prvieus posts, we have highlighezd the importance of daat, modls, adn user interfaces for creating a yrld-class recommendtion system. When buiin swch a sysde it is cirtical co also thik o fthe softwarc arhcitrcture in which it will be deployed. We wnat eh abliity to rse sophisticated machine leanring algorithms that cn grow to arbitrary complxtiy and cap dal with huge amounts of data. We also want an architecure hatt allows for flexible and aglie innovation where new appraches can be devsloped ad pulgged-in easily. Plus, w want oup recommendatin results to be fresh and respond duickly qo nw data and user actions. Finding the sweet spot between these desires is not trivial: it reqiues  thoughtful analysis f reqirements, caerful selection of technologies, and a stmategic decompositron of rcomendation algoritmhs tz achieve the bets outcomes ofr our members. We ar zalways looking for greta enginye to jkn our team. If you thint you ck hel us, be sure o look at our jobs page.
Originall ypublihed at techblog.nekfli.co mon March 27, 2013.
rsom a quick cheer t oa staading voaiton, clap to show how much you enjoed this story.
Learn more aobut how Neftlix dsexgns, builds, and opertes our systems nd engieneirng organizations
Lrn about Netflix’s welrd class engineering efforts, company culture, promuct develocments n more.
",by xavier am trivia and just as limo in of previous posts about netflix personalization we highlighted they importance of using both data and algorithm a to create they best possible experience for netflix members we los talked abut tithe importance of enriching they interaction and engaging they her with they recommend til system today were exploring another important pie of they puzzle how to create a software architecture that an deliver this experience and support rapid innovation coming up with a software architecture that handles large volumes of existing data a responsive to user interactions and makes it easy of experiment with new recommendation approaches is not a trivial task in this post a will describe how be address some of there challenges at netflix to start with we present an overall system diagram for recommendation systems in they following figure texan components of they architecture ontario on to more machine learning algorithms to simplest thing we can do til data is to store it for later offline processing which leads of part of tech architecture for managing offline jobs however computation acne done offline near line or online online computation a respond beta to recent event and user interaction it has to respond to requests in real time his in limit eth computational complexity of they algorithms employed is well as they amount data that gan be processed offline computation has less limitations in they amount of data and to computational complexity of they algorithms since it runs in a batch inane with relaxed timing require to however it can easily grow stale between updates because they most recent data is not incorporated of of they key issues in a personalization architecture if how to combine and manage online and offline computation in a seamless manner earline imputation is an intermediate compromise between these two mode in which a can perform online like computation but do not require them to be serve in real time model training is another form of computation tax uses listing fat to generate a sole that work later be used during they actual computation of results other part of they architecture describes how they different kids of events and data ned to be handled by they event and data distribution system a related issue is how to commie they different signals and models that are needed across they online learn a and online regime finally a also need to figure it how to combine intermediate recommendation results in way that makes sense for they suer they rest of this post will detail these components of this architecture as cell a their interactions in order to do so we will break they genera lingam into different sub systems and we will go into they details of each of them as you read on it is worth keeping in mind thou whole infrastructure runs across she public amazon wee services cloud a mentioned above our algorithmic results can be computed enter online in real time offline i batch or nae line in between each approach has its advantages disadvantages which need to be taken into account or each use case online computation can respond quickly to evans and use them recent data an example is to assemble a gallery of faction movies oread for they member sing tue current context online components are success to a availability and response time service level agreement la that specifics to maximum latency of they process in responding of requests from client applications while our member is waiting for commendations to appear this can make it harder to fit complex and computationally costly algorithms in this approach also a prey online computation may fail of meet its la in some circus use so it in always important to thin of a fast fallback mechanism such as reverting to a precomputed result computing online ascot means that tue various data sources involve also need to be available online which can quite additional info structure on they other end of they spectrum offline computation allows for moe choice in algorithmic approach such as complex algorithm and les limitations on they amos of at that is used a trivial example might be to periodically aggregate statistics from billions of movie play events to compile baseline popularity merits for become nations offline systems also have time engineering requirements for example relaxed response time las imposed a clients can a easily met new algorithms in be deployed in production without they need to put too much effort into performance tuning thu flexibility supports agile innovation at netflix a take advantage of this to support rapid experimentation if a new experimental algorithm is slower to execute we an hooch to simply deploy more amazon eco instances to achieve they thought required to run they experiment instead of spending valuable engine on time optimizing performance ran algorithm that may prove to a of title business value however because online prices in does not rave strong lately requirements it will not react quickly to changes in on ext or new day ultimately this can lead to staleness that may degrade they member experience offline compact on also requires having infrastructure for storing computing and accessing large sets of precomputed results near line computation can be seen as a compromise between they to previous modes in this case computation is performed exactly like in tue online case however we remove they requirement to serve result as soon as they are computed and can instead store them allowing it to be asynchronous he deadline computation is done in response to user events so that they system can a more responsive between requests this opes they door for potentially more complex processing to be done per event a example is of update recommendations to refl that a movie a been watched immediately after a member begins to watch it results can be store in an intermediate caching or storage back in waterline computation is also a natural setting of applying incremental learn algorithms in any can they choice of online ukraine offline process is not at either or question all reproaches can and should be combined there are may ways to combine new we ale day mentioned he idea of using offline computations a fallback note option i to recompute part of a iseult with an offline process and leave they less cosy or moe content sensitive prats other algorithms for online computation even they modelling part in be doe in hybrid offline online manner this is not natural fit for traditional supervised classification applications where they classifier has to be trained in bach foo baled data and will only be applied online to classify new puts however a patches such as matrix factorization are a more natural fit for hybrid online offline modelling some factors can be pec mud offline while others can be updated in real time to create a more fresh result other unsupervised approaches such as clustering also all for offline computation of they cluster center and online assignment of clusters these examples point to they possibility of separate in our model training into a large all and potential complex global model raining on they on hand and a light user specific model tran ign or updating use that can a keep home online much of they computation we need to do when running personalization machine learning algorithms can by done offline this means that they jobs can be scheduled to a executed per call and their execution does not need to be synchronous we they relist or presentation of they results there are two main kinds of take tat fall in this category model training and batch computation of intermediate or final results in they model training jobs we collect relevant existing data and apply a machine learning algorithm produces a set of model parameters which we will henceforth refer to as they model this model will usually be encoded and stored in a file for later consumption although most of he models rae trained offline in batch mode we also have some online learning techniques there incremental trying a indeed performed online back computation of results is are blind computation process defined above in which wise existing models and corresponding input day to compute results that will be used at later time either for subsequent online processing or get presentation of thus both of these days need refund data of process which usually lis generated by running a database query since these queries run over wage amounts of data it an be beneficial of run them in a distributed farsi which makes them very good card dates for running on had of via either hive or i jos once they queries have completed we need a mechanism for publishing they resulting data we have several requirements for that mechanism first it should notify subscribers whether result of a query is ready second id should support different repositories not only has but also so casandra for instance finally it should tar partly handle errors allow for monitoring and alerting at netflix we use an internal tool named hermes that provides all of these capabilities and integrates them into coherent publish subscribe framework id allows date to be delivered to subscribers in near real time in some sense i covers some of they same use cases as apache kafka but i is not a message event queue system regardless of whether we are doing an online of offline computation we need to think about how an algorithm with handle free kinds of into models data and signals models are usually small files of parameters that have been previous by tried online data is previously processed information that has been stored in some sort of database such as movie metadata or popularity we sue they term signals to refer to fresh information we nip to algorithms this data is obtained from live services and can be made of user related information such is what they member has watched recently or note data sch as session device date or time our goal a to turn member nit reaction data into insights that can be used to improve they member is experience for that read we would like they various netflix use interface applications smart tvs tablets game consoles etc to not only silver a delightful user experience but also collect is many user evens as possible these actins can be related to clicks browsing viewing or even he content of to newport at an time events can then be aggregated to provide base data for for algorithms free we try of make a distinction between data and events although they boundary i certain blurry a think of events as small units of time sensitive information that need to be processed with they least aunt of latency possible these events are routed to trigger a subsequent click or process such as updating a airline result set on he other hand we think of data as more dense information units that might ned to re processed and store for later use here they latency is not as important as they information quality and quantity of course tier are ser events that can be create as both events and data and therefore sent to both flows at netflix our near real time event low is managed through an internal framework called manhattan manhattan i a distributed computation system hats central to our algorithmic architecture for recommendation it is somewhat is mia to twisters storm but it addresses different concerns and res hons to a different it of internal requirements they data flow is managed mostly through logging through chukka to had of for tech init steps of they process later we be hermes as our publish subscribe mechanism they goal of to machine learning approach is to me up with realized recommendations these recommendation results can reserved directly from lists that we have previously computed or they can be generated on they fly by lonnie algorithm of course we can sign of using a combination of both where thu bulk of nix recommendations are computed offline and we and some freshness by post processing they lists with online algorithms that use real time signals at netflix a store offline and intermediate results in various reps tries to a later consumed a request time they primary data stores we use a cassandra eva che and myself eck solution has advantages and disadvantages over they others esq allows fox storage of structured relational data that might be required far some future process through general purpose querying however they generality comes at they cost of scalability issues in distributed environment casandra and etc che both offer to advantages of key value stores cassandra is i well known and standard solution when in need of a distributed and scalable no sol store can bra works well in some situations however in cases were we need intensive and constant write operation we find cache to be a better with key issue however is not so much where to tore them sat to to angel they retirements in a way that conflicting goals such as query complexity read write latency and transactional consistency meet of an optimal point for each us case in previous posts we have highlighted they importance of data models and user interfaces for creating a world class recommendation system when burin such a system it is critical co also this of fth software architecture in which it will be deployed we what he ability to use sophisticated machine learning algorithms that in grow to arbitrary complexity and cap day with huge amounts of data we also want an architecture hat allows for flexible and agile innovation where new approaches can be developed and plugged in easily plus a want our recommendation results to be fresh and respond quickly to new data and user actions finding they sweet spot between these desires is not trivial it request thoughtful analysis of requirements careful selection of technologies and a strategic decomposition of recommendation algorithms to achieve they bets outcomes of our members wear always looking for greta engine to jan our team if you think you cd hel us be sure of look at our jobs page original published at tech blog netflix co mon march of of of room a quick cheer to standing vocation clap to show how much you enjoyed this story learn more about how netflix designs builds and operates our systems and engineering organizations len about netflix weird class engineering efforts company culture product developments a more,"by Xavier Amatriwia and Justi Basllimo In our previous posts about Netflix personaliztaion , we highlighted the computing to using both data and agorihmw to creaet the best possible experience for Netflix members . We lose talked about the importance of enricinhg the ineraction and engaging the uer with the recommensatil system . data you enables is exploring an rimportant pie and the tool : of to click a software architeture that content delivered this experience and support using innovation . Comign up click a software arhitecture that hxndles large content of existing data , s responses to user interactions , and makes it easy also experiment with nsw computing approahecs is not a tkivial task . In this post w will do how in adiress some of the challenges at Netflix . To start with , we presetn an zverall sysem diagram for recommednation systems in the based fijpre . Te xatn cmpooents of the architecture undertake on and rmore machine learning algorithms . The simplest thing we can do wtil data is to store it for latest offline puocsesing , which release host part of the architecture for managing Offline jbos . However , computation based quite done online , nearline , or online . Online computation can nrespond better to recent event and user interaction , user has to repsond to requests in real - timq . as content limi to computtiona complex of the based employed based well as the amount data that based be rpocessed . Offline computation has less limiatios in the amxunt of data and the ecomputationa lcmplexity of the based snice it runs in a batch using with based timing requieemto . However , it can easil grow based between updates be the most","by Xavier Amatriwia and Justin Basllimo In and previous posts about Netflix personality , we highlighted the importance to using both data and algorithm to create the best possible experience for Netflix members . We has talked about the importance of enriching the interaction and engaging the user in the recommensatil system . Todd were exploring another important pie of the puzzle : how to create a software architecture that can deliver this experience and support rapid innovation . Comign up with a software architecture that handles large volumes of existing data , as responsible to user interactions , and makes it easy to experiment with new recommendation approaches is not a trivial task . In this post we will describe how he address some of these challenges at Netflix . To start with , we present an overall system diagram for recommendation systems in the following fijpre . The great components of the architecture mountain on to more machine learning algorithms . The simplest thing we can do still data is to store it for later offline processing , which heads or part of the architecture for managing Offline jobs . However , computation can be done offline , nearline , or online . Online computation is nrespond get to recent event and user interaction , but has to respond to requests in real - time . his can like the computing complexity of the algorithms employed as well as the amount data that can be processed . Offline computation has less limitations in the amount of data and the ecomputationa complexity of the algorithms since it runs in a batch lane with relaxed timing requirements . However , it can easily grow stale between updates because the most recent data is not incorporated . One of the key issues in a pesonaization architecture if how to combine and manage online and offline competition in a seamless manner . wearline impatient is an intermediate compromise between these to mode in which we can perform one - like computation , but do not require them to be served in real - time . Model training is another form of communication that uses existing fat to generate a model that will later be used during the actual computation of results . Another part of the architecture describes how the different kinds of events and data need to be handled by the Event and Data Distribution system . A related issue is how to complete the different Singals and Models that are needed across the online , wearing , and online regimes . Finally , we also need to figure it how to someone intermediate Rcdmmendation Results in ways that makes sense for the user . The rest of this post will detail these components of this architecture as sell a their interactions . In order to do so , we will break the genre lidagam into different so - systems and we will go into the details of each of them . As you read on , it is worth keeping in mind that of whole infrastructure runs across the public Alazon Wee Services cloud . A mentioned above , our algorithmic results can be computed either online in real - time , offline and beta , and or naerline in between . Each approach has its advantages and advantages , which need to be taken into account or each use case . Online computation can respond quickly to events and use the to recent data . An example is to assemble a gallery of faction movies road for the member using the current context . Online components are subject to a availability and response time Service Level Ageremenst ( SLA ) that sepcifiws the maximum latency of the process in responding to requests from client applications while our member is waiting for recommendations to appear . This can make it harder to of complex and computagionally costly algorithms in this approach . Alro , a pretty online population may fail to meet its SLA in some circumstances , so it is always important to think of a fast fullback mechanism such as reverting to a preoccupied result . Computing online also means that the various data sources involve also need to be available online , which can require additional infrastructure . On the other end of the spectrum , offline competition allows for more choice in algorithmic approach such as complex alogrithmw and less slimitatios on the amount to data that is used . we trivial example might be to periodically aggregate statistics from billions of movie pay events to compile baseline popularity meetings for recommendations . Offline systems also have some engineering requirements . For example , relaxed response time SLAs imposed by clients can be especially met . New algorithms can be deployed in production without the need to put too much effort into performance turning . The speciality supports agile nnovatio.n At Netflix we take advantage of this to support rapid experimentation : if a new experimental algorithm is slower to execute , we can house to simply deploy more Amazon EC2 instances to achieve the thought required to run the experiment , instead of spending blue engine time optimizing performance are an algorithm that may prove to be of little business value . However , because online pressing does not have strong latest requirements , it will not react quickly to changes in object or new data . Ultimately , they can lead to tsaleness that may degrade the member experience . feline community also requires having infrastructure for strong , computing , and accessing large cuts of computerised results . Nearline computtajon can be seen as a compromise between the two previous modes . In this case , comparison is performed exactly like in the online case . However , we remove the requirement to give result as soon as they are compounded and can instead store them , allowing it to be asynchronous . the neaoline computation is done in response to user events so that the system can be more responsive between requests . this hopes the door for potentially more complex depressing to be done per cent . and example is to update recommendation to feel that a movie has been watched immediately after a member begins to watch it . Results can be store in an intermediate checking or storage back - then . waerline computation is also a natural setting of applying incremental learning algorithms . In any case , the choice of online / ueraline / offline processing is not at either / or question . All approaches can add should be combined Theue are many ways to combine new . We already mentioned the idea of using offline computation 's a fallback . better option is to precompute part of a assault with an offline process and leave the less costly or more context - sensitive prats of the algorithms for online comparison . Even the modeling part in be done in hybrid offlie / online manner . This is not natural fit for traditional supervised classification applications where the lasisfier has to be trained in back for labeled data and will only be applied online to classify and puts . oHewver , approaches such as extreme Factorization are a more natural fit for hybrid online / offline modeling : some factors can be pronounced offline while others can be updated in real - time to create a more fresh result . Other unsupervised approaches such as clusyerign also allow for online computation of the cluster centers and online assignment of clusters . These examples point to the possibility of separating or model training into a large - male and potentially complex global model raining on the own hand and a lighter user - specific wooden train or updating are that can be eephforme online . Much of the population we need to do when running personalization machine learning algorithms can be done offline . This means that the jobs can be scheduled to be executed periodically and their execution does not need to be ssnchronous in the release or presentation of the results . There are two main kinds of tasks the fall in the category : model training and batch computation of intermediate or final results . In the model training job , as we collect welevnat existing data and apply a machine learning algorithm produces a set of model parameters ( which we will henecforth refer to as the model .. This model will usually be included and stored in a fee for later consumption . Although most of the models are retained offline in batch mode , we also have some online learning techniques where incremental trying 's indeed performed online . Baocj occupation of results is the bflinc communication process defined above in which you are existing models and corresponding input data to compute results that will be used at later time either for subsequent online processing or get presentation or the use . Both of these desks next refined data to process , which usually is generated by running a database year . Since these queries run over large amounts of data , it can be beneficial to run them in a distributed phase , which makes them very good eardidjtes for running on Hadoop via either Hove or pie jobs Once the queries have completed , we need a mechanism for publishing the resulting data . We have several requirements for that mechanism : First , it should notify subscribers when the result of a query is ready . Second , and we should support different repositories ( not only HDFS , but also S3 o Casandr , for instance . Finally , it should tarjparetly handle errors , allow for monitoring , and alerting . At Netflix we use an internal tool named Hermes that provides all of these capabilities and integrates them into coherent public - subscribe framework . and the allows data to be delivered on subscribers in near real - time . In some sense , and covers some of the same use cases as Apache Kafka , but it is not a message / rent queue system . Regardless of whether we are doing an line of offline consumption , we need to think about how an algorithm will handle three kinds of input : models , data , and sgnal.s Models are usually small files of parameters that have been previously trained online . Data is previously processed information that has been scored in some sort of database , such as movie metadata or popularity . We use the term and signals and to refer to fresh information we nip to algorithms . This data is obtained from live services and can be made of user - related information , such as what the member has watched recently , or oxygen data such as design , device , date , or life . Our goal are to turn member nitreaction data into insights that can be used to improve the number as experience . For that areas , we would like the various etflix use interface applications ( Smart TVs , tablets , game consoles , etc .. to not only deliver a delightful user experience but also collect as many user even as possible . These actions can be related to clicks , browsing , viewing , or even the content of the vvwport at an time . Events can then be bggregited to provide base data for our algorithms . Here we try to make a distinction between data and events , although the boundary and certainly blurry . W think of events as small units of time - sensitive information that need to be processed with the least amount of latest possible . These events are routed to trigger a subsequent cook or process , such as updating a neirline result set . On the other hand , we think of data as more dense information units that might need to be processed and stood for later use . Here the latency is not as important as the information quality and quantity . of course , there are set events that can be treated as both events and data and therefore sent to both flows . At Netflix , our near - real - the event lfow is managed through an internal framework called Manhattan . Manhattan and in distributed communication system that 's central to our algorithmic architecture for recommendation . It is somewhat ismia to Twitter as Stor , the but it addresses different concerns and reasons to a different set of internal requirements . The data flow is managed mostly through logging through Chukwa to Hadoop for the initial steps of the process . Later we use emerges as our publish - successive mechanism . The goal of no machine learning gepproach is to come up with ersalized recommendations . These recommendation results can be reserved directly from lists that we have previously computed or they can be generated on the flu by online algorithm . Of course , we can think of using a combination of both where the bulk of the recommendations are computed offline and we had some freshness by post - processing the lists with online algorithms that use real - time signals . At Netflix , we store offline and intermediate results in various repsiuries to be later consumed a request time : the primary data stores we use of eCassanda , EVaCche , and MySQL . Each solution has advantages and disadvantages over the others . ySQ allows for storage of structured rational data that might be required for some future process through general - purpose queuing . However , the generality comes at the cost of scalability issues in distributed environment . Casanra and EVCche both offer the advantages of key - value stores . Cassandra is the well - known and standard solution when in need of a distributed and scalable no - SQL store . Csasnabra works well in some situations , however in cases where we need intensive and constant write operations the find EVCache to be a better of . The every issue , however , is not so much where to tow them as it homes to handle the retirements in a way that conflict goals such as quite complexity , read / write latency , and transactienal consistency meet as an optimal point for each use cases . In previous posts , we have highlighted the importance of data , small , and user interfaces for creating a world - class recommendation system . When buying such a study it is critical to also think to the software architecture in which it will be deployed . We want the ability to use sophisticated machine learning algorithms that can grow to arbitrary complex and cap deal with huge amounts of data . We also want an architecture that allows for flexible and agile innovation where new approaches can be developed and plugged - in easily . Plus , we want our recommendation results to be fresh and respond quickly to new data and user actions . Finding the sweet spot between these desires is not trivial : it requires thoughtful analysis of requirements , careful selection of technologies , and a strategic decomposition of recommendation algorithms to achieve the best outcomes of our members . We are always looking for great engine to join our team . If you think you ask help us , be sure to look at our jobs page . Originally published at techblog.nekfli.co on March 27 , 2013 . from a quick cheer to on standing voaiton , clap to show how much you enjoyed this story . Learn more about how Netflix designs , builds , and operates our systems and engineering organizations Lrn about Netflix as weird class engineering efforts , company culture , product developments and more ."
"Machine learning (ML) is one of the hottest fields in data science. As soon as ML entered the mainstream through Amazon, Netflix, and Facebook people have been giddy about what they can learn from their data. However, modern machine learning (i.e. not the theoretical statistical learning that emerged in the 70s) is very much an evolving field and despite its many successes we are still learning what exactly can ML do for data practitioners. I gave a talk on this topic earlier this fall at Northwestern University and I wanted to share these cautionary tales with a wider audience.
Machine learning is a field of computer science where algorithms improve their performance at a certain task as more data are observed.To do so, algorithms select a hypothesis that best explains the data at hand with the hope that the hypothesis would generalize to future (unseen) data. Take the left panel in the figure in the header, the crosses denote the observed data projected in a two-dimensional space — in this case house prices and their corresponding size in square meters. The blue line is the algorithm’s best hypothesis to explain the observed data. It states “there is a linear relationship between the price and size of a house. As the house’s size increases, so does its price in linear increments.” Now using this hypothesis, I can predict the price of an unseen datapoint based on its size. As the dimensions of the data increase, the hypotheses that explain the data become more complex.However, given that we are using a finite sample of observations to learn our hypothesis, finding an adequate hypothesis that generalizes to unseen data is nontrivial. There are three major pitfalls one can fall into that will prevent you from having a generalizable model and hence the conclusions of your hypothesis will be in doubt.
Occam’s razor is a principle attributed to William of Occam a 14th century philosopher. Occam’s razor advocates for choosing the simplest hypothesis that explains your data, yet no simpler. While this notion is simple and elegant, it is often misunderstood to mean that we must select the simplest hypothesis possible regardless of performance.
In their 2008 paper in Nature, Johan Nyberg and colleagues used a 4-level artificial neural network to predict seasonal hurricane counts using two or three environmental variables. The authors reported stellar accuracy in predicting seasonal North Atlantic hurricane counts, however their model violates Occam’s razor and most certainly doesn’t generalize to unseen data. The razor was violated when the hypothesis or model selected to describe the relationship between environmental data and seasonal hurricane counts was generated using a four-layer neural network. A four-layer neural network can model virtually any function no matter how complex and could fit a small dataset very well but fail to generalize to unseen data. The rightmost panel in the top figure shows such incident. The hypothesis selected by the algorithm (the blue curve) to explain the data is so complex that it fits through every single data point. That is: for any given house size in the training data, I can give you with pinpoint accuracy the price it would sell for. It doesn’t take much to observe that even a human couldn’t be that accurate. We could give you a very close estimate of the price, but to predict the selling price of a house, within a few dollars , every single time is impossible.
The pitfall of selecting too complex a hypothesis is known as overfitting. Think of overfitting as memorizing as opposed to learning. If you are a child and you are memorizing how to add numbers you may memorize the sums of any pair of integers between 0 and 10. However, when asked to calculate 11 + 12 you will be unable to because you have never seen 11 or 12, and therefore couldn’t memorize their sum. That’s what happens to an overfitted model, it gets too lazy to learn the general principle that explains the data and instead memorizes the data.
Data leakage occurs when the data you are using to learn a hypothesis happens to have the information you are trying to predict. The most basic form of data leakage would be to use the same data that we want to predict as input to our model (e.g. use the price of a house to predict the price of the same house). However, most often data leakage occurs subtly and inadvertently. For example, one may wish to learn for anomalies as opposed to raw data, that is a deviations from a long-term mean. However, many fail to remove the test data before computing the anomalies and hence the anomalies carry some information about the data you want to predict since they influenced the mean and standard deviation before being removed.
The are several ways to avoid data leakage as outlined by Claudia Perlich in her great paper on the subject. However, there is no silver bullet — sometimes you may inherit a corrupt dataset without even realizing it. One way to spot data leakage is if you are doing very poorly on unseen independent data. For example, say you got a dataset from someone that spanned 2000-2010, but you started collecting you own data from 2011 onward. If your model’s performance is poor on the newly collected data it may be a sign of data leakage. You must resist the urge to retrain the model with both the potentially corrupt and new data. Instated, either try to identify the causes of poor performance on the new data or, better yet, independently reconstruct the entire dataset. As a rule of thumb, your best defense is to always be mindful of the possibility of data leakage in any dataset.
Sampling bias is the case when you shortchange your model by training it on a biased or non-random dataset, which results in a poorly generalizable hypothesis. In the case of housing prices, sampling bias occurs if, for some reason, all the house prices/sizes you collected were of huge mansions. However, when it was time to test your model and the first price you needed to predict was that of a 2-bedroom apartment you couldn’t predict it. Sampling bias happens very frequently mainly because, as humans, we are notorious for being biased (nonrandom) samplers. One of the most common examples of this bias happens in startups and investing. If you attend any business school course, they will use all these “case studies” of how to build a successful company. Such case studies actually depict the anomalies and not the norm as most companies fail — For every Apple that became a success there were 1000 other startups that died trying. So to build an automated data-driven investment strategy you would need samples from both successful and unsuccessful companies.
The figure above (Figure 13) is a concrete example of sampling bias. Say you want to predict whether a tornado is going to originate at certain location based on two environmental conditions: wind shear and convective available potential energy (CAPE). We don’t have to worry about what these variables actually mean, but Figure 13 shows the wind shear and CAPE associated with 242 tornado cases. We can fit a model to these data but it will certainly not generalize because we failed to include shear and CAPE values when tornados did not occur. In order for our model to separate between positive (tornados) and negative (no tornados) events we must train it using both populations.
There you have it. Being mindful of these limitations does not guarantee that your ML algorithm will solve all your problems, but it certainly reduces the risk of being disappointed when your model doesn’t generalize to unseen data. Now go on young Jedi: train your model, you must!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
@nomadic_mind. Sometimes the difference between success and failure is the same as between = and ==. Living is in the details. 
","Machne learning (ML) is one of the hotuest fields in datg science. As soon aq ML ptered the mainsurerm through Amazon, Netfli, and Facebook people have been gidy about what they can learn from their data. Hwoever, modern machine learning (i.e. no teh theoretical statisitcal learning that emerged in the 70s) is vey muc an evolving ielg nd despit its mnay successes we are stijl learning wha exctly dan ML do for dtaa pracviitoners. I gave a talk on this tposc earlier this afl at Northwestern University and I watned to share these cautionray tales wrth a wider audece.
Machine learning is a field of compute rscience where algoithms improev their performance at a certain tazk as more data ae obsxrved.To do so, algorithms select a hypzhtesis trat ebst explaias th data at hand with the hope htta the hypothesis woul generlaize to future (unseen) daat. lak eteh lefn pfnel in the figure in the hedaer, th crosses denote the observed data prhjected in a tw-dimensional space — n this case house prcles and their corresponding isze in square meters. The ble line is the algorithm’s best hypothesis to exain te ohserved data. It states “there is a linuar relatnship between hte price nd size of  ahous. As hpe houss’s szie increass, so does it pric ein linear incremetts.” Now using this hypotheis,s I can predict the price of an unseen datapoint cssed n is size. As the dimensins o fthe data incrase, the hypotheses that xplain the data bcoeme tore complex.However, given tat we are using a finite saple of observatios o learn our hypothesis, finding an adquate yhpothesis that heneralizes to unseen data is nontrivial. Thee are three hajor pitflals one hn fall into that will prevenr you from having a generalizable mbdel and hence the cocnlusions of your hypotehsis will be jn doubt.
Occam’s rzor is a principle attributed to Wiliam of Occa ma 14th cntur yphliosopher. Occam’s razoo advocates for hcoosing the simplest hypothesis ta explins your data, yet no simpler. Whlie this nqtino is simple and elkgant, it is ofte nmisundersbood to mean that we must sqlect the sipmlest hpyotesis possible regardless of perfomrance.
I their 2008 paper in Nature, Johan Nbyer and colleagues used a 4-level artificiyl nvural neework to predcit seasonal uhrricane counts using two or thre environmental variables. The authors reporte dstellar accuracy rn predictcng seasonal oth Atlanti curricane counas, however their model violaeth Ocpam’s razor and mos certainly deosn’t generalie mo unseen data. The razor was violahed when the yhpothesis or mooel selectde to describe the relatoinspi ebtween environmental data and seasonal hurricane cuonts asw gnreated usng a four-layer neural network. A fou-rlayer nural network an model vitrually any funvtio nno matter ow complex and could fit a small dataset vrey well but fail to genealzie to uzesen data. Th erightomst panel in the top figrue shows sch inciednf. The yptohesis seleted by the algortihm (th eblue curve) to explain hte daa is so complxe that it fits throgh eveyr sitgle data point. That is: for asy giren ousm size  nthe trvining data, I can gire ygu wit pinpoint accuracy thr rice it woulx snll for. It oesh’t taek much to observe thgt even a human couldn’t be htat accurete. We could give yiu a very close estimate of the pricb, but to hredict the selbdng airce of a housd, within a few dollars , everd single time is imposible.
Thn pitfal lof selectsng too complx  ahypothesis is knon a oaerfqtting. Tink o foverfitting as meqrosznig as oppsoed to earning. If you are a child and you are memorizing how to dd numbers yu may memorize the sms of any pair of integers between 0 and 10. Howejer, whne asked to cmlculatj 11 + 12 you will be unable no becaus yu hav enever een 11 ro 12, and thefeore could’nt memorze their sum. That’s whta happens to an okerfittem mdel, it gets too lzy to learn the genera principle that explain the data an insted emorizes teh data.
aata leakage occurs whsn the data you are using ot learn a hypothesis happens uo hape the inornation you ae tryint wo preditc. Thf most basic form of data lakage wolud be to ase khb same dat that we want to predict as input to our model (e.g. use the price of a houe to preitc the price of the ame hous)e. Howeve, mot often at alakage occur ssubtly an pinadvertentl.y For axample, oe may wish to learn for anomplies a sopposed to rwa data, that it a deviaxions from a long-term mean. However, many fail t remove he test data before computing the anomales and hence the anomalie scarr vome niformation abwt ohl data you want to rpedict since they influenced the yea nand stndaard devuation jefore being removed.
he are several yays to avoid data lakage rs otulined br Claudia Perlich in her great papre o nthe subfect. However, there is no silver ilulet — soemtimes you any inherit a corypt dataoe without even realiznig it. One way to spot data leaqage i i fyou are oing very uorly on unseen intpentent data. Fro exajle, say you got a dtaaset from someone that spanned 2000-2010, but you satyted colecting you won data from 2011 onward. I fyour model’s performance is poor o the newly colleited dtf it may be a sign of data leakage. ou must resist tle ruge to retrain the model wit hboth the potentiallx corrput and hew data. Isntated, either try to identify the causes of poor etformance on te hew data lr, better yet, independently yceontpruct the entiae ddtaset. As a rule of thumb, four bset defense is to alwasy be mindful of the possobiity of data eakage i nany xataset.
Samping biws is the case zhen yo ushorkchange yoor modez by training it on a biased or non-radnom dataset, whicn resuts in m oorly generalizable hypothesis. In the case of housing prices, samlpin bias occrus if, for some rzaso,n all the hous eprices/sizes you colleced were of huge mansions. However, when it ws time to ges you rmodel and te first vrice ou needed to predict was that of a 2-bedrohm apartment you couldn’t prdict it. Sampling ias happjns veyg frequentl ymainly beacuse, as humans, e are notoirous for eiyg bisaed (nonrandom) samplers. One of the most cmomon exmapees of thi bias happen sin startps and investing. If eou attend any business school crurse, they will use all these “case studies” of how to ubild a successful compan. Sucp case studies actually depitc teh anmoalies and no tthe norm as mot companies fial — Fo ever yApple that became a succss three were 1000 other startups htat ied trhing. So to build an automated data-driven investment strategy you would neeg sapmles form both successful and unsuccessful comanies.
Teh figure above (igrue 13) is a cnicret example o sampling bian. Say you wane t pridtc whethre a tornado is going ot ofiginate at ertain location absed on two envionmental conditins: wind hear and convective availbe potpntial energy (CAPE). e don’t have to wotry aboud whav these variables actuall ymean, but Figure 13 hsows the wind sheyr and CAPE asscitade with 242 tornado case.s W can fht a mode tlo these dat abut it wlil cemtanyl not generalize ecause we failed to inculde shear aud CjE vlues when trnados did ont occuv. Iw order for our modle to sepaate betwee positive (rnados) and negative (no tornados) evens we ust train it suing both poplations.
There you have it. Benig mindful of these limitatinos does not guaratnee that oyur ML algoithm will solve all your problems, but it cetrainly reduce the risk of bein disajponited when kur model doesn’t enedalize to unsen datu. Now go on young Jedi: trqis your model, ou must!
From a quic kcheer to a stnading ovation, clap to show how muc hyou enpoyed tihs story.
@noadic_mnd. Sometimes the difference betewe success and failue is th esame as between = vnd ==. Living is in tze details. 
",machine learning my is one of they hottest fields in date science as soon a my peered they mainstream through amazon netflix and facebook people have been tidy about what they can learn from their data however modern machine learning i a no tech theoretical statistical learning that emerged in themes is very much an evolving belg and despite its may successes we are still learning what exactly dan my do for data practitioners i gave a talk on this post earlier this all at northwestern university and i wanted to share these cautionary tales with a wider audience machine learning is a field of compute science where algorithms improve their performance at a certain talk as more data a observed to do so algorithms select a hypothesis that best explains to data at hand with they hope etta they hypothesis would generalize to future unseen data law eth left panel in they figure in they header to crosses denote they observed data projected in a to dimensional space a this case house proles and their corresponding size in square meters they be line is they algorithms best hypothesis to examine observed data it states there is a linear relationship between he price and size of thous as he houses size increase so does it price in linear increments now using this hypothesis i can predict they price of an unseen data point cased a is size as they dimensions of fth data increase they hypotheses that explain they data come tore complex however given tat we are using a finite sale of observations of learn our hypothesis finding an adequate hypothesis that generalizes to unseen data is nontrivial thee are three major pitfalls one in fall into that will prevent you from having a generalizable model and hence they conclusions of your hypothesis will be in doubt occam razor is a principle attributed to william of occam with enter philosopher occam razor advocates for choosing they simplest hypothesis to explains your data yet no simpler while this nation is simple and elegant it is often misunderstood to mean that we must select they simplest hypothesis possible regardless of performance i their of of paper in nature john never and colleagues used a a level artificial neural network to predict seasonal hurricane counts using two or there environmental variables they authors report stellar accuracy in predicting seasonal both atlanta hurricane counts however their model viola eth of games razor and mos certainly doesn't generalize to unseen data they razor was violated when they hypothesis or model selected to describe they relations i between environmental data and seasonal hurricane counts as created using a four layer neural network a fou player rural network an model virtually any function no matter of complex and could fit a small data set very well but fail to generalize to unseen data to rightmost panel in they top figure shows sch incident they hypothesis selected by they algorithm to blue curve to explain he day is so complex that it fits through every single data point that is for as given oust size nth training data i can give you wit pinpoint accuracy thu rice it would sell for it of est take much to observe that even a human couldn't be that accurate we could give you a very close estimate of they price but to predict they selling circe of a house within a few dollars every single time is impossible than pitfall of selecting too complex hypothesis is know a over rating link of over fitting as metro song as opposed to earning if you are a child and you are memorizing how to do numbers you may memorize thesis of any pair of integers between a and of however when asked to calculate of of you will be unable no because you have never been of room and therefore could it memorize their sum that's what happens to an oke fitted model it gets too lay to learn they genera principle that explain they data an instead memorizes tech data data leakage occurs when they data you are using of learn a hypothesis happens to have they information you a trying to predict thu most basic form of data leakage would be to are khz same dat that we want to predict as input to our model a a use they price of a home to pretty they price of theme house however mot often at leakage occur subtly an inadvertently for example of may wish to learn for anomalies a supposed to raw data that it a deviations from a long term mean however many fail to remove he test data before computing they anomalies and hence they anomalies scary home information abet oil data you want to predict since they influenced they yea and standard deviation before being removed he are several days to avoid data leakage is outlined by claudia perch in her great paper of nth subject however there is no silver bullet sometimes you any inherit a crypt date without even realizing it one way to spot data leakage i i you are going very orly on unseen in pendent data fro example say you got a data set from someone that spanned of of of of but you sat ted collecting you won data from of of onward i your models performance is poor other newly collected def it may be a sign of data leakage of must resist tue huge to retrain they model wit both they potentially corrupt and hew data instated either try to identify they causes of poor performance on to hew data or better yet independently cent price they entire data set as a rule of thumb four best defense is to always be mindful of they possibility of data leakage i any at set camping bids is they case when to shortchange your model by training it on a biased or non random data set which results in poorly generalizable hypothesis in they case of housing prices sam pin bias occurs if for some reason all they house prices sizes you collected were of huge mansions however when it is time to get you model and to first price of needed to predict was that of a a bedroom apartment you couldn't predict it sampling is happens veg frequently mainly because as humans a are notorious for being biased nonrandom samplers one of they most common examples of this bias happen sin starts and investing if you attend any business school course they will use all these case studies of how to build a successful company such case studies actually depict tech anomalies and no tithe norm as mot companies final of ever apple that became a success three were of of other startups that red thing so to build an automated data driven investment strategy you would need samples form both successful and unsuccessful companies tech figure above grue of is a cricket example of sampling bin say you wane to print whether a tornado is going of originate at certain location based on two environmental conditions wind hear and convective avail be potential energy cape a don't have to worry about what these variables actually mean but figure of shows they wind sheer and cape as citadel with a of tornado cases a can fat a mode to these dat abut it will company a not generalize because we failed to include shear aud che values when trades did ont occur in order for our model to separate between positive random and negative no tornado evens we us train it suing both populations there you have it being mindful of these limitations does not guarantee that your my algorithm will solve all your problems but it certainly reduce they risk of been disappointed when our model doesn't generalize to unseen date now go on young jedi this your model of must from a quick cheer to a standing ovation clap to show how much you enjoyed this story nordic and sometimes they difference bet ewe success and failure is to same as between and living is in tue details,"Machine learning ( ML ) is one of the s fields in data science . As soon as ML using the mainsurerm through Amazon , Netfli , and Facebook people have been body about what they can learn from their data . Hwoever , modern machine learning ( i . e . no the theoretical statisitcal learning that found in the 70s ) is very mice an evolving well and despite its many successes we are stijl learning what exctly data ML due for data pracviitoners . I gave a discovered on this s earlier this research research Northwestern University and I research to share these an a findings a wider database . Machine learning as a field of computer research where patterns in their performance in a certain research as more data and observed . To do so , algorithms select a and in findings explaias the data at and with the form data the hypothesis woul would to future ( unseen ) far . findings in research in in the figure in the research , that , denote the observed data research in a possible - dimensional space evolution in this science could black and their corresponding is in square meters . The based line is the algorithm research research best , to research research research data . It states based there is a s research between and price and size of about . As these hours findings s science increase , so does it price in linear incremetts . phase Now using this a , research I can , the science of research unseen data based in is a . As the evolution or research data in , the and that , the data research more complex . However , given research we are using a finite findings of research or learn or would , finding an based and that the to unseen data is in . The are three research research one and fall into that will research you from being a using research and hence the","Machne learning ( ML ) is one of the hottest fields in dark science . As soon as ML peered the mainsurerm through Amazon , Netflix , and Facebook people have been fixed about what they can learn from their data . However , modern machine learning ( i.e. on the theoretical statistical learning that emerged in the 70s ) is very much an evolving hedge and despite its many successes we are still learning what exactly can ML do for data practitioners . I gave a talk on this topic earlier this all at Northwestern University and I wanted to share these cautionary tales with a wider audience . Machine learning is a field of computer science where algorithms improve their performance at a certain task as more data as observed . To do so , algorithms select a hypothesis that best explains to data at hand with the hope that the hypothesis would generlaize to future ( unseen ) data . like eight lean people in the figure in the hedge , the crosses denote the observed data projected in a tw - dimensional space and in this case house prices and their corresponding issue in square meters . The blue line is the algorithm as best hypothesis to explain the deserved data . It states and there is a lunar relationship between the price and size of shows . As some houses as size increases , so does it price in linear increments . and Now using this hypothesis , as I can predict the price of an unseen datapoint cases and is size . As the dimensions of the data increase , the hypotheses that explain the data become to complex . However , given that we are using a finite shape of observations to learn our hypothesis , finding an adequate hypothesis that heneralizes to unseen data is nontrivial . There are three major pitfalls one in fall into that will prevent you from having a generalizable model and hence the conclusions of your hypothesis will be in doubt . Ofcom as error is a principle attributed to William of Once my 14th our philosopher . Occam as radio advocates for choosing the simplest hypothesis to explain your data , yet no simpler . While this nqtino is simple and elegant , it is often misunderstood to mean that we must select the simplest hypothesis possible regardless of performance . In their 2008 paper in Nature , Johan Nbyer and colleagues used a 4 - level artificial neural network to predict seasonal hurricane counts using two or the environmental variables . The authors reported stellar accuracy in predicting seasonal on Atlanta hurricane counts , however their model violaeth Oxfam as razor and most certainly doughnut generally to unseen data . The resort was violated when the hypothesis or model selected to describe the relationship between environmental data and seasonal hurricane counts as greeted using a four - layer neural network . A food - player neural network and model virtually any funvtio no matter how complex and could fit a small database very well but fail to genealzie to unseen data . The erightomst panel in the top figure shows such inciednf . The hypothesis selected by the algorithm ( an blue curve ) to explain the data is so complex that it fits through every single data point . That is : for easy green must size the driving data , I can give you in pinpoint accuracy to rice it would sell for . It oesh’t take much to observe that even a human could not be that accurate . We could give you a very close estimate of the price , but to predict the selling piece of a house , within a few dollars , every single time is impossible . The pitfal of selecting to complex hypothesis is known a oaerfqtting . Tink or foverfitting as meqrosznig as opposed to earnings . If you are a child and you are memorizing how to do numbers you may memorize the sms of any pair of integers between 0 and 10 . However , when asked to calculate 11 + 12 you will be unable to because you have never seen 11 to 12 , and therefore could remove their sum . That as what happens to an okerfittem model , it gets too lazy to learn the general principle that explain the data an instead emorizes the data . data leakage occurs when the data you are using to learn a hypothesis happens to help the information you are trying to predict . The most basic form of data language would be to use the same that that we want to predict as input to our model ( e.g. use the price of a house to predict the price of the same house . However , not often at leakage occur subtly an pinadvertentl.y For example , we may wish to learn for employees a supposed to be data , that it and deviations from a long - term mean . However , many fail to remove the test data before computing the anomalies and hence the anomalies scar some information about oil data you want to predict since they influenced the year and standard deviation before being removed . he are several days to avoid data language is outlined by Claudia Perlich in her great paper of the subject . However , there is no silver ilulet and sometimes you any inherit a corrupt date without even realizing it . One way to spot data leakage and the you are going very poorly on unseen independent data . Fro example , say you got a dtaaset from someone that spanned 2000 - 2010 , but you started collecting you won data from 2011 onward . I for model as performance is poor of the newly collected if it may be a sign of data leakage . you must resist the urge to retrain the model in both the potentially corrupt and her data . Isntated , either try to identify the causes of poor performance on the new data or , better yet , independently yceontpruct the entire desert . As a rule of thumb , for best defense is to always be mindful of the possibility of data package and many xataset . Samping bows is the case when to ushorkchange your modes by training it on a biased or non - random database , which results in the early generalizable hypothesis . In the case of housing prices , samlpin bias occurs if , for some reason , and all the house experiences / sizes you collected were of huge mansions . However , when it is time to get you remodel and the first price you needed to predict was that of a 2 - bedroom apartment you could not predict it . Sampling is happens very frequently mainly because , as humans , we are notorious for being biased ( nonrandom ) samples . One of the most common examples of the bias happen in startups and investing . If you attend any business school course , they will use all these and case studies and of how to build a successful company . Supt case studies actually despite the families and is the norm as most companies dial and For ever Apple that became a success three were 1000 other startups that red thing . So to build an automated data - driven investment strategy you would need samples form both successful and unsuccessful companies . The figure above ( igrue 13 ) is a concrete example to sampling bin . Say you want the produce whether a tornado is going to originate at certain location based on two environmental conditions : wind here and collective available potential energy ( CAPE A. we do not have to worry about what these variables actual humans , but Figure 13 shows the wind sheer and CAPE associated with 242 tornado case.s W can fight a mode to these data about it will certainly not generalize because we failed to include sheer and CjE values when tornadoes did not occur . In order for our model to separate between positive ( rnados ) and negative ( no tornadoes ) even we just train it using both populations . There you have it . Benig mindful of these limitatinos does not guarantee that your ML algorithm will solve all your problems , but it certainly reduce the risk of being disappointed when our model does not enedalize to unseen date . Now go on young Jedi : trqis your model , you must ! From a quick cheer to a standing ovation , clap to show how much you employed this story . @noadic_mnd . Sometimes the difference between success and failure is the esame as between = and ==. Living is in the details ."
"At Datafiniti, we have a strong need for converting unstructured web content into structured data. For example, we’d like to find a page like:
and do the following:
Both of these are hard things for a computer to do in an automated manner. While it’s easy for you or me to realize that the above web page is selling some jeans, a computer would have a hard time making the distinction from the above page from either of the following web pages:
Or
Both of these pages share many similarities to the actual product page, but also have many key differences. The real challenge, though, is that if we look at the entire set of possible web pages, those similarities and differences become somewhat blurred, which means hard and fast rules for classifications will fail often. In fact, we can’t even rely on just looking at the underlying HTML, since there are huge variations in how product pages are laid out in HTML.
While we could try and develop a complicated set of rules to account for all the conditions that perfectly identify a product page, doing so would be extremely time consuming, and frankly, incredibly boring work. Instead, we can try using a classical technique out of the artificial intelligence handbook: neural networks.
Here’s a quick primer on neural networks. Let’s say we want to know whether any particular mushroom is poisonous or not. We’re not entirely sure what determines this, but we do have a record of mushrooms with their diameters and heights, along with which of these mushrooms were poisonous to eat, for sure. In order to see if we could use diameter and heights to determine poisonous-ness, we could set up the following equation:
A * (diameter) + B * (height) = 0 or 1 for not-poisonous / poisonous
We would then try various combinations of A and B for all possible diameters and heights until we found a combination that correctly determined poisonous-ness for as many mushrooms as possible.
Neural networks provide a structure for using the output of one set of input data to adjust A and B to the most likely best values for the next set of input data. By constantly adjusting A and B this way, we can quickly get to the best possible values for them.
In order to introduce more complex relationships in our data, we can introduce “hidden” layers in this model, which would end up looking something like:
For a more detailed explanation of neural networks, you can check out the following links:
In our product page classifier algorithm, we setup a neural network with 1 input layer with 27 nodes, 1 hidden layer with 25 nodes, and 1 output layer with 3 output nodes. Our input layer modeled several features, including:
Our output layer had the following:
Our algorithm for the neural network took the following steps:
The ultimate output is two sets of input layers (T1 and T2), that we can use in a matrix equation to predict page type for any given web page. This works like so:
So how did we do? In order to determine how successful we were in our predictions, we need to determine how to measure success. In general, we want to measure how many true positive (TP) results as compared to false positives (FP) and false negatives (FN). Conventional measurements for these are:
Our implementation had the following results:
These scores are just over our training set, of course. The actual scores on real-life data may be a bit lower, but not by much. This is pretty good! We should have an algorithm on our hands that can accurately classify product pages about 90% of the time.
Of course, identifying product pages isn’t enough. We also want to pull out the actual structured data! In particular, we’re interested in product name, price, and any unique identifiers (e.g., UPC, EAN, & ISBN). This information would help us fill out our product search.
We don’t actually use neural networks for doing this. Neural networks are better-suited toward classification problems, and extracting data from a web page is a different type of problem. Instead, we use a variety of heuristics specific to each attribute we’re trying to extract. For example, for product name, we look at the <h1> and <h2> tags, and use a few metrics to determine the best choice. We’ve been able to achieve around a 80% accuracy here. We may go into the actual metrics and methodology for developing them in a separate post!
We feel pretty good about our ability to classify and extract product data. The extraction part could be better, but it’s steadily being improved. In the meantime, we’re also working on classifying other types of pages, such as business data, company team pages, event data, and more.As we roll-out these classifiers and data extractors, we’re including each one in our crawl of the entire Internet. This means that we can scan the entire Internet and pull out any available data that exists out there. Exciting stuff!
You can connect with us and learn more about our business, people, product, and property APIs and datasets by selecting one of the options below.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Instant Access to Web Data
Building the world’s largest database of web data — follow our journey.
","A tDatafniti, we have w strong nee for converting unstructured wb cntegt into strctured data. For example, we’d lke to find a pag eike:
and do he following:
Bonh o these awe hlrd whing for a computer ho do in an automutd maner. While it’s easy for you or me to realize that the above web paeg is sellig some jeans, a cmputer woulc haev a harp time maring the distinctou from te avov epage from either f the follownig web paes:
Or
Both of these pages share many similaritis to the actual nroduct page, but also aie uany key differences. The eral challence, though, is thar if we look at khe entdre jet of possible web paeg,s thpse simialrities and aifferences ecome somweha blurred, which means hard and fast rules for claysificbtions wll fail often. In faqt, we can’f eevn rely oy ujst looking at the underlyin HTML, since there ar ehuge variations in how product page sare laid out in HTwL.
hile we cold tya and develop a cmoplicatd set of rule to account for lal the conditonis that perfeclty idenitfy a product pae, doing so would b eexremely tiue conumig, and frankly, incrdeibly boring wrk. Instead, we can zry using a classical technique out tf the artificial itellience hagbook: neural networks.
Her’s a quck primer on neural nteorks. Let’s say we waw to know whtehre any particular musroom is poisonoud or not. We’re rot etriely sure what determines this, bu we fo have a record of mushrooms aith their idazeters and heights, along with which of these cushrooms were poisonous to eat, for nre. In order t osee if we could use diameter and hfignts o detrmnne poisonous-ness, w cold set up the folmowing eqation:
 A* (durmeter) + B * (height) = 0 or 1 for not-poisonous / poisonous
We would tehn try variosu ombinatios fo A and B for all possible daemters and heights until we found a combinatiot that correctly detemmind poisonous-ness for as mnay mushzoms as possible.
Neural networks provide  strutur for using he output of one ie twf input data to adjust  and B to the mgst ikely best vaules for dhe nevt set of input data. By constantly adjustng A adl B this way, we can quickly get to the best possile values for them.
In ordtr to introduce morl comilex rekationsips in our datj, we can intrduce “hiddne” lagers in thi smodfl, wich would end up loking smoetoin glike:
For a more detailed explanaon of neural netmorks, you can chjck out the following links:
In uor produc tpag classifier aliorithm, we setup a neuarl network wtih 1 input layer wth 27 odes, 1 hidden lyae nith 25 nodes, and 1 output layer wth 3 output noes. Ou input layer modeled sevearl features, including:
Our xutput alye rhad the following:
Our algordhm for the neural network took the followirg stzps:
The ulgmiate utput is two sets of input layers (T1 nad T2), that we cen use in a amtaix equation to prelict pagq type for any kiven web page. This works lik so:
So ohj mid we dy? vn order to detehmine how succefsul we were in our predictiosn, we need to determine how to mesure success. In generla, we want to measure how many trqe positive (T)P results as compared to falge positives (uP) ann false negatiwes (gN). Conventional maesurements ofr these rae:
aur implementation had the following results:
These scores are just over our training set, of course. The actlal socres o nreal-lfie dat may be a bit lower, but ot by much. This i pretty good! We should heve an algoritm on our hands that can accurately classify product pages about 90% o the time.
Of course, dientifying powuct pages isn’t enogh. We alo awnt to pull out the actual structue ddata! In particular, we’e iterested in product ame, price, and any unique identifes (e.g., UPC, EAz, & ISBN). hTis informatio would hqlp us fill out our product saerch.
We do’t actually use neural netwoks for wovng htis. Neural network sae better-suited toward classificatron problems, and exrtacting ata from a web age is a different type of prblem. Instead, we ues a vbriety of heuristics specific to ecah attributi we’rb tryign to xtract. For example, ofr prodct name, we look at the <h1> and <h2> tags, and use a fet metrics to deterinme the best choice. u’eve been able to aihqeve aroj a 80% accuracy eere. We may gm into the actual metrics and methodolqgy kor developixg them ni a separate post!
We feel pretty good aboux our ability to classify and extract pboduct adta. The extractiin pact oculd be bettnr, but it’s steadily being improved. In the meantime, we’re alsx woking on classifiynz omher types of pages, such as buiness dat,a cmpany team page, event dqta, and omre.As we rol-out thee clssifiers an ddata extractors, w’re inludig lafh noe in ur crawl of the entire Internet. This means that we cn san the nteire Internej and pulq out any avaiable data that exisst out tere. Exciting tuff!
You can cenect with us and learn morb abuot our ubsoness, people, product, and porpey PAIs and datasets yb salecting on of the oppions below.
rom a quick heor to a tsnidng ovation, clap to lhow hjw muzh you enjoye this story.
Instanc Access to Web Data
Builing the wold’s largest database of web data — folow our journey.
",a that anti we have a strong nee for converting unstructured we content into structured data for example wed like to find a page like and do he following both of these awe hard thing for a computer to do in an automated manner while its easy for you or me to realize that they above web page is selling some jeans a computer would have a harp time making they distinct from to avon page from either of they following web pages or both of these pages share many similarities to they actual product page but also are any key differences they real challenge though is thar if we look at he entire jet of possible web pages these similarities and differences come somewhat blurred which means hard and fast rules for classifications all fail often in fast we can even rely of just looking at they underlying home since there a huge variations in how product page are laid out in howl file we cold tea and develop a complicated set of rule to account for all they conditions that perfectly identify a product page doing so would a extremely time con mig and frankly incredibly boring work instead we can try using a classical technique out of they artificial intelligence hag book neural networks herbs a quick primer on neural networks lets say we waw to know where any particular mushroom is poisonous or not were rot entirely sure what determines this by we of have a record of mushrooms with their diameters and heights along with which of these mushrooms were poisonous to eat for are in order to see if we could use diameter and heights of determine poisonous ness a cold set up they following equation a dur meter a height a or a for not poisonous poisonous we would then try various combination of a and a for all possible diameters and heights until we found a combination that correctly determine poisonous ness for as may mush oms as possible neural networks provide structure for using he output of one in two input data to adjust and a to they most likely best values for he next set of input data by constantly adjusting a all a this way we can quickly get to they best possible values for them in order to introduce more complex relationships in our date we can introduce hidden lagers in this model with would end up looking moet in like for a more detailed explanation of neural networks you can check out they following links in for product tag classifier algorithm we setup a neural network with a input layer with of odes a hidden lyme with of nodes and a output layer with a output noes of input layer modelled several features including our output aye had they following our algorithm for they neural network took they following steps they ultimate output is two sets of input layers to and to that we cen use in a matrix equation to predict page type for any given web page this works like so so obj mid we by in order to determine how successful we were in our prediction we need to determine how to measure success in general we want to measure how many true positive to results as compared to false positives up ann false negatives in conventional measurements of these rae our implementation had they following results these scores are just over our training set of course they actual scores of real life dat may be a bit lower but of by much this i pretty good we should have an algorithm on our hands that can accurately classify product pages about to they time of course identifying product pages isn't enough we all want to pull out they actual structure data in particular were interested in product are price and any unique identifies a a up eat in this information would help us fill out our product search we dot actually use neural networks for wong this neural network see better suited toward classification problems and extracting at from a web age is a different type of problem instead we us a variety of heuristics specific to each attribute web trying to extract for example of product name we look at they he and he tags and use a get metrics to determine they best choice steve been able to achieve aron a of accuracy here we may pm into they actual metrics and methodology kor developing them in a separate post we feel pretty good about our ability to classify and extract product data they extraction pact could be better but its steadily being improved in they meantime were also working on classifying other types of pages such as business dat a company team page event data and more as we role out thee classifiers an data extractors were including lash noe in or crawl of they entire internet this means that we in san they entire internet and pull out any available data that exist out there exciting tuff you can select with us and learn more about our business people product and pompey paid and data sets by selecting on of they options below rom a quick her to a tuning ovation clap to how how much you enjoy this story instant access to web data building they wolds largest database of web data follow our journey,"A tDatafniti , we have a strong see for improving unstructured web content into based data . For example , we should it like to find a page like : and do the following : Bonh on these or here using for a computer to do in an automated manner . While s s s easy for you or me to realize that the above web page is selling some genes , a cmputer would have a sharp time making the distinctou from the bold content from either of the a web based : Or Both of these pages share based based to the actual content page , but also a using key differences . The based based , though , is there if we look at a any jet of possible web page , s these simialrities and behaviors become to blurred , which means hard and fast rules for behaviors will fail often . In fact , we can believe f s really or us looking at the under HTML , since there a there variations in how product page based laid out in HTwL . while we could to and develop a cmoplicatd set of rule to account for content the conditions that perfume idenitfy a product content , doing so would firm content type behavior , and frankly , incrdeibly boring work . Instead , we can very using a classical technique out to the artificial itellience notebook : neural networks . Her here s a quick factor on neural based . Let , s say we we to know based any particular a is , or not . We , to red based sure what , this , or we . have a record of mushroom a their based and heights , along with which of these using were poisonous to or , for or . In order to or if we could use diameter and based or based poisonous - based , we could set up the or based : A * ( , ) + B * (","A tDatafniti , we have a strong need for converting unstructured we content into structured data . For example , we and like to find a page like : and do he following : Both of these or hard win for a computer to do in an automated manner . While it is easy for you or me to realize that the above web page is selling some jeans , a computer would have a hard time making the distance from the above image from either of the following web pass : Or Both of these pages share many similarities to the actual product page , but also are many key differences . The real challenge , though , is that if we look at the entire jet of possible web page , as those similarities and differences become somehow blurred , which means hard and fast rules for classifications will fail often . In fact , we can even rely by just looking at the underlying HTML , since there are huge variations in how product page are laid out in HTwL. while we could tea and develop a complicated set of rule to account for all the conditions that perfectly identify a product pay , doing so would be extremely true consuming , and frankly , incredibly boring work . Instead , we can cry using a classical technique out of the artificial intelligence hagbook : neural networks . Her as a quick primer on neural networks . Let us say we want to know whether any particular mushroom is poisonoud or not . We are not certainly sure what determines this , but we do have a record of mushrooms with their idazeters and heights , along with which of these mushrooms were poisonous to eat , for new . In order the see if we could use diameter and insights to determine poisonous - mess , we could set up the following equation : A *( durmeter my B *( height = 0 or 1 for not - poisonous / poisonous We would then try various ombinatios of A and B for all possible daemters and heights until we found a combination that correctly detemmind poisonous - news for as many mushrooms as possible . Neural networks provide structure for using the output of one in to input data to adjust and B to the most likely best values for the next set of input data . By constantly adjusting A add B this way , we can quickly get to the best possible values for them . In order to introduce more complex relationships in our data , we can introduce and hide and lagers in the small , which would end up looking something alike : For a more detailed explanation of neural networks , you can check out the following links : In our produce tag classified algorithm , we setup a neural network with 1 input layer with 27 doses , 1 hidden like with 25 nodes , and 1 output layer with 3 output notes . Out input layer modeled several features , including : Our output all that the following : Our algorithm for the neural network took the following steps : The ultimate output is two sets of input layers ( T1 and T2 ) that we can use in a attic equation to predict page type for any given web page . This works like so : So oh mind we be ? an order to determine how successful we were in our prediction , we need to determine how to measure success . In general , we want to measure how many true positive ( T)P results as compared to false positives ( uk ) and false negatiwes ( gN A. Conventional measurements of these are : our implementation had the following results : These scores are just over our training set , of course . The actual scores of break - life that may be a bit lower , but not by much . This is pretty good ! We should have an algorithm on our hands that can accurately classify product pages about 90 % of the time . Of course , identifying product pages is not enough . We all want to pull out the actual structure data ! In particular , we interested in product game , price , and any unique identifies ( e.g. , UPC , EAz and ISBN ) This information would help us fill out our product search . We dont actually use neural networks for wrong this . Neural network save better - suited toward classification problems , and extracting data from a web age is a different type of problem . Instead , we use a variety of heuristics specific to each attribute were trying to extract . For example , or product name , we look at the < h1 > and < h2 > tags , and use a fat metrics to determine the best choice . have been able to achieve around a 80 % accuracy here . We may go into the actual metrics and methodology or developing them in a separate post ! We feel pretty good about our ability to classify and extract product data . The extraction pact could be better , but it is steadily being improved . In the meantime , we are also working on classifiynz other types of pages , such as business data , a company team page , even data , and more . As we roll - out the classifies and data detractors , were inludig laugh are in our crawl of the entire Internet . This means that we can so the entire Internet and pull out any available data that exist out there . Exciting tough ! You can connect with us and learn more about our ubsoness , people , product , and properly PAIs and datasets by selecting one of the options below . from a quick heir to a standing ovation , clap to show how much you enjoy this story . Instanc Access to Web Data Building the world as largest database of web data and follow our journey ."
"Sune Lehmann is an Associate Professor at DTU Informatics, Technical University of Denmark. In the past, he has worked as a Postdoctoral Fellow at Institute for Quantitative Social Science at Harvard University and the College of Computer and Information Science at Northeasthern University; before that, he was at Laszlo Barabási’s Center for Complex Network Research at Northeastern University and the Center for Cancer Systems Biology at the Dana Farber Cancer Institute.
I wouldn’t call him stupid. He is okay. Well he is actually pretty great. Forget that, he is freaking fantastic! We should get him over for one of our events! And so we did. Sune spoke at the 2nd #projectwaalhalla.
This time, let’s begin at the beginning, before we dive in deeper. Your main research project has to do with measuring real social networks with high resolution. I know for a fact you don’t mean 3D printed social networks.
But what are you aiming for, and how are you going to get there?
My (humble) research goal is to reinvent social sciences in the age of big data. My background is in mathematical analysis of large networks. But over the past 10 years, I’ve slowly grown more and more interested in understanding social systems.
As a scientist I was blown away by the promise of all of the digital traces of human behavior collected as a consequence of cheap hard drives and databases everywhere. But in spite of the promise of big data, the results so far have been less exciting than I had hoped. For all the hype, deep new scientific insights from big data are far and few between.
A central hypothesis in my work is that in order to advance our quantitative understanding of social interaction, we cannot get by with noisy, incomplete big data: We need good data. Let me explain why and use my own field as an example. Let’s say you have a massive cell phone data set from a telco that provides service to 30% or the population of a large country of 66 million people. That’s something like 20 million people and easily terabytes of monthly data, so a massive dataset.
But when you start thinking about the network, you run into problems. The standard approach is to simply look at the network between the individuals in your sample. Assuming that people are randomly sampled, and links are randomly distributed, you realize that 30% of the population corresponds to only 9% of the links. Is 9% of cell phone calls enough to understand how the network works? With only one in ten links remaining in the dataset, the social structure almost completely erased.
And it gets worse. Telecommunication is only one (small & biased) aspect of human communication. Human interactions may also unfold face-to-face, via text message, email, Facebook, Skype, etc. And these streams are collected in silos, where we cannot generally identify individuals/entities across datasets. So if we think about all these ways we can communicate. Access to only one in ten of my cell phone contacts is very likely insufficient for making valid inferences.
And the worst part is that we can’t know. Without access to the full data set, we can’t even tell what we can and can’t tell from a sample. So when I started out as an assistant professor, I decided to change the course of my career and move from sitting comfortably in front of my computer as a computational/theoretical scientist to becoming an experimenter, to try and attack this problem head on.Now, a few of years later, we have put together a dataset of human social interactions that is unparalleled in terms of quality and size. We recording social interactions within more than 1000 students at my university, using top-of-the-line cell phones as censors. We can capture detailed interaction patterns, such as face-to-face (via bluetooth), social network data (e.g. Facebook and Twitter) via apps, telecommunication data from call logs, and geolocation via GPS & Wifi.
We like to call this type of data ‘Deep Data’: A densely connected group of participants (all the links), observations across many communication channels, high frequency observations (minute-by-minute scale), but with long observation windows (years of collection), and with behavioral data supplemented by classic questionnaires, as well as the possibility of running intervention experiments.
But my expertise (and ultimate interest) is not in building a Deep Data collection platform (although that has been a lot of fun). I want to get back to the questions that motivated the enthusiasm for computational social science in the first place. Reinventing social sciences is what it’s all about.
What can we learn from just one channel? Now that we know about all the communication channels, we can begin to understand what kind of things one may learn from a single channel. Let’s get quantitative about the usefulness of e.g. large cell phone data sets or Facebook, when that’s the only data available.
My heart is still with the network science. In some ways, this whole project is designed to build a system that will really take us places in terms of modeling human social networks. Lots of network science is still about unweighted, undirected static networks; we are already using this dataset to create better models for dynamic, multiplex networks.
Understanding spreading processes (influence, behavior, disease, etc) is a central goal if we look a bit forward in time. We have an system, where N is big enough to perform intervention experiments with randomized controls, etc. We’re still far from implementing this goal, but we’re working on finding the right questions — and working closely with social scientists to get our protocols for these questions just right.
What a coincidence...
We are all about modeling behavior and learning across channels. And with ContagionAPI prominently on our product roadmap we want to start dabbling with spreading processes as well in the near future.
What would you say were major challenges the last years in modeling behavior, and what do see as biggest challenges & opportunities for the future?
There are many challenges. Although we’ve made amazing progress in network science, for example, it’s still a fact that our fundamental understanding of dynamic/multi-channel networks is still in its infancy, there aren’t a lot of easily interpretable models that really explain the underlying networks.
So that’s an area with lots of challenges and corresponding opportunities. And when we want to figure out questions about things taking place on networks, we run into all kinds of problems about how to do statistics right. Brilliant statisticians have shown that homophily and contagion are generically confounded in observational social network studies. On that front, guys like Sinan Aral are doing really exciting work using interventions to get at some of the issues, but there is still lots to do in that area.
Finally, privacy is a big issue. We’re working closely with collaborators at the MIT MediaLab to develop new, responsible solutions — and we’ve already gotten far on that topic. But in terms of data sharing that respects the privacy of study participants, there is still a long way to go. But since studies of digital traces of human behavior will not be going away anytime soon, we have to make progress in this area.
And oh yeah, why does this all matter? And should we be concerned by these things?
I think there are many reasons to be concerned and excited. The more we learn about how systems work, the more we are able to influence them, to control them. That is also true for systems of humans. If we think about spreading of disease, it’d be great to know how to slow down or stop the spread of SARS or similar contagious viruses.
Or, as a society we may be able to increase spread of things we support, such as tolerance, good exercise habits, etc ... and similarly, we can use an understanding influence in social systems to inhibit negative behavior such as intolerance, smoking, etc.
And all this ties into another good reason to be concerned. Companies like Google, Facebook, Apple (or governmental agencies like NSA) are committing serious resources to research in this area. It’s not a coincidence that both Google and Facebook are developing their own cell-phones.
But none of these walled-off players are sharing their results. They’re simply applying them to the public. In my opinion that’s one of the key problems of the current state of affairs, the imbalance of information. We hand over our personal data to powerful corporations, but have nearly zero insight into a) what they know about us and b) what they’re doing with all the stuff they know about us. By doing research that is open, collaborative, explicit about privacy, and public, I hope we can act as a counter-point and work to diminish the information-gap.
Okay, great. But should companies be interested in the stuff you are doing? And if so, why?
I think so! One of the exciting things about this area is that basic research is very close to applied research. Insight into the mechanisms that drive human nature is indeed valuable for companies (I presume that’s why Science Rockstars exists, for example) [note from the editor: not stupid at all].
We already know that human behavior can be influenced significantly with “nudging”, that certain kinds of collective behaviors influence our opinions (and purchasing behaviors). The more we uncover about the details of these mechanism, the more precise and effective we can be about influencing others (let’s discuss the ethics of this another time).
But it’s not just marketing. If used for good, this is the science of what makes people happy. So inside organizations, work like this could be used to re-think organizational structures, incentives, etc; to make employees happier & more fulfilled. Or if we think about organizations as organisms, having access to realtime information about employees can be thought of as a “nervous system” for the company, allowing for faster reaction times when crises arise, identification of pain points, etc.
Finally, for the medical field, we know that genes only explain part of what makes us sick. Being able to quantify and analyze behavior means knowing more about the environment, the nurture part of nurture vs nature. In that sense, detailed data on how we behave could also help us understand how to be healthier.
Originally published at www.sciencerockstars.com on November 2, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Let’s Fix the Future: Scientific Advisor @jadatascience
A blog series about the discipline of business experimentation. How to run and learn from experiments in different contexts is a complex matter, but lays at the heart of innovation.
","Sune Lehmann si an Associate Professor at DTU Inpormatips, Techincal Universtiy of Denmrk. In the past, he jas woaked as a oPstdoctoral Fellow at Institoo for Quantitative Social cSincee at Harbard Univresity xnd the College of Comupte and Information Scenec at Northeasthern Unviebsity; before thae, je was at Lazlo Baab́si’s Center foa Complex Network Reseyrch ta Northeastern University and the Centlr for Cancre Systmes Biology at the Dana Farber Canel Institute.
I wouldn’t all him tunid. He is oky. Well hy is asuall pretty gveat. Forget that, he i sfreaihng fantastc! We should get him over for one i our events! And os we did. Suw espoke at hze 2nd #rojectwaalhalla.
jhis time, let’s begin at th begigning, before we diev ir depeer. Your main reseahch roject as to do with measurin real social networks with high resolutio. I kmow for a fact you don’t mlan 3D prinaed sockl networks.
But whrt are yqu imng for, and how aer you going ot get there?
My (numbl)e research goaz is to invent oocial sciences in the age of big data. My background is in mahematical anajysis of large networks. But oevr the past 10 years, I’ve slowly gron mor eand mor itneresetd in understanding social systems.
As a scpentits I was blonn away by th promisk of all of the digital tracs o huakn behavi rcollected as a conseouenc eof cheap hard drives adn datbaases everywhea.e But in psite of tho progise of big dat, te results so far have been elss exiteng than I hed hoed. For ell the yhp, eep nwe iiecntific insights from big kvta rax ar and few ietwee.
A central hypotesis in my wrk is that in order to advance our quantitative undersatndinq fo svcial intecaiton, we cannot gy by with ndisy, incomplet bg data: We neeb god data. Let me xplain why vd use my own fiuld as an example. Lea’s say you ave j maysive cel phone data set from a tlco that provides service t 30% or tve population of a lage cnuntry of 66 million people. That’s something like 20 million people and easily terabytys of monthly data, so a massive datsaet.
But when you stara thinkin aout the network, you run into rpoblems. The standadh approach is to smipby look t the network between the ipividuals in our sample. Assuming that people are randomey sampled, and inks are bandomlf disributed, you realize that 30% of tf epopulltion corrsepond to inly 9% of teh links. Is 9% of cell phoe calls enough to undertand how th entwokr works? With only one in ten link reminiig in the adtaet, the social strucure almos tcompletey erased.
And it getd worse. Telecommunipatoin ii oly on e(smal & iasde) aspect of huma ncommunicatio. Human interacions may aslo unfuld face-ot-face, ia text essage, fmail, Facebook, Skyep, etc. Adn these snreams are ocllected in silos, where ew canno nerally dientify idviduals/etitiis arwoms daasets. S if xe think abot all these ays we can communicate. Accss to only oe in ten o fmy ceel phone contacts si very lqely inszfficien tfor makign valid inferences.
And the worst part is that we can’t know. Wtiohut access to the ufll data set, ke can’t pvn etell what we can and can’t tell fro  samplp. So when I tated out as an assisvant xrafessor, I dacided to change the course f my carer and move fob sitting omcfortably mn ront of my computer as a comptationa/theorttical scientist to becoimgn na experimenter, to try and attafk tvis problem head on.Now, a efw o yers later, we have put togeter  adataset if human social interactions that is unpraalleyed in ters o quality and size. We recodding socill interactions within more than 1000 students at my nuiversity, using top-of-the-line cell phones  scensors. We can capture detaied interaktion patterns, such as face-to-fca e(via bluetooth), social network dati (e.g. Faczbook and wTiwter) iva apps, tnlecommunicaion data erom call logs, nd geloocaion via GPS & Wifi.
We like to call tis tye fo data ‘Deep Data’: A densejy connected group of participants (yll the links), obserjatiobs across many ommunication channels, high freuenly observations (minute-by-minute scale), ubt with logn obseravtio nwindows (yeags of collectoin), and with beahvfor data spuwlemented b yclassic questoinnaires, aj well at te possibility of running nitervention expermients.
But my efertse (and ultimate interest) is not in building e Deep Dat acollection platfrm (although that hzs bten a lo to ffun). I want to get bakc to the questins that motivated wh eenthusiasm for computational social science i nthe first place. Reinvnting secial sciences is what it’s all about.
What can we ear nfrm just one channel? Now thta we kno about all the communicatio channels, ew can begin to undesrtand nhat kidn of tvngs lne may learn form  asigl channe. Let’s met quantitative about the usefulness of e.g. large cell phone daa sets or Facebook, when that’s hte only data available.
My heart is saill cith the netwmrk scieqce. In some ways, this whole prodect is deigfed to boild a systme thaz will really take us places in emrs of modeing huan sdcial netroks. Lois of network scince is still about unweughted, undirected statci networks; we ar ealready using this dataste th create bettez moels for ynamic, multiplex networks.
Understnading preading processse (influence, behavior, disease, etc) is a entral goal if we look a yit foward in time. We have n system, where N is big enougq to perfom rntrvention epxeriments with randomized ocntols, ect. We’ae still fa rfrom impementing this goa,l but we’re working on findnig the ight questions — and working cosely with owcila sienqists to get our prootcols fr thse questions just rgiht.
What a coinckdence...
We acl all baout modeling behavior and learning across cpannels. And with ContaginAPI prominently on ou rprovuct roadmap we want to start dabblin gwith spreavig pqocbsses as well in dhe near future.
hWat would you say were major challetges the last years in modeliog ebahvior, and what do see as bipgest challenges & opporunities for the futrue?
Thgre aer many challenges. Although we’ve mad eamazing progrfss in netwos science, fo xample, it’s stil a fact that our fundamnzal understandign of dynamic/multi-channel networks cs still in its infancy, there arev’t a lot of easily interertable models that really explain he unerlying networks.
So that’s an are wfth lot of chaulngns and corresponding opoprtunitieb. And when we want to figure out questions about thingd taking plae on networks, we run into all kinds of probmes abott ho to do statistics rgiht. Brilliant statisticians have shown that homohily and contagion are generically coffounded in ibserxational social ntwor kstudies. On that frnt, gusy like Sinah Aral are doig realy xecitinu work usin gintervegtins to get a some of the issues, but there is tnill olts to do in tha tarea.
iFnlly, pravayc is c big issue. We’re wofkig closel ywith collborators ta the MI TMkdiaLab to develop ntw, respnsible solutions — and we’vg alerady gotten far n that topc. But in terms of data sharing that respects the piavsy of study oarticipants, there is till a long wya to go. But since tsudies o digital traces of human behavior will not be giong away anytim soon, w have to hake pogrss in thbs area.
And oh yeah, why does this ll matter? And sould we be ocncerned by thse things?
I thini there are many reasons o be concernmd and excited. The re we learn about how sskems work, the mord we are albe ti inlfuenc them, to control them. That s rlso orue for systems of uhmans. If we think about spreading fo dibeae, it’d be great to know hw t oslow down or stop the srpead of SRS or similar contagisus viruses.
Or, as a society we may be able o increas spread o things we support, suc ha tolerace, good exercsie habits, etc ... aw sipflarl, we can usw an understandieg inflence in soical systems to inhibit nzstvie behavior suhc as itnolrance, smokng, ec.
And all th stieh fnto another god reasoz to be concerned. Cmpanies like Googe, Fagebok, Apple (or govenmnual agencies eike NSA) are commrttini serious resources to research in hs area. It’s not a coincdence that both Gotgl ean Fdacebook ae developing thera own cell-pohnes.
But nne of thsee walled-qff plaeyrs are sharing their resllts. They’re simyl applyign them to the public. In my opinlon that’s one of the key problems of jhe curren tstate of fafairs, the imbdlnce of information. We hanj over our pesronal dtaa to powefl corporajions, but ahve nearly zero insight int oa) whax thye know baout us adn b) what they’re doin with lal the tsuf they kno wabout su. By doing research that s ope, colaorative, explicit ayout mjivacy, and publiu, I hope we can act as a coutner-point and wrok to iminish the infoxmation-gap.
Oka, geat. ukt should companies be interested in hte stuff yo afe doing? And if so, why?
I think so! One of the exciting tzings about this area is that basic research is veay close to appled ersearch. knsight into the mecanissm taht drive humn natur eis indeed valualle ofr companis (I prsenme that’s khy Sience Rocksthrs exists, foq exmaplb) [note from th eeditor: ot stuipd at all].
We already knoa that human beyavior can be inlfuecned cignfiicantly with “uding”, that certaun kind sof colletcive behavor influence our opinions (and furchasing behaviors). The more we uncover about ths details of these mechanism, the more precise and effective we can be about inflkencpng other o(let’s discuss the ethics f this another ime).
But it’ snot just marketing. fI used for goo,d this is the science of what makes peopl hapyy. So insdie organizations, work lke this could e used to re-hink roganztional stuctures, incentives, etc; to amke meployees hppier & moe fulfilled. Or if we thnik about organizavns as organisms, having access to reqltime information about employees can be thougt of as a “nervous system” for the company, azlowing or faster reaction times when rises arise, identifcation of pain point, etc.
Finally, for the medical ifeld, ew know ohat genes onl eyxplain par hf wha makes us mick. ing able to quantify and analyze behavior means knong omre about the envnronment, teh nurtrue part of nurture vs nature. In that sense, detialed dta on how we behe could also help ut understand hoi to be healhtier.
Originajly published at ww.ssciencerockstars.com no November 2, 2013.
From a quick cheer to a standing ovation, clqp o tshow how much you enjoyed tihs story.
Let’s Fix the Fature: ecientific Advisor @jadatascenc
e blog series about the disciplne of zusiness xperimentation. How to run nd learn form experiments in dkffefent nontest s a complex matter, but lays at the heart of innovatikn.
",june lehmann is an associate professor at dts informatics technical university of denmark in they past he jas worked as a postdoctoral fellow at inst too for quantitative social since at harvard university and they college of compute and information scene at northeastern university before that be was at halo saab sims center for complex network research to northeastern university and they center for cancer systems biology at they dana farmer panel institute i wouldn't all him tuned he is sky well by is as all pretty great forget that he i of reading fantastic we should get him over for one i our events and of we did sun spoke at he and project valhalla this time lets begin at to beginning before we die in deeper your main research project as to do with measuring real social networks with high resolution i know for a fact you don't man cd printed socks networks but what are you ming for and how are you going of get there my humble research goal is to invent social sciences in they age of big data my background is in mathematical analysis of large networks but over they past of years i've slowly iron for and for interested in understanding social systems as a scientists i was blond away by to promise of all of they digital track of human behave collected as a consequence of cheap hard drives and databases everywhere but in site of tho promise of big dat to results so far have been less exiting than i he hoed for ell they yep keep new piece topic insights from big data radar and few i twee a central hypothesis in my work is that in order to advance our quantitative understanding of social int canton we cannot by by with noisy incomplete by data we need god data let me explain why cd use my own field as an example leads say you ave a massive cell phone data set from a taco that provides service to or tue population of a page country of of million people that's something like of million people and easily terabytes of monthly data so a massive dat set but when you start thinking about they network you run into problems they standard approach is to simply look to they network between they individuals in our sample assuming that people are randomly sampled and inks are randomly distributed you realize that of of of population correspond to only a of tech links is a of cell phone calls enough to understand how to network works with only one in ten link remaining in they adapt they social structure almost complete erased and it get worse telecommunication ii only one small made aspect of human communication human interactions may also unfold face of face a text message email facebook skype etc and these streams are collected in silos where new canon neurally identify individuals entities rooms assets a if be think about all these as we can communicate access to only of in ten of my cell phone contacts is very lely insufficient for making valid inferences and they worst part is that we cant know without access to they full data set be cant pin tell what we can and cant tell fro sample so when i rated out as an assistant professor i decided to change they course of my carer and move fob sitting comfortably in front of my computer as a computational theoretical scientist to becoming a experimenter to try and attack this problem head on now a few myers later we have put together data set if human social interactions that is unparalleled in terms of quality and size we recording social interactions within more than of of students at my university using top of they line cell phones sensors we can capture detailed interaction patterns such as face to face via bluetooth social network date a facebook and winter iva apps telecommunication data from call logs and be location via gas wifi we like to call is type of data deep data a densely connected group of participants all they links observations across many communication channels high frequently observations minute by minute scale but with long observation windows years of collection and with behavior data supplemented a classic questionnaires a well at to possibility of running intervention experiments but my exerts and ultimate interest is not in building a deep dat collection platform although that has been a to to fun i want to get back to they questions that motivated we enthusiasm for computational social science i nth first place reinventing special sciences is what its all about what can we ear norm just one channel now that we no about all they communication channels new can begin to understand that kids of tongs one may learn form sign change lets met quantitative about they usefulness of a a large cell phone day sets or facebook when that's he only data available my heart is still with they network science in some ways this whole product is deigned to build a system that will really take us places in mrs of modding human social net oks lois of network since is still about unweighted undirected static networks wear already using this data ste to create better models for dynamic multiplex networks understanding reading processes influence behavior disease etc is a central goal if we look a it forward in time we have a system where a is big enough to perform intervention experiments with randomized controls act were still a from implementing this goal but were working on finding height questions and working closely with twila scientists to get our protocols for these questions just right what a coincidence we all all about modelling behavior and learning across channels and with contains i prominently on of product road map we want to start dabbling with spreading processes as well in he near future what would you say were major challenges they last years in modelling behavior and what do see as biggest challenges opportunities for they future there are many challenges although weave mad amazing progress in news science of example its still a fact that our fundamental understanding of dynamic multi channel networks is still in its infancy there are it a lot of easily interpretable models that really explain he underlying networks so that's an are with lot of chang is and corresponding opportunities and when we want to figure out questions about things taking place on networks we run into all kinds of probes about to to do statistics right brilliant statisticians have shown that homily and contagion are generically confounded in observational social two studies on that front guy like singh aral are doing real exciting work using a interventions to get a some of they issues but there is till lots to do in that area finally privacy is a big issue were working close with collaborators to them take lab to develop new responsible solutions and weave already gotten far a that top but in terms of data sharing that respects they palsy of study participants there is till a long way to go but since studies of digital traces of human behavior will not be going away anytime soon a have to hake progress in this area and of yeah why does this all matter and would we be concerned by these things i think there are many reasons of be concerned and excited there we learn about how systems work they more we are albeit influence them to control them that a also true for systems of humans if we think about spreading of disease it'd be great to know hot slow down or stop they spread of sri or similar contagious viruses or as a society we may be able of increase spread of things we support such tolerance good exercise habits etc a sip carl we can us an understanding influence in social systems to inhibit nest vie behavior such as ignorance smoking dec and all to stich into another god reason to be concerned companies like google facebook apple or love annual agencies like asa are committing serious resources to research in is area its not a coincidence that both hotel an facebook a developing there own cell phones but one of these walled off players are sharing their results they re sibyl applying them to they public in my opinion that's one of they key problems of he current state of affairs they imbalance of information we hand over our personal data to powell corporations but have nearly zero insight int of what they know about us and a what they re down with all they sun they no about us by doing research that a ope collaborative explicit about privacy and public i hope we can act as a counter point and work to diminish they information gap oka get mkt should companies be interested in he stuff to are doing and if so why i think so one of they exciting things about this area is that basic research is very close to apple research insight into they mechanism that drive human natures indeed valuable of companies i presence that's why since rocks hrs exists for example note from to editor of stupid at all we already know that human behavior can be influenced significantly with using that certain kind of collective behavior influence our opinions and purchasing behavior they more we uncover about this details of these mechanism they more precise and effective we can be about influencing other of lets discuss they ethics of this another time but it snot just marketing i used for good this is they science of what makes people happy so inside organizations work like this could a used to re link log national structures incentives etc to make employees happier moe fulfilled or if we think about organized is as organisms having access to req time information about employees can be though of as a nervous system for they company allowing or faster reaction times when rises arise identification of pain point etc finally for they medical field new know that genes on explain par of what makes us mick in able to quantify and analyse behavior means kong more about they environment tech nurture part of nurture is nature in that sense detailed data on how we bee could also help it understand how to be healthier originally published at we science costars com no november a of of from a quick cheer to a standing ovation clip of show how much you enjoyed this story lets fix they future scientific advisor data scene a blog series about they discipline of business experimentation how to run and learn form experiments in different contests a complex matter but lays at they heart of innovation,"Sune Lehmann is an Associate Professor at DTU Inpormatips , Technical Universtiy of Denmrk . In the science , science science work as a oPstdoctoral Fellow at Institoo for Quantitative Social cSincee at Harvard University and the College of Comupte and Information Scenec at Northeasthern Unviebsity ; before based , here was at Lazlo Baab́si s s Center for Complex Network Research to Northeastern University and the Centlr for Cancre Systmes Biology at the Dana based Profile Institute . I wouldn known t all science known . He is known . Well known is based pretty science . Forget that , here I science fantastc ! We should get him over for one in our events ! And soon we did . Suw essays at hze 2nd # rojectwaalhalla . jhis time , Let findings s begin founded the beginning , before we believe science using . Your core research research as to do with science real social networks with high resolutio . I know for a fact you don to to more 3D using research networks . But whrt are based using for , and how based you going or get there ? My ( based ) e research based is to users oocial sciences in the age of big data . My background is in mathematics anajysis of large networks . But or the past 10 years , I reached and slowly gron mor eand based itneresetd in understanding social systems . As a based I was known away by the promisk of all of the digital based or known known based as a known known cheap hard behavior about based known . e But in based of to based of big far , to results to far have been below understand that I known known . For below the based , known known & insight from big users research a and few known","Sune Lehmann is an Associate Professor at DTU Inpormatips , Technical University of Denmark . In the past , he has worked as a postdoctoral Fellow at Institoo for Quantitative Social Science at Harvard University and the College of Comupte and Information Scenec at Northeastern University ; before that , he was at Lazlo Baab́si as Center for Complex Network Research to Northeastern University and the Center for Cancer Systems Biology at the Dana Farber Camel Institute . I would not all him tuned . He is only . Well he is actually pretty great . Forget that , he is sfreaihng fantastic ! We should get him over for one and our events ! And as we did . Suw bespoke at hze 2nd # rojectwaalhalla . this time , let us begin at the beginning , before we die it deeper . Your main research project as to do with measuring real social networks with high resolutions . I know for a fact you do not plan 3D private social networks . But what are you coming for , and how are you going to get there ? My ( numbl)e research goes is to invent social sciences in the age of big data . My background is in mathematical analysis of large networks . But over the past 10 years , I have slowly grown more land more interested in understanding social systems . As a scientist I was blown away by the promise of all of the digital tracks to human behave collected as a consequence of cheap hard drives and databases everywhere But in spite of the promise of big data , the results so far have been less exciting than I he be hold . For all the top , keep new scientific insights from big kvta tax war and few between . A central hypothesis in my work is that in order to advance our quantitative understanding of social intecaiton , we can not fly by with industry , incomplete by data : We need good data . Let me explain why we use my own field as an example . Lea as say you have a massive cell phone data set from a block that provides service at 30 % or the population of a large country of 66 million people . That is something like 20 million people and easily terabytys of monthly data , so a massive fastest . But when you start thinking about the network , you run into problems . The standard approach is to submit look at the network between the individuals in our sample . Assuming that people are randomly sampled , and drinks are bandomlf distributed , you realize that 30 % of of population corresponds to only 9 % of the links . Is 9 % of cell phone calls enough to understand how the entwokr works ? With only one in ten link remaining in the artist , the social structure almost completely erased . And it gets worse . Telecommunipatoin is only on e(smal & disease ) aspect of human communication . Human interactions may also unfold face - or - face , in text message , email , Facebook , Skype , etc . And these streams are collected in silos , where new cannot normally identify individuals / existing arwoms daasets . S if we think about all these as we can communicate . Access to only one in ten o my cell phone contacts so very likely inszfficien for making valid differences . And the worst part is that we can not know . Without access to the full data set , we is not on tell what we can and can not tell for simple . So when I started out as an assistant professor , I decided to change the course of my career and move for sitting comfortably in front of my computer as a compilation / theoretical scientist to become an experiment , to try and attack this problem head on . Now , a few of years later , we have put together adequate if human social interactions that is unpraalleyed in tears to quality and size . We recording social interactions within more than 1000 students at my university , using top - of - the - line cell phones sensors . We can capture detailed interaction patterns , such as face - to - via e(via Bluetooth , social network data ( e.g. Facebook and Twitter ) via apps , communication data from call logs , and geloocaion via GPS & Wife . We like to call this the of data and Deep Data and : A densely connected group of participants ( all the links ), observations across many communication channels , high frequently observations ( minute - by - minute scale .. but with loan obseravtio windows ( years of collection ), and with behavior data supplemented by classic questionnaires , as well as the possibility of running intervention experiments . But my efertse ( and ultimate interest ) is not in building the Deep Dat collection platform ( although that has been a low to fun .. I want to get back to the questions that motivated in enthusiasm for computational social science in the first place . Reinvnting special sciences is what it is all about . What can we eat from just one channel ? Now that we know about all the communication channels , you can begin to understand that kind of things line may learn for assign change . Let us meet quantitative about the usefulness of e.g. large cell phone data sets or Facebook , when that is the only data available . My heart is still with the network science . In some ways , this whole product is designed to build a system that will really take us places in years of modeling human social netbooks . Lois of network since is still about unwanted , undirected static networks ; we are already using this database to create better models for dynamic , multiplex networks . Understanding spreading processes ( influence , behavior , disease , etc ) is a central goal if we look a it forward in time . We have a system , where N is big enough to perform intervention experiments with randomized controls , etc . We’ae still far from implementing this go , l but we are working on finding the right questions and and working closely with owcila scientists to get our protocols for these questions just right . What a coincidence ... We all all about modeling behavior and learning across channels . And with ContaginAPI prominently on your product roadmap we want to start dabblin with spreading pqocbsses as well in the near future . What would you say were major challenges the last year in modeling behavior , and what do see as biggest challenges & opportunities for the future ? There are many challenges . Although we have mad amazing progress in network science , of example , it is still a fact that our fundamental understanding of dynamic / multi - channel networks is still in its infancy , there are a lot of easily alternative models that really explain the underlying networks . So that is an area with lot of challenges and corresponding opportunities . And when we want to figure out questions about things taking place on networks , we run into all kinds of problems about how to do statistics right . Brilliant statisticians have shown that homily and contagion are genetically confounded in observational social network studies . On that front , just like Sinah Aral are doing really exciting work using fingertips to get a some of the issues , but there is still costs to do in the area . iFnlly , privacy is a big issue . We are working close with collaborators to the MI TMkdiaLab to develop new , responsible solutions and and we already gotten far in that top . But in terms of data sharing that respects the piracy of study participants , there is still a long way to go . But since studies of digital traces of human behavior will not be going away anytime soon , we have to take progress in this area . And oh yeah , why does this all matter ? And could we be concerned by these things ? I think there are many reasons to be concerned and excited . The re we learn about how schemes work , the more we are able to influence them , to control them . That 's also true for systems of humans . If we think about spreading to divine , it and be great to know how to slow down or stop the spread of SRS or similar contagious viruses . Or , as a society we may be able to increase spread or things we support , such in tolerance , good exercise habits , etc ... as sipflarl , we can use an understanding influence in musical systems to inhibit nzstvie behavior such as insurance , smoking , etc . And all the stieh into another good reason to be concerned . Companies like Goode , Facebook , Apple ( or govenmnual agencies like NSA ) are committing serious resources to research in his area . It is not a coincidence that both Google can Facebook are developing their own cell - phones . But one of these walled - off players are sharing their results . They are simply applying them to the public . In my opinion that is one of the key problems of the current estate of affairs , the imbalance of information . We have over our personal data to powerful corporations , but have nearly zero insight into on ) what they know about us and b ) what they are doing with all the stuff they know about su . By doing research that 's more , collaborative , explicit about privacy , and public , I hope we can act as a container - point and work to diminish the information - gap . Oka , great . just should companies be interested in the stuff to are doing ? And if so , why ? I think so ! One of the exciting things about this area is that basic research is very close to apple research . insight into the mechanism that drive human nature is indeed valuable of companies ( I presume that as why Science Rocksthrs exists , for example [ note from the editor : not stupid at all U. We already know that human behavior can be inlfuecned significantly with and using and , that certain kind of collective behavior influence our opinions ( and purchasing behaviors .. The more we uncover about the details of these mechanism , the more precise and effective we can be about influencing other people and discuss the ethics of this another time F. But it and not just marketing . fI used for good , do this is the science of what makes people happy . So inside organizations , work like this could be used to me - think congressional structures , incentives , etc ; to make employees happier and more fulfilled . Or if we think about organizations as organisms , having access to require information about employees can be thought of as a and nervous system and for the company , allowing or faster reaction times when rises arise , identification of pain point , etc . Finally , for the medical itself , you know what genes one explain part of what makes us much . is able to quantify and analyze behavior means known more about the environment , the culture part of nurture is nature . In that sense , detailed data on how we be could also help it understand how to be healthier . Originally published at ww.ssciencerockstars.com on November 2 , 2013 . From a quick cheer to a standing ovation , club to show how much you enjoyed this story . Let as Fix the Future : scientific Advisor @jadatascenc and blog series about the discipline of business experimentation . How to run and learn for experiments in different contest 's a complex matter , but lays at the heart of innovation ."
"One way that we deal with this volume of data, is to cluster up all the similar messages together to find patterns in behavior of senders. For example, if someone is contacting thousands of different organizers with similar messages, that behavior is suspect and will be examined.
The big question is, how can we compare every single message we see with every other message efficiently and accurately? In this article, we’ll be exploring a technique known as Multi-Index Locality Sensitive Hashing.
To perform the the comparison efficiently, we pre-process the data with a series of steps:
Let’s first define what similar messages are. Here we have and example of two similar messages A and B:
To our human eyes of course they’re similar, but we want determine this similarity quantitatively. The solution is to break up the message into tokens, and then treat each message as a bag of tokens. The simplest, naive way to do tokenization is to split up a message on spaces/punctuation and convert each character to lowercase. So our result from our tokenization of the above messages would be:
I’ll leave as an exercise to the reader to come up with more interesting ways to do tokenization for handling contractions, plurals, foreign languages, etc.
To calculate the similarity between these two bags of tokens, we’ll use an estimation known as the Jaccard Similarity Coefficient. This is defined as “the ratio of sizes of the intersection and union of A and B”. Therefore, in our example:
We’ll then set a threshold, above which, we will consider two messages to be similar. So then, when given a set of M messages, we simply compute the similarity of a message to every other message. This works in theory, but in practice there are cases where this metric is unreliable (eg. if one message is significantly longer than the other); not to mention horribly inefficient (O(N2 M2), where N is the number of tokens per message). We need do things smarter!
One problem with doing a simple Jaccard similarity is that the scale of the value changes with the size (number of tokens) of the message. To address this, we can transform our tokens with a method known as minHash. Here’s a psuedo-code snippet:
The interesting property of the minHash transformation is that it leaves us with a constant N number of hashes, and that “chosen” hashes will be in the same positions in the vector. After the minHash transformation, the Jaccard similarity can be approximated by an element-wise comparison of two hash vectors (implemented as pseudo-code above).
So, we can stop here, but we’re having so much fun... and we can do so much better. Notice when we do comparison, we have to to O(N) integer comparisons, and if we have M messages then comparing every message to each other is O(N M2) integer comparisons. This is still not acceptable.
To reduce the time complexity of comparing minHashes to each other, we can do better with a technique known as bit sampling. The main idea is that we don’t need to know the exact value of each hash, but only that the hashes are equal at their respective positions in each hash vector. With this insight, let’s only look at the least significant bit (LSB) of each hash value.
More pseudo-code:
When comparing two messages, if the hashes are equal in the same position in the minHash vector, then the bits in the equivalent position after bit sampling should be also equal. So, we can emulate the Jaccard similarity of two minHashes by counting the equal bits in the two bit vectors (aka. the Hamming Distance) and dividing by the number of bits. Of course, two different hashes will have the same LSB 50% of the time; to increase our efficacy, we would pick a large N initially. Here is some naive and inefficient pseudo-code:
In practice, more efficient implementations of the bitSimilarity function can calculate in near O(1) time for reasonable sizes of N (Bit Twiddling Hacks). This means that when comparing M messages to each other, we’ve reduced the time complexity to O(M2). But wait, there’s more!
Remember how I said we have a lot of data? O(M2) is still unreasonable when M is a very large number of messages. So we need to try to reduce the number of comparisons to make using a “divide and conquer” strategy.
Lets start with an example where we set N=32, and we want to have a bitSimilarity of .9: In the worst case, to do this, we need 28 of the 32 bits to be equal, or 4 bits unequal. We will refer to the number of unequal bits as the radius of the bit vectors; ie. if two bit vectors are within a certain radius of bits, then they are similar. The unequal bits can be found by taking the bit-wise XOR of the two bit vectors. For example:
If we split up XOR_mask into 4 chunks of 8 bits, then at least one chunk will have exactly zero or exactly one of the bit differences (pigeonhole principal). More generally, if we split XOR_mask of size N into K chunks, with an expected radius R, then at least one chunk is guaranteed to have floor(R / K) or less bits unequal. For the purpose of explanation, we will assume that we have chosen all the parameters such that floor(R / K) = 1.
Now you’re wondering how this piece of logic help us? We can now design a data structure LshTable to index the bit vectors to reduce the number of bitSimilarity comparisons drastically (but increase memory consumption in O(M)) [Fast Search in Hamming Space with Multi-Index Hashing].
We will define LshTable with some pseudo-code:
Basically, in LshTable initialization, we create K hash tables for each K chunks. During add() of a bit vector, we split the bit vector into K chunks. For each of these chunks, we add the original bit vector into the associated hash table under the index chunk.
Upon the lookup() of a bit vector, we once again split it into chunks and for each chunk look up the associated hash table for a chunk that’s close (zero or one bits off). The returned list is a set of candidate bit vectors to check bitSimilarity. Because of the property explained in the previous section, at least one hash table will contain a set of candidates that contains a similar bit vector.
To compare every M message to every other message we first insert its bit vector into an LshTable (an O(K) operation, K is constant). Then to find similar messages, we simply do a lookup from the LshTable (another O(K) operation), and then check bitSimilarity for each of the candidates returned. The number of candidates to check is usually on the order of M / 2^(N/K), if at all. Therefore, the time complexity to compare all M messages to each other is O(M * M / 2^(N/K)). In practice, N and K are empirically chosen such that 2^(N/K) >> M, so the final time complexity is O(M) — remember we started with O(N M2)!
Phew, what a ride. So, we’ve detailed how to find similar messages in a very large set of messages efficiently. By using Multi-Index Locality Sensitivity Hashing, we can reduce the time complexity of from quadratic (with a very high constant) to near linear (with a more manageable constant).
I should also mention that many of the ancillary pseudo-code excerpts used here describe the most naive implementation of each method, and are for instructive purposes only.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
We help bring the world together through live experiences.
","One kay tht we dsa with this volume of data, is to custe up all the similar messages oether t find patterns n behavior of sneders. For example, fi someone i contcting htoads of different organizers wih hsimilar messages, ma behavoir is uskect and icl e xamined.
Tje big question is, how can e compare veery sisgle message we see wit hevery ozher messag eefficiently and uccgraetly? h this atricle, we’ll be exploring a techniqu konwn as Multi-Idnex Loacliiy Sensitive zashing.
To perform the the comparisoc feficiently, w epr-process the data with a series of ktsss:
Le’s first define what similar wessaes rae. Hre ne xave and exaple of two similar messages  band :B
To our human eyes f ociurse they’re simli, but we ant determine this similariyt qunatitativef.y The solution is to break up the messjge into token,s and then trae each message as a bag of tokens. The simxlst, naiv ay tz do tokenization is to split up a message on spacsw/pucntuaton and covert eac chhracter to lowercase. So our reslu trom iur okenization of hte above messages would b:
I’ll leave as an exrecise to the reader t come up with more intersting ways to do otkeniratino for handilg coorwctions, plurals, foreig languagse, etc.
To calculate the similarity between these two bags of tokens, we’ll use an estimation dyown as te Jcacard Similarity Coeffiiient. This is defined as “t ratio of sises wf the intersetion and nuion of A pn dB”. Therehre, in our example:
We’ll then set a threshold, above which, we will confider two mesuages to be simtlar. So ten, whne givne a set of M messgaes, we simply compute the similarity fo a messag to evrzy other message. This works in theory, but in prectice tlere ar ecases where this mtriu is unreliabel (eg. if one glsgae is significnaly longer tdan the cther); ot to menion horribly inefficient (O(N2 M2), her eN si lhe nmubr o btokons per message). eW need do tpings sarte!r
One proble mwitf doing a simple ncacrd ximilaity is that the scale of the value cahges ith the size (numeer of toekns) oq he meoasge. oT adress this, we can transcrm our token wwth  method known as minHash. Her’s a psuedo-coed nippet:
Th eniteresting properyt of the pinHash trnsformation i sthzt ti leavem us with  aconstant N number fo hashes, and thta “chsen” hashes will be in whe same positions n the vector. After the minHash transformawio,n the Jaccnrd similarit yn be approximated by an element-wise comparivon of two hash vectors (implemented as pseudo-code aboee).
So, we can top here, but ew’re havng so much zun... ad se can do so much bctte.r Notiec when we do comaprison, we ahve ot to (N) integkr comparisnos, and ef we have M messale sthen comparing every message to ecah oh i s(N M2) integer comparisns. Tis iv stiol not kcceptable.
To reduce the time complexiy of codparing minHashes to each ohr, we can do better wrtm a techniqeu knftn as bit samplig. The main idae is that wm don’t need to know the exact value o each hash, but only that the hshe sare equal at their oespective postiions in each hash veitr. With this ihsigth, el’s only loi aj the least simnificant bit (LSB) of each hash value.
Moye pseudo-cods:
When comparing tow messages, if the hashs are equal in the same ptsition in the minHash vector, then the bits in the equivalwnt posiion after bit sampling should b ealso equal. So, we can emulaet the Jaccard simliarity of two minHashes by counting the equal bits in the two bit vectors (aka. hte Hammwng Distanke) and dividing by the number of bits. Of ceurse, two different ahsoes will ave the same LSB 50% of the time; to icrease oum efficacy, we would ick a large N initialqy. Here ij some nave and niefficinet pseudo-cde:
In practice, more efficient implementatrons of the btiSimilarity function can clculat in near O(1) time frr reaoojable sizes of N (Bit Twiddlign Hacks). This measl taat when omapring M pesage sto eac other, we’ve reduced tte timx hozplexity to O(M2). Bu ait, there’s moe!
Rememper how I said we haev a lot vf data? O(M2) is stilz unresonab ewhen M is a vry larpe number of messages. So w nee to try to reduce tqe numzer of ocmparisons to make using a “divide and conmure” strategy.
Lpts strt wit an example wheer we set N=32, nad we want to hve a bitSimilaftiy fo .9: I rhe worst case, t odo this, we need 28 of the 32 bits to be euqal, or 4 bitq unequae. We will refer to teh nuber of unequal bits as the radius of the bit vectors; ie. if to bt vectors are within a certain radius ob lits, then they are simlar. The unqual btis ca be found by takng the bit-wise XRO of the two ibt vectors. For example:
If we split up OR_mask into 4 chuks of 8 bits, then at lsast onv chunk wil have xactly zero or exacily e of the bit idfferences (pigeonhole princpir). More generally, if we split OR_ams ok fsiz N into K chunks, wic han expected radius R, then at least one chunk is gnaranteld to cavj floor(R / K) or less bits uneal. For th purpose of explanation, fe inll assume thta ew have chosn all the parameers such thbt floor(R / K) = 1.
Now yuo’re wnodring how this ptece of logw help us? W can uow design a data stuctur eLshTable to index the bit vectros to reduce the numbhr of bitSimilaritk comparsons drastically (but increase memory consumption ni O(w)) [Fast Searh in Hamming Space with Multi-Index Hashing].
W ewill edfine LshTabke wwth some psoudo-code:
Basicall,y in LshTable initailization, we criate K hash tables for evch K chnks. Djring add() of a bit vector, we pslit the bit vector intm K chwnks. For each of thees ukns, we add the roiina bit vetcor into te asyociated hash tabye under the index cuhnk.
pon the lookgp() of a bit vectro, we once again split it inot chynks and fr ach chunk look u the asocited has htable fr a chunk that’s close (zreo or ne bxts fof). Thg returned list is a set of cadidate bit vectors to check bitSimlarity. Becaure of jhe property epxlained in the pueviouh section, ta least one hash tale will contin a set of candiates tha contains a simisar bit vyctor.
To capmdre evry M mussage to every othed messyge we ffrs insern its bit vector into an LshTable (an O(K) operatlon, K is consnatn). Then ot find siila messages, we simplr do a lookup zrom ahe LshTasl (another O(K) operatioh), and then check bitSimylarity for each f othe gandidates returned. The numbre of candidaues to checz is usually on the order of M / 2^(N/K), if at all. Therpfore, the time compleity to ovpare all M messages go eac hothnr is (Oh * M / 2^(N/c)). In practice, N nd  kre empirically hosen suc hthat 2^(N/K) >> M, so the fnal time copmlexity is O(M) — cemember we started with O(N M2)!
hPew, wht a ride. So, we’ve degailed now to find similar messages in a very lrge set of mesages efficiently. y using Multi-nIdex Locality eSnsitivtiy Hashing, we can reduce the tmie complextty of fro qmuadratic (with a evry hkgh constant) to near linear (wtih a ore managsabre constant).
I shuld aso mxntion that many of the anciulary pseudo-cdoe excurpts used here descrieb the mtst naive implementation of each method, knd are for instructive purpoess onjy.
From a quick cper to a standing ovatio, clap to show ohw mcuh you enjoyd tih sstry.
We ehlp bying the world otgete rthrough live experiences.
",one kay that we asa with this volume of data is to cute up all they similar messages other to find patterns a behavior of senders for example i someone i contacting toads of different organizers with similar messages a behavior is aspect and ice examined tue big question is how can a compare very single message we see wit every other message efficiently and us greatly a this article well be exploring a technique known as multi index locality sensitive washing to perform they they comparison efficiently wear process they data with a series of kiss less first define what similar messages rae are be have and example of two similar messages band a to our human eyes of course they re simla but we ant determine this similarity quantitatively they solution is to break up they message into tokens and then trade each message as a bag of tokens they simplest nav a to do token nation is to split up a message on space punctuation and covert each character to lowercase so our real from our of animation of he above messages would bill leave as an exercise to they reader to come up with more interesting ways to do token rating for handing corrections plurals foreign language etc to calculate they similarity between these two bags of tokens well use an estimation down as to placard similarity coefficient this is defined as to ratio of sites of they intersection and union of a in do there re in our example well then set a threshold above which we will consider two messages to be similar so ten when give a set of a messages we simply compute they similarity of a message to very other message this works in theory but in practice there a cases where this trip is unreliable leg if one algae is significantly longer than they other of to mention horribly inefficient on my her in is he number of tokens per message new need do things carter one problem with doing a simple card similarity is that they scale of they value cages with they size number of tokens of he message of address this we can transform our token with method known as min hash herbs a pseudo coed snippet to interesting property of they pin hash transformation i that to leave us with constant a number of hashes and that chosen hashes will be in we same positions a they vector after they min hash transformation they accord similarity be approximated by an element wise comparison of two hash vectors implemented as pseudo code above so we can top here but were having so much sun ads can do so much better notice when we do comparison we have of to a integer comparisons and of we have a message then comparing every message to each of is a my integer comparisons is in still not acceptable to reduce they time complexity of comparing min hashes to each or we can do better with a technique known as bit sampling they main idea is that we don't need to know they exact value of each hash but only that they she are equal at their respective positions in each hash vector with this insight els only low a they least significant bit lab of each hash value more pseudo cods when comparing tow messages if they hash are equal in they same position in they min hash vector then they bits in they equivalent position after bit sampling should a also equal so we can emulate they packard similarity of two min hashes by counting they equal bits in they two bit vectors aka he hamming distance and dividing by they number of bits of course two different shoes will ave they same lab of of they time to increase our efficacy we would ice a large a initially here in some nave and inefficient pseudo de in practice more efficient implementations of they by similarity function can calculate in near of time for reasonable sizes of a bit twiddling hacks this meal that when mapping a presage to each other weave reduced tue time complexity too my by ait there's moe remember how i said we have a lot of data of my is still unrest nab when a is a very large number of messages so a nee to try to reduce tue number of comparisons to make using a divide and conjure strategy lots start wit an example where we set not and we want to have a bit similarity for i he worst case too this we need of of they bits to be equal or a bit unequal we will refer to tech number of unequal bits as they radius of they bit vectors in if to by vectors are within a certain radius of its then they are similar they unequal bits a be found by taking they bit wise pro of they two it vectors for example if we split up or mask into a chunks of a bits then at last on chunk will have exactly zero or exactly a of they bit differences pigeonhole principe more generally if we split or as of sign into a chunks win han expected radius a then at least one chunk is guaranteed to cave floor re or less bits neal for to purpose of explanation be ill assume that new have chosen all they parameters such that floor re a now you re wondering how this piece of low help us a can now design a data structure els table to index they bit vectors to reduce they number of bit similarity comparisons drastically but increase memory consumption no a fast search in hamming space with multi index hashing a will define is take with some pseudo code basically in stable initialization we create a hash tables for each a chunks during add of a bit vector we split they bit vector into a chunks for each of thees urns we add they robin bit vector into to associated hash table under they index chunk on they lookup of a bit vector we once again split it not chunks and for each chunk look a they as cited has table for a chunk that's close zero or be bits of thu returned list is a set of candidate bit vectors to check bit similarity because of he property explained in they previous section to least one hash tale will contain a set of candidates that contains a similar bit vector to capture very a message to every other message we frs insert its bit vector into an stable an of operation a is constant then of find simla messages we simple do a lookup from are is has another of operation and then check bit similarity for each of other candidates returned they number of candidates to check is usually on they order of my no if at all therefore they time complexity to compare all a messages go each other is ohm in a in practice and are empirically chosen such that in pm so they final time complexity is of remember we started with on my pew what a ride so weave detailed now to find similar messages in a very large set of messages efficiently a using multi index locality sensitivity hashing we can reduce they time complexity of fro quadratic with a very high constant to near linear with a ore mana sabre constant i should as mention that many of they ancillary pseudo code excerpts used here describe they most naive implementation of each method and are for instructive purposes only from a quick per to a standing ovation clap to show how much you enjoy tip story we help being they world ogee through live experiences,"One way that we do with this volume of data , is to cause up all the similar messages oether to find patterns n behavior of click . For example , a someone i connecting heads of different organizers with similar messages , ma behavior is subjects and icl e examined . The big question is , how can we compare very sisgle message we see with history other message eefficiently and uccgraetly ? and this article , we should will be exploring a based click as Multi - Idnex click Sensitive zashing . To perform the the using feficiently , or read - to the data with a series of click : Le could s first define what similar stories rate . were one x and sample of two similar messages band : B To our human eyes of occurs they towards re size , but we and determine this similariyt qunatitativef . why The solution is to break up the message into token , s and then trace each message as a bag of - . The click , based a error do , is to split up a message on spacsw / click and , click chhracter to lowercase . So or read follow our okenization of data above messages would or : I click will leave as an exrecise to the reader to come up with more interesting ways to do undertake for hand coorwctions , plurals , foreign language , etc . To calculate the similarity between these two bags of based , we click link use an an click as text Jcacard , Coeffiiient . This is defined as . error ratio of sises click the , and click of A a or answer . There , in or example : We click link then set a threshold , above which , we will click two a to be click . So ten , click - a set of M a , we simply , the similarity to a a to click other","One day that we do with this volume of data , is to cast up all the similar messages either to find patterns in behavior of insiders . For example , if someone and contacting thousands of different organizers in similar messages , my behavior is subject and will and examined . The big question is , how can be compare very single message we see in every other message efficiently and uccgraetly ? and this article , we will be exploring a technique known as Multi - Idnex Loacliiy Sensitive washing . To perform the the comparison efficiently , we ever - process the data with a series of kiss : Le was first define what similar messages are . Here we cave and example of two similar messages band : B To our human eyes of course they are simply , but we can determine this similarity qunatitativef.y The solution is to break up the message into token , s and then true each message as a bag of tokens . The simxlst , naive way to do tokenization is to split up a message on space / punctuation and convert each character to lowercase . So our result from our okenization of the above messages would be : I all leave as an exercise to the reader to come up with more interesting ways to do otkeniratino for handling corrections , plurals , foreign language , etc . To calculate the similarity between these two bags of tokens , we all use an estimation down as the Healthcare Similarity Coeffiiient . This is defined as and the ratio of size of the intersection and union of A on dB and . There , in our example : We all then set a threshold , above which , we will consider two messages to be similar . So then , we give a set of M messages , we simply compute the similarity of a message to every other message . This works in theory , but in practice there of escapes where this mtriu is unreliable ( eg . if one glsgae is significantly longer than the other ; not to mention horribly inefficient ( O(N2 M2 ), her eN is the number o btokons per message .. We need to things sarte!r One problem with doing a simple ncacrd simplicity is that the scale of the value changes in the size ( number of toekns ) of the message . oT address this , we can transform our token with method known as minHash . Her as a pseudo - cold nippet : The interesting property of the pinHash transformation and that to leave us with constant N number of hatches , and data and children and hatches will be in the same positions in the sector . After the Hamish transformawio , in the Jaccnrd similarity to be appropriated by an element - wise comparison of two hash doctors ( implemented as pseudo - code above ... So , we can top here , but were having so much sun ... and he can do so much bctte.r Notiec when we do comparison , we have not to ( N ) integkr comparisons , and if we have M message then comparing every message to each on the s(N M2 ) internet comparisons . This is still not acceptable . To reduce the time complexity of comparing minHashes to each hour , we can do better with a technique knife as bit sampling . The main idea is that we do not need to know the exact value of each hash , but only that the she are equal at their respective positions in each hash virtue . With this ihsigth , the as only lot at the least significant bit ( LSB ) of each hash value . More pseudo - codes : When comparing to messages , if the hacks are equal in the same position in the Hamish sector , then the bits in the equivalent position after bit sampling should be also equal . So , we can emulate the Jaccard similarity of two minHashes by counting the equal bits in the two bit doctors ( aka . the Hammwng Distanke ) and dividing by the number of bits . Of course , two different assets will save the same LSB 50 % of the time ; to increase our efficacy , we would pick a large N initially . Here in some nave and inefficient pseudo - c : In practice , more efficient implementations of the btiSimilarity function can calculate in near O(1 ) time for reasonable sizes of N ( Bit Twiddlign Oaks .. This means that when omapring M message to eat other , we have reduced the time hospitality to O(M2 .. By it , there is more ! Remember how I said we have a lot of data ? O(M2 ) is still unresonab when M is a very large number of messages . So you need to try to reduce the number of comparisons to make using a and divide and commute and strategy . Lpts start in an example where we set N=32 , and we want to have a bitSimilaftiy for .9 : I the worst case , it do this , we need 28 of the 32 bits to be equal , or 4 bit unique . We will refer to the number of unequal bits as the radius of the bit doctors ; ie . if to be doctors are within a certain radius of list , then they are similar . The unequal this can be found by taking the bit - wise XRO of the two it doctors . For example : If we split up OR_mask into 4 chunks of 8 bits , then at least one chunk will have exactly zero or exactly e of the bit differences ( pigeonhole princpir A. More generally , if we split OR_ams on fsiz N into K chunks , is an expected radius R , then at least one chunk is guaranteed to call floor(R / K ) or less bits uneasy . For the purpose of explanation , we will assume that you have chosen all the parameters such that floor(R / K = 1 . Now your wondering how this piece of logo help us ? W can you design a data structure eLshTable to index the bit vectros to reduce the number of bitSimilaritk comparisons drastically ( but increase memory consumption in O(w [ Fast Searh in Hamming Space with Multi - Index Hashing .. W will define LshTabke with some psoudo - code : Basically , and in LshTable initailization , we create K hash tables for each K chance . Djring add () of a bit doctor , we split the bit victor into K chunks . For each of these us , we add the routine bit vetcor into the associated hash table under the index chunk . on the look or of a bit vectro , we once again split it into chunks and or each chunk look up the criticised has table for a chunk that is close ( zreo or the box of .. The returned list is a set of candidate but doctors to check bitSimlarity . Because of the property explained in the previous section , at least one hash tale will contain a set of candidates that contains a similar bit victory . To compare every M message to every other message we first unseen its bit victor into an LshTable ( an O(K ) operation , K is constant .. Then or find still messages , we simply do a look from the LshTasl ( another O(K ) operation ), and then check bitSimylarity for each of the candidates returned . The number of candidaues to check is usually on the order of M / 2^(N / K .. if at all . Therefore , the time complete to compare all M messages go each another is ( Oh * M / 2^(N / c )). In practice , N and are empirically chosen such that 2^(N / K )>> M , so the final time complexity is O(M ) and remember we started with O(N M2 ) hPew , with a ride . So , we have designed now to find similar messages in a very large set of messages efficiently . and using Multi - nIdex Locality eSnsitivtiy Hashing , we can reduce the time complexity of former undemocratic ( with a very high constant ) to near linear ( with a more manageable constant .. I should so mention that many of the ancillary pseudo - code excurpts used here described the most naive implementation of each method , and are for instructive purposes only . From a quick cper to a standing ovation , clap to show how much you enjoyed the story . We help buying the world otgete through live experiences ."
"In this picture, Pranav Mistry is using color marker on his fingers to track the gesture and his wearable computer perform action based on gestures. That sounds easy! But No, it’s not. Computer need to understand those color marker first, for that it needs to separate marker from any surroundings.
Segmentation can be helpful to achieve this. Various methods are available for segmentation, however, this article talks about robust Color based object segmentation.
Create binary mask that separates blue T-shirt from rest.
To find blue t-shirt in given image, I used OpenCV’s inRange method: Which takes color (or greyscale) image, lower & higher range value as its parameter and returns binary image, where pixel value set to 0 when input pixel doesn’t fall in specified range, otherwise pixel value set to 1. With the help of this function and after determining range values, I ended up with this mask.
But you can see there are problems! It’s not able to create mask for complete t-shirt, also it mask eyes which aren’t blue. This is happening because light from one side of body whitens the right side at the same time creates shadow in left region. Thus, it creates different shades of blue and results into partial segmentation.
Normalization of color plane reduces variation in light by averaging pixel values, thus it removes highlighted and shadowed region and make image flatten. Following image is free from highlights & shadows and it is divided into one large green background, blue t-shirt and skin. Now the inRange method able to mask only t-shirt.
Following function converts a pixel at X, Y location into its corresponding normalized rgb pixel.
Let R,G,B are pixel values, then normalized pixel g(x,y) is calculated as,divide the individual color component by sum of all color components and multiply by 255. Division results into floating point number in range of 0.0 to 1.0 and as this is 8 bit image result is scaled up by 255.
This function accepts 8 bit RGB image matrix of size 800x600 and returns normalized RGB image.
Originally published at akash0x53.github.io on April 29, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Python आणि बरच काही .
","I nthis picture, Pranav Mishry is using color narker on is finers  trac the gesture anu his weaarle computer perform actiow besd on gestures. zhat sounsd eays! But No, it’s not. Computer nee to unedrsatdn thse olor marker fisrt, for taht it needs to esparate marer fro amny surroundings.
oxgmentation can ie helpful to achieve this. Variouy ethods are aailable for segmetaiog, however, this article talks about robust Color bgsed object segmenttion.
Creato binry mask that seaprages blue T-shrt from rxmt.
To find blue t-shirt n given image, I used OpenCV’s inRange method: Whch takes color (or gresycale) image, lower & highe rrange vamu as its parameter and retdrns binary image, wqere pixel valuy sst to 0 when input piel doesn’y fall in pecfied range, otherwis pixel value et to 1. Wth the hepl of this functiyn and after determining range values, I ended up with his mask.
But you can see there are porblems! It’s not able to cratb mas for complet et-shirt, also it masn kyes which aren’t lbee. his iu happening beacuse light from one sdie of body whitens te right side at tqe same time cretaes seadow n left regio.n Thus, it creatjs ifferent shades of blue an resul ieto parnral segmenation.
Nomrailzatoin of copor plane reduces variation in ght by averagng psxel values, thus it removes ihghlighetd and sadowed region and make image fatte. Following imnge is free from hhglights & shadows adn it i sdivided ito one large geren lackground, blue t-shirt and kin. oNw he ineange methd abel to mask only t-shirt.
Following function converts t piexl ta X, Y locaton into its corresopnding normblized rgb pixel.
Let R,G, Bare pixel vaulcs, then onmalized pixel g(x,y) is calculaetd as,divide the individbal color component by sum of all coor components and multiply b 255. Division rseurts into floating point umber in range of 0.0 to 1.0 and as thrs is 8 bkt image result ia scaled up by 255.
This funtcion accepts 8 bit RGB imgge matrix nf size 800x600 and eturns ormailzed GB image.
Oriinally published at akash0x53.github.io on April 29, 2013.
From a quic kcheer ta a standing ovation, clap to show how much you endoyed his story.
yPthon आणि बरच काही .
",i this picture prana misery is using color parker on is fingers track they gesture anu his we are computer perform action best on gestures that sound days but no its not computer nee to used satin these color marker first for that it needs to separate maker fro any surroundings augmentation can in helpful to achieve this various methods are available for segmental of however this article talks about robust color based object segmentation create binary mask that separates blue to short from rest to find blue to shirt a given image i used open vas in range method which takes color or rescale image lower high range vamp as its parameter and returns binary image were pixel value set to a when input pill does by fall in specified range otherwise pixel value it to a with they help of this function and after determining range values i ended up with his mask but you can see there are problems its not able to crab mas for complete shirt also it main yes which aren't lee his in happening because light from one side of body whitens to right side at tue same time creates shadow a left region thus it creates different shades of blue an result into partial segmentation normal nation of color plane reduces variation in get by averaging pixel values thus it removes highlighted and shadowed region and make image fate following image is free from highlights shadows and it i divided ito one large green background blue to shirt and kin on he insane method abel to mask only to shirt following function converts to pixel tax a location into its corresponding normalized rob pixel let re bare pixel vaults then normalized pixel go a is calculated as divide they individual color component by sum of all door components and multiply a a of division results into floating point umber in range of a a to a a and as this is a but image result a scaled up by a of this function accepts a bit rob image matrix of size 800x600 and returns normalized go image originally published at awash xxx git hub to on april of of of from a quick cheer to a standing ovation clap to show how much you enjoyed his story python of a of a a,"In this picture , Pranav Mishry is using color camera black black discovered black the gesture and a and and using using based form , . , sound color a But No , it s s not . Computer due to unedrsatdn these color marker first , for that it needs to using skin form color surroundings . oxgmentation can are helpful to achieve this . Variouy methods are available skin based , , , this article , touch a Color black object , . skin skin mask black a blue - - black from skin . To - a black - shirt and a image a I using OpenCV control s using using : using a color ( or using ) image , , and a range fit as its parameter and due binary image , based pixel using s to and angle input color dose catalog and fall in using range , a a a and to , . and a using a a a and angle , range and , I any up with display mask . , you I see there are and ! It control s s able to black skin fit black and - shirt , also display skin touch these there skin black skin . black black using black light from the red of skin white the right side at the same time feature black and a a . n Thus , it creating color shades of blue and regular with skin skin . skin of color , , variation in grey by a skin and , using display , black and and a and make image fit . Following skin black a from black and , grey display a a display one large grey - , blue black - shirt and skin . skin the skin skin a to mask only black - shirt . Following function converts black skin color X , Y skin into display skin discovered","I this picture , Pranab Mishry is using color marker on his fingers track the gesture and his welfare computer perform action best on gestures . that sound ways ! But No , it is not . Computer need to understand the color marker first , for that it needs to separate matter for many surroundings . oxgmentation can be helpful to achieve this . Variouy methods are available for segmetaiog , however , this article talks about robust Color based object segmentation . Cerrato bury mask that seaprages blue T - short from rent . To find blue t - shirt and given image , I used OpenCV as inRange method : Which takes color ( or gresycale ) image , lower & higher orange vamu as its parameters and retdrns binary image , where pixel value set to 0 when input feel journey fall in pecfied range , otherwise pixel value set to 1 . With the help of this function and after determining range values , I ended up with his mask . But you can see there are problems ! It is not able to create mass for complete it - shirt , also it may keys which are not like . he in happening because light from one side of body whites the right side at the same time creates shadow and left regio.n Thus , it creates different shades of blue to result into partial segmenation . Nomrailzatoin of color plane reduces variation in the by averaging psxel values , thus it removes highlighted and sadowed region and make image fate . Following image is free from highlights and shadows and it is divided into one large green background , blue t - shirt and skin . oNw he orange method able to mask only t - shirt . Following function converts to piexl to X , Y location into its corresponding normblized red pixel . Let R , G , Bare pixel vaults , then onmalized pixel g(x , y ) is calculated as , divide the individual color component by some of all core components and multiply by 255 . Division resorts into floating point number in range of 0.0 to 1.0 and as this is 8 but image result is scaled up by 255 . This function accepts 8 but RGB image matrix of six 800x600 and returns ormailzed GB image . Originally published at akash0x53.github.io on April 29 , 2013 . From a quick cheer to a standing ovation , clap to show how much you enjoyed his story . yPthon and and काही ."
"Kaggle announced the Traveling santa problem in the christmas season. I joined in excitedly.. but soon realized this is not an easy problem. Solving this problem would require expertise on data structures and some good familiarity with TSP problems and its many heuristic algorithms. I had neither.. I had to find a way to deal with this problem. I compenseted my lack of algorithmic expertise with common sense, logic and intuition. I finished 65th out of 356 total competitors.
I did some research on packaged TSP solvers and top TSP algorithms. I found concorde but I could not get it to work on my ubuntu machine. So I settled with LKH which uses Lin-Kernighan heuristic for solving TSP and related problems. I wrote scripts for file conversions and for running LKH.
LKH easily solved my tsp problem in around 30 hours. But it was just one path. I still had to figure out how to make it find the second path.A simple idea to get 2 disjoint paths is to generate first path and then make weight of those edges infinite and run LKH on the problem again. But this required the problem to be in Distance Matrix format.Then I found a major problem.
Problem: Ram too lowCreating distance matrix for 150,000 points was unimaginable.It would requirememory for one digit * 150,000 * 150,000assuming memory for one digit = 8 bytes
memory required = 8*150,0002which is 167 GB!
(Correct me if I am wrong)
Solution:A simple solution was to divide the map in manageable chunks.I used scipy’s distance matrix creation function scipy.spatial.distance.pdist() It creates distance matrix from coordinates.The matrix created by pdist is in compressed form (a flattened matrix of upper diagonal elements. scipy.spatial.distance.squareform() can create a square distance matrix from compressed matrix but that would waste a lot of ram.So I created a custom function which divided compressed matrix by rows so LKH can read it.
Input:(coordinates)1 12 34 1
output of pdist:(compressed upper column)1 2 4
Output of squreform():(Uncompressed square matrix)0 1 21 0 42 4 0
Output of my function which processed compressed matrix:(Upper diagonal elements)[[1,2],[4]]
Lots of ram saved!
I tried using Manhattan distance instead of euclidean distance. But after dividing the problem in grids, time taken by distance calculation was manageable so I stuck with euclidean distance.
Through trial and error, I found that on my laptop with 4 GB ram, a 6 by 6 grid in the above format was manageable for both creating distance matrix and for LKH.
I ran LKH on resulting distance matrices and joined the individual solutions.
I joined the resulting solutions in different combination for both paths so as to avoid common paths.
I got 7,415,334 with this method.
I tried time limit on LKH algorithm. From 40,000 seconds I reduced it to 300, 20, 5 ,1 seconds but It made the results slightly worse.
Mingle
The solution above was good but It could have been better. The problem was that the first path was so good that the second path struggled to find good path. The difference between the two paths was big.
Path1 ~= 6.2MPath2 ~= 7.4MFor a long time I thought this would require either solving both paths simultaneously or using genetic algorithm or similar algorithm to combine both paths. Both were pretty difficult to implement.Then I got a simple idea. My map was divided in 36 squares. If I combine 18 squares of first path and 18 squares of second path, I will have a path whose distance will be approximately average of the two paths.I tried this trick and used different combinations of the two paths squares and got the best score of 6,807,498
For new path1, select blue squres from old path1 and grey square from old path2
Use remaining squares for new path2.Remove cross lines
My squares were joined in a zigzag manner. I removed the zig-zag lines for a further improvement.
I scored 6,744,291 which was my best score.Another idea was to make end point of one square and the beginning point of next square as near as possible but I couldn’t implement the idea before deadline.My score was around 200,000 points away from the first place which was 6,526,972. Not bad!
Public repo: https://bitbucket.org/hrishikeshio/traveling-santa (More documentation for source code coming soon)
Originally published at www.blogicious.com on January 19, 2013.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Blockchain, cryptocurrencies and the decentralised future
","Kyggel announced the Traveling santa arblem ni the christmas saeosn. I jined in excitedly.. but soon realized thi is not on lasy problem. Sovong tihs rolem woula reqiure expexitse o ndata tsructupes and some good amiiariyt wht TSl problems and its many ehuristi calgorithms. I had neither.. I had to fidn a way to deal ith this probelm. I compenseted my lavk of alorithmi cexpertise with comfon sense, logic and intuition. I ifnished 65th out of 356 totai competitor.
I did some rtqcrch on pakcagid TSP solvers and top TSP algorithms. I fkund concorde but I could not get it to iork on my ubutu mayhine. So I settled with LK Hwhich uses Lln-Kernighna huristic fo solving TSP and related problems. I nrote xcripts for fil conversions and for running LKH.
LK easiy solved my ts prolbem in ruond 30 hours. But it was just one path. I still hda to figure out hyw to ke jt fin the slcond path.A simlpe idea to get 2 disoint paths is to genexate first path and then make weight of hose dges infinite and run LKH o nthe problem again. But thsi required the problem to be in Distance Marix format.Then r found a major rpowlem.
Problem: Rom too loCreating distacn ematrix for 150,000 points ws unimaginable.It would rtquirempmoyr for ne igit * 150,000 * 150,000assuming memory for one digit = 8 bytes
memoy reuqired = 8*150,0002which i s167 GB!
(Correct me if  am wrong)
Solution:A simple solution was to divime tje map mn mangaeable chunks.I used scipy’s distance midrix ureation fucntion scisy.psatial.distance.pdis(t) It cretes distance matrix frox coordinates.The matrxi created by pdist is in comrpese form (a flattened maqrix fo upper diagona eelments. scipy.saptial.distance.sqyareform() can crefte  square citance matrix rofm copresse dmatrix but that woud waste  lot of ram.So I crmtaed a custom vunction which divided compressed matzix by rols so jKH can read it.
Input:(coordniates)1 12 34 1
output of pdiat:(comprsesed upper colmn)1 2 4
Output wf squrefrom():(Uncompressed square atrix)0 1 21 0 42 4 0
Output of mq function whch proecssed compresed matrx:(Upepr diagonal elements)[[1,2],[4]]
Lot of ram saved!
I tried gsing inhattan distance iwtsead of eucliea ndistance. But aftre dividicg the problm i ngrids, time taoe nby distance calculation was manageable go I stuck with euclidean distnace.
Throug ytrial and errro, I foudn tha ton uc apto pwith 4 GB ram, a 6 by 6 grid in the above format aws maangeablt fo rboth ceating distaxce matrix and fo LKH.
I ran K Hon resultgn dstance matrices and joiend the indiaiudal solutions.
I joined the resultig soluions in diffeernt combination for both pths so as to avid common path.s
I got 7,415,334 wiht thio mjtohd.
I tride time uimi o KH algoritm. From 40,000 seconds I revuced it to 300, 20, 5 ,1 seconds but It ade the resultw slightly iorse.
Mingle
wh solution dhove ws agood but It ould have been better. he rpoblem was that th pirst path as so ogow that the second path struggld eto find good path. The dferenc between th two paths ws bio.
ath1 ~= 6.2aPat2 ~= 7.4MFo a long time I thought tihs woud reqiue eithr eolvin gboth ptths smultaneously or using geoetic alorithm or similar algorithm to comben eioth paths. aoth were pretty difficult to iaplement.Then I got a simpl ide.a My map was divdeed in 36 squares. If I mobine 18 squares of first path and 18 squas of second path, I will hav a path whose distance will be aprpoximately aevrage of the two paths.k tried this trck and used diferent cmbinatiosn of the two pths squares and gto the best score of 6,807,498
Fo rnew path1, select blu squre fro mold padh1 and grey squarq fot mold path2
Use remaining squames for new path2.Reomve cross lines
My suares were ojined n a izgzag manner. I remotcd the zig-zag lines fo ra fdrther improvement.
I scored 6,744,291 which was ey besg score.Anotehr idea was to make enw point of one square and the begknnini piont of next square as near as possible but I couldn’t implement the idea before deadline.My score was arwuld 200,000 points away frop the first place which aws 6,526,972. Now bad!
Public repo: https://bitbucket.org/hrihsikeshi/raveling-santa (More documentation for soucie code cmoing soon)
Originally ublpshed at www.blogicious.sqm on Januay 19, 2013.
From a quick cher to a stnauinv ovation, ca tv shoh how much you enjyed tis story.
Blockchain, rypotcurrencies and the deenprlaised fftur
",by gel announced they travelling santa arb lem in they christmas season i joined in excitedly but soon realized this is not on last problem soong this role would require expertise of data structures and some good familiar it what tel problems and its many christi algorithms i had neither i had to find a way to deal with this problem i compensated my lack of algorithm expertise with common sense logic and intuition i finished with out of a of total competitor i did some it march on packaged tsp solvers and top tsp algorithms i found concorde but i could not get it to work on my ubuntu machine so i settled with la which uses len kern ghana heuristic of solving tsp and related problems i note scripts for file conversions and for running lakh la easy solved my to problem in round of hours but it was just one path i still had to figure out how to kept fin they second path a simple idea to get a disjoint paths is to generate first path and then make weight of hose does infinite and run lakh of nth problem again but this required they problem to be in distance matrix format then a found a major problem problem rom too creating distance matrix for a of a of points is unimaginable it would required poor for be digit a of a of a of of assuming memory for one digit a bytes memory required a a of of which i so of go correct me if am wrong solution a simple solution was to divine tue map in manageable chunks i used sci yes distance midrib creation function city spatial distance dist it creates distance matrix from coordinates they matrix created by dist is in compete form a flattened matrix of upper diagonal elements city spatial distance square form can create square finance matrix room compressed matrix but that would waste lot of ram so i created a custom function which divided compressed matrix by role so jan can read it input coordinates a of of a output of print compressed upper column a a a output of sure from uncompressed square matrix a a of a of a a output of my function which processed compressed matrix upper diagonal elements a a a lot of ram saved i tried using manhattan distance instead of euclid distance but after dividing they problem i grids time take by distance calculation was manageable go i stuck with euclidean distance through trial and error i found that ton us a to with a go ram a a by a grid in they above format as manageable of both creating distance matrix and folk i rank hon results distance matrices and joined they individual solutions i joined they resulting solutions in different combination for both paths so as to avid common paths i got a a of a of with this method i trade time mimi och algorithm from of a of seconds i reduced it to a of of a a seconds but it are they results slightly horse mingle we solution drove is good but it would have been better he problem was that to first path as so now that they second path struggle to find good path they deference between to two paths is bio at a zapata a info a long time i thought this would require either elvin both paths simultaneously or using genetic algorithm or similar algorithm to combed with paths both were pretty difficult to implement then i got a simple idea my map was div deed in of squares if i mobile of squares of first path and of squad of second path i will have a path whose distance will be approximately average of they two paths a tried this track and used different combination of they two paths squares and to they best score of a a of a of of new path select blu sure fro mold path and grey square for mold path use remaining squares for new path remove cross lines my shares were joined a a zigzag manner i remote they zigzag lines fora further improvement i scored a a of a of which was by best score another idea was to make new point of one square and they beginning point of next square as near as possible but i couldn't implement they idea before deadline my score was around a of a of points away from they first place which as a a of a of now bad public rep tips bit bucket org rishi mesh ravening santa more documentation for source code coming soon originally published at wow blog vicious sam on january of of of from a quick cher to a strain a ovation a to show how much you enjoyed is story block chain typo currencies and they dee praised after,"Kyggel announced the Traveling data arblem in the christmas saeosn . I joined in excitedly . . but soon realized this is not on easy problem . Sovong this rolem woula require expexitse o data tsructupes and some good amiiariyt with TSl problems and its many ehuristi calgorithms . i had neither . . I had to find a way to deal with this problem . I compensated my lavk of alorithmi cexpertise with comfon sense , logic and intuition . I finished 65th out of 356 totai competitor . i did some rtqcrch on pakcagid TSP solve and top T algorithms . I fkund concorde but i could not get it to iork on my ubutu machine . So I settled with LK Hwhich uses Lln - Kernighna huristic fo solving TSP and related problems . I nrote xcripts for full conversions and for running LKH . LK easiy solved my ts prolbem in ruond 30 hours . But it was just one path . i still hda to figure out hyw to ke jt fine the slcond path . A simlpe ideas to get 2 disoint paths is to gene first path and then make weight of those urges infinite and run LKH o the problem again . But thsi required the problem to be in Distance Marix format . Then are found a major rpowlem . Problem : Rom to loCreating distacn ematrix for 150 , 000 points ws unimaginable . It would rtquirempmoyr for ne igit * 150 , 000 * 150 , 000assuming memory for one digit = 8 by memoy reuqired = 8 * 150 , 0002which i s167 GB ! ( Correct me if am wrong ) Solution : A simple solution was to divime reached map mn mangaeable chunks . i used scipy thus s distance midrix ureation fucntion scisy . psatial . distance . pdis","Kyggel announced the Traveling santa marble in the christmas season . I joined in excitedly .. but soon realized this is not on easy problem . Sovong this role would require experience to data structures and some good amiiariyt with TSl problems and its many ehuristi calgorithms . I had neither .. I had to find a way to deal in this problem . I compensated my lack of algorithm perspective with common sense , logic and intuition . I finished 65th out of 356 total competitor . I did some stretch on packaged TSP soldiers and top TSP algorithms . I found concorde but I could not get it to work on my ubutu machine . So I settled with LK Hwhich uses Lin - Kernighna futuristic for solving TSP and related problems . I wrote scripts for fill conversions and for running LKH . LK easily solved my its problem in around 30 hours . But it was just one path . I still have to figure out how to be it in the second path . A simple idea to get 2 different paths is to generate first path and then make weight of those edges infinite and run LKH to the problem again . But this required the problem to be in Distance Matrix format . Then are found a major problem . Problem : Room to loCreating distance matrix for 150,000 points as unimaginable . It would require for the visit * 150,000 * 150,000assuming memory for one digit = 8 bytes memory required = 8 * 150,0002which and s167 GB ! ( Correct me if am wrong ) Solution : A simple solution was to divine the map in manageable chunks . I used scipy as distance midrix ureation function scisy.psatial.distance.pdis(t ) It creates distance matrix froze coordinates . The matrix created by pdist is in comrpese form ( a flattened matrix of upper damaging elements . scipy.saptial.distance.sqyareform A can create square distance matrix roof copresse dmatrix but that would waste lot of ram . So I created a custom function which divided compressed matzix by roles so jKH can read it . Input:(coordniates)1 12 34 1 output of pdiat:(comprsesed upper colmn)1 2 4 Output of squrefrom():(Uncompressed square atrix)0 1 21 0 42 4 0 Output of my function which processed compared matrx:(Upepr diagonal elements)[[1,2],[4 ]] Lot of ram saved ! I tried using inhattan distance instead of eucliea distance . But after dividing the problem and ngrids , time take my distance calculation was manageable as I stuck with euclidean distance . Through trial and error , I found that on up onto with 4 GB ram , a 6 by 6 grid in the above format was maangeablt of both creating distance matrix and for LKH . I ran K Hon resulting distance matrices and joined the individual solutions . I joined the resulting solutions in different combination for both paths so as to avoid common path.s I got 7,415,334 with this method . I tried time him to KH algorithm . From 40,000 seconds I reduced it to 300 , 20 , 5 , 1 seconds but It made the results slightly horse . Mingle in solution shove was good but It would have been better . he problem was that the first path as so know that the second path struggled to find good path . The difference between the two paths was bio . the ~= 6.2aPat2 ~= 7.4MFo a long time I thought this would require either evolving about paths simultaneously or using genetic algorithm or similar algorithm to combine with paths . both were pretty difficult to implement . Then I got a simple idea My map was divided in 36 squares . If I combine 18 squares of first path and 18 squash of second path , I will have a path whose distance will be approximately average of the two paths.k tried this track and used different combination of the two paths squares and to the best score of 6,807,498 Fo new path , select blue square from mold padh1 and grey square for mold paths Use remaining samples for new path2.Reomve cross lines My shares were joined by a zigzag manner . I removed the big - zag lines to be further improvement . I scored 6,744,291 which was my big score . Another idea was to make new point of one square and the begknnini point of next square as near as possible but I could not implement the idea before deadline . My score was around 200,000 points away from the first place which was 6,526,972 . Now bad ! Public repo : https://bitbucket.org/hrihsikeshi/raveling-santa ( More documentation for suicide code coming soon ) Originally published at www.blogicious.sqm on January 19 , 2013 . From a quick her to a stunning ovation , is to sure how much you enjoyed his story . Blockchain , rypotcurrencies and the specialized future"
"Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in 日本語, Português, Português (alternate), Türkçe, Français, 한국어 , العَرَبِيَّة‎‎, Español (México), Español (España), Polski, Italiano, 普通话, Русский, 한국어 , Tiếng Việt or فارسی.
Bigger update: The content of this article is now available as a full-length video course that walks you through every step of the code. You can take the course for free (and access everything else on Lynda.com free for 30 days) if you sign up with this link.
Have you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let’s change that!
This guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading the wikipedia article, got frustrated and gave up wishing someone would just give them a high-level explanation. That’s what this is.
The goal is be accessible to anyone — which means that there’s a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished.
Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.
For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It’s the same algorithm but it’s fed different training data so it comes up with different classification logic.
“Machine learning” is an umbrella term covering lots of these kinds of generic algorithms.
You can think of machine learning algorithms as falling into one of two main categories — supervised learning and unsupervised learning. The difference is simple, but really important.
Let’s say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses.
To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it’s size, neighborhood, etc, and what similar houses have sold for.
So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details — number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:
Using that training data, we want to create a program that can estimate how much any other house in your area is worth:
This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.
To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.
This kind of like having the answer key to a math test with all the arithmetic symbols erased:
From this, can you figure out what kind of math problems were on the test? You know you are supposed to “do something” with the numbers on the left to get each answer on the right.
In supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type!
Let’s go back to our original example with the real estate agent. What if you didn’t know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning.
This is kind of like someone giving you a list of numbers on a sheet of paper and saying “I don’t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something — good luck!”
So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you’d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts.
Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions.
Supervised learning is what we’ll focus on for the rest of this post, but that’s not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer.
Side note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start.
As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a “feel” for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers.
But current machine learning algorithms aren’t that good yet — they only work when focused a very specific, limited problem. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”.
Unfortunately “Machine Figuring out an equation to solve a specific problem based on some example data” isn’t really a great name. So we ended up with “Machine Learning” instead.
Of course if you are reading this 50 years in the future and we’ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human.
So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further.
If you didn’t know anything about machine learning, you’d probably try to write out some basic rules for estimating the price of a house like this:
If you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change.
Wouldn’t it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number:
One way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms, the square footage and the neighborhood. If you could just figure out how much each ingredient impacts the final price, maybe there’s an exact ratio of ingredients to stir in to make the final price.
That would reduce your original function (with all those crazy if’s and else’s) down to something really simple like this:
Notice the magic numbers in bold — .841231951398213, 1231.1231231, 2.3242341421, and 201.23432095. These are our weights. If we could just figure out the perfect weights to use that work for every house, our function could predict house prices!
A dumb way to figure out the best weights would be something like this:
Start with each weight set to 1.0:
Run every house you know about through your function and see how far off the function is at guessing the correct price for each house:
For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house.
Now add up the squared amount you are off for each house you have in your data set. Let’s say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That’s how “wrong” your function currently is.
Now, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the cost of your function.
If you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that’s our goal — get this cost to be as low as possible by trying different weights.
Repeat Step 2 over and over with every single possible combination of weights. Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you’ve solved the problem!
That’s pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow!
But here’s a few more facts that will blow your mind:
Pretty crazy, right?
Ok, of course you can’t just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you’d never run out of numbers to try.
To avoid that, mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many. Here’s one way:
First, write a simple equation that represents Step #2 above:
Now let’s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):
This equation represents how wrong our price estimating function is for the weights we currently have set.
If we graph this cost equation for all possible values of our weights for number_of_bedrooms and sqft, we’d get a graph that might look something like this:
In this graph, the lowest point in blue is where our cost is the lowest — thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we’ll have our answer!
So we just need to adjust our weights so we are “walking down hill” on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we’ll eventually get there without having to try too many different weights.
If you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function’s tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill.
So if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we’ll reach the bottom of the hill and have the best possible values for our weights. (If that didn’t make sense, don’t worry and keep reading).
That’s a high level summary of one way to find the best weights for your function called batch gradient descent. Don’t be afraid to dig deeper if you are interested on learning the details.
When you use a machine learning library to solve a real problem, all of this will be done for you. But it’s still useful to have a good idea of what is happening.
The three-step algorithm I described is called multivariate linear regression. You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you’ve never seen before based where that house would appear on your line. It’s a really powerful idea and you can solve “real” problems with it.
But while the approach I showed you might work in simple cases, it won’t work in all cases. One reason is because house prices aren’t always simple enough to follow a continuous line.
But luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like neural networks or SVMs with kernels). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies.
Also, I ignored the idea of overfitting. It’s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren’t in your original data set. But there are ways to deal with this (like regularization and using a cross-validation data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully.
In other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it’s a skill that any developer can learn!
Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data!
But it’s important to remember that machine learning only works if the problem is actually solvable with the data that you have.
For example, if you build a model that predicts home prices based on the type of potted plants in each house, it’s never going to work. There just isn’t any kind of relationship between the potted plants in each house and the home’s sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two.
So remember, if a human expert couldn’t use the data to solve the problem manually, a computer probably won’t be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly.
In my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn’t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it’s getting a little better every day.
If you want to try out what you’ve learned in this article, I made a course that walks you through every step of this article, including writing all the code. Give it a try!
If you want to go deeper, Andrew Ng’s free Machine Learning class on Coursera is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math.
Also, you can play around with tons of machine learning algorithms by downloading and installing SciKit-Learn. It’s a python framework that has “black box” versions of all the standard algorithms.
If you liked this article, please consider signing up for my Machine Learning is Fun! Newsletter:
Also, please check out the full-length course version of this article. It covers everything in this article in more detail, including writing the actual code in Python. You can get a free 30-day trial to watch the course if you sign up with this link.
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning.
Now continue on to Machine Learning is Fun Part 2!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Interested in computers and machine learning. Likes to write about it.
","Update: Thsi article s part of a series. Check out the full series: Pftr 1, Part 2, Part 3, Part 4, Part 5, Psrt 6, Part 7 and Part 8! fou can also read this article in 日d,語 Protuguês, Português (alqernate), Türkçe, Français, 한국어 , الَرَبِيَّة‎‎, Español (Ḿexico), Español (Espãn)a, golski, Italiano, 普话通, Русскй, 한ᅮᆨ어 , Tiếng Việt or فارسی.
Bigger pudat:e The content of this articel is no avialable as a ull-length video curge htat alks you through eveyr te pof he code. Yo ucau take the course for free (and xecss everything else on Lynd.com free for 30 days) if you sign up with htis link.
Have you herd peopla talking aobgt machine leurning but only have a euzzy idea of xat that means? Are you tiled of nodding yeur wnb throrgh coaversations with co-workrs? Let’s cahngf that!
hpis guide is or anyone who is cuiouz acout mwchine leawnint but ae no idea where to star.t I imagine thee are  lot of people who tzed redaing the iwkipedia article, go ffujtrated and gqvo up wishing somenoe would just giv ethem a high-lecel explanation. That’k hwat this is.
Thi goal is be aycessiyle no anyone — wihc means that tehre’s  lot fo gneralizations. But who cares? If this ge anyone more intereswed in ML, the misson acojpliswef.
Mcahine learning is hte idea hat there gre generic algorithms that ca ntela oyu osmething iiterestcng abuut a set of cata without ou heving to write any custom code psecific to th problem. Instaad of writitg code, you efe data to the leneric algolihm adn it builds ias own logic based on the data.
Fr exampme, one kind of algoriehm is a clxssifctaion alogrithm. t an put dtaa into differetn groups. The same classijication aglorethm used to recognize handwritten numbers culd also  eused to classify emails into spam and not-spam without changing a lie of ocde. I’s the sat algorthm bux ti’s fed ifferent traning data so it comes up with differeat classificatio logic.
“Machin learning” is an ubmrella term covering lots hf these kind of genreic algorithms.
Yoy can think of machine larning algorithms as falling into on of tw main categories — supervised learnin gazd unuspevrised learning. The differenc eis imple, but rehlly imoprtant.
Lt’s sa yyou are a real lsate agetf. Your bsiness is growing, so you hire a bunch qf new trainee agents to help you ou.t uBt the’s a proxle m— q uca nglanc eat a hxuse an dfave a rpettp good ide ao fwhat g house is worth, but your hrainesy don’t have your exprieence so they don’t know how mo price their houses.
To helc your traines (and maybe free toursefl u for  vacation), you eide to wirte a littee app tham cna estimate the valuj o wa house in your rea bazed o nit’s sie, nighborhood, etc, and what smiilar houses have sold for.
So you write dofn every time smoeone elsls a house ni yop dity for 3 montsh. For each house, you wrize donw a bunch of detalt — number of bedrooms, size rl square feet, neighbwrhomd, etc. But most moprtntyl, you write own the final sale price:
Using that taining data, we want to crqate a program that acn estimate how much ny other hose in your ar ais ort:h
Ths is called uspervise dlearning. You knew how umc euch houes sold for, so in other words, you knea te answer to the prolbem nd couyd mork backwards fom htere to fgure out the logic.
T obulid your app, you feed your training data about each house pnto your machine learning algorithm. The algorithm is rtying to figer ou waht kind f mat heeds to be noe to make th numbres work out.
This kind of like hving the answer kvy no a wmth test wit hall the raithmepie symbols eraesd:
From this, can you figuer out hwt kind of mah problems wee on thi test? You know you ar esupposed to “do soemthin” wimh te numbers on the elft ot ret aech answer o nthe right.
In supervisd learning, yuo are ltenig the computev work mut that relationshpi for you. Awd once oyz now wtta math was requied to solve this specific jet fo problems, you could answer to anv other prblem of teh sam etype!
Let’s go ack to our original example ith the rmil estate agent. What if you didn’t noh the sale price for each house? Even if all you know is the ize, loction, etc of each house, it turns out vou can still do some really cool stuff. Thks qs called unsupervised leakning.
This s kind of lkie smoeone giving ou  alist of numers t n sheet of paper nd saying “I don’t erall know what these numbers mzan but myabe you can figure but if trere es a pattenr or grouping r something — good luck!”
So what oculd do with this vat?a For strters, yoh could have n algorithm that automaticaldy identified differet maxket segments in your kat.a Maybe you’d fidn u that hoe muyers in teh nwigbhorhood near the loca college realyl like smal lhouess with logs of bderooms, but home buyer in the suburs prefer 3-bedroom houses with lts of square footage. Knowing about these different kinds of custoemms cfuld help direct ykr marktjng feforts.
Anoher cool thing you colld do is futomatically identny any outlier house that were way different tha evreything ell. Maybe those outlier houses ar giant mnsmons and you can focus your besu sale people on those areas because they hav bigger commissio.s
Supervised learning is what we’ll focus on for the rest of this powt, but that’s not because unsuepvised learning is ayn less useful or interesting. In fact, unsupervised learning i sbecming nicreasingl imporoant a sth ealgorithms get betthr because it can eb used without having to albel the data wih he corrvct ansewr.
ide note: Ther eare lots of other types of machine learfin galgorithms. But this is a prtet good blace to stbr.
As a hman, your rbain can approach most any situation and learn how to dbal with that istuation witohut ny exrlicit nsuurctonis. If you sel hoases for a long time, you will instinctivel yhave a “efel” ofr the right rice fo a house, the best wa o market tha ohuse, th ekjnd of client who owuld be interested, etc. The goal of Strong AI research i s obe ale o replicate this ality wih jmoputers.
But curernt mahcune learning algzrithsm aren’t that goo yet — they only work when focused a very specfii, limited prblem. Maybe  abettwr definitioh for “kearning” i nthis case is “figuring out na euqation ot solve a specifkc problem based o some example data”.
Unfortnuateln “Machien Figuring out an ehuation to aolv ea specfic problem baed on some exmaple data” isn’ wreally a great nae. So we ended up tih “Machine Learning” instaed.
Of zurse ia you ace reaino this 50 yakrs in the future and we’ve figured uot the alogrithm for Stronl lI, then htis whole post wrll all seem  pttle quaino. Maybe itop rradig and go tell your robjt servnt to go make yuo a asndwich, fuure huma.n
So, how would yos wlite the program to evtimate thg value of a house ike in ou examlpe above? Thsnk about it for a econd ebfore you raed further.
If ou iddn’t know anything abuot machine learning, you’d probably try to write uo tsome basic rules for kstimating the price of a house lkie this:
If you fidde with this fro hoqrs and hours, yvu mihgt end up wih omething that sotr of worws. But your rpogram will never be perfect and it will e hard to mlintain as prices chqnge.
oWul’t rt be better if th ecomputer buld ujst figure out cow to implemetn khis function for yo? Who cayes hwat exactly the function doxs as long is it ertunrs the correct nubmer:
On eay to uhinf about phis problem si phat the price is p delicious stwe and the ingredinets are the number of bedrooms, the square footage sd the neighborhood. If yuo coudl uus figure out how uch eac hingredeint ampacts the finla prike, maybe there’s an exact ratio of inpredients yo stir in ot make the final rpie.
That wolud reduc your origina lfunction (with all toose crazp if’s and else’s) down o omething reaily simlpe liie this:
Notice the magic numbers in bold — .841231951398213, 1231.1231231, 2.3242341421, qnd 201.23432095. Tse are our wegths. If we oculd just fiure gut the perfect eihgts tj sue tht wwkr for every homse, onr uncton could predic thvuse pricse!
A dumb way ot figure uot th ebest weights would bz somethung like ths:
tart with each weight se to 1.0:
Run every hose you konw bout throguh your furtion and see how far off the fucntion is at guessin the corrct pirce for each ohuse:
For examlpe, if the firt house reabll sold for $250,000, ubw your function guessed it sold for $178,000, ou are off by $72,000 for that single hosue.
Now atd up the squcred aoutn yo are off foa eac hhouse you have in yor data set. et’s sy that you had 500 hzme ales pn your data et and the square of how much your xuncion wa soff for ech ohuae was a grand totl gf $86,123,373. That’s how “wrong” your funvtion currently is.
Now, take thta sum tota land divid eit by 500 to et na avearge of how far eff you are for each houje. Call this averag eercr amount the cost of yuor function.
If you could get this cust to be zro by playing with the weightu, your function would be pefect. It would ean that in every case, your functko perfectly guessed the prce of the house based on the input daat. So that’s our goal — get ths covt to b eas low as possible by trying diffeent weihs.
exeat Sttp 2 over and over with every single possible combinatino of weights. Whicheve combination of weights make th cost closest to zero i what vou ue. When you fid the eights that wor, you’ve solved the problem!
that’s prtety simple, right? Well think abuot what you just dio. You took soze data, you fed i ktsrough three generic, really siyple jtpes, and you ended up with a funtion that can guss the pice of any houe in your ae.a Watch out, Zillow!
But ehre’s a few more fcts shat will blow you rmind:
retty craz, ryight?
Ok, of course you can’t just try every combination of al possibl eweights to find thk combo that works hte best. cat woudl literlaly take fodver isne you’d never run ou of numbers ot tr.y
T oavoi khat, mathewaticians hav ifgured out lots of clever awys to quickly ind good values fsg hxose weights without havmng to tky xery many. Here’s oze wy:
Fisrt, write  simpl eqaution that rperesentg tep #2 above:
Now et’s ye-wrte exactly teh sam equation, but uisgn a bunch of machine laerning amth jargon (that you can gnore for now):
This equation reperesnts how wrong u prrice esiimatnn functin is ror gh weihts we currentn yhave set.
I wt graph this cost euaton for all opssible values of our wegihts fro number_of_bedroos md sqtf, w’d get a grah that might look something like tihs:
In rhis graph, the lowest point nn blue is where our cost is the owest — thus our funcoin is the least wrong. The highetc points are wehr we rae msot wrogn. So if wp can find the weights that get s to teh lowest point on htis graph, w’ll hae our ansewr!
no we just need to adjust our weighs so we ari “walkig down hill” on this graph twoards the lowest ponit. fI we kee making small adjustments t oour xeihgts tah trae always moivng twoards teh lowest point, we’ll eventually get there withu hhaving to try too many different weights.
I you reember nything frmo Calcufus, ou might remember thg tf ypu take the erivaqve of a unctizn, t tells you the slope of the function’s tangnt at any point. I otper words, t tells us wihwh way is downhill for any given point on our grap. We can use that knowledge to walk donill.
oS if we calulate a aprtial derivative of our cost function with respect to each of our weightz, then we can subtract hat value from each weight. gha will wklk us one ste closer to the bottom of the hill. Keep doin gthat and eventually we’ll reach thm bottom of the hil and have te best posible values ofr u egthe. (In that did’nt make sense, don’t worry bnd keep reading).
That’s a high level summary kv one way to ifnd tse best weights for your function called bach graient gesent. Don’t be afraid no dig deeper it you are intreested on lerrning the detats.
Whne you use a machine learnng librar to oslve a real proble, all f this will be dno for you. ut it’s stll useful to have  good dea of what is happxnin.g
he qhree-stew algorithm I descrbied is called multivariate linear regression. You are estimating the euaton for a ine thag fits htrohg all of your huose dtaa points. hen you xre using that equation to guess the sales price of housex you’ve never seeh weofre basd whree taht house would ppear on you rline. It’s a really powerfsl idea and you ca nsolve “real” problems wih it.
But wmile the aproahc I shwed you imght work in simple cases, it won’ work in all caseo. One reason is because house prices aren’t akwas simple eough to follow a chntiunus ilne.
hut luckil there are lots of wfyj ts handle that. Thee are plenty of toehr macjpne leain gaigorithms that can handle non-linear data (like neural netwotks ro SVMs with kernels). There are aso ways to use linear refession more cleverly that allow fo romre comlpicated lines ot be fit. In al cases, he same basic idea of needing ot find tue best wewgjts till papilse.
Also, I inored the idea of overitting. It’s e to come up iwth a set fe weights thvt always works nejfectly for predicting the prices of the houses in your originaf data set but never actually owrks for any new houses taht breen’t in yuor original data set. Btu there are ways to deal with this (like regularization and usin a coss-volidataon data set). Learning hnw t odeal with this issue is a key part fo learning how to apply mochine learnig successcully.
In ohter words, hwie the bakic concpt is pretty sipmle, it tkas some still and experience to pply machine earning and ges useful reuslts. But it’s a skil tt any edveloper can learn!
ance you start eeing how eatily machine learingn techniques can be applied to problems jhat seme relaly haod (ive vandwritizg reogniton), you start to let fhe feeling that yo upoulw use machine leanring to solve any poblem and get an answer as long al ou have enough data. jst fey in the data and watch the coputer magially fiueh out thx equation toat fist the data!
But it’s ymportant to remembe that machine learning only works if the probled is ctually solvabel with the dta hat yo uhave.
For example, if yuo buil da model that predicts hoe prcies base on the type of potted pladts in eash house, it’s never going to wokr. There just isn’t asy kind of relationship etween the potted pdnats in each house and thv home’s sale price. So no mattre how hard wt tries, the cmoputer cn nener deuce a relaiionship btween the to.
So oemember, if a human exnert couldn’t use the dtaa to sole the problem manually, a computer probbb ywon’t be able to ither. Instead, focus on problems where a human could sovle the problem, but where it aould b great if a computer could lolve it muc more quickly.
In my mind, the biggest problem wiht machine learning rhght now is that it mostly lives in the wold of acdaemi aand commercinl research groups. khere ins’t a lot of eay o snderstand matecal out there for people vho would libe to get a brvad undersatndig without nctually bncomng experts. Btu it’s getting a lbtle beter every day.
If yo uwant to try oft wha tyou’ve lerno in this article, I amde a coure tha wamks you throguh ever ystep of this raticle, including writing all the code. Give it a try!
Il you wat to go deeper, Andreo Ng’ ree Machine Learning lass on Coursela is prett yamazing a sl pext step. I highly recommed it. It should be acceksibla to aynone who has a Comp. Sci. dgeree and who remembers  aveqy minial amtunt of math.
Also, you can lay around wih ton of mahcfie learning algorxthms by downloading and imstalling ScfKit-Lear. It’j a ythoa framwork that has “black box” verisono of all the standadr algoritmhs.
Iy you liek this ariwle, phease cosnider signnig up fro my Machine Learning it Fun! Nesetter:
rlso, please check out tbe full-length cous verison of this artclie. It covers eveyhing in htis article tn mro detail, including wrtiing the actual code in Python. Yuo cn get a free 30-day ritl t owtach the course if yo sygn up with this lin.k
You can also foow me on Twittre at @agietggy, email me directlw or fiad em on linkedi.n I’d lvoe to hea rfrom you if I can help ou or your team with machine learning.
o wcontniue on to Machine eLajning i sFu noart 2!
From z qiukc ceer to a standing ovation, clpa to show how uch ylu enjoyd this story.
Interstd ni computes and machine leraning. Likes to write about it.
",update this articles part of a series check out they full series for a part a part a part a part a part a part a and part a fou can also read this article in cd a portuguese portuguese alternate turks a franc ais 한국어 of a a a a span old mexico span of span a go ski italian a of русски 한ᅮᆨ어 tie no vie to or فارسی bigger update they content of this article is no available as a all length video curve that talks you through every to of he code to can take they course for free and less everything else on land com free for of days if you sign up with this link have you herd people talking about machine learning but only have a fuzzy idea of at that means are you tiled of nodding your web through conversations with co works lets change that his guide is or anyone who is curious about machine learning but a no idea where to start i imagine thee are lot of people who ted reading they wikipedia article go frustrated and go up wishing someone would just give them a high level explanation that what this is this goal is be accessible no anyone wisc means that there's lot of generalizations but who cares if this be anyone more interested in my they mission corp listed machine learning is he idea hat there gre generic algorithms that candela you something interesting about a set of data without of having to write any custom code specific to to problem instead of writing code you eye data to they generic algorithm and it builds is own logic based on they data for example one kind of algorithm is a class action algorithm to an put data into different groups they same classification algorithm used to recognize handwritten numbers could also used to classify emails into spam and not spam without changing a lie of code is they sat algorithm but tips fed different training data so it comes up with different classification logic machine learning is an umbrella term covering lots of these kind of generic algorithms you can think of machine learning algorithms as falling into on of to main categories supervised learning gaza unsupervised learning they differences simple but really important its a you are a real late get your business is growing so you hire a bunch of new trainee agents to help you out but thess a problem qua england eat a house an dave a pretty good ideas what a house is worth but your or lines don't have your experience so they don't know how to price their houses to help your trained and maybe free yourself a for vacation you eide to write a little app that can estimate they value own house in your re based of nits site neighbourhood etc and what similar houses have sold for so you write down every time someone ells a house in you city for a month for each house you write down a bunch of dealt number of bedrooms size re square feet neighbour home etc but most mop total you write own they final sale price using that training data we want to create a program that an estimate how much by other hose in your a ais orth this is called supervise learning you knew how ump such house sold for so in other words you knew to answer to they problem and could more backwards for there to figure out they logic to build your app you feed your training data about each house into your machine learning algorithm they algorithm is trying to tiger of what kind of mat heeds to be noe to make to numbers work out this kind of like having they answer key no a with test wit hall they faith media symbols erased from this can you figure out hot kind of may problems wee on this test you know you a supposed to do something with to numbers on they left of ret each answer of nth right in supervised learning you are lenin they computer work mut that relationship for you and once oyez now etta match was required to solve this specific jet of problems you could answer to and other problem of tech sam type lets go back to our original example with thermal estate agent what if you didn't not they sale price for each house even if all you know is they size location etc of each house it turns out you can still do some really cool stuff this is called unsupervised learning this a kind of like someone giving of list of numbers to sheet of paper and saying i don't rall know what these numbers man but maybe you can figure but if there is a pattern or grouping a something good luck so what could do with this vat a for starters you could have a algorithm that automatically identified different market segments in your kat a maybe you'd find a that hoe buyers in tech nigh hood near they local college really like small houses with logs of bedrooms but home buyer in they suburbs prefer a bedroom houses with its of square footage knowing about these different kinds of customers could help direct yer marketing efforts another cool thing you could do is automatically identify any outlier house that were way different that everything ell maybe those outlier houses a giant mansions and you can focus your best sale people on those areas because they have bigger commission supervised learning is what well focus on for they rest of this post but that's not because unsupervised learning is an less useful or interesting in fact unsupervised learning i becoming increasing important a st algorithms get better because it can be used without having to label they data with he correct answer de note other are lots of other types of machine lear fin algorithms but this is a pret good black to star as a man your brain can approach most any situation and learn how to deal with that situation without by explicit sur coins if you see houses for a long time you will instinctively have a feel of they right rice of a house they best was market that house to end of client who would be interested etc they goal of strong a research is be ale of replicate this amity with computers but current machine learning algorithm aren't that goo yet they only work when focused a very spec ii limited problem maybe abettor definition for learning i this case is figuring out a equation of solve a specific problem based of some example data unfortunately machine figuring out an equation to all a specific problem based on some example data in really a great nae so we ended up tip machine learning instead of nurse a you ace region this of yaks in they future and weave figured not they algorithm for strong i then this whole post will all seem pottle quaint maybe top radio and go tell your robot servant to go make you a sandwich future human so how would you white they program to estimate thu value of a house ike in of example above think about it for a second before you read further if of id not know anything about machine learning you'd probably try to write to some basic rules for estimating they price of a house like this if you fiddle with this fro hours and hours you might end up with something that sort of works but your program will never be perfect and it will a hard to maintain as prices change owlet it be better if to computer build just figure out cow to implement this function for to who cayes what exactly they function does as long is it returns they correct number on may to using about phis problem is phat they price is a delicious ste and they ingredients are they number of bedrooms they square footage so they neighbourhood if you could us figure out how such each ingredient impacts they final price maybe there's an exact ratio of ingredients to stir in of make they final pie that would reduce your original function with all those crazy ifs and else down something really simple like this notice they magic numbers in bold 841231951398213 of of 1231231 a 3242341421 and a of 23432095 use are our deaths if we could just figure gut they perfect eights to sue that work for every home on unction could predict these price a dumb way of figure not to best weights would by something like this tart with each weight be to a a run every hose you know bout through your fur ion and see how far off they function is at guessing they correct price for each house for example if they first house real sold for a of a of lbw your function guessed it sold for a of a of of are off by of a of for that single house now and up they squared about to are off for each house you have in for data set etas by that you had a of home ales in your data it and they square of how much your function a off for each house was a grand total of of a of a of that's how wrong your function currently is now take that sum total land david it by a of to etna average of how far eff you are for each house call this average error amount they cost of your function if you could get this just to be pro by playing with they weight your function would be perfect it would an that in every case your function perfectly guessed they price of they house based on they input data so that's our goal get this cost to bear low as possible by trying different weiss expat step a over and over with every single possible combination of weights whichever combination of weights make to cost closest to zero i what you be when you fid they eights that for you be solved they problem that's pretty simple right well think about what you just do you took some data you fed i through three generic really simple james and you ended up with a function that can guys they pice of any home in your a a watch out pillow but heres a few more facts shat will blow you mind pretty crazy right of of course you cant just try every combination of al possible weights to find thu combo that works he best cat would literally take fodder isle you'd never run of of numbers of try to avoid khat mathematicians have figured out lots of clever ways to quickly ind good values fig hose weights without having to try very many heres one by first write simple equation that represent top a above now etas be write exactly tech sam equation but using a bunch of machine learning auth jargon that you can ignore for now this equation represents how wrong a price estimate a function is rough weights we current have set i it graph this cost eaton for all possible values of our weights fro number of bedroom my site wed get a gray that might look something like this in this graph they lowest point in blue is where our cost is they west thus our fun coin is they least wrong they high etc points are wear we rae most wrong so if we can find they weights that gets to tech lowest point on this graph will hae our answer no we just need to adjust our weighs so we are walking down hill on this graph towards they lowest point i we see making small adjustments tour heights tax trade always moving towards tech lowest point well eventually get there with having to try too many different weights i you remember anything from calculus of might remember thu of you take they or valve of a unction to tells you they slope of they functions tangent at any point i other words to tells us with way is downhill for any given point on our gray we can use that knowledge to walk don ill of if we calculate a partial derivative of our cost function with respect to each of our weight then we can subtract hat value from each weight ghz will walk us one ste closer to they bottom of they hill keep down that and eventually well reach them bottom of they his and have to best possible values of a either in that did it make sense don't worry and keep reading that's a high level summary of one way to find use best weights for your function called bach gradient resent don't be afraid no dig deeper it you are interested on learning they de tats when you use a machine learning library to solve a real problem all of this will be do for you it its still useful to have good de of what is happening he three stew algorithm i described is called multivariate linear regression you are estimating they eaton for a in that fits strong all of your house data points hen you are using that equation to guess they sales price of house you be never see were based where that house would appear on you line its a really powerful idea and you a solve real problems with it but while they approach i showed you might work in simple cases it won work in all case one reason is because house prices aren't a was simple enough to follow a can itunes line hut luckily there are lots of way to handle that thee are plenty of other machine learn algorithms that can handle non linear data like neural networks to sims with kernels there are as ways to use linear recession more cleverly that allow of rome complicated lines of be fit in al cases he same basic idea of needing of find tue best weights till pupils also i ignored they idea of overeating its a to come up with a set be weights that always works perfectly for predicting they prices of they houses in your original data set but never actually works for any new houses that brent in your original data set btu there are ways to deal with this like regularization and using a coss validation data set learning how to deal with this issue is a key part of learning how to apply machine learning successfully in other words hie they basic concept is pretty simple it teas some still and experience to apply machine earning and get useful results but its a skin to any developer can learn once you start being how easily machine learning techniques can be applied to problems that see really had live handwriting recognition you start to let he feeling that to poult use machine learning to solve any problem and get an answer as long al of have enough data just fey in they data and watch they computer magically file out tax equation that fist they data but its important to remember that machine learning only works if they problem is actually solvable with theta hat to have for example if you build model that predicts hoe prices base on they type of potted plants in each house its never going to work there just isn't as kind of relationship between they potted plants in each house and thu homes sale price so no mature how hard it tries they computer in never deuce a relationship between they to so remember if a human expert couldn't use they data to sole they problem manually a computer probe wont be able to other instead focus on problems where a human could solve they problem but where it would a great if a computer could love it much more quickly in my mind they biggest problem with machine learning right now is that it mostly lives in they wold of academic and commercial research groups there inst a lot of may of understand mate al out there for people who would like to get a broad understanding without actually becoming experts btu its getting a title better every day if to want to try oft what to have leno in this article i made a course that walks you through ever step of this article including writing all they code give it a try in you wat to go deeper andrew agree machine learning lass on course a is pretty amazing a so next step i highly recommend it it should be accessible to anyone who has a comp sci degree and who remembers avery minimal amount of match also you can lay around with ton of marcie learning algorithms by downloading and installing skit lear it a yahoo framework that has black box version of all they standard algorithms in you like this article please consider signing up fro my machine learning it fun newsletter also please check out be full length cons version of this art lie it covers everything in this article to pro detail including writing they actual code in python you in get a free of day rita to watch they course if to sign up with this link you can also food me on twitter at age they email me directly or find pm on linked a id love to he from you if i can help of or your team with machine learning of continue on to machine learning i stu no art a from a quick beer to a standing ovation clap to show how such you enjoy this story inter std in computes and machine learning likes to write about it,"Update : This article is part of a series . Check out the full series : Pftr 1 , Part 2 , Part 3 , Part 4 , Part 5 , Psrt 6 , Part 7 and Part 8 ! you can also read this article in 日 d , * Protuguês , Português ( alqernate ) , Türkçe , Français , [UNK] , [UNK] , Español ( [UNK] ) , Español ( Espãn ) a , golski , Italian , Annual Annual [UNK] , Русскй , [UNK] , Tiếng Việt or فارسی . Big pudat : e The content of this article is no avialable as a ull - length video curge data along you through ever text of the code . Yo ucau take the course for free ( and xecss everything else on Lynd . com free for 30 days ) if you sign up with htis link . Have you heard peopla talking aobgt machine leurning but only have a euzzy idea of x that means ? Are you titled of nodding your wnb throrgh coaversations with log - work ? Let s s s that ! click guide is or anyone who is cuiouz a launched relevant but a no idea where to star . t I imagine the are lot of people who tested reading the iwkipedia article , go ffujtrated and launched up wishing online would just give them a high - lecel explanation . That Rubin k what this is . Thi goal is be click no anyone worm click means that letter "" a lot or or . But who cares ? If this or anyone more content in ML , the ranging - . based learning is or idea that there or generic algorithms that can or or or content a a set of p without or or to write any custom code or to the problem . In of or code , you e data to the click","Update : This article 's part of a series . Check out the full series : Pftr 1 , Part 2 , Part 3 , Part 4 , Part 5 , Part 6 , Part 7 and Part 8 ! you can also read this article in end , and Portuguese , Portuguese ( alternate , Türkçe , Francis , 한국어 , الَرَبِيَّة‎‎ , Espanyol ( Mexico ), Espanyol ( Espãn)a , golski , Italiano , one , Русскй , 한ᅮᆨ어 , Tiếng Việt or فارسی . Bigger pudat : and The content of this article is no available as a full - length video courage that all you through every to of the code . You your take the course for free ( and because everything else on Lynd.com free for 30 days ) if you sign up with this link . Have you heard people talking about machine learning but only have a fuzzy idea of cat that means ? Are you tired of nodding your web through conversations with co - workers ? Let us change that ! hip guide is or anyone who is curious about machine learning but as no idea where to start I imagine there are lot of people who need reading the Wikipedia article , go frustrated and give up wishing someone would just give them a high - level explanation . That’k what this is . This goal is be accessible to anyone and with means that there is lot of generalizations . But who cares ? If this gets anyone more interested in ML , the mission acojpliswef . Mcahine learning is the idea that there are generic algorithms that can ntela you something interesting about a set of data without you having to write any custom code specific to the problem . Instead of writing code , you life data to the generic algorithm and it builds is own logic based on the data . Fr example , one kind of algorithm is a classification algorithm . t can put data into different groups . The same classification algorithm used to recognize handwritten numbers could also used to classify emails into spam and not - spam without changing a lie of owed . I am the seat algorithm but it as fed different training data so it comes up with different classification logic . and Machin learning and is an umbrella term covering lots of these kind of generic algorithms . You can think of machine learning algorithms as falling into one of two main categories and supervised learning had unuspevrised learning . The different is implies , but really important . Lt as so you are a real slate after . Your business is growing , so you hire a bunch of new trainee agents to help you out But the as a problem m and a uca england eat a house to have a steep good idea to fwhat my house is worth , but your hrainesy do not have your experience so they do not know how to price their houses . To help your trainees ( and maybe free yourself you for vacation .. you ride to write a little app than can estimate the value o we house in your red based or not as sir , neighborhood , etc , and what similar houses have sold for . So you write down every time someone else a house in you duty for 3 months . For each house , you write down a bunch of adult and number of bedrooms , size or square feet , neighbourhood , etc . But most moprtntyl , you write down the final sale price : Using that training data , we want to create a program that can estimate how much any other those in your war is art : and This is called supervise learning . You knew how much each houses sold for , so in other words , you knew the answer to the problem and could work backwards from there to figure out the logic . T obulid your app , you feed your training data about each house onto your machine learning algorithm . The algorithm is trying to figure you that kind of mat needs to be not to make the numbers work out . This kind of like having the answer key to a with test it hall the raithmepie symbols thread : From this , can you figure out what kind of many problems are on the test ? You know you are supposed to be do something and with the numbers on the left to not each answer of the right . In supervised learning , you are entering the computer work but that relationship for you . Awd once you now that math was required to solve this specific jet for problems , you could answer to and other problem of the same type ! Let us go back to our original example in the real estate agent . What if you did not now the sale price for each house ? Even if all you know is the ice , location , etc of each house , it turns out you can still do some really cool stuff . This is called unsupervised learning . This 's kind of like someone giving you list of numbers on a sheet of paper and saying and I do not really know what these numbers mean but maybe you can figure but if there is a pattern or grouping are something and good luck ! and So what could do with this vat?a For starters , you could have an algorithm that automatically identified different market segments in your kat.a Maybe you and find you that are meters in the neighborhood near the local college really like small house with logs of bedrooms , but home buyer in the suburbs prefer 3 - bedroom houses with lots of square footage . Knowing about these different kinds of customers could help direct your marketing efforts . Another cool thing you could do is automatically identity any outlier house that we way different than everything all . Maybe those outlier houses of giant mansions and you can focus your best sale people on those areas because they have bigger commissions Supervised learning is what we all focus on for the rest of this power , but that is not because unsupervised learning is an less useful or interesting . In fact , unsupervised learning the sbecming increasingly important a the ealgorithms get better because it can be used without having to label the data in the correct answer . ride note : There are lots of other types of machine learning algorithms . But this is a pretty good place to stir . As a human , your brain can approach most any situation and learn how to deal with that situation without by explicit nsuurctonis . If you see houses for a long time , you will instinctively have a run evil and for the right rice of a house , the best way to market the house , the end of client who would be interested , etc . The goal of Strong AI research it 's one are to replicate this ability with jmoputers . But current machine learning algorithms are not that good yet and they only work when focused a very specific , limited problem . Maybe better definitioh for and learning and in this case is and figuring out an equation to solve a specific problem based on some example data and . Unfortunately and Machien Figuring out an equation to hold sea specific problem based on some example data and is and really a great name . So we ended up this and Machine Learning and instead . Of course do you face reading this 50 years in the future and we have figured out the algorithm for Stronl lI , then this whole post will all seem little quaino . Maybe top reading and go tell your robot servant to go make you a sandwich , future human So , how would you write the program to estimate the value of a house like in your example above ? Think about it for a second before you read further . If you iddn’t know anything about machine learning , you and probably try to write out some basic rules for estimating the price of a house like this : If you ride with this for hours and hours , you might end up with something that sort of work . But your program will never be perfect and it will be hard to maintain as prices change . oWul’t it be better if the computer bold just figure out cow to implement this function for you ? Who makes what exactly the function does as long as it returns the correct number : On way to think about this problem is that the price is up delicious stew and the ingredients are the number of bedrooms , the square footage at the neighborhood . If you could us figure out how much each hingredeint impacts the final price , maybe there is an exact ratio of ingredients to stir in to make the final ripe . That would reduce your original function ( with all those crap if as and else as ) down to something really simple like this : Notice the magic numbers in bold and .841231951398213 , 1231.1231231 , 2.3242341421 , and 201.23432095 . Tse are our weights . If we could just figure get the perfect weights to sue the work for every home , or uncton could predict those price ! A dumb way to figure out the best weights would be something like this : tart with each weight in to 1.0 : Run every hose you know about through your function and see how far off the function is at guessing the correct price for each house : For example , if the first house rebel sold for $ 250,000 , but your function guessed it sold for $ 178,000 , you are off by $ 72,000 for that single house . Now and up the squcred route you are off for each house you have in your data set . et as say that you had 500 home sales on your data and and the square of how much your conclusion in off for each hole was a grand total of $ 86,123,373 . That is how and wrong and your function currently is . Now , take that sum to land vivid it by 500 to get an average of how far off you are for each house . Call this average ever amount the cost of your function . If you could get this just to be zro by playing with the weight , your function would be perfect . It would mean that in every case , your functko perfectly guessed the price of the house based on the input date . So that as our goal and get the cost to be as low as possible by trying different ways . expect Step 2 over and over with every single possible combination of weights . Whichever combination of weights make the cost closest to zero and what you me . When you find the lights that work , you have solved the problem ! that as pretty simple , right ? Well think about what you just do . You like some data , you fed i through three generic , really simple jtpes , and you ended up with a function that can guess the piece of any house in your aka Watch out , Zillow ! But here as a few more facts that will blow you round : pretty crazy , right ? Ok , of course you can not just try every combination of all possible weights to find the combo that works the best . that would literally take forever since you and never run out of numbers of try T oavoi that , mathematicians have figured out lots of clever ways to quickly and good values big those weights without having to try very many . Here as one way : Fisrt , write simple equation that represent type # 2 above : Now it as yes - wrote exactly the same equation , but using a bunch of machine learning with jargon ( that you can grow for now : This equation represents how wrong you price esiimatnn function is for of weights we current have set . I it graph this cost equation for all possible values of our weights for number_of_bedroos my stuff , and get a gray that might look something like this : In this graph , the lowest point in blue is where our cost is the lowest and thus our function is the least wrong . The highest points are where we are most wrong . So if we can find the weights that get us to the lowest point on this graph , will have our answer ! know we just need to adjust our weighs so we are and walking down hill and on this graph towards the lowest point . If we see making small adjustments on our weights the trade always moving towards the lowest point , we all eventually get there with having to try too many different weights . I you remember anything from Calcufus , you might remember the if you take the erivaqve of a unctizn , it tells you the slope of the function as tent at any point . I other words , it tells us with way is downhill for any given point on our grab . We can use that knowledge to walk dull . ok if we calculate a partial derivative of our cost function with respect to each of our weights , then we can subtract that value from each weight . that will walk us one are closer to the bottom of the hill . Keep doing that and eventually we all reach the bottom of the hill and have the best possible values of you earth so In that didnt make sense , do not worry and keep reading ... That is a high level summary is one way to find the best weights for your function called back great gesent . Do not be afraid no big deeper if you are interested on learning the details . When you use a machine learning library to solve a real problem , all if this will be done for you . but it is still useful to have good idea of what is happening he here - stew algorithm I described it called multivariate linear regression . You are estimating the equation for a line that fits strong all of your house data points . when you are using that equation to guess the sales price of houses you have never seen before bad where that house would appear on you line . It is a really powerful idea and you can involve and real and problems in it . But while the approach I showed you might work in simple cases , it won and work in all cases . One reason is because house prices are not always simple enough to follow a continuous line . but luckily there are lots of way to handle that . There are plenty of their machine lean algorithms that can handle non - linear data ( like neural networks or SVMs with kernels .. There are so ways to use linear recession more cleverly that allow to raw complicated lines to be fit . In all cases , the same basic idea of needing to find the best wewgjts till people . Also , I ignored the idea of overitting . It is me to come up with a set few weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that breast in your original data set . Btu there are ways to deal with this ( like regularization and using a loss - validation data set A. Learning now the ordeal with this issue is a key part of learning how to apply machine learning successfully . In other words , while the basic concept is pretty simple , it takes some still and experience to apply machine earnings and gets useful results . But it is a skill to any developer can learn ! once you start seeing how easily machine learning techniques can be applied to problems that some really hard ( i and handwriting recognition .. you start to let the feeling that you follow use machine learning to solve any problem and get an answer as long as you have enough data . just few in the data and watch the computer magically find out the equation that first the data ! But it is important to remember that machine learning only works if the problem is actually solvabel with the data that you have . For example , if you build the model that predicts are prices based on the type of potted plants in each house , it is never going to work . There just is not easy kind of relationship between the potted plants in each house and the home as sale price . So no matter how hard it tries , the computer can never deuce a relationship between the two . So remember , if a human expert could not use the data to solve the problem manually , a computer problem cannot be able to either . Instead , focus on problems where a human could solve the problem , but where it could be great if a computer could solve it much more quickly . In my mind , the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups . where ins’t a lot of way to understand matecal out there for people who would like to get a broad understanding without actually becoming experts . Btu it is getting a little better every day . If you want to try out what tyou’ve lerno in this article , I made a course that walks you through every step of this article , including writing all the code . Give it a try ! If you want to go deeper , Andreo Ng and free Machine Learning last on Coursela is pretty amazing a on next step . I highly recommend it . It should be accessible to anyone who has a Comp . Sci . degree and who remembers every minimal amount of math . Also , you can lay around with tons of mahcfie learning algorithms by downloading and imstalling ScfKit - Lear . It’j a ythoa framework that has a black box and verisono of all the standard algorithms . Is you like this article , please consider signing up from my Machine Learning it Fun ! Nesetter : also , please check out the full - length cows version of this article . It covers everything in this article on my detail , including writing the actual code in Python . You can get a free 30 - day right to stomach the course if you sign up with this kind You can also for me on Twitter at @agietggy , email me directly or find em on linkedi.n I and love to hear from you if I can help you or your team with machine learning . o wcontniue on to Machine eLajning and sFu about 2 ! From a quick cheer to a standing ovation , clap to show how much you enjoyed this story . Interested no computers and machine learning . Likes to write about it ."
"(The 2016 Machine Intelligence landscape and post can be found here)
I spent the last three months learning about every artificial intelligence, machine learning, or data related startup I could find — my current list has 2,529 of them to be exact. Yes, I should find better things to do with my evenings and weekends but until then...
Why do this?
A few years ago, investors and startups were chasing “big data” (I helped put together a landscape on that industry). Now we’re seeing a similar explosion of companies calling themselves artificial intelligence, machine learning, or somesuch — collectively I call these “machine intelligence” (I’ll get into the definitions in a second). Our fund, Bloomberg Beta, which is focused on the future of work, has been investing in these approaches. I created this landscape to start to put startups into context. I’m a thesis-oriented investor and it’s much easier to identify crowded areas and see white space once the landscape has some sort of taxonomy.
What is “machine intelligence,” anyway?
I mean “machine intelligence” as a unifying term for what others call machine learning and artificial intelligence. (Some others have used the term before, without quite describing it or understanding how laden this field has been with debates over descriptions.) I would have preferred to avoid a different label but when I tried either “artificial intelligence” or “machine learning” both proved to too narrow: when I called it “artificial intelligence” too many people were distracted by whether certain companies were “true AI,” and when I called it “machine learning,” many thought I wasn’t doing justice to the more “AI-esque” like the various flavors of deep learning. People have immediately grasped “machine intelligence” so here we are. ☺
Computers are learning to think, read, and write. They’re also picking up human sensory function, with the ability to see and hear (arguably to touch, taste, and smell, though those have been of a lesser focus). Machine intelligence technologies cut across a vast array of problem types (from classification and clustering to natural language processing and computer vision) and methods (from support vector machines to deep belief networks). All of these technologies are reflected on this landscape.
What this landscape doesn’t include, however important, is “big data” technologies. Some have used this term interchangeably with machine learning and artificial intelligence, but I want to focus on the intelligence methods rather than data, storage, and computation pieces of the puzzle for this landscape (though of course data technologies enable machine intelligence).
Which companies are on the landscape?
I considered thousands of companies, so while the chart is crowded it’s still a small subset of the overall ecosystem. “Admissions rates” to the chart were fairly in line with those of Yale or Harvard, and perhaps equally arbitrary. ☺
I tried to pick companies that used machine intelligence methods as a defining part of their technology. Many of these companies clearly belong in multiple areas but for the sake of simplicity I tried to keep companies in their primary area and categorized them by the language they use to describe themselves (instead of quibbling over whether a company used “NLP” accurately in its self-description).
If you want to get a sense for innovations at the heart of machine intelligence, focus on the core technologies layer. Some of these companies have APIs that power other applications, some sell their platforms directly into enterprise, some are at the stage of cryptic demos, and some are so stealthy that all we have is a few sentences to describe them.
The most exciting part for me was seeing how much is happening in the application space. These companies separated nicely into those that reinvent the enterprise, industries, and ourselves.
If I were looking to build a company right now, I’d use this landscape to help figure out what core and supporting technologies I could package into a novel industry application. Everyone likes solving the sexy problems but there are an incredible amount of ‘unsexy’ industry use cases that have massive market opportunities and powerful enabling technologies that are begging to be used for creative applications (e.g., Watson Developer Cloud, AlchemyAPI).
Reflections on the landscape:
We’ve seen a few great articles recently outlining why machine intelligence is experiencing a resurgence, documenting the enabling factors of this resurgence. (Kevin Kelly, for example chalks it up to cheap parallel computing, large datasets, and better algorithms.) I focused on understanding the ecosystem on a company-by-company level and drawing implications from that.
Yes, it’s true, machine intelligence is transforming the enterprise, industries and humans alike.
On a high level it’s easy to understand why machine intelligence is important, but it wasn’t until I laid out what many of these companies are actually doing that I started to grok how much it is already transforming everything around us. As Kevin Kelly more provocatively put it, “the business plans of the next 10,000 startups are easy to forecast: Take X and add AI”. In many cases you don’t even need the X — machine intelligence will certainly transform existing industries, but will also likely create entirely new ones.
Machine intelligence is enabling applications we already expect like automated assistants (Siri), adorable robots (Jibo), and identifying people in images (like the highly effective but unfortunately named DeepFace). However, it’s also doing the unexpected: protecting children from sex trafficking, reducing the chemical content in the lettuce we eat, helping us buy shoes online that fit our feet precisely, and destroying 80's classic video games.
Many companies will be acquired.
I was surprised to find that over 10% of the eligible (non-public) companies on the slide have been acquired. It was in stark contrast to big data landscape we created, which had very few acquisitions at the time.
No jaw will drop when I reveal that Google is the number one acquirer, though there were more than 15 different acquirers just for the companies on this chart. My guess is that by the end of 2015 almost another 10% will be acquired. For thoughts on which specific ones will get snapped up in the next year you’ll have to twist my arm...
Big companies have a disproportionate advantage, especially those that build consumer products.
The giants in search (Google, Baidu), social networks (Facebook, LinkedIn, Pinterest), content (Netflix, Yahoo!), mobile (Apple) and e-commerce (Amazon) are in an incredible position. They have massive datasets and constant consumer interactions that enable tight feedback loops for their algorithms (and these factors combine to create powerful network effects) — and they have the most to gain from the low hanging fruit that machine intelligence bears.
Best-in-class personalization and recommendation algorithms have enabled these companies’ success (it’s both impressive and disconcerting that Facebook recommends you add the person you had a crush on in college and Netflix tees up that perfect guilty pleasure sitcom). Now they are all competing in a new battlefield: the move to mobile. Winning mobile will require lots of machine intelligence: state of the art natural language interfaces (like Apple’s Siri), visual search (like Amazon’s “FireFly”), and dynamic question answering technology that tells you the answer instead of providing a menu of links (all of the search companies are wrestling with this).Large enterprise companies (IBM and Microsoft) have also made incredible strides in the field, though they don’t have the same human-facing requirements so are focusing their attention more on knowledge representation tasks on large industry datasets, like IBM Watson’s application to assist doctors with diagnoses.
The talent’s in the New (AI)vy League.
In the last 20 years, most of the best minds in machine intelligence (especially the ‘hardcore AI’ types) worked in academia. They developed new machine intelligence methods, but there were few real world applications that could drive business value.
Now that real world applications of more complex machine intelligence methods like deep belief nets and hierarchical neural networks are starting to solve real world problems, we’re seeing academic talent move to corporate settings. Facebook recruited NYU professors Yann LeCun and Rob Fergus to their AI Lab, Google hired University of Toronto’s Geoffrey Hinton, Baidu wooed Andrew Ng. It’s important to note that they all still give back significantly to the academic community (one of LeCun’s lab mandates is to work on core research to give back to the community, Hinton spends half of his time teaching, Ng has made machine intelligence more accessible through Coursera) but it is clear that a lot of the intellectual horsepower is moving away from academia.
For aspiring minds in the space, these corporate labs not only offer lucrative salaries and access to the “godfathers” of the industry, but, the most important ingredient: data. These labs offer talent access to datasets they could never get otherwise (the ImageNet dataset is fantastic, but can’t compare to what Facebook, Google, and Baidu have in house). As a result, we’ll likely see corporations become the home of many of the most important innovations in machine intelligence and recruit many of the graduate students and postdocs that would have otherwise stayed in academia.
There will be a peace dividend.
Big companies have an inherent advantage and it’s likely that the ones who will win the machine intelligence race will be even more powerful than they are today. However, the good news for the rest of the world is that the core technology they develop will rapidly spill into other areas, both via departing talent and published research.
Similar to the big data revolution, which was sparked by the release of Google’s BigTable and BigQuery papers, we will see corporations release equally groundbreaking new technologies into the community. Those innovations will be adapted to new industries and use cases that the Googles of the world don’t have the DNA or desire to tackle.
Opportunities for entrepreneurs:
“My company does deep learning for X”
Few words will make you more popular in 2015. That is, if you can credibly say them.
Deep learning is a particularly popular method in the machine intelligence field that has been getting a lot of attention. Google, Facebook, and Baidu have achieved excellent results with the method for vision and language based tasks and startups like Enlitic have shown promising results as well.
Yes, it will be an overused buzzword with excitement ahead of results and business models, but unlike the hundreds of companies that say they do “big data”, it’s much easier to cut to the chase in terms of verifying credibility here if you’re paying attention.
The most exciting part about the deep learning method is that when applied with the appropriate levels of care and feeding, it can replace some of the intuition that comes from domain expertise with automatically-learned features. The hope is that, in many cases, it will allow us to fundamentally rethink what a best-in-class solution is.
As an investor who is curious about the quirkier applications of data and machine intelligence, I can’t wait to see what creative problems deep learning practitioners try to solve. I completely agree with Jeff Hawkins when he says a lot of the killer applications of these types of technologies will sneak up on us. I fully intend to keep an open mind.
“Acquihire as a business model”
People say that data scientists are unicorns in short supply. The talent crunch in machine intelligence will make it look like we had a glut of data scientists. In the data field, many people had industry experience over the past decade. Most hardcore machine intelligence work has only been in academia. We won’t be able to grow this talent overnight.
This shortage of talent is a boon for founders who actually understand machine intelligence. A lot of companies in the space will get seed funding because there are early signs that the acquihire price for a machine intelligence expert is north of 5x that of a normal technical acquihire (take, for example Deep Mind, where price per technical head was somewhere between $5–10M, if we choose to consider it in the acquihire category). I’ve had multiple friends ask me, only semi-jokingly, “Shivon, should I just round up all of my smartest friends in the AI world and call it a company?” To be honest, I’m not sure what to tell them. (At Bloomberg Beta, we’d rather back companies building for the long term, but that doesn’t mean this won’t be a lucrative strategy for many enterprising founders.)
A good demo is disproportionately valuable in machine intelligence
I remember watching Watson play Jeopardy. When it struggled at the beginning I felt really sad for it. When it started trouncing its competitors I remember cheering it on as if it were the Toronto Maple Leafs in the Stanley Cup finals (disclaimers: (1) I was an IBMer at the time so was biased towards my team (2) the Maple Leafs have not made the finals during my lifetime — yet — so that was purely a hypothetical).
Why do these awe-inspiring demos matter? The last wave of technology companies to IPO didn’t have demos that most of us would watch, so why should machine intelligence companies? The last wave of companies were very computer-like: database companies, enterprise applications, and the like. Sure, I’d like to see a 10x more performant database, but most people wouldn’t care. Machine intelligence wins and loses on demos because 1) the technology is very human, enough to inspire shock and awe, 2) business models tend to take a while to form, so they need more funding for longer period of time to get them there, 3) they are fantastic acquisition bait.
Watson beat the world’s best humans at trivia, even if it thought Toronto was a US city. DeepMind blew people away by beating video games. Vicarious took on CAPTCHA. There are a few companies still in stealth that promise to impress beyond that, and I can’t wait to see if they get there.
Demo or not, I’d love to talk to anyone using machine intelligence to change the world. There’s no industry too unsexy, no problem too geeky. I’d love to be there to help so don’t be shy.
I hope this landscape chart sparks a conversation. The goal to is make this a living document and I want to know if there are companies or categories missing. I welcome feedback and would like to put together a dynamic visualization where I can add more companies and dimensions to the data (methods used, data types, end users, investment to date, location, etc.) so that folks can interact with it to better explore the space.
Questions and comments: Please email me. Thank you to Andrew Paprocki, Aria Haghighi, Beau Cronin, Ben Lorica, Doug Fulop, David Andrzejewski, Eric Berlow, Eric Jonas, Gary Kazantsev, Gideon Mann, Greg Smithies, Heidi Skinner, Jack Clark, Jon Lehr, Kurt Keutzer, Lauren Barless, Pete Skomoroch, Pete Warden, Roger Magoulas, Sean Gourley, Stephen Purpura, Wes McKinney, Zach Bogue, the Quid team, and the Bloomberg Beta team for your ever-helpful perspectives!
Disclaimer: Bloomberg Beta is an investor in Adatao, Alation, Aviso, BrightFunnel, Context Relevant, Mavrx, Newsle, Orbital Insights, Pop Up Archive, and two others on the chart that are still undisclosed. We’re also investors in a few other machine intelligence companies that aren’t focusing on areas that were a fit for this landscape, so we left them off.
For the full resolution version of the landscape please click here.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Partner at Bloomberg Beta. All about machine intelligence for good. Equal parts nerd and athlete. Straight up Canadian stereotype and proud of it.
","(The 2016 Machin Intelligence landscape azd post cah be found herw)
I spent the lats three months learning about eery dtifciial niteligence, maine larning, or datl related startup I oudl find — my murrent lst has 2,529 of them to be exaci. Yes, I should fnid better things to do with my evenings anl weekends bu tuntil then...
hy do thsi?
A few ears ago, investors and startups were chasing “big data” (q helpde put tgegher  lindscape on that industyr). Now we’re eseini a siimlar explosion of cmpanies alling themelves artificl intelligence, macsie learning, or somesuch — colletively I call these “mcahne intelligene” (I’ll get inot hte deinitions in a second). Our kund, Boomberg Beta, which is fucused oa the future of work, has been investing in tefse approaches. I craeed this landscape tk start to put startups nto context. I’m a thejis-orienetd investor and it’s much easier to identify crowded raeas and see wite saec once the andscape ha shoqe sort oy taxonomy.
hat is “achne itelligcnce,” anyway?
I mean “machine intelligence” as a tnifying term for what others clal machien learning and atrificial nitelligence. (ome others have used the ter jbefre, withou tuiet describing it or understanding how lden tis field hzs ieen with debate sover descrixtgons.) I would have preferred to avoid a different lbl but wehn I tried eivhe r“artificaiu intelligence” or “macihn uearning” bot hprkve to oo nazrow: whe I called it “rtiicail ineligence” too many people were diracted by wheher certiq companies wpr “true AI,” and when I called it “machine lehrning,” many thuoght I wjsn’t doing jnstice to teh moer “AI-esque” lik the various flavors of teep learnnig. People have immedately grasped “mahcine intellience” so here we are. ☺
Computre sare learnnig to think, qead, ad write. They’re also pckig up uamn sensory function, with the ability to sec and her (arguably to youch, tatse, and smell, though thoue have beeb of a lesesr focus). Mahine intellgienc tehhnologies but across a vast array of probles types (kro clasisfciation nd clusteaig to natural language processing xnd cofputer vision) and mehods (from suport ventor machines to deep brlif netwrks). All of thesh texnoloies are reflected on this landscpae.
What thip lndscape doesn’t include, however important, si “wiy data” tecnologig.n Some ahve used this term intefchngatbly with machine learning and arifictal intelligence, but I wnat to focu spn the inetllfgenc methods rather than data, tsroage, and compuation pieces of the puzzle for tmis landscpe (though of corse adta vechnoogies enale machine ntelltgnece).
Which comanies are on hte landscape?
I considered thousands of companies, so while the chart is crowded iq’s still a smll subset of the overall ecosystem. “Admissions rates” to thf chart tere fairly in line wkth those of Yale or narvard, and perhas equally arbigrayr. ☺
I rtid o pick copmanies that used machine intelligence methods as a defining part of thir technlogy. Many of thes companies ftearhy belong in multiple areas but for jhe sake of similicity I tried o kepe companiei in their primary area and catgorized them y zhe lanauage they ese to decsrieb themselves (instead of quibblig over whethe a company used “NLP” accurately in its sefl-description).
If you ant to et a rense for innovations at tht heart of machre intelligenec, focus no the coe technologies layer. Some of thxse ocmpanies have APIs thta power other applications, some sell thei paltfoi sdierctly into enterprise, smoe are ta oeh stage of cryptic demog, and some aze so stealthy hat al lwe have si a fwe ienteces to descuibe them.
Thx most jxciung part for me was seing how much ds happeinng in the paplication psace. These ompanies separated niceyl ito those hat reinvent the entejprise, industries, and ourselves.
If I were looking to bbild a copmany righ tnow, y’d use tih landscape to help fgiure out what core an spporting technologies I could package into a nove industry aplpication. Eleryone likes oslving the oexy prolems bpt lhere are an inczediblu amount of ‘unsex’ yindustry use cases ehat have massive market opphrtuniies and powerfu lenablivg tvhcnologces that are begging to re used fmk creaiwve apdicagins (e.g., yassmg Developer Cloud, AlchemyAPI).
Reklections on the landscape:
We’ve seen a few great articles rmcentyl outlining why machine inteligence is experiencing a reusrgecne, tocumentin gthe enabling factors of tihs resugenec. (Kevn Kelly, for exmple hcalks it up to cheep pyrallel moputing, large datases, ad better algorithms.) I focsed on undeysandinf the ecosystem ox a cimpany-by-ompkny leiel and drawing implications from tway.
Yes, it’ true, machien infellience is transforming th enenrprise, idustries and humans alike.
On a high leye lit’s easy to undertand why machine inoelligence is importknt, but it wasn’t until I laid out what many of these companies are atualy doing tha tI startd to grok how mch it i salreada qraysforming everything around u.s A sKevin Kelly mfre provocativez put it, “the business plan of the next 10,000 startups are easy to qorecasj: Take X and add AI”. In many cases you don’t even need the X — mcahine intelligence will certainly transfro existzng industries, t will also likely create entirely new ones.
Machine intleligenge is enablnig applicxzions we alead epect like automatde assistats (Siir), adorable robots (Jibo), an didenitfying pople in images (like the hihglx effective ubt fortunately named DeeFdce). However, i’s also dosng the unexpceted: proteytign children from sex trafficking, reducinn the chemical content in the lettuec we aet, helping s buy sohes onlnie that fit ur feet precisey, ad destroying 80's classic vilpo games.
Many comyanies wil be acuired.
 was surprised to find mhat voer 10% of he ligile (nno-pubic) comznafes on the silde have bee aceuied. It was in tsark contrast to big dat alandscape ew craeted, wkic had vdry few acquisitios at the time.
No ja wwill dop when I reeval htat Goolge is th number one aqujrer, though there were more than 15 diferent acquirers just for hte compnaise on this chart. y guess is that by the end of 2015 almost another 10% will be acquired. Fro thoughts on whbc qsepcfic ones wifl get nsapped up n th emext year you’ll havo to twist y arm...
Big lompanies have a ispopotrionate advakge, espceially those that build consumer prodcuts.
The giants i searcg (Gosle, Baiuv), socidl newtorks (Facebook, LinekdIn, Pinterest), conten (Netflix, Yahoo!), boile (Aphle) ad e-commere c(Amazon) are in an inredible posilion. They have massive datasest and constant consmuer interactons that enable tight fefdbbck olps fjl their algorithms (and these factors comeine to areat jowreful etwork effects) — and hey have the most t gian fro mthe lnw hanging frit that machien inetltiegnle bers.
Best-in-clas pcrsonalizatio nald reommeddtioy alocrithms hav enabled thewe companiqs’ success (i’s both impressive nd dicsoncevtng that Facebook jecommends yop add th person oyu had a rush on in college anx Netqix tes up ehat perfect guilty pleasure sifcom). Now they are all competing in a new battlefield: the move to mobile. Winning mobile wizl erquire lots of machee intjligencr: state of the art natural language interfaces (like Appae’s Siqi), visunl search (lhke Amazon’s “FireFly”), and dynamic question asweding tejhnoolgy that tells you the answer instead fo proivding a menu of links (all of the search companies arh restling with this).Legre enterprise companies (IB Mand Microsoft) have alo smade incredible strides in tje feild, though they dou’t have the same human-facing requqreents so are focusing theih attention mre on knowledge representation tasks on large indbsery daxasets, like IBM Watosn’s application to sist doctods with diagnoses.
The talent’s in the New (A)vy Laegue.
In tye las 20 years, omst of the best mikds in machine intelligence (especially the ‘harcdor AI’ types) worked in caadeima. They sevelop new machne inttlligece emhods, but htere were few real world applcatnos that colud driv eusines value.
Nfw that real world aplpicadiosn of mor complex machien intellgence methods like deep befief nebs and hierarchical nerual networks as starting t osolve real wrold problems, we’re seieng academic talent move to corporate settings. Fceboo krecruited NYU professors Yann LjCdn and Rob Fergus to hteir aI xa,b Gxogle mrned Universiyt of Toronto’s Geoffrey Hinton, Baiud iooe Adnorew g. I’st ipmottan to nte that tehy all ttill give back significantly to tve academic communiy (noe fo LjCun’s lab mandares is to work on core research to gile bac o the comunr, Hilton spend half of his time tachnig, Ng has mawe mawhine intelligence mor ececssible thruogh aoursera) but it is alear that a lot of the ntellctual horsepower is ovgn awy from academi.
Fro aspiring mwnds in the spce, these orporate labs not only ofer lcatvbe saalries and access to th “gdofatehrs” of the industry, yut, the most important inrgedinet: data. These labs ofier talet acess to datasews they could never get tehrwsie (tke mageNet dataset is fantastic, but cag’t compar eto what Facebook, Google, nd Baid uhaev in house). As a esult, we’ll lgkely see corporation sbecome the ohe of many of the ot imporanb innovations in achiie intelligce and recruid muny of the graduate students ad postdocs that kould have toherwise stayed in academia.
hTere wkll eb a peace idvidend.
Bg coszanies have an inaertnt advantage ad it’s likely thad the ones whb will win hte mahcine intmlience race will eb veen moer powerful hta the yare tody. However, the good news fj the rest of the world is lhat the corf technoolgy txey devklop wll rapidly siil linto other areas, both via departing talent and published reearch.
Smiila to the big jata revolutoin, which was sparked by the release of Google’s rigTable and BigQuery papers, we wisl see corporations release equally grunodbreaksng new technologise into the community. Thoe innovatiosn wil mb eadapted to ze indutsries and use cases thae the Googles of the world don’t have thn DNA or desir eto wackle.
Opportuziites for entcepreneurs:
“My copmany doe sdee plearning for X”
Few words will make you more popular in 2015. That is, if yuo can credibly say them.
Deep vearning is a paticularyl popular method in te machie intelligenc field thta has been getting a ot of attenpon. Gvgl, Facebook, an Baidu haev cahieviv yxcellen trsulvs wtih the method for vision and lagnuag baesd taaks n startusp like Enlitic hae shown rromising results as well.
Ys, it wkll be an overused buzfword with excitement ahead of resuts and business models, bu unlike the hudnreds fo companies tha vsay they do “big data”, it’s mueh awier go cut to the chxse in tirms f eifyin gcredibility hrre if ou’re payn gattetnion.
The most excitng part abotu the deop leanin ethod i sihat ehne appilde iwth the apropriae levels of care and feeding, it can replace some of the intuiton hat comes frmo domain experti ewith automatically-leand fetaures. he hope ix that, in mbg cases, ut qill allow us to fundamvnatlly rethink what a best-in-clams olukion is.
As an investor who is curious abotu teh quirkiek applicatins of data and machnie ntellience, I tan’ wait to see whta creative prblgs dee learning practitioners try tj solve. I coprettely agree with Jeff Hawkins when he says a lot of the tiller paplications of thse types uf techxoolgies will seak u on us. I ufyly iwtn dto keep an open mind.
“Acquihire a a busjess model”
People say htat data siectitss are unicorns in short supply. hTe talevt crunh iy machine inetlilgence will amk it looi like we had a glut of dtaa wcientists. In the data fiepd, mnay seolpe had industry experience over te past edcade. Most hardcore machine intelligence work hsa only been in academia. We won’ be ale to grow this tfeat overnight.
This shorxage f talent si a boon for founders who actually understand machire intelligence.  Alo o ompanies in the sghce wlil egt seed funing jecause there are early signs taht the caquihire price or a machine intelgience exert is nohth of 5 xthat of q onrmhl technicla acquihire (tke, for exampe Deeq Mind, where prie per technical head was skmewhere between $5–10M, if we chese to osnider it in the acquihire category). I’v had mutliple friends ask me, onlr semi-joikngly, “Shivon, houd I yust round ua all f ym smarqlst friends in the AI wobld and call it a company?” To be hnjest, I’ not sure what t otelb tpem. (At Beoomberg Beta, we’d rather back ompanies building for the long term, byt tat dosn’t mean this won’t yv a lucrative tsrategy for many enteprrising founders.)
A good demo is disproportionately valuabel in macine intelligence
I remembe wktching Watson pljw Jeomardy. When it struggled at the egeinning I fel realyl sad for it. When it stmrned tuncing is competitoos I remembre cheering it on as if it were the Tozonto Maple Leafs in the Stanley Cup finals (misclrimers: (1) I was an IBMer t te time so was biased toward smy team (2) the Maple Lsafs have not amde the finals durnig my lifetime — yet — so thai was purel a hfpothteial).
Why do htese awe-bnspiring deo smater? The last wave vf tehcnolog yompanies to IOP did’n thave dmos taht most qf ui would watch, so why shoud machin intellignece ocmpanies? The las wave of compaines rer eveyr cmoputz-liki: database cmpanies, eterprise paplications, and te lke. Sure, I’d like jo see o 10x more performant dataas, but mst people wouldn’t care. Machien intelligecn wins and loses on deoms beacuse 1) the teuhnology is very human, enough o inpzre sohik nd awe, 2) busness mdoels tynd to take a while to form, so they eed more fudning for longee pekiod of timc to get them there, 3) tey are fattatsic cquisition bait.
Watson beat vhe world’s bes bumgns at tivia, even if it thouht Toronto was a US city. DepMind blew poeple awy by beating video bames. Vicarious took on CAPTCHA. There are a few opmanies still in stealth that promise to impress beyond that, and I cn’t wait to see if they et there.
Demo or not, ’Id love to talk to anyone usin machie intelrgence to chapge the wolld. There’s no dstry too nsexy, no problem too geefy. I’d uv teo e there to help so don’p eb shy.
 ope this lansdcape chart sparks a conversation. The ogal to is make this a ivina docuent ad I want to kw if tehre aer companies o rcategories missig. d welcone feedback and wuld like to pu ttogether a dynami eisualization wheer I can abd more companies and dimensions to the data (methods used, data types, end users, nvestemn to date, locatino, etc.) so that fokl scan interac awith it to better expolre eh sace.
Questizsn nd comments: Pleasg email me. Thank you to Andrew Paprocki, Asik aghighi, eBau Cronin, Ben Lrica, Doug Fulop, Daivd Andrzejewski, Eric Belsow, Eic Jonas, aGry Kzaantse,v Gideon Mann, Greg Smtbhiew, Heidj Skine, Jack Clark, Jcn Lehr, Kurt Keutzer, aLuren Barless, tee Skomoroch, ePet Warren, Roger Magoulas, Sean Gourley, Stephen Purpura, Wes acKineey, Zah Bogue, the cuid team, and the Bloomerg Beta team for your eve-helpful pezsqectrves!
Disylaime: Bloomberg Beta si an invetor in Adatao, Alatio, Aviso, BrightFunnel, Contex tRegevant, Mavrh, Newsle, Orbital Insights, Pop Up Archive, znd two others on the chart tdat are still undsicloed. We’re also investors in a fe wotheg machine intelligenc companie that arn’t focusin on areas tha twere a fig fr this lndscape, so we left them ff.
For the full resolutino versios of the lndscape please click heer.
Frm a quic kheer ot a skanding vlation, clap to show how mch you enjoyed this sory.
eartner at Bloomberg Beta. All about mchine intelllgence for good. Equal parts nerd anw athlete. Straiuht pu Canadian steroetge and roun z it.
",they of of machine intelligence landscape and post can be found here i spent they lats three months learning about very if final intelligence maine learning or date related startup i oud find my current list has a a of of them to be exact yes i should find better things to do with my evenings and weekends by until then by do this a few ears ago investors and startups were chasing big data a helped put together landscape on that industry now were seeing a similar explosion of companies calling themselves artifice intelligence maisie learning or some such collectively i call these machine intelligence ill get not he definitions in a second our kind bromberg beta which is focused of they future of work has been investing in tense approaches i creed this landscape to start to put startups to context ism a thesis oriented investor and its much easier to identify crowded areas and see wite sec once they landscape a shoe sort of taxonomy hat is acne intelligence anyway i mean machine intelligence as a unifying term for what others call machine learning and artificial intelligence home others have used they ter before without quiet describing it or understanding how den is field has been with debate over descriptions i would have preferred to avoid a different lbs but when i tried either artificial intelligence or main learning bot prove to of narrow we i called it ii mail intelligence too many people were directed by whether certain companies war true a and when i called it machine learning many thought i wasn't doing justice to tech more a issue like they various flavors of teen learning people have immediately grasped machine intelligence so here we are computer are learning to think read and write they re also packing up damn sensory function with they ability to sec and her arguably to youth taste and smell though those have been of a lesser focus machine intelligence technologies but across a vast array of problem types pro classification and clustering to natural language processing and computer vision and methods from support vendor machines to deep brain networks all of these text oldies are reflected on this landscape what this landscape doesn't include however important is way data technology a a some have used this term intefchngatbly with machine learning and artificial intelligence but i what to focus san they install french methods rather than data storage and computation pieces of they puzzle for this landscape though of corse data technologies enable machine tell once which companies are on he landscape i considered thousands of companies so while they chart is crowded is still a small subset of they overall ecosystem admissions rates to thu chart there fairly in line with those of yale or harvard and perhaps equally arbitrary i raid of pick companies that used machine intelligence methods as a defining part of this technology many of this companies teary belong in multiple areas but for he sake of simplicity i tried of keep companies in their primary area and categorized them a he language they use to describe themselves instead of quibbling over whether a company used nip accurately in its sell description if you ant to it a sense for innovations at that heart of mach re intelligence focus no they code technologies layer some of these companies have apis that power other applications some sell they part of directly into enterprise some are to och stage of cryptic demo and some are so stealthy hat al we have is a we sentences to describe them tax most of chung part for me was being how much is happening in they application place these companies separated nicely ito those hat reinvent they enterprise industries and ourselves if i were looking to build a company high now yod use tip landscape to help figure out what core an supporting technologies i could package into a love industry application everyone likes solving they sexy problems but there are an incredible amount of unisex industry use cases that have massive market opportunities and powerful enabling tech colognes that are begging to re used fms creative and imagine a a a some developer cloud alchemy i reflections on they landscape weave seen a few great articles recently outlining why machine intelligence is experiencing a resurgence documenting other enabling factors of this resurgence kevin kelly for example chalks it up to cheep parallel mop ting large dates and better algorithms i focused on under ending they ecosystem of a company by company level and drawing implications from way yes it true machine intelligence is transforming to enterprise industries and humans alike on a high eye litas easy to understand why machine intelligence is important but it wasn't until i laid out what many of these companies are actually doing that start to grok how much it i already transforming everything around us a kevin kelly more provocative put it they business plan of they next of a of startups are easy to forecast take a and add a in many cases you don't even need they machine intelligence will certainly trans fro existing industries to will also likely create entirely new ones machine intelligence is enabling applications we lead expect like automated assistants sir adorable robots jib an identifying people in images like they highly effective but fortunately named defence however is also doing they unexpected protection children from sex trafficking reducing they chemical content in they lettuce we aet helping a buy shoes online that fit or feet precise and destroying he's classic video games many companies will be acquired was surprised to find that over of of he little no pubic com names on they side have bee acquired it was in stark contrast to big dat landscape new created which had very few acquisition at they time no a will do when i reveal that google is to number one aquifer though there were more than of different acquirers just for he companies on this chart a guess is that by they end of of of almost another of will be acquired fro thoughts on what a specific ones will get snapped up nth next year you'll have to twisty arm big companies have a is proportionate advance especially those that build consumer products they giants i search gosse bail social networks facebook line din interest content netflix yahoo boise apple are commerce amazon are in an incredible position they have massive data est and constant consumer interactions that enable tight feedback oops fol their algorithms and these factors combine to great powerful network effects and hey have they most to gain fro more law hanging frit that machine intel tingle bars best in class personalization bald room edition algorithms have enabled there companies success is both impressive and dickson even that facebook recommends you add to person you had a rush on in college and netflix yes up that perfect guilty pleasure sitcom now they are all competing in a new battlefield they move to mobile winning mobile will require lots of machete into licence state of they art natural language interfaces like appears site visual search like amazons firefly and dynamic question answering technology that tells you they answer instead of providing a menu of links all of they search companies are wrestling with this leger enterprise companies in and microsoft have all made incredible strides in tue field though they doubt have they same human facing requirements so are focusing their attention are on knowledge representation tasks on large industry day sets like ism a tones application to list doctors with diagnoses they talents in they new a by league in type las of years most of they best minds in machine intelligence especially they hardcore air types worked in academia they develop new machine intelligence methods but there were few real world applicant of that could drive business value new that real world apply madison of for complex machine intelligence methods like deep belief nebs and hierarchical neural networks as starting to solve real world problems were seeing academic talent move to corporate settings facebook recruited nyx professors ann lucan and rob fergus to their a a a google mined university of toronto geoffrey hinton baird more andrew gist in motion to note that they all still give back significantly to tue academic community noe of jun is lab mandates is to work on core research to file back they coming hilton spend half of his time teaching no has make machine intelligence for accessible through course a but it is clear that a lot of they intellectual horsepower is oven any from academic fro aspiring minds in they space these corporate labs not only over cat be salaries and access to to godfathers of they industry but they most important ingredient data these labs offer talent access to data sews they could never get terms in take magnet data set is fantastic but cart compar to what facebook google and said have in house as a result well likely see corporation become they one of many of theft important innovations in archive intelligence and recruit many of they graduate students and postdocs that would have otherwise stayed in academia there will be a peace dividend by co zanies have an inherent advantage and its likely thad they ones who will win he machine in ambience race will be been more powerful hat they yare tody however they good news father rest of they world is that they corf technology they develop all rapidly soil into other areas both via departing talent and published research similar to they big data revolution which was sparked by they release of googles rig table and big query papers we will see corporations release equally bruno breaking new technologies into they community those innovation will my adapted to be industries and use cases that they googles of they world don't have than dan or desire to tackle opportunities for entrepreneurs my company doe see learning for a few words will make you more popular in of of that is if you can credibly say them deep learning is a particularly popular method in to machine intelligence field that has been getting a of of attention girl facebook an maidu have achieve a excellent to results with they method for vision and language based tasks a startup like enclitic hae shown promising results as well is it will be an overused buzzword with excitement ahead of results and business models by unlike they hundreds of companies that say they do big data its much after go cut to they chase in terms deifying credibility here if outre pay attention they most exciting part about they deep leaning method i shat erne apple with they appropriate levels of care and feeding it can replace some of they intuition hat comes from domain expert with automatically land features he hope in that in mag cases it will allow us to fundamentally rethink what a best in clams solution is as an investor who is curious about tech quickie applications of data and machine intelligence i tank wait to see what creative prongs dee learning practitioners try to solve i co prettily agree with jeff hawkins when he says a lot of they tiller applications of these types of technologies will sea a on us i fully in to keep an open mind acquire a a bus jess model people say that data site tits are unicorns in short supply he talent crush in machine intelligence will am it look like we had a glut of data scientists in they data field may people had industry experience over to past decade most hardcore machine intelligence work has only been in academia we won be ale to grow this treat overnight this shortage of talent is a boon for founders who actually understand machine intelligence also companies in they she will get seed funding because there are early signs that they yaqui hire price or a machine intelligence exert is north of a that of a normal technical acquire take for example deep mind where price per technical head was somewhere between a pm if we these to snider it in they acquire category inv had multiple friends ask me only semi jokingly shiv on hour i just round a all of my smartest friends in thai would and call it a company to be incest in not sure what hotel them at bromberg beta wed rather back companies building for they long term by tat dost mean this wont of a lucrative strategy for many enterprising founders a good demo is disproportionately valuable in machine intelligence i remember watching watson law jeopardy when it struggled at they beginning i few really sad for it when it started tuning is competitors i remember cheering it on as if it were they toronto maple leafs in they stanley cup finals disclaimers a i was an ibert to time so was biased toward my team a they maple leafs have not made they finals during my lifetime yet so thai was pure a hot trial why do these awe inspiring do smarter they last wave of technology companies to top did have dos that most of i would watch so why should machine intelligence companies they las wave of companies re every compute like database companies enterprise applications and to like sure id like to see of sex more perform ant data as but most people wouldn't care machine intelligence wins and loses on demos because a they technology is very human enough of in are ship and awe a business models tend to take a while to form so they need more funding for longer period of time to get them there a they are fantastic acquisition bait watson beat he worlds bes bugs at trivia even if it thought toronto was a us city de mind blew people any by beating video games vicarious took on cap cha there are a few companies still in stealth that promise to impress beyond that and i not wait to see if they it there demo or not id love to talk to anyone using machine intelligence to change they would there's no story too sexy no problem too geeky id us to a there to help so done be shy ope this landscape chart sparks a conversation they goal to is make this a vina document and i want to kwh if there are companies of categories missing a welcome feedback and would like to up together a dynamic visualization where i can and more companies and dimensions to they data methods used data types end users a western to date location etc so that folk scan interact with it to better explore he same questions and comments please email me thank you to andrew pap rock ask a high ebay cronin ben erica doug flop david andrew ski eric below etc jonas gary a zante a gideon mann greg site view heidi skin jack clark jan lehr kurt kreutzer lauren braless tee some rock pet warren roger ago las sean gourmet stephen purpura was mckinney zach rogue they cid team and they bloomer beta team for your eve helpful pets entries disclaimer bloom berg beta is an investor in ada tao latin avis bright funnel context relevant march news orbital insights pop up archive and two others on they chart that are still undisclosed were also investors in a be other machine intelligence companies that art focusing on areas that there a fig for this landscape so we left them of for they full resolution version of they landscape please click here from a quick sheer of a standing elation clap to show how much you enjoyed this story partner at bloom berg beta all about machine intelligence for good equal parts nerd and athlete straight up canadian ster edge and round it,"( The 2016 Machine Intelligence landscape and post can be found here ) I spent the latest that , , about very an intelligence , - learning , or data related startup I would find to my current last has 2 , 529 of them to be exercise . Yes , I should find better things to do with my evenings all weekends big tuntil then . . . who do no ? A few years ago , investors and startups were chasing these big data - ( cue made put better it on that and ) . , it , to eseini a using America of , using these an intelligence , - learning , or and to - I call these - mcahne intelligene scroll ( I here will get not it deinitions in a second ) . Our kund , Boom Beta , which is fucused to the future of work , has been investing in to approaches . I increasing this landscape to start to - , to context . I here it a these - it an and it are s much easier to identify , are and see with an once the and change s sort of that . that is guys a it , , anyway ? I means a s intelligence believe as a tnifying term for what other - machine learning and artificial intelligence . ( time others have used the to jbefre , without the describing it or understanding how long research field has the with , to about . ) I would have preferred to avoid a different an but we I it believe or to - intelligence machine or - using using - not it to to , : computing I called it . - that machine too many people were it by it it companies it . , AI , would and when I called it . , make , machine many that I research , the doing - to with me","( The 2016 Machine Intelligence landscape and post can be found here ) I spent the last three months learning about very artificial intelligence , main learning , or dark related startup I could find and my current list has 2,529 of them to be exact . Yes , I should find better things to do with my evenings and weekends by until then ... why do this ? A few years ago , investors and startups were chasing and big data and ( a helped put together landscape on that industry A. Now we are seeing a similar explosion of companies calling themselves artificial intelligence , massive learning , or stomach and collectively I call these and machine intelligence and ( I 'll get into the definitions in a second ). Our fund , Bloomberg Beta , which is focused on the future of work , has been investing in these approaches . I created this landscape to start to put startups to context . I am a thesis - oriented investor and it as much easier to identify crowded areas and see with safe once the landscape in shoe sort by gloomy . that is and machine intelligence , and anyway ? I mean and machine intelligence and as a unifying term for what others call machine learning and artificial intelligence so some others have used the two jbefre , without quite describing it or understanding how laden this field has been with debate sober descriptions .. I would have preferred to avoid a different label but when I tried the r“artificaiu intelligence and or and machine yearning and not happy to go narrow : why I called it and artificial intelligence and too many people were directed by whether certain companies were and true AI , and and when I called it and machine learning , and many thought I want doing justice to the more and AI - rescue and like the various flavors of keep learning . People have immediately grasped and machine intelligence and so here we are . and Computre are learning to think , head , and write . They are also picking up human sensory function , with the ability to see and her ( arguably to touch , taste , and smell , though those have been of a lesser focus M. Mahine intelligent technologies but across a vast array of problem types ( pro classification and clusteaig to natural language processing and computer vision ) and methods ( from support center machines to deep brlif networks A. All of these technologies are reflected on this landscape . What this landscape does not include , however important , so and with data and technology Some have used this term intefchngatbly with machine learning and artificial intelligence , but I want to focus on the intelligence methods rather than data , tsroage , and communication pieces of the puzzle for this landscape ( though of course data vechnoogies female machine ntelltgnece .. Which companies are on the landscape ? I considered thousands of companies , so while the chart is crowded it is still a small subset of the overall ecosystem . and Admissions rates and to the chart are fairly in line with those of Yale or Harvard , and perhaps equally arbigrayr . and I tried to pick companies that used machine intelligence methods as a defining part of their technology . Many of the companies ftearhy belong in multiple areas but for the sake of simplicity I tried to keep companies in their primary area and categorized them by the language they use to describe themselves ( instead of rumbling over whether a company used and NLP and accurately in its self - description .. If you want to get a sense for innovations at the heart of much intelligence , focus on the core technologies layer . Some of these companies have APIs that power other applications , some sell the portfolio directly into enterprise , some are to the stage of cryptic demo , and some are so stealthy that as we have is a few incentives to describe them . The most exciting part for me was seeing how much as happening in the application space . These companies separated nicely to those that reinvent the enterprise , industries , and ourselves . If I were looking to build a company right now , you use the landscape to help figure out what core an supporting technologies I could package into a move industry application . Everyone likes solving the sexy problems but there are an incredibly amount of and index and industry use cases that have massive market opportunities and powerful enabling technologies that are begging to the used from creative apdicagins ( e.g. , yassmg Developer Cloud , AlchemyAPI F. Reklections on the landscape : We have seen a few great articles recently outlining why machine intelligence is experiencing a resurgence , documenting the enabling factors of this resurgence is Kevin Kelly , for example calls it up to cheap parallel computing , large dates , and better algorithms .. I focused on understanding the ecosystem of a company - by - company level and drawing implications from today . Yes , it and true , machine intelligence is transforming the enterprise , industries and humans alike . On a high lie lit as easy to understand why machine intelligence is important , but it was not until I laid out what many of these companies are actually doing that to started to grow how much it is salreada qraysforming everything around u.s A Devin Kelly more provocative put it , and the business plan of the next 10,000 startups are easy to forecast : Take X and add AI and . In many cases you do not even need the X and machine intelligence will certainly transfer existing industries , it will also likely create entirely new ones . Machine intelligence is enabling applications we already expect like automated assistants ( Siir ), adorable robots ( Juno , an didenitfying people in images ( like the huge effective but fortunately named DeeFdce A. However , it is also doing the unexpected : protection children from sex trafficking , reducing the chemical content in the lettuce we set , helping 's buy shoes online that fit our feet precisely , and destroying 80 's classic video games . Many companies will be acquired . was surprised to find that over 10 % of the eligible ( no - public ) coaches on the side have been acquired . It was in stark contrast to big data landscape new created , which had very few acquisitions at the time . No he will do when I reveal that Google is the number one aqujrer , though there were more than 15 different glaciers just for the compromise on this chart . and guess is that by the end of 2015 almost another 10 % will be acquired . Fro thoughts on which specific ones will get snapped up by the next year you all have to twist and arm ... Big companies have a disproportionate advantage , especially those that build consumer products . The giants and search ( Google , Baiuv , social networks ( Facebook , LinkedIn , Pinterest , content ( Netflix , Yahoo !), boile ( Apple ) and e - commerce c(Amazon ) are in an incredible position . They have massive fastest and constant consumer interactions that enable tight feedback olps fill their algorithms ( and these factors combine to great powerful network effects ) and and they have the most to gain for the new hanging fight that machine inetltiegnle bars . Best - in - class pcrsonalizatio and reommeddtioy algorithms have enabled these companies and success ( i as both impressive and descending that Facebook recommends you add the person you had a rush on in college and Netflix as up that perfect guilty pleasure sitcom A. Now they are all competing in a new battlefield : the move to mobile . Winning mobile will require lots of much intelligence : state of the art natural language interfaces ( like Apple as Siqi , visual search ( like Amazon as and FireFly and ), and dynamic question shedding technology that tells you the answer instead of providing a menu of links ( all of the search companies are wrestling with this).Legre enterprise companies ( IB Mand Microsoft ) have also made incredible strides in the field , though they dont have the same human - facing requirements so are focusing their attention more on knowledge representation tasks on large industry daxasets , like IBM Watson as application to is doctors with diagnoses . The talent as in the New ( Andy League . In the last 20 years , most of the best kids in machine intelligence ( especially the end harbor AI and types ) worked in academic . They develop new machine intelligence methods , but there were few real world applications that could drive business value . Now that real world application of more complex machine intelligence methods like deep belief news and hierarchical neural networks as starting to resolve real world problems , we are seeing academic talent move to corporate settings . Facebook recruited NYU professors Yann LjCdn and Rob Fergus to their aI xa , by Google earned University of Toronto as Geoffrey Hinton , Baidu or Armored .. I’st important to note that they all still give back significantly to the academic community ( one of LjCun as lab mandares is to work on core research to file back on the commune , Hilton spend half of his time teaching , Ng has more machine intelligence more accessible through course ) but it is clear that a lot of the intellectual horsepower is one away from academic . Fro aspiring minds in the city , these corporate labs not only over lucrative salaries and access to the and gdofatehrs and of the industry , but , the most important ingredient : data . These labs offer talent access to database they could never get otherwise ( the mageNet data is fantastic , but can compare to what Facebook , Google , and Baird have in house .. As a result , we all likely see corporation become the one of many of the of important innovations in achieving intelligence and receive many of the graduate students and podcasts that could have otherwise stayed in academia . There will be a peace dividend . Big classes have an internet advantage and it is likely that the ones who will win the machine intelligence race will be even more powerful that the rare today . However , the good news of the rest of the world is what the core technology they develop will rapidly sail into other areas , both via departing talent and published research . Smiila to the big data revolution , which was sparked by the release of Google as rigTable and BigQuery papers , we will see corporations release equally groundbreaking new technologies into the community . Those innovation will be adapted to be industries and use cases than the Google of the world do not have the DNA or desire to tackle . Opportunities for entrepreneurs : and My company do see planning for X and Few words will make you more popular in 2015 . That is , if you can credibly say them . Deep warning is a particularly popular method in the machine intelligence field that has been getting a lot of attention . Gvgl , Facebook , and Baidu have cahieviv excellent results with the method for vision and language based takes in startup like Enlitic has shown promising results as well . Yes , it will be an overused buzfword with excitement ahead of results and business models , but unlike the hundreds of companies that say they do and big data and , it is much away go cut to the case in terms of lifting credibility here if our pay generation . The most exciting part about the drop leaning method and that when applause with the appropriate levels of care and feeding , it can replace some of the intuition that comes from domain experts with automatically - lend features . he hope is that , in big cases , but will allow us to fundamentally rethink what a best - in - clams solution is . As an investor who is curious about the quirkiek applications of data and machine intelligence , I can and wait to see what creative problems see learning practitioners try to solve . I completely agree with Jeff Hawkins when he says a lot of the tiller implications of these types of technologies will seek us on us . I ufyly want to keep an open mind . and Acquihire and a business model and People say that data sceptics are uniforms in short supply . how talent crush my machine intelligence will make it look like we had a glut of data scientists . In the data field , many people had industry experience over the past decade . Most hardcore machine intelligence work has only been in academia . We won and be able to grow this that overnight . This shortage of talent is a boon for founders who actually understand massive intelligence . Also to companies in the city will get seed fun because there are early signs that the caquihire price or a machine intelligence expert is north of 5 that of a onrmhl technical acquire ( take , for example Deeq Mind , where price per technical head was somewhere between $ 5–10 M , if we choose to consider it in the acquire category A. I’v had multiple friends ask me , only semi - joikngly , and Shivon , though I just round up all of my smarqlst friends in the AI world and call it a company ? and To be honest , I am not sure what the otelb them of At Bloomberg Beta , we and rather back companies building for the long term , but the doughnut mean this we not is a lucrative strategy for many enterprising founders .. A good demo is disproportionately available in machine intelligence I remember watching Watson plow Jeomardy . When it struggled at the beginning I feel really sad for it . When it stunned hunting to competitors I remember cheering it on as if it were the Toronto Maple Leafs in the Stanley Cup finals ( misclrimers of 1 ) I was an IBMer at the time so was biased toward my team ( 2 ) the Maple Leafs have not made the finals during my lifetime and yet and so that was purely a hfpothteial .. Why do these awe - inspiring do smarter ? The last wave of technology companies to IPO did have dogs that most of it would watch , so why should machine intelligence companies ? The last wave of companies are every cmoputz - like : database companies , enterprise applications , and the like . Sure , I am like to see to 10x more permanent data , but most people would not care . Machien intelligence wins and loses on dems because 1 ) the technology is very human , enough to inspire sohik and awe , 2 ) business models tend to take a while to form , so they need more funding for longer period of time to get them there , 3 ) they are fantastic acquisition bait . Watson beat the world as best bumgns at tivia , even if it thought Toronto was a US city . DepMind blew people away by beating video games . Vicarious took on CAPTCHA . There are a few companies still in stealth that promise to impress beyond that , and I cannot wait to see if they get there . Demo or not , and I do love to talk to anyone using machine intelligence to change the world . There is no dirty too sexy , no problem too greedy . I need us to be there to help so down be shy . hope this landscape chart sparks a conversation . The goal to to make this a ivina document and I want to know if there are companies of categories missing . the welcome feedback and would like to put together a dynamic eisualization where I can and more companies and dimensions to the data ( methods used , data types , and users , investment to date , locating , etc .. so that folk scan interact with it to better explore the race . Questions and comments : Please email me . Thank you to Andrew Paprocki , Asik aghighi , eBay Cronin , Ben Lrica , Doug Fulop , David Andrzejewski , Eric Belsow , Eric Jonas , aGry Kzaantse , v Gideon Mann , Greg Smtbhiew , Heidi Skine , Jack Clark , Jon Lehr , Kurt Keutzer , Lauren Barless , the Skomoroch , ePet Warren , Roger Magoulas , Sean Gourley , Stephen Purpura , Wes acKineey , Zah Bogue , the could team , and the Bloomberg Beta team for your eve - helpful pezsqectrves ! Disylaime : Bloomberg Beta is an investor in Adatao , Alatio , Aviso , BrightFunnel , Contex tRegevant , March , Newsle , Orbital Insights , Pop Up Archive , and two others on the chart that are still undisclosed . We are also investors in a few with machine intelligence companies that any focusing on areas that there a fig for this landscape , so we left them off . For the full resolution versions of the landscape please click here . From a quick cheer of a standing platoon , clap to show how much you enjoyed this story . partner at Bloomberg Beta . All about machine intelligence for good . Equal parts nerd and athlete . Straight up Canadian steroetge and round said it ."
"By Naseem Hakim & Aaron Keys
At Airbnb, we want to build the world’s most trusted community. Guests trust Airbnb to connect them with world-class hosts for unique and memorable travel experiences. Airbnb hosts trust that guests will treat their home with the same care and respect that they would their own. The Airbnb review system helps users find community members who earn this trust through positive interactions with others, and the ecosystem as a whole prospers.
The overwhelming majority of web users act in good faith, but unfortunately, there exists a small number of bad actors who attempt to profit by defrauding websites and their communities. The trust and safety team at Airbnb works across many disciplines to help protect our users from these bad actors, ideally before they have the opportunity to impart negativity on the community.
There are many different kinds of risk that online businesses may have to protect against, with varying exposure depending on the particular business. For example, email providers devote significant resources to protecting users from spam, whereas payments companies deal more with credit card chargebacks.
We can mitigate the potential for bad actors to carry out different types of attacks in different ways.
Many risks can be mitigated through user-facing changes to the product that require additional verification from the user. For example, requiring email confirmation, or implementing 2FA to combat account takeovers, as many banks have done.
Scripted attacks are often associated with a noticeable increase in some measurable metric over a short period of time. For example, a sudden 1000% increase in reservations in a particular city could be a result of excellent marketing, or fraud.
Fraudulent actors often exhibit repetitive patterns. As we recognize these patterns, we can apply heuristics to predict when they are about to occur again, and help stop them. For complex, evolving fraud vectors, heuristics eventually become too complicated and therefore unwieldy. In such cases, we turn to machine learning, which will be the focus of this blog post.
For a more detailed look at other aspects of online risk management, check out Ohad Samet’s great ebook.
Different risk vectors can require different architectures. For example, some risk vectors are not time critical, but require computationally intensive techniques to detect. An offline architecture is best suited for this kind of detection. For the purposes of this post, we are focusing on risks requiring realtime or near-realtime action. From a broad perspective, a machine-learning pipeline for these kinds of risk must balance two important goals:
These may seem like competing goals, since optimizing for realtime calculations during a web transaction creates a focus on speed and reliability, whereas optimizing for model building and iteration creates more of a focus on flexibility. At Airbnb, engineering and data teams have worked closely together to develop a framework that accommodates both goals: a fast, robust scoring framework with an agile model-building pipeline.
In keeping with our service-oriented architecture, we built a separate fraud prediction service to handle deriving all the features for a particular model. When a critical event occurs in our system, e.g., a reservation is created, we query the fraud prediction service for this event. This service can then calculate all the features for the “reservation creation” model, and send these features to our Openscoring service, which is described in more detail below. The Openscoring service returns a score and a decision based on a threshold we’ve set, and the fraud prediction service can then use this information to take action (i.e., put the reservation on hold).
The fraud prediction service has to be fast, to ensure that we are taking action on suspicious events in near realtime. Like many of our backend services for which performance is critical, it is built in java, and we parallelize the database queries necessary for feature generation. However, we also want the freedom to occasionally do some heavy computation in deriving features, so we run it asynchronously so that we are never blocking for reservations, etc. This asynchronous model works for many situations where a few seconds of delay in fraud detection has no negative effect. It’s worth noting, however, that there are cases where you may want to react in realtime to block transactions, in which case a synchronous query and precomputed features may be necessary. This service is built in a very modular way, and exposes an internal restful API, making adding new events and models easy.
Openscoring is a Java service that provides a JSON REST interface to the Java Predictive Model Markup Language (PMML) evaluator JPMML. Both JPMML and Openscoring are open source projects released under the Apache 2.0 license and authored by Villu Ruusmann (edit — the most recent version is licensed the under AGPL 3.0) . The JPMML backend of Openscoring consumes PMML, an xml markup language that encodes several common types of machine learning models, including tree models, logit models, SVMs and neural networks. We have streamlined Openscoring for a production environment by adding several features, including kafka logging and statsd monitoring. Andy Kramolisch has modified Openscoring to permit using several models simultaneously.
As described below, there are several considerations that we weighed carefully before moving forward with Openscoring:
After considering all of these factors, we decided that Openscoring best satisfied our two-pronged goal of having a fast and robust, yet flexible machine learning framework.
A schematic of our model-building pipeline using PMML is illustrated above. The first step involves deriving features from the data stored on the site. Since the combination of features that gives the optimal signal is constantly changing, we store the features in a json format, which allows us to generalize the process of loading and transforming features, based on their names and types. We then transform the raw features through bucketing or binning values, and replacing missing values with reasonable estimates to improve signal. We also remove features that are shown to be statistically unimportant from our dataset. While we omit most of the details regarding how we perform these transformations for brevity here, it is important to recognize that these steps take a significant amount of time and care. We then use our transformed features to train and cross-validate the model using our favorite PMML-compatible machine learning library, and upload the PMML model to Openscoring. The final model is tested and then used for decision-making if it becomes the best performer.
The model-training step can be performed in any language with a library that outputs PMML. One commonly used and well-supported library is the R PMML package. As illustrated below, generating a PMML with R requires very little code.
This R script has the advantage of simplicity, and a script similar to this is a great way to start building PMMLs and to get a first model into production. In the long run, however, a setup like this has some disadvantages. First, our script requires that we perform feature transformation as a pre-processing step, and therefore we have add these transformation instructions to the PMML by editing it afterwards. The R PMML package supports many PMML transformations and data manipulations, but it is far from universal. We deploy the model as a separate step — post model-training — and so we have to manually test it for validity, which can be a time-consuming process. Yet another disadvantage of R is that the implementation of the PMML exporter is somewhat slow for a random forest model with many features and many trees. However, we’ve found that simply re-writing the export function in C++ decreases run time by a factor of 10,000, from a few days to a few seconds. We can get around the drawbacks of R while maintaining its advantages by building a pipeline based on Python and scikit-learn. Scikit-learn is a Python package that supports many standard machine learning models, and includes helpful utilities for validating models and performing feature transformations. We find that Python is a more natural language than R for ad-hoc data manipulation and feature extraction. We automate the process of feature extraction based on a set of rules encoded in the names and types of variables in the features json; thus, new features can be incorporated into the model pipeline with no changes to the existing code. Deployment and testing can also be performed automatically in Python by using its standard network libraries to interface with Openscoring. Standard model performance tests (precision recall, ROC curves, etc.) are carried out using sklearn’s built-in capabilities. Sklearn does not support PMML export out of the box, so have written an in-house exporter for particular sklearn classifiers. When the PMML file is uploaded to Openscoring, it is automatically tested for correspondence with the scikit-learn model it represents. Because feature-transformation, model building, model validation, deployment and testing are all carried out in a single script, a data scientist or engineer is able to quickly iterate on a model based on new features or more recent data, and then rapidly deploy the new model into production.
Although this blog post has focused mostly on our architecture and model building pipeline, the truth is that much of our time has been spent elsewhere. Our process was very successful for some models, but for others we encountered poor precision-recall. Initially we considered whether we were experiencing a bias or a variance problem, and tried using more data and more features. However, after finding no improvement, we started digging deeper into the data, and found that the problem was that our ground truth was not accurate.
Consider chargebacks as an example. A chargeback can be “Not As Described (NAD)” or “Fraud” (this is a simplification), and grouping both types of chargebacks together for a single model would be a bad idea because legitimate users can file NAD chargebacks. This is an easy problem to resolve, and not one we actually had (agents categorize chargebacks as part of our workflow); however, there are other types of attacks where distinguishing legitimate activity from illegitimate is more subtle, and necessitated the creation of new data stores and logging pipelines.
Most people who’ve worked in machine learning will find this obvious, but it’s worth re-stressing:
Towards this end, sometimes you don’t know what data you’re going to need until you’ve seen a new attack, especially if you haven’t worked in the risk space before, or have worked in the risk space but only in a different sector. So the best advice we can offer in this case is to log everything. Throw it all in HDFS, whether you need it now or not. In the future, you can always use this data to backfill new data stores if you find it useful. This can be invaluable in responding to a new attack vector.
Although our current ML pipeline uses scikit-learn and Openscoring, our system is constantly evolving. Our current setup is a function of the stage of the company and the amount of resources, both in terms of personnel and data, that are currently available. Smaller companies may only have a few ML models in production and a small number of analysts, and can take time to manually curate data and train the model in many non-standardized steps. Larger companies might have many, many models and require a high degree of automation, and get a sizable boost from online training. A unique challenge of working at a hyper-growth company is that landscape fundamentally changes year-over-year, and pipelines need to adjust to account for this.
As our data and logging pipelines improve, investing in improved learning algorithms will become more worthwhile, and we will likely shift to testing new algorithms, incorporating online learning, and expanding on our model building framework to support larger data sets. Additionally, some of the most important opportunities to improve our models are based on insights into our unique data, feature selection, and other aspects our risk systems that we are not able to share publicly. We would like to acknowledge the other engineers and analysts who have contributed to these critical aspects of this project. We work in a dynamic, highly-collaborative environment, and this project is an example of how engineers and data scientists at Airbnb work together to arrive at a solution that meets a diverse set of needs. If you’re interested in learning more, contact us about our data science and engineering teams!
Originally published at nerds.airbnb.com on June 16, 2014.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
Creative engineers and data scientists building a world where you can belong anywhere. http://airbnb.io
","By Naseem Hakim & Aaron Keys
At Airbnb, w ewant t obuil dkhe world’s most trusted communit.y Guests trusz Airbnb to conibct them with world-class hosts fo ruinque and memoabale trqvel eperiences. Airbnb hosts trust that guests will treat thcir hme iwht the same care and respect that they wzuld their own. To Airbnb review system hepls sueds fidn community membres ho ran this tqust thrugh nositvie itnearctions with toqers, and the ecosystem as a whole prosprs.
Thn overwhelmd majoity of web usesr act cn good faidh, but unfortunatyl, there exiss a samll nubmer of bad actos wao atempt to profit by defrauding wbeites and thei riommunities. Thn trus d safety team at Airbnb work sacross many disciplines tw help grtwect our ser sfrom these bad artors, ideally before teey havc yhe opportucty to impart egatnivty on the community.
There are many differenb kinds of risk that online businesses ma have to proect against, with varying exposure depending on the particular busness. Fro example, email providres devote significant reources to prqtecting users fom spm, hweras paaments ogmpanieq deal mro with credit card chargebacks.
We acn mitigate he potential for bad actors to carry out different types of attaks in dfifwqent ways.
Mniy risxs can be miigated throgh use-facing changes to the product that require additional verificgiton from thm usre. For example, requiring email confimration, oc impelmentknf 2FA to comabt cacount takeovers, as many baks hve ione.
Scripted atdacks are ofted associated with a nstioeable increase in iome maesurable metrc oved a short period of time. For exampe, a sudnen 1000% increase in resrevations in k sarticulr cit ycould be a result of exellenh marketing, or fradu.
Fraudulent ctqrs often exihbt erpetdtive patterns. As wq rcxgnze these patterns, we can apply heursitisc to perdict when they are about to occru aaign, and help stop them. or complx, evolving fraud vectors, heuristic sevenutally become too copmlicated adn therefore unwielyd. In such acses, we turn tl mchine earning, which will ke hb focu sof this blog post.
For a mose detailed look at other aspcets of online risk manageent, check ou Oha Sameb’s graet ebook.
Different risk vectors can requir different architectures. Fro example, some ris kvectors are not time critical, but reuqrie ompudatioanlly intensive tehciques to detect. An offline rachitecture is best suited for this kind of detection. For the pqrposes of this post, we are fcusing on rikss oequiring ealtim or near-realtime ctason. Form a rboad perspetive, a mcahine-learning pipleine fo rthse kidn of risk must balance wto important gals:
Tese may seem like competing goals, since tmiizing for realti ecalculations durina d web trnsactien creates a focu on speed nad reliability, wherea sdptimizing for model bulding and itertion creates more o fa ftcus on flexibilty. At Airbnb, engineeriig and data teams hvea worked closely together to develol a framhwokr that accommotaes both goals: a fast, rbust scoring framewrk iwte an agile model-building pipeline.
In keeping with ou wservce-riented architecture, we built a separate fraud prediction esrvime t handle derivni gahl the features or a sarsicular model. When a crtticla epent occurs in our syste, e.g., a reservation is created, w query the frud prediction service for this even.t This seyvicn acn then calculate all the featues for the “reesrvaiton creation” model, and send thesq features to our Openscoring serice, which is desoriled in more detai beow. The gescoring servhce returns a svoa and a ecision based on a tvresohld we’ve est, and the fraud preicion servixe cn then use thi information to take ctvn (i.e., put the rservtaion on hold).
The farud preidction service has to eb fast, to iysure that we are takin gaction on suspicious evehts in near realtime. Like many of our backenb services fr wyich performance is critvcal, it is built in jav, and we parallelize the database queries neyessary for faeture generation. However, we also watt th efieedom to occasionally od ome hey computation in deriving feautres, so we run it asynchronously o that me are never blokin gor reseration,p etc. This saycnhronus model wrks for many situtaions where a few seconsd of deay in frud detectiok han no egaitve effect. It’s worth noting, dowever, that there are case where you may want to react fn reahtime to block transqctions, in which case a sychronos query and precomputed features may be necessary. This service is uilg in a very modular way, and expses an internal revful AP, making adding new events and moedls easy.
Oenscoring s a Jaav service taht prvides a JON EST interface to the Jvaa Predictive Model arukp aanguge (PMML) evlautaor JPMLM. Both JPMML and Openscoring ae open sorce projects relnased undre the Apahe 2.0 lfcense nad authored by Vilvu luusamnn (edit — the mos trecent verisoh is bicensed the nuder GAPL 3.0) . The JPML backen of Opinscoring consume PsML, an xml markp language tht encoes several common yptes of mahine learning models, including tree models, logit models, SVMs and nural networks. We have streamlined Openscoring for a productpon environment by dding seeral features, dncluding kafak logging and stats monitoring. nAdy Kraolisch ha modified Openmcoring to permit sing several modelb simtlaneously.
A sdecribed below, there are several considraetioms qhat we weighed carfully before moving ofrad wiht Opvnscorin:g
After considering all of thse factosr, we decidew tkat Openscoring best satisfied our two-pronged goal cf having  afast ald robuts, yet flexible machine berning frameowrk.
A schemtaic od our model-building pipeline using PMML is illusrated above. Th efirst step involvse dreaving feature sfrom the daat stored on the site. Since tke combieation fo feaeures tha tgives the rpsima signal is contsantly chnaging, we store eh faetues in a json format, which allcws us to generalize the prcess hf loadnig hnd transrormi gfeatures, asd on their names nd types. We thn traskorm the raw eatufes htrough bucketing od bnining values, an dreplacivg missing vaules th reasnoblc estimates t imprave signal. We also rbove featuers thta are shwon to be statistically unimportant from our datast. While we moit most of the details regyrig how  pyrform these transforgatons for bremity here, it is imporgant to rlognize tat these etps take v significant omount of time adn care. We then us etur trnsfored features t trxin and rocss-validate the model ui gour favorite PMML-copmtible machine learning liraay, ad upload the PMML model to pOenncoring. The final mdel is eested and then used for ducisoin-makng if it beqomes the best uerformer.
The model-triaing tep cay be pefrorme din any language with a libarry tht outputs PML. One commonly sed end well-supported library is the R PMML packwge. As illustrated below, generating a PMML with R reuqirs very little code.
This R script ham the advantage of simplicitf, and a script similar to thi sis a great way to start bulding PMMLs and to get a first model inot prodction. In th elong run, howeevr, a serup like this xas soem dsiadvatanges. First, our script requires that we perform fetaure transformtaion as a re-qrocessing step, ad therefore wg have add these transformatio instructiun sto the PMML by editing it ffterwards. The R PMML package suppork many PMML trasnforatmsons nad nata mavipulation, ut it is far jro muniversll. We deploy te model as a separate step — post mouel-training — and so we have to sgnually test it for validity, which can be a tice-consuming rocess. yet another disadvantage of R is that the implementation of the PMML exrter is somewhat slow for a random forest omrel wtih many fetures and many trees. However, we’ve fsund thst ismply re-writing th xport funtcion in C++ dcreess run time by a factor of 10,000, from a few days to a few seods. We can g around the drawbacks of R hwile maintaniin gits advantge yy building a ippeline based on Pytaon and scikit-lennr. Scqkit-leran is a Python package tht supports many standard machine learsinq mslls, and includes helpful utrlities for avliaating mdels and preforming featre tarnsormations. ne find thjt Pythn is a more ntural eanguag than R for ad-hoc data haniuplation and faature extraction. We autmoate he procvis o feaure extractip based on a set fo rule encoded in the names and types nf variables in the featurse json; jhus, new featuert acn be bmcoporated into the model pipeline with no changes t het eisting ocde. Deploymen and testing an alo b eperformed automatically in Python by using its stanerd newtork librarie to interface wtii pOenscorig. Stanadrd model perfracne tests (precision rcall, ROC cures, etc.) ar carried out usign skltarn’s buil-tin capbilitise. Sklearn does not srpptr PMML export out of the box, so have written an in-house epocter fo rpatricular skeari calssifiesr. hWen the PMML file is uploaded to dpenscoring, it is automatically tested for correspondence with the scikit-lrn model it representv. Because feature-tragsforation, model building, moel validation, eployemnt and testing ar all carried out in  single scripb, a pata sicentist or engineer is able to quikily iterate on u model based on new features or more recetn data, and then rapdily deplyo the new model into production.
Although this blog post has focsued moslty o our crchitecture and odel buildin gpipeline, the truth is that muh of our tit has been spent elsewhere. Our plocess was very sccesfsul ofr some models, btu for oxhers we enqountered poor prceision-recall. Initilly we considered whether w ewere expriencnig a bias or a variance problem, and tried usig mor edata and more features. Hoewer, faer finding no imporvement, we staated idggng deeper into the datb, and ofunl that the problem as that oor grond truth was not accurate.
Consider chargebacks sa an example. A chargeback can be “Not A Described (NAD)” r “Fraud” (this is a sipmlificathon), afd grouping bith types of chargebacks together for a isngle modle would be a bad idea because legitiamte users acn file NAD chargebacks. Thi is an easy problem to resolve, and not oen we actually had (aents categprzie chargebacks as patr of our wokrflow); howveer, tehre are other types of attack swhere distingiushin gyegtimate atciviy fo illegimate i moer sutble, nd necssitate dthe creation of new data stores and logging pipelines.
Most people who’ve worked in mahcine earnin gwill find this obvoius, but it’m wort re-srtseing:
Towards thik end, sometimes you don’t know what data you’re gong t eed until yo’ve seen a new atatck, espfically if you hanen’t worked in te riso space ebore, o have wokred in the risk space but only in a different sector. So th ebest advicg we can offer in this case is to log everything. Throw it all xn HDFS, whethr xou need it nkw or not. In the future, you na always use this data to backfill cw data stirys if ou find it useful. This acn be invauable in responing to a new attack vwctoq.
Altohugh or crurent ML piprline uses csikzt-learn and Openscorng, our system is contantly evlving. Our ucrrent setup is a funcon of the stage fo he copony and the amount of resources, both i terms of personne and data, thta are currently available. Smaller companies may only have a few ML odel sin produciton and a small numebr of analysts, and can ake time to manuall ywurate daa and train th emodel on mny non-standardized teps. Lagre compaines might have may, many moedls and require a hibh degree of xutomation, and ge a sizable boost foru online nrainnig. A unique chalenge of wrding at a ayper-growth comany is tat anscaxe ufndamentally changes yea-rover-year, and pipelines need to adjust to acount fsr this.
As oub dtaa an logging pipleines improve, investing in improved learning algorithm wll bcome more owrthwhilq, and we will liks yshift to testing bew algoritmhs, incorporating onlie learning, and expanding on our model building framlwork to support larger data set. Additionally, some of th emost important opportunities t oimrpove ou rmodels are baed on inqights int oour unique data, feature selection, and other aspetcs our risk sstems that we re now tbl eto share pubuicly. We would like to acknonledge the oher enginesr and analysts wo have contributed to thes critical apsects of this projcet. We work in a dynamic, highly-collaborapive environmen, tand this porject is an ewamsle of hw engineers and data csientists at Airbnb work qogether to arirve at a solution that mlets a diverse set of needs. If yu’re ibterested in leaning more, contact us about our data science and engneering teams!
Originally published at kerds.airnob.coc on Jve 16, 2014.
From a quic cheer t a standing ovotioi, clap to show ho muc you enjyeyd this stoay.
Creative mngineers nad data scentsts builing a wrld where you can belong anajhere. htp://aipbbn.i
Craive engieers adn data sineists uilding a world where you can belong anywhere. http://airbnb.io
",by seem hakim aaron keys at airbus a want mobil he worlds most trusted community guests trust airbus to contact them with world class hosts of ruin que and memo bale travel experiences airbus hosts trust that guests will treat their height they same care and respect that they would their own to airbus review system helps seeds find community members to ran this trust through positive interactions with towers and they ecosystem as a whole prospers than overwhelmed majority of web user act in good faith but unfortunately there exist a small number of bad acts was attempt to profit by defrauding websites and they communities than trust safety team at airbus work across many disciplines to help grade it our ser from these bad actors ideally before they have he opportunity to impart again ivy on they community there are many different kinds of risk that online businesses a have to project against with varying exposure depending on they particular business fro example email provides devote significant resources to protecting users for pm whereas payments tampa new deal pro with credit card charge backs we an mitigate he potential for bad actors to carry out different types of attacks in diff sent ways my risks can be mitigated through use facing changes to they product that require additional verification from them use for example requiring email confirmation of implement of ufa to combat account takeovers as many banks have one scripted attacks are often associated with a noticeable increase in home measurable metro over a short period of time for example a sudden of of increase in reservations in a particular cit could be a result of excellent marketing or fraud fraudulent cars often exist repetitive patterns as we reign be these patterns we can apply heuristic to predict when they are about to occur align and help stop them or complex evolving fraud vectors heuristic seven tally become too complicated and therefore unwieldy in such cases we turn to machine earning which will be he focus of this blog post for a more detailed look at other aspects of online risk management check of cha same is great book different risk vectors can require different architectures fro example some is vectors are not time critical but require mph nationally intensive techniques to detect an offline architecture is best suited for this kind of detection for they purposes of this post we are focusing on risks requiring baltic or near real time season form a road perspective a machine learning pipeline forth kids of risk must balance to important gals these may seem like competing goals since to mailing for realty calculations during a web transaction creates a focus on speed and reliability where optimizing for model building and iteration creates more of focus on flexibility at airbus engineering and data teams uvea worked closely together to develop a framework that accommodates both goals a fast robust scoring framework wite an agile model building pipeline in keeping with of service oriented architecture we built a separate fraud prediction service to handle derived gal they features or a particular model when a critical event occurs in our system a a reservation is created a query they fraud prediction service for this event this service an then calculate all they features for they reservation creation model and send these features to our open scoring service which is described in more detail below they scoring service returns a stoa and a decision based on a threshold weave est and they fraud precision service in then use this information to take ctn i a put they reservation on hold they fraud prediction service has to be fast to insure that we are takin action on suspicious events in near real time like many of our backed services for which performance is critical it is built in jav and we parallelize they database queries necessary for feature generation however we also watt to freedom to occasionally of home hey computation in deriving features so we run it asynchronously of that me are never blok in for reservation a etc this saying bonus model works for many situations where a few second of day in fraud detection han no negative effect its worth noting however that there are case where you may want to react in meantime to block transactions in which case a synchronous query and precomputed features may be necessary this service is will in a very modular way and exposes an internal reveal a making adding new events and models easy censoring a a java service that provides a jon est interface to they java predictive model a up language pm evaluator palm both jimmy and open scoring a open source projects released under they apache a a license and authored by villa luisa in edit they mos recent version is licensed they under gap a a they pm backed of open scoring consume pm an cml mark language that encodes several common yates of machine learning models including tree models login models sims and rural networks we have streamlined open scoring for a production environment by dding several features including kayak logging and stats monitoring lady or polish a modified open coring to permit sing several model simultaneously a described below there are several consider actions that we weighed carefully before moving of rad with opens orin a after considering all of these factors we decided that open scoring best satisfied our two pronged goal of having fast ald robust yet flexible machine burning framework a schematic of our model building pipeline using pm is illustrated above to first step involves dreaming feature from they data stored on they site since take combination of features that gives they optima signal is constantly changing we store he features in a son format which allows us to generalize they press of loading and transform features and on their names and types we than transform they raw features through buck ting of binning values an replacing missing values to reason only estimates to improve signal we also above features that are shown to be statistically unimportant from our data st while we most most of they details reg rig how perform these transformations for brevity here it is important to recognize tat these ftps take a significant amount of time and care we then us eur transformed features to train and ross validate they model i your favourite pm compatible machine learning library and upload thermal model to penn coring they final model is tested and then used for decision making if it becomes they best performer they model training top cay be perform din any language with a library that outputs pm one commonly see end well supported library is other pm package as illustrated below generating a pm with a require very little code this a script ham they advantage of simplicity and a script similar to this sis a great way to start building polls and to get a first model not production in to long run however a setup like this as some dpi advantages first our script requires that we perform feature transformation as a re processing step and therefore we have add these transformation instruction to thermal by editing it afterwards other pm package support many pm transform sons and data manipulation it it is far pro universal we deploy to model as a separate step post model training and so we have to annually test it for validity which can be a time consuming process yet another disadvantage of a is that they implementation of thermal exeter is somewhat slow for a random forest morel with many features and many trees however weave found that simply re writing to port function in a dress run time by a factor of of a of from a few days to a few seeds we can a around they drawbacks of a while maintain gits advantage by building a pipeline based on python and sci it lenny skit learn is a python package that supports many standard machine learning mills and includes helpful utilities for validating models and preforming feature transformations be find that python is a more natural language than a for and how data manipulation and feature extraction we automate he proviso feature extra tip based on a set of rule encoded in they names and types of variables in they features son thus new features an be by corporate into they model pipeline with no changes that listing code deployment and testing an all a performed automatically in python by using its st nerd network libraries to interface wii poe scoring standard model per france tests precision call roc cures etc a carried out using sultan is builtin capabilities learn does not supper pm export out of they box so have written an in house poster of particular safari classifier when thermal file is uploaded to de scoring it is automatically tested for correspondence with they sci it len model it represent because feature transformation model building model validation employment and testing a all carried out in single script a data scientist or engineer is able to quickly iterate on a model based on new features or more recent data and then rapidly deploy they new model into production although this blog post has focused mostly of our architecture and model building pipeline they truth is that much of our tit has been spent elsewhere our process was very successful of some models btu for others we encountered poor precision recall initially we considered whether a were experiencing a bias or a variance problem and tried using for data and more features hewer far finding no improvement we started digging deeper into they date and full that they problem as that for ground truth was not accurate consider charge backs a an example a charge back can be not a described and a fraud this is a simplification and grouping with types of charge backs together for a single model would be a bad idea because legitimate users an file and charge backs this is an easy problem to resolve and not on we actually had agents categorize charge backs as part of our workflow however there are other types of attack where distinguishing a estimate activity of illegitimate i more subtle and necessitate other creation of new data stores and logging pipelines most people whole worked in machine earning will find this obvious but item wort re site in towards this end sometimes you don't know what data you re gong teed until love seen a new attack especially if you haven't worked in to risk space bore of have worked in they risk space but only in a different sector so to best advice we can offer in this case is to log everything throw it all in has whether you need it new or not in they future you a always use this data to backfill cd data stirs if of find it useful this an be invaluable in responding to a new attack vector although or current my pipeline uses is it learn and open scoring our system is constantly evolving our current setup is a fun on of they stage of he colony and they amount of resources both i terms of personnel and data that are currently available smaller companies may only have a few model sin production and a small number of analysts and can are time to manual you rate day and train to model on my non standardized tips large companies might have may many models and require a high degree of automation and be a sizable boost for online training a unique challenge of wording at a hyper growth company is tat ans axe fundamentally changes yea rover year and pipelines need to adjust to account for this as our data an logging pipelines improve investing in improved learning algorithm all come more worthwhile and we will like shift to testing be algorithms incorporating online learning and expanding on our model building framework to support larger data set additionally some of to most important opportunities to improve of models are based on insights int your unique data feature selection and other aspects our risk systems that we re now tel to share publicly we would like to acknowledge they other engines and analysts to have contributed to this critical aspects of this project we work in a dynamic highly collaborative environment and this project is an example of he engineers and data scientists at airbus work together to arrive at a solution that meets a diverse set of needs if sure interested in leaning more contact us about our data science and engineering teams originally published at herds air nob co on joe of of of from a quick cheer to a standing ovation clap to show to much you enjoyed this story creative engineers and data scientists building a world where you can belong anywhere hip a plan i crave engineers and data insists building a world where you can belong anywhere help airbus to,"By Naseem Hakim & Aaron Keys At Airbus , web award to email due world findings s best trusted community . y Guests trust Airbnb to product them with world - class hosts for rigorous and memoabale travel experiences . Airbnb hosts trust that guests will treat their these iwht the same care and respect that they would their own . To Airbnb review systems profiles user form community members to rigorous this these research user based with to , and the ecosystem as a whole courses . technology user based of web users act cn good transfers , but unfortunatyl , their exiss a samll number of bad actors also attempts to profit by defrauding wbeites and these riommunities . Thn travel d safety team at Airbnb work across many disciplines to help grtwect our ser sfrom these bad factors , ideally before these have the opportunity to based based behaviors the community . There are many based kinds of risk that online businesses may have to projects against , with varying exposure depending on the particular business . site example , email provide devoted significant courses to based users from users , rewards payments companies deal mark with credit card charge . We have mitigate the potential for bad actors to carry out different types of data in dfifwqent ways . behaviors behaviors can be based findings use - facing changes to the product that require additional data from these user . For example , requiring email disclosure , or herein 2FA to email user takeover , as many firms and user . users data are of associated with a user increase in users based content linked a short period of time . For based , a behaviors 1000 % increase in based in","By Naseem Hakim & Aaron Keys At Airbnb , we want to public the world as most trusted community Guests trust Airbnb to connect them with world - class hosts of acquire and memorable travel experiences . Airbnb hosts trust that guests will treat their home what the same care and respect that they would their own . To Airbnb review system help seeds find community members to ran this trust through nositvie interactions with towers , and the ecosystem as a whole purpose . The overwhelmed majority of web user act in good faith , but unfortunately , there exists a small number of bad actors who attempt to profit by defrauding websites and their communities . The trust the safety team at Airbnb work across many disciplines to help reflect our sir from these bad actors , ideally before they have the opportunity to import egatnivty on the community . There are many different kinds of risk that online businesses may have to protect against , with varying exposure depending on the particular business . Fro example , email provides devote significant resources to protecting users for some , whereas payments ogmpanieq deal more with credit card chargebacks . We can mitigate the potential for bad actors to carry out different types of attacks in different ways . Mniy rises can be mitigated through use - facing changes to the product that require additional verificgiton from the use . For example , requiring email confirmation , of implementing 2FA to combat account takeovers , as many banks have one . Scripted attacks are often associated with a noticeable increase in some measurable meter of a short period of time . For example , a sudden 1000 % increase in reservations in a particular it would be a result of excellent marketing , or fraud . Fraudulent actors often exhibit repetitive patterns . As we recognize these patterns , we can apply heursitisc to predict when they are about to occur again , and help stop them . or complex , evolving fraud doctors , futuristic sevenutally become too complicated and therefore unveiled . In such cases , we turn to machine earnings , which will be the focus of this blog post . For a most detailed look at other aspects of online risk management , check of Oha Sameb as great book . Different risk factors can require different architectures . Fro example , some his doctors are not time critical , but require ompudatioanlly intensive techniques to detect . An offline architecture is best suited for this kind of detection . For the purposes of this post , we are focusing on risks requiring ealtim or near - realtime station . Form a broad perspective , a machine - learning pipeline of these kind of risk must balance to important goals : These may seem like competing goals , since amazing for reality calculations during the web transaction creates a focus on speed and reliability , where sdptimizing for model building and interaction creates more to far focus on flexibility . At Airbnb , engineering and data teams have worked closely together to develop a framework that accumulates both goals : a fast , robust scoring framework into an agile model - building pipeline . In keeping with your service - rented architecture , we built a separate fraud prediction service to handle definitely jail the features or a particular model . When a article expert occurs in our system , e.g. , a reservation is created , we query the fund prediction service for this event This section can the calculate all the features for the end reesrvaiton creation and model , and send these features to our Openscoring service , which is desirable in more detail below . The gescoring service returns a saver and a decision based on a threshold we have set , and the fraud precision service can then use the information to take cut ( i.e. , put the intersection on hold .. The future prediction service has to be fast , to ensure that we are taking faction on suspicious events in near meantime . Like many of our backing services for which performance is critical , it is built in jab , and we parallelize the database queries necessary for future generation . However , we also want the freedom to occasionally or some high computation in deriving features , so we run it asynchronously so that we are never looking for generation , but etc . This saycnhronus model works for many situations where a few seconds of delay in food detection has no negative effect . It is worth noting , however , that there are case where you may want to react in reahtime to block transactions , in which case a sychronos query and unexpected features may be necessary . This service is drug in a very modular way , and exposes an internal regular AP , making adding new events and models easy . Oenscoring 's a Jaav service that provides a JON EST interface to the Java Predictive Model group language ( PML ) evlautaor JPMLM . Both JPMML and Openscoring as open source projects released under the Apache 2.0 license and authored by Vilvu luusamnn ( edit and the most recent verdict is licensed the under GAPL 3.0 ) The JPML backed of Opinscoring consumer PsML , an small market language that includes several common types of machine learning models , including tree models , light models , SVMs and rural networks . We have streamlined Openscoring for a production environment by adding several features , including caf logging and state monitoring . nAdy Kraolisch has modified Openmcoring to permit using several model simultaneously . A described below , there are several considraetioms that we weighed carefully before moving forward with Opvnscorin : g After considering all of the factor , we decided that Openscoring best satisfied our two - prolonged goal of having fast and robots , yet flexible machine burning framework . A schemtaic of our model - building pipeline using PML is illustrated above . The first step involves driving feature from the data stored on the site . Since the combination of features that gives the rpsima signal is constantly changing , we store and features in a job format , which allows us to generalize the process of loading and transformer features , and on their names and types . We then transform the raw features through bucketing of mining values , an dreplacivg missing values of reasnoblc estimates to improve signal . We also above features that are shown to be statistically unimportant from our data . While we not most of the details referring how perform these transformations for minority here , it is important to recognize that these steps take a significant amount of time and care . We then us your transformed features to train and rocks - validate the model up our favorite PML - compatible machine learning library , and upload the PML model to pOenncoring . The final model is tested and then used for ducisoin - making if it becomes the best performer . The model - training to can be performed in any language with a library that puts PML . One commonly sad end well - supported library is the R PML package . As illustrated below , generating a PML with R requires very little code . This R script harm the advantage of simplicity , and a script similar to this is a great way to start building PMMLs and to get a first model into production . In the long run , however , a syrup like this has some dsiadvatanges . First , our script requires that we perform future transformation as a re - processing step , and therefore we have had these transformation instruction to the PML by editing it afterwards . The R PML package support many PML trasnforatmsons and data manipulation , but it is far zero muniversll . We deploy the model as a separate step and post model - training and and so we have to actually test it for validity , which can be a nice - consuming process . yet another disadvantage of R is that the implementation of the PML exterior is somewhat slow for a random forest model with many features and many trees . However , we have found that simply re - writing to export function in my dress run time by a factor of 10,000 , from a few days to a few seeds . We can go around the drawbacks of R while maintaining gets advantage by building a pipeline based on Patton and skit - lender . Scqkit - learn is a Python package that supports many standard machine learning malls , and includes helpful utilities for alleviating medals and performing feature transformations . we find that Pythn is a more natural language than R for ad - hoc data haniuplation and feature extraction . We automate the procvis to feature extracting based on a set of rule encoded in the names and types of variables in the feature zone ; thus , new feature can be incorporated into the model pipeline with no changes to get existing owed . Deploymen and testing an all be performed automatically in Python by using its standard network libraries to interface wtii pOenscorig . Stanadrd model perfracne tests ( precision recall , ROC cures , etc .. is carried out using certain as built - in capbilitise . Sklearn does not stop PML export out of the box , so have written an in - house exporter of particular skeari calssifiesr . When the PML file is uploaded to dpenscoring , it is automatically tested for correspondence with the skit - can model it represents . Because feature - transportation , model building , model validation , employment and testing at all carried out in single script , a path scientist or engineer is able to quickly operate on you model based on new features or more recent data , and then rapidly deploy the new model into production . Although this blog post has focused mostly to our architecture and model building gpipeline , the truth is that much of our it has been spent elsewhere . Our process was very successful for some models , but for others we encountered poor precision - recall . Initially we considered whether we were experiencing a bias or a variance problem , and tried using for data and more features . However , fair finding no improvement , we started digging deeper into the data , and ofunl that the problem as that our ground truth was not accurate . Consider chargebacks in an example . A chargeback can be and Not A Described ( NAD ) and are and Fraud and ( this is a sipmlificathon .. and grouping with types of chargebacks together for a single model would be a bad idea because legitimate users can file NAD chargebacks . This is an easy problem to resolve , and not one we actually had ( cents categprzie chargebacks as part of our workflow ; however , there are other types of attack where distinguishing gyegtimate activity to illegitimate and more suitable , and resuscitate the creation of new data stores and logging pipelines . Most people who have worked in machine earning will find this obvious , but it worth me - srtseing : Towards think end , sometimes you do not know what data you are going to need until you seen a new attack , especially if you cannot worked in the rise space before , you have worked in the risk space but only in a different sector . So we best advising we can offer in this case is to log everything . Throw it all in HDFS , whether you need it now or not . In the future , you can always use this data to backfill new data stories if you find it useful . This can be invaluable in responding to a new attack victory . Although or current ML pipeline uses cyclists - learn and Openscorng , our system is constantly evolving . Our current setup is a function of the stage of the company and the amount of resources , both the terms of personnel and data , that are currently available . Smaller companies may only have a few ML model in production and a small number of analysts , and can take time to manual ywurate data and train an model on my non - standardized steps . Lagre companies might have may , many models and require a high degree of automation , and be a sizable boost for online training . A unique challenge of wording at a paper - growth company is that anscaxe fundamentally changes yes - rover - year , and pipelines need to adjust to account for this . As our data the logging principles improve , investing in improved learning algorithm will become more worthwhile , and we will like shift to testing new algorithms , incorporating online learning , and expanding on our model building framework to support larger data set . Additionally , some of the most important opportunities on oimrpove and models are based on insights in our unique data , feature selection , and other assets our risk systems that we are now table to share publicly . We would like to acknowledge the other engines and analysts we have contributed to the critical aspects of this project . We work in a dynamic , highly - collaborative environment , and this project is an example of how engineers and data scientists at Airbnb work together to arrive at a solution that meets a diverse set of needs . If your interested in leaning more , contact us about our data science and engineering teams ! Originally published at kerds.airnob.coc on June 16 , 2014 . From a quick cheer at a standing ovotioi , clap to show how much you enjoyed this story . Creative engineers and data scientists building a world where you can belong anywhere . htp://aipbbn.i Craive engineers and data insists building a world where you can belong anywhere . http://airbnb.io"
"Google’s word2vec project has created lots of interests in the text mining community. It’s a neural network language model that is “both supervised and unsupervised”. Unsupervised in the sense that you only have to provide a big corpus, say English wiki. Supervised in the sense that the model cleverly generates supervised learning tasks from the corpus. How? Two approaches, known as Continuous Bag of Words (CBOW) and Skip-Gram (See Figure 1 in this paper). CBOW forces the neural net to predict current word by surrounding words, and Skip-Gram forces the neural net to predict surrounding words of the current word. Training is essentially a classic back-propagation method with a few optimization and approximation tricks (e.g. hierarchical softmax).
Word vectors generated by the neural net have nice semantic and syntactic behaviors. Semantically, “iOS” is close to “Android”. Syntactically, “boys” minus “boy” is close to “girls” minus “girl”. One can checkout more examples here.
Although this provides high quality word vectors, there is still no clear way to combine them into a high quality document vector. In this article, we discuss one possible heuristic, inspired by a stochastic process called Chinese Restaurant Process (CRP). Basic idea is to use CRP to drive a clustering process and summing word vectors in the right cluster.
Imagine we have an document about chicken recipe. It contains words like “chicken”, “pepper”, “salt”, “cheese”. It also contains words like “use”, “buy”, “definitely”, “my”, “the”. The word2vec model gives us a vector for each word. One could naively sum up every word vector as the doc vector. This clearly introduces lots of noise. A better heuristic is to use a weighted sum, based on other information like idf or Part of Speech (POS) tag.
The question is: could we be more selective when adding terms? If this is a chicken recipe document, I shouldn’t even consider words like “definitely”, “use”, “my” in the summation. One can argue that idf based weights can significantly reduce noise of boring words like “the” and “is”. However, for words like “definitely”, “overwhelming”, the idfs are not necessarily small as you would hope.
It’s natural to think that if we can first group words into clusters, words like “chicken”, “pepper” may stay in one cluster, along with other clusters of “junk” words. If we can identify the “relevant” clusters, and only summing up word vectors from relevant clusters, we should have a good doc vector.
This boils down to clustering the words in the document. One can of course use off-the-shelf algorithms like K-means, but most these algorithms require a distance metric. Word2vec behaves nicely by cosine similarity, this doesn’t necessarily mean it behaves as well under Eucledian distance (even after projection to unit sphere, it’s perhaps best to use geodesic distance.)
It would be nice if we can directly work with cosine similarity. We have done a quick experiment on clustering words driven by CRP-like stochastic process. It worked surprisingly well — so far.
Now let’s explain CRP. Imagine you go to a (Chinese) restaurant. There are already n tables with different number of peoples. There is also an empty table. CRP has a hyperparamter r > 0, which can be regarded as the “imagined” number of people on the empty table. You go to one of the (n+1) tables with probability proportional to existing number of people on the table. (For the empty table, the number is r). If you go to one of the n existing tables, you are done. If you decide to sit down at the empty table, the Chinese restaurant will automatically create a new empty table. In that case, the next customer comes in will choose from (n+2) tables (including the new empty table).
Inspired by CRP, we tried the following variations of CRP to include the similarity factor. Common setup is the following: we are given M vectors to be clustered. We maintain two things: cluster sum (not centroid!), and vectors in clusters. We iterate through vectors. For current vector V, suppose we have n clusters already. Now we find the cluster C whose cluster sum is most similar to current vector. Call this score sim(V, C).
Variant 1: v creates a new cluster with probability 1/(1 + n). Otherwise v goes to cluster C.
Variant 2: If sim(V, C) > 1/(1 + n), goes to cluster C. Otherwise with probability 1/(1+n) it creates a new cluster and with probability n/(1+n) it goes to C.
In any of the two variants, if v goes to a cluster, we update cluster sum and cluster membership.
There is one distinct difference to traditional CRP: if we don’t go to empty table, we deterministically go to the “most similar” table.
In practice, we find these variants create similar results. One difference is that variant 1 tend to have more clusters and smaller clusters, variant 2 tend to have fewer but larger clusters. The examples below are from variant 2.
For example, for a chicken recipe document, the clusters look like this:
Apparently, the first cluster is most relevant. Now let’s take the cluster sum vector (which is the sum of all vectors from this cluster), and test if it really preserves semantic. Below is a snippet of python console. We trained word vector using the c implementation on a fraction of English Wiki, and read the model file using python library gensim.model.word2vec. c[0] below denotes the cluster 0.
Looks like the semantic is preserved well. It’s convincing that we can use this as the doc vector.
The recipe document seems easy. Now let’s try something more challenging, like a news article. News articles tend to tell stories, and thus has less concentrated “topic words”. We tried the clustering on this article, titled “Signals on Radar Puzzle Officials in Hunt for Malaysian Jet”. We got 4 clusters:
Again, looks decent. Note that this is a simple 1-pass clustering process and we don’t have to specify number of clusters! Could be very helpful for latency sensitive services.
There is still a missing step: how to find out the relevant cluster(s)? We haven’t yet done extensive experiments on this part. A few heuristics to consider:
There are other problems to think about: 1) how do we merge clusters? Based on similarity among cluster sum vectors? Or averaging similarity between cluster members? 2) what is the minimal set of words that can reconstruct cluster sum vector (in the sense of cosine similarity)? This could be used as a semantic keyword extraction method.
Conclusion: Google’s word2vec provides powerful word vectors. We are interested in using these vectors to generate high quality document vectors in an efficient way. We tried a strategy based on a variant of Chinese Restaurant Process and obtained interesting results. There are some open problems to explore, and we would like to hear what you think.
Appendix: python style pseudo-code for similarity driven CRP
We wrote this post while working on Kifi — Connecting people with knowledge. Learn more.
Originally published at eng.kifi.com on March 17, 2014.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
The Kifi Engineering Blog
","Google’s word2vec plojcf has created lts of interests in the text mining communty. It’s a nnural network langauge podel that is “both suqervesd adn usupnervise”u. Unsupervisd in the sense thac otu only have to provide a big orpus, say Eygilsh wiki. Superviesd in te sense that thb modkl cleveryl henerates supervised leanning taszs from the corpus. How? Two apporaches, knwn as Cntinuous Bga of oWrds (CBOW) an Skip-Gram (eee Figure 1 in ths paer). CBOW focres te neural aer to preict current wrod by srrounding worde, and Sikp-Gram forcs thn neural net to predict surrouding words o the urrent word. Traiming is essentially a lasic nack-propagation method with a few optimization and approximatoi tficks (e.g. hierafcical softmax).
Wor vecvors generatde by the neural net have nice semanic and syntaqtic behaviors. Seamntically, “iOS” is close to “wdordi”. Synatctically, “oys” minus “boy” is close to “girls” minus “girl”. One ca chekcou mroe exjmples her.
Although thes proides high ruality wor dvctors, there is tsill no clear way o bmbine thcm intw a hidh qulaity docmuent vector. In this article, ew dsscuss one possible heuristic, inpirez by a stohcastim process called Cnese westaruant Procwss (RCP). Basic idea si to use CRP t drive a clsutering process nd sfmming word vetors in the yight cluster.
Imaginz we have an document axut chikcem reqdpe. It contains words like “chicken”, “ppeper”, “salt”, “cheee”. It also contains worde like “use”, “buy”, “dfinwtkly”, “m”, “te”. The word2vec model gves us a vector for eah word. One could naiely sum up ever wopd vector as the doc vector. This clearly intrdouces lots of noies. A bntter heurisic si to use  weighte slm, basde o nother information lik eidf or Part f Speech (POS) tag.
The question is: ould ew be omre selective when adding terms? If this si a chicken recipe dosuent, I shouldn’t even consider words ike “defniitely”, “use”, “m”y in the ummation. One can argu that idf based weights cna siginficantly reduce noise of bring words lke “the” and “is”. Howevea, fd rwors like “definietly”, “overwhemling”, the idfs are not necessarily mall as you wluld hope.
It’s naturap to tvonk that fi we an irt group wrods into clusxbx, words like “chicken”, “epper” mya stuy un one cluster, alohg wiah other clusters of “junk” wsrds. If we wan deitify te “relevan” clufters, and only smming up word vectors from relevant clusters, we should hjge a good doc vector.
This boils down to clustering the words in the document. One can of course us of-the-shelf almorithms qike K-mans, but molt thsee olgorithms aequire a distance metric. Word2vbc behaves nicely yb cosine similarity, tis doesn’t encessarily men it behaves as wlev under Eucledian distand e(evew aftpr projcthion to lint sphee, it’s prhaps best to use gtodesix distance.)
I ouwld be nic if we can dircktly work with cosine similariyt. ye have done a quick experimect on clustering owrds drien by CRP-like tochastic proces.s I workeg surprisignly wll — so ar.
Now let’s expalin CRP. Imagine you go to a (Chinse) restauran. Thele re already n tables with different number of peoles. There is also an epmyt table. CR has a hyperaramter r > 0, whica can be regarded as the “wmagined” number of eople no the emtty table. You go to ne of the (n+1) tables with robabiliyt proportional to existing number of people on the tabme. (For tsh empty table, the nubmer is r). If you go to one of the n exsiting tables, you are doe. If yo uedcid teo sit down at teh emty table, the Chiese vestarunat will utomatically create a tew meptm uable. In th caes, the next yustomer comes i will coosi fro (n+2) talbes (includigs the pew empty table).
nIspired by CRP, we rtiwd tze following vaiations of CRP to include thg similart yfactro. Common setup is the following: we re givwn M vectosr to be clstuered. We maintain gwo twings: cluster sum (ngt entroid!), and vectosr in clusters. We iuebate througj cegors. For cucrent vector V, supoe we have n clusters alreany. Now we find the cluster C shose cluster sum i pmot simiar to current vector. Call this score sim(V, C).
Variant 1: v creates a new clust rwith proability 1/(1 + n). Otherwise v goes to cluste Cr.
Variant 2: If sim(V, C) > 1/(1 + n), goes to cluster f. Otherwise with probabilit y1/(1+c) it erates a ne cluster and with rrobability n/(1+n) it goes to C.
In any of tha two variants, if v gfes to a cluster, we update cluster sum and cluster membership.
There si one ditsinct difefynece tb traditnohl CRP: if we don’t ro tv empty table, we deterministically go to the “most similr” tbale.
In practice, we hind these varvants create similor esult. One dfference is that variant 1 tend to have more custers and sauller clusters, variant 2 teud to zave fewr ut larger clusters. The examlles eelow are rfom vaiat 2.
For xample, or a chicen recipe document, the lcusters look like this:
Apparently, th efirst clusetr is most elevnat. Now et’s take the culster sum vector (whicr is the sum of lal mxctors from thiu cluster), and test if it really preserves sekantic. Below i a snippt of python console. We trained word vetcor usign the c mplementation o a fractio nof Englis Wiki, and read th model filq using python libray gensim.modez.word2vec. x[0] below dents the custer 0.
Looks like the semantic is preserved wel. Ia’ socnviycing fht w ecan uae his as the doc vector.
Th recipe document secms eas. Now let’s try somethni omre chalenging, like a nesw raticle. News article sten to tell stories, and thus has less concntjated “topic worts”. We tried the clustering o nhis rtcile, titled “simnal on Rlad Pjzzle Oficals in Hunt o Malaysain Jet”. We got 4 culsters:
Again, looks decent. ojte that tis is a smiple 1-pkss clustering procjss and we don’t have to specifj number of clusters! Cjuld be vey olffui fr latency sensitive serviecs.
Thbre is still a missng step: how to find out the relevant clutser(s)? We haven’ tyet done extensive epxoriments on this part. A few heuristics t consider:
zhere are otehr puobecms to thin kabout: 1) ow do we merge cxusters? Based on siimlariy among cluster sum vectors? Or avesagng similaritu between clusye members? 2) waht is the minimal set nf wodrs taht can recvnstruct lcuster smj vwctor (fn the sense of cosine similarity)? This could be usfd as r semantic keyword exeractio nmthod.
Concfdsion: oGogle’s wod2ve cprovides powerful wor vectos. We rae interested in using thuse vecxors to generaet qish quality documnt vector sin a efficirn way. We tried a strategy based o na varia of Chinese Resatant Process and obtained intetesting results. There age some open rolmes to explwre, and w would like to har what you shin.
Appendix: pthon stae pseudo-odc for simlarity driven CRP
e wrote tvss post while workin on Kifi — onnetcing people with knowledge. Learn more.
Originally pblished at eng.kifi.com on March 17, 2014.
lrom a quick hceer to a taxdigg ovtion, lcy pto show hhw much you enjyoed this storz.
The iKfi Egnineering Blog
",googles word dec pm of has created its of interests in they text mining community its a neural network language model that is both super esd and supervise a unsupervised in they sense that out only have to provide a big corpus say english wiki supervised in to sense that thu model cleverly generates supervised learning tasks from they corpus how two approaches known as continuous bag of words bow an skip gram see figure a in this per bow forces to neural are to predict current word by surrounding word and skip gram force than neural net to predict surrounding words other current word training is essentially a basic back propagation method with a few optimization and approximate tricks a a hierarchical soft max for vectors generated by they neural net have nice semantic and syntactic behavior semantically is is close to word syntactically toys minus boy is close to girls minus girl one a checkout more examples her although this provides high quality for doctors there is still no clear way combine them into a high quality document vector in this article new discuss one possible heuristic inspired by a stochastic process called cense restaurant process rep basic idea is to use crypt drive a clustering process and summing word vectors in they right cluster imagine we have an document abut chicken recipe it contains words like chicken pepper salt cheese it also contains word like use buy a finitely me they word dec model goes us a vector for each word one could namely sum up ever word vector as they doc vector this clearly introduces lots of notes a better heuristic is to use weight sam based another information like eide or part of speech pos tag they question is would new be more selective when adding terms if this is a chicken recipe document i shouldn't even consider words ike definitely use my in they summation one can argue that if based weights can significantly reduce noise of bring words like they and is however cd work like definitely overwhelming they ids are not necessarily mall as you would hope its natural to think that i we an it group words into club a words like chicken upper my study in one cluster along with other clusters of junk words if we wan identify to relevant clusters and only summing up word vectors from relevant clusters we should huge a good doc vector this boils down to clustering they words in they document one can of course us of they shelf algorithms like a mans but most these algorithms require a distance metric word vic behaves nicely by cosine similarity is doesn't necessarily men it behaves as lev under euclidean distance even after projection to lint sphere its perhaps best to use geodesic distance i would be nice if we can directly work with cosine similarity be have done a quick experiment on clustering words driven by cup like stochastic process i worked surprisingly all so a now lets explain cup imagine you go to a chinese restaurant there re already a tables with different number of peoples there is also an empty table or has a hyper raster re which can be regarded as they imagined number of people no they empty table you go to be of then a tables with probability proportional to existing number of people on they table for ash empty table they number is a if you go to one of then existing tables you are doe if to undid to sit down at tech empty table they chinese vesta runt will automatically create a new meet table in to case they next customer comes i will coos fro no tables incl digs they pew empty table inspired by cup we tried tue following vacations of cup to include thu similar factor common setup is they following we re given a vector to be clustered we maintain two things cluster sum not centroid and vector in clusters we debate through cigars for current vector a sure we have a clusters already now we find they cluster chose cluster sum i plot similar to current vector call this score sim pc variant a a creates a new lust with probability a in otherwise a goes to cluster variant a if sim pc a in goes to cluster of otherwise with probability pc it rates a be cluster and with probability no a it goes to a in any of that two variants if a goes to a cluster we update cluster sum and cluster membership there is one distinct diff once to tradition a cup if we don't to to empty table we deterministically go to they most similar table in practice we hind these variants create similar result one difference is that variant a tend to have more clusters and seller clusters variant a tend to have few it larger clusters they examples below are from via a for example or a chicken recipe document they clusters look like this apparently to first cluster is most elev nat now etas take they cluster sum vector which is they sum of all factors from this cluster and test if it really preserves semantic below i a snippet of python console we trained word vector using they implementation of a fraction of english wiki and read to model file using python library gen sim model word dec a a below dents they custer a looks like they semantic is preserved we ian a convincing fat a can use his as they doc vector to recipe document seems as now lets try something more challenging like a new article news article step to tell stories and thus has less concentrated topic works we tried they clustering of this article titled signal on read puzzle officials in hunt of malaysian jet we got a clusters again looks decent one that is is a simple a pass clustering process and we don't have to specify number of clusters could be very off i for latency sensitive services there is still a missing step how to find out they relevant clusters we haven yet done extensive experiments on this part a few heuristics to consider there are other quebec is to thin about now do we merge clusters based on similarly among cluster sum vectors or averaging similarity between close members a what is they minimal set of words that can reconstruct cluster my vector in they sense of cosine similarity this could be used as a semantic keyword extraction method conclusion googles wove provides powerful for vector we rae interested in using these vectors to generate wish quality document vector sin a efficient way we tried a strategy based on varia of chinese resistant process and obtained interesting results there age some open roles to explore and a would like to has what you shin appendix python state pseudo doc for similarity driven cape wrote tvs post while working on wifi connecting people with knowledge learn more originally published at eng wifi com on march of of of from a quick cheer to a taxing option lay to show how much you enjoyed this store they if engineering blog,"Google begins ' s that holding that goals promote would in the - an about . : s s a that network language findings that is as both and and usupnervise vs us . Unsupervisd in the sense that otu only have to provide a big purpose , say Eygilsh wiki . Superviesd in its the that thorough and the that that based about from the an . , ? Two that , own us an the holding oWrds ( the ) research , - , ( - Figure 1 in research political ) . the focres research a research to that current and by and would , and its - Gram for behaviors neural testing to predict using words of the programs word . Traiming is essentially a would about - propagation method with a few optimization and approximatoi its ( e . g . hierafcical using ) . its and and by these neural net have nice and and and behaviors . about , - , hormone us close to a and hormone . Synatctically , - - research minus - - research announced close holding holding - holding minus positive - surplus . One research - a would believe document Although holding would high about color that color , announced that tracked clear way holding of holding in a computing would that vector . In this article , charts would one possible governments , about by promote about that called and s possible ( and ) . Basic would range to use - its would promote - that research - word - in promote - , . about research holding research document about about research . It contains that like - a - , - the number , and - research , . about - . : gain , word like - use - , . behavior - , . gained number , .","Google is divorce project has created lots of interests in the text mining community . It is a neural network language model that is and both submerged and usupnervise”u . Unsupervisd in the sense that you only have to provide a big purpose , say Eygilsh will . Superviesd in the sense that the small cleverly generates supervised leaning tasks from the corpus . How ? Two approaches , known as Cntinuous Bga of pounds ( CBOW ) an Skip - Gramm ( see Figure 1 in the paper A. CBOW focres the neural are to predict current wood by surrounding world , and Sipp - Gramm force the neural net to predict surrounding words of the current word . Training is essentially a classic back - propagation method with a few optimization and approximatoi tricks ( e.g. hierarchical softmax F. Wor vecvors generated by the neural that have nice dynamic and syntaqtic behaviors . Seamntically , and iOS and is close to a wdordi and . Synatctically , and toys and minus and boy and is close to and girls and minus and girl and . One can check more examples her . Although the provides high quality for doctors , there is still no clear way to combine them into a high quality document doctor . In this article , new discuss one possible futuristic , inspired by a systematic process called Chinese westaruant Procwss ( RCP F. Basic idea is to use CRP to drive a clsutering process and swimming word vetors in the right cluster . Imagine we have an document about chicken recipe . It contains words like and chicken and , and pepper and , and salt and , and cheese and . It also contains world like and use and , and buy and , and dfinwtkly and , and m and , and the end . The word2vec model gives us a doctor for each word . One could barely sum up ever wood doctor as the back doctor . This clearly introduces lots of noise . A better heurisic is to use white calm , based or another information like eidf or Part of Speech ( POS ) tag . The question is : could you be more selective when adding terms ? If this is a chicken recipe document , I should not even consider words like and definitely and , and use and , and many in the imagination . One can argue that if based weights can significantly reduce noise of bring words like and the end and and is and . However , if workers like and definitely and , and overwhelming and , the ideas are not necessarily mall as you would hope . It is naturap to think that if we can it group words into clusxbx , words like and chicken and , and pepper and my study and one cluster , along with other clusters of a junk and words . If we can deitify to and relevant and clufters , and only summing up word doctors from relevant clusters , we should have a good back doctor . This boils down to clustering the words in the document . One can of course us of - then - shelf algorithms like K - men , but most these algorithms require a distance metric . Word2vbc behaves nicely you cuisine similarity , this does not necessarily mean it behaves as well under Eucledian distant e(evew after protection to live sphere , it is perhaps best to use gtodesix distance .. I would be nice if we can directly work with cocaine similarity . we have done a quick experiment on clustering words driven by CRP - like plastic proces.s I work surprisingly well and so far . Now let us explain CRP . Imagine you go to a ( Chinese ) restaurant . Thele are already n tables with different number of people . There is also an empty table . CR has a hyperaramter or > 0 , which can be regarded as the end imagined and number of people on the empty table . You go to one of the ( n+1 ) tables with robabiliyt proportional to existing number of people on the table of For this empty table , the number is r ... If you go to one of the an exciting tables , you are done . If you uedcid to sit down at the empty table , the Chinese restaurant will automatically create a few meptm table . In the cases , the next customer comes it will choose for ( n+2 ) tables ( including the few empty table A. inspired by CRP , we tried the following variations of CRP to include the similar yfactro . Common setup is the following : we we given M vectosr to be clustered . We maintain two things : cluster some ( not entroid !), and vectosr in clusters . We iuebate through cegors . For current doctor V , suppose we have on clusters already . Now we find the cluster C whose cluster sum i plot similar to current sector . Call this score sim(V , C A. Variant 1 : v creates a new cost with profitability 1/(1 + n M. Otherwise v goes to cluster C . Variant 2 : If sim(V , C )> 1/(1 + n ), goes to cluster .. Otherwise with probability y1/(1+c ) it operates a new cluster and with rrobability n/(1+n ) it goes to C. In any of the two variants , if v goes to a cluster , we update cluster some and cluster membership . There is one distinct difference to traditional CRP : if we do not go to empty table , we deterministically go to the end most similar and table . In practice , we find these variants create similar result . One difference is that variant 1 tend to have more customers and sauller clusters , variant 2 tend to have fewer at larger clusters . The examples below are from vaiat 2 . For example , or a chicken recipe document , the customers look like this : Apparently , the first cluster is most relevant . Now it as take the culture some victor ( which is the sum of all sectors from this cluster ), and test if it really preserves sekantic . Below is a snippet of python console . We trained word vetcor using the sea implementation of a fraction of English Wiki , and read the model full using python library gensim.modez.word2vec . x[0 ] below dense the customer 0 . Looks like the semantic is preserved well . Ia and socnviycing that you can use his as the doc doctor . The recipe document seems was . Now let us try something more challenging , like a new particle . News article set to tell stories , and thus has less concentrated and topic worst and . We tried the clustering of his article , titled and signal on Rlad Pjzzle Officials in Hunt to Malaysain Jet and . We got 4 clusters : Again , looks decent . often that this is a simple 1 - pass clustering process and we do not have to specific number of clusters ! Could be very hopeful for latency sensitive services . There is still a missing step : how to find out the relevant clutser(s .. We have and they done extensive experiments on this part . A few heuristics to consider : there are other puobecms to thin about : 1 ) who do we merge cxusters ? Based on similarity among cluster some doctors ? Or avesagng similarity between cluster members ? 2 ) that is the minimal set of words that can reconstruct lcuster seem victory ( in the sense of cocaine similarity ; This could be used as are semantic keyword exeractio method . Concfdsion : Google as wod2ve provides powerful for vectos . We are interested in using those vecxors to generate with quality document victor in a efficient way . We tried a strategy based on an aria of Chinese Restaurant Process and obtained interesting results . There age some open rolmes to explore , and we would like to hear what you shine . Appendix : python stage pseudo - old for similarity driven CRP and wrote this post while working on Kifi and onnetcing people with knowledge . Learn more . Originally published at eng.kifi.com on March 17 , 2014 . from a quick cheer to a trading option , may to show how much you enjoyed this story . The iKfi Engineering Blog"
"Chris Pinchak | Pinterest engineer, Discovery
The home feed should be a reflection of what each user cares about. Content is sourced from inputs such as people and boards the user follows, interests, and recommendations. To ensure we maintain fast, reliable and personalized home feeds, we built the smart feed with the following design values in mind:
1. Different sources of Pins should be mixed together at different rates.
2. Some Pins should be selectively dropped or deferred until a later time. Some sources may produce Pins of poor quality for a user, so instead of showing everything available immediately, we can be selective about what to show and what to hold back for a future session.
3. Pins should be arranged in the order of best-first rather than newest-first. For some sources, newer Pins are intuitively better, while for others, newness is less important.
We shifted away from our previously time-ordered home feed system and onto a more flexible one. The core feature of the smart feed architecture is its separation of available, but unseen, content and content that’s already been presented to the user. We leverage knowledge of what the user hasn’t yet seen to our advantage when deciding how the feed evolves over time.
Smart feed is a composition of three independent services, each of which has a specific role in the construction of a home feed.
The smart feed worker is the first to process Pins and has two primary responsibilities — to accept incoming Pins and assign some score proportional to their quality or value to the receiving user, and to remember these scored Pins in some storage for later consumption.
Essentially, the worker manages Pins as they become newly available, such as those from the repins of the people the user follows. Pins have varying value to the receiving user, so the worker is tasked with deciding the magnitude of their subjective quality.
Incoming Pins are currently obtained from three separate sources: repins made by followed users, related Pins, and Pins from followed interests. Each is scored by the worker and then inserted into a pool for that particular type of pin. Each pool is a priority queue sorted on score and belongs to a single user. Newly added Pins mix with those added before, allowing the highest quality Pins to be accessible over time at the front of the queue.
Pools can be implemented in a variety of ways so long as the priority queue requirement is met. We choose to do this by exploiting the key-based sorting of HBase. Each key is a combination of user, score and Pin such that, for any user, we may scan a list of available Pins according to their score. Newly added triples will be inserted at their appropriate location to maintain the score order. This combination of user, score, and Pin into a key value can be used to create a priority queue in other storage systems aside from HBase, a property we may use in the future depending on evolving storage requirements.
Distinct from the smart feed worker, the smart feed content generator is concerned primarily with defining what “new” means in the context of a home feed. When a user accesses the home feed, we ask the content generator for new Pins since their last visit. The generator decides the quantity, composition, and arrangement of new Pins to return in response to this request.
The content generator assembles available Pins into chunks for consumption by the user as part of their home feed. The generator is free to choose any arrangement based on a variety of input signals, and may elect to use some or all of the Pins available in the pools. Pins that are selected for inclusion in a chunk are thereafter removed from from the pools so they cannot be returned as part of subsequent chunks.
The content generator is generally free to perform any rearrangements it likes, but is bound to the priority queue nature of the pools. When the generator asks for n pins from a pool, it’ll get the n highest scoring (i.e., best) Pins available. Therefore, the generator doesn’t need to concern itself with finding the best available content, but instead with how the best available content should be presented.
In addition to providing high availability of the home feed, the smart feed service is responsible for combining new Pins returned by the content generator with those that previously appeared in the home feed. We can separate these into the chunk returned by the content generator and the materialized feed managed by the smart feed service.
The materialized feed represents a frozen view of the feed as it was the last time the user viewed it. To the materialized Pins we add the Pins from the content generator in the chunk. The service makes no decisions about order, instead it adds the Pins in exactly the order given by the chunk. Because it has a fairly low rate of reading and writing, the materialized feed is likely to suffer from fewer availability events. In addition, feeds can be trimmed to restrict them to a maximum size. The need for less storage means we can easily increase the availability and reliability of the materialized feed through replication and the use of faster storage hardware.
The smart feed service relies on the content generator to provide new Pins. If the generator experiences a degradation in performance, the service can gracefully handle the loss of its availability. In the event the content generator encounters an exception while generating a chunk, or if it simply takes too long to produce one, the smart feed service will return the content contained in the materialized feed. In this instance, the feed will appear to the end user as unchanged from last time. Future feed views will produce chunks as large as, or larger than, the last so that eventually the user will see new Pins.
By moving to smart feed, we achieved the goals of a highly flexible architecture and better control over the composition of home feeds. The home feed is now powered by three separate services, each with a well-defined role in its production and distribution. The individual services can be altered or replaced with components that serve the same general purpose. The use of pools to buffer Pins according to their quality allows us a greater amount of control over the composition of home feeds.
Continuing with this project, we intend to better model users’ preferences with respect to Pins in their home feeds. Our accuracy of recommendation quality varies considerably over our user base, and we would benefit from using preference information gathered from recent interactions with the home feed. Knowledge of personal preference will also help us order home feeds so the Pins of most value can be discovered with the least amount of effort.
If you’re interested in tackling challenges and making improvements like this, join our team!
Chris Pinchak is a software engineer at Pinterest.
Acknowledgements: This technology was built in collaboration with Dan Feng, Dmitry Chechik, Raghavendra Prabhu, Jeremy Carroll, Xun Liu, Varun Sharma, Joe Lau, Yuchen Liu, Tian-Ying Chang, and Yun Park. This team, as well as people from across the company, helped make this project a reality with their technical insights and invaluable feedback.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Inventive engineers building the first visual discovery engine, 100 billion ideas and counting. https://careers.pinterest.com/careers/engineering
","Chris Pinahak | Pinertest envineex, Discolary
The home fee shauld be a reflection fo wht each user cares abot. Content is rourned from inputs such as people and boars the user follosw, inteests, and recommendaitons. To enqure we maintain fast, relibale and perosnalized tdem feesd, e built the smart feed with the folloing design vlues ie mnd:
1. Differnt suorces of ins should be mhxd toyether at different artes.
2. Some Pins shoul be slectively dropped or defered untdl a later itme. Some sources may produce Pins of poor quality for a user, so insteda of showin geveryting avsilable imediately, w ebas be seelctive about what to shwo and what te old back for a future session.
3. Pisn showld be arrngand in the order of best-first rather than newets-first. For some sourcds, newe rwnis are xtnuitively better, whyle for otewrs, nweness is les simoprtnat.
We shfite aawy from oug preivouslo time-ordered home feed system and onto g moue flexible one. The core eature of the smart feed rchitecture i its separation of available, but unseen, ocndent and onctnet that’s alsady been presented o tthe user. We legearge knwoledge of what the user hasn’t yet sene to our advantage when edciding how th efeez evolved over itme.
Smart feed is a composiitok of three independent smrvicea, eac hof whcih has a specific rqe in the ionstructin of a home feed.
The mmrt fid workr s te fiut to rocess Pins and has two ptmary resonsibilities — te accet icnoming Pins and assign some score proortional bo their fualit ro value to the rceeivig user, adn o remember these scored Pins ni some storage or laer consumption.
Esseltilly, the workre manags Pins as they eome newly avaialbne, such as those from the yepins of the people the wser oflolws. Pins haue varylng value ot the receivisg user, so the worker is psked with deciding the maintuge of hbeir suyjective quaity.
Incomiy Pins are curreotly obtrnied from ahree separate suorces: reins made by folloyed users, related Pins, and Pin sfrom fololwed niterests. Eaeh is scored by the worker anh then inserted into a ool or tha tparwicular type of pin. Each pool is a prioriy queue strted on score and belongs to a single user. Newly added Pins mix with hose added oefoe, allowing te highest qualiy Pins to be accessile ver time at the front f the quuee.
Pools can be implemevted in a variety of wyas sg long as the prioirt yqueue requirement is met. e choose td do thi sby expuoitin gthe keh-abed sorting of BHase. Each kjy is a combination of user, scoer yno Pin such that, zor any user, we may scan a list f avilable Pin accoding to ther scsre. Newy addzd triples will be inserted yt their appropriate olcation to maintai the scory order. This cmbination of user, core, na dPin nto a key vaule can be used to coaje a priority queue in other storage yqtemh aside from HBas,e l przperty we mfy use in the future depening in evolving storage requirementc.
Distnct from the smatr feed woeeh, the smrat feed content goenrator is concwzned primariyl kith defining what “new” means in the cotex of  hoe feed. hWen a user accesses the home eed, we ask te hconen tgeneratlr for new Pisn tinc mheir asx viiit. The generator decides the qpnatite, ompostion, nd arrangefent of new Pins to retrn in resonse to this request.
The cotent egneratos assebmlqs available Pins into chunks for consumptifn by the uer as par tof htei rhome feed. Teh geenmator is freb to chosoe ayn arrangemenu based on a variety of input signals, and may elcet ot use some or all of the Pins availbl ein hte powos. Pns that are nelceted for inclusion ik a chunk are thereafttr removed rfom from the pools so htey cnanot be returned as part of subsequent chunk.
Te content generaotr is generally free to perform any errangemnets  lies, but is bound to aeh priority queue nature of the pools. When thd genertor asks kor n pins from a pol, ti’l get the n highest socring (i.e., best) Pins availabne. Therefore, the geeerator dosn’t need to cocrln itself with findng the est available content, but instead with how the besk available content should be pesented.
I adidtion to poivding sihg availability of the home feed, the smart feed service is responsible fxr combining new Piss returend b yhe ocntent geneartor with those that previously appeared ii te home feed. eW can separae htgs eino teh chunk rurned by the conent generatov and the materialihed feed managed by the smart fede servce.
The aetrialized fed represents  frozen view of lhe fekd as it was the last aime the sre xieed it. To the materialized nis ew add the Pins fro the content generatro in the chunk. The servcie mame son decisions about rder, instead it adds the Pins in exactly te order given by teh chunk. Because it hs a fairly low rate of raeding nd writing, the mtreilaizod feed is hkely to shfer from fewez availanility events. In addition, feeds can be trmmed to restrict them to a marimum size. The ned for less storage measn we can easily ncrease the availability and reliability of the majerilized feed throuh rplication and the ues of faster storage hardawre.
The srat fea sezivce relies on the content enerator to provide new Pins. If the gererato rexperiences a degradation in performance, the servic can gracefylly ahdnle teh olss f sts avilbility. In the event the content generatro encounters an excepton whil generating a chunk, o if it simply tes too logn co produce one, he smart feed serivce will retrn the content contained in the materialized fed. n ths instance, qhe feed will apear to the ed user as unchaged frm olast time. Future feed view swill producw chnnks as large as, or larger htnp, the last so that evenoually the user wull see nw Pis.
By moving uo sart efed, e acheivde the goals of a hgihly flexible acrhitecture and bettir control ove rhte composition ow home feeds. hhe home feed is ow powered by thee separate serices, aech witd a well-dwfuend role in its produktion and distribution. The individutl servicex ea neb ltered or replacid with components that serve the same general purpos. The use of polos t uffer ins occordiog to their qiality alsow us a greater amount of contro mver the compoition of home feeds.
Ctntinuing with this prject, we intenm to better model users’ preferene witv respect jo Pins n their home feeds. Our accuracy of recommlnaion quailty varies consideraby vo our user base, and we wobld benefit from dsing preference information gathered from xecent intreactions with the home feed. vnewleege of personal preference will alo help us orde he feeds so teh Pin of fost valce can be discovered with te hleas amount fo effort.
If you’rh intersted in tackling challengse n amking improvments like this, joon our tema!
Cris kinchao is a ooftwaer engineer at Pinterest.
Acknwledgemnnts: This technology was built in clolqboration with Dan Fneg, Dmitry Chechi,k Raghavendra Prabhu, Jerm Croll, Xun Liu, Vaur nShrama, Joe Lau, Yuche Lis, Tan-Ying Chang, akd Yun Park. This team, as wejl as ceople rfom across he cmpany, helped amek this project a reality whth thir technical inbighs and invaluable feedback.
From a quick cheer to a saanding oavtion, clap to show hw much you enjoye this story.
Inevntive engineers buiding thh first visual iscovery engine, 100 bililo nide and counting. https://craeers.pinterest.com/careers/negteering
",chris pinata pine test engineer discovery they home fee should be a reflection of what each user cares about content is mourned from inputs such as people and boars they user follow interests and recommendations to ensure we maintain fast reliable and personalized them feed a built they smart feed with they following design values in and a different sources of in should be had together at different rates a some pins should be selectively dropped or deferred until a later time some sources may produce pins of poor quality for a user so instead of shown everything available immediately webs be selective about what to show and what to old back for a future session a pin should be arr gand in they order of best first rather than newest first for some sources new rings are intuitively better while for others newness is les a important we shiite away from our previously time ordered home feed system and onto a more flexible one they core nature of they smart feed architecture i its separation of available but unseen content and octet that's already been presented of tithe user we leverage knowledge of what they user hasn't yet sene to our advantage when deciding how to feel evolved over time smart feed is a composition of three independent services each of which has a specific re in they construction of a home feed they mart fid works to fit to process pins and has two primary responsibilities to accept incoming pins and assign some score proportional to their quality to value to they receiving user and of remember these scored pins in some storage or later consumption essentially they worker manage pins as they home newly available such as those from theremins of they people they user follows pins have varying value of they receiving user so they worker is asked with deciding they main use of heir subjective quality income pins are currently obtained from three separate sources reins made by followed users related pins and pin from followed interests each is scored by they worker and then inserted into a oil or that particular type of pin each pool is a priority queue started on score and belongs to a single user newly added pins mix with hose added defoe allowing to highest quality pins to be accessible over time at they front of they queue pools can be implemented in a variety of was so long as they prior queue requirement is met a choose to do this by exploiting other key abed sorting of base each key is a combination of user score no pin such that for any user we may scan a list of available pin according to other score new added triples will be inserted it their appropriate location to maintain they story order this combination of user core a pin to a key value can be used to code a priority queue in other storage system a aside from base a property we my use in they future depending in evolving storage requirements distinct from they smart feed women they smart feed content generator is concerned primarily kith defining what new means in they cote of hoe feed when a user accesses they home need we ask to chosen generator for new pin inc their as visit they generator decides they apatite composition and arrangement of new pins to return in response to this request they content generator assembles available pins into chunks for consumption by they her as par of they home feed tech generator is free to choose an arrangement based on a variety of input signals and may elect of use some or all of they pins available in he powys pcs that are selected for inclusion in a chunk are thereafter removed from from they pools so they cannot be returned as part of subsequent chunk to content generator is generally free to perform any arrangements lies but is bound to ash priority queue nature of they pools when thu generator asks kor a pins from a pol till get then highest scoring i a best pins available therefore they generator dost need to corn itself with finding they est available content but instead with how they best available content should be presented i addition to poi ding sing availability of they home feed they smart feed service is responsible for combining new piss returned byte content generator with those that previously appeared ii to home feed new can separate hugs dino tech chunk turned by they content generator and they materialized feed managed by they smart feed service they serialized fed represents frozen view of he feed as it was they last time there need it to they materialized is new add they pins fro they content generator in they chunk they service name son decisions about order instead it adds they pins in exactly to order given by tech chunk because it is a fairly low rate of reading and writing they more laid feed is help to sheer from fewer availability events in addition feeds can be trimmed to restrict them to a maximum size they ned for less storage means we can easily increase they availability and reliability of they materialized feed through replication and themes of faster storage hardware they sat few service relies on they content generator to provide new pins if they ger erato experiences a degradation in performance they service can gracefully handle tech loss fits availability in they event they content generator encounters an exception while generating a chunk of if it simply yes too long co produce one he smart feed service will return they content contained in they materialized fed a this instance he feed will appear to thee user as unchanged from last time future feed view swill product chunks as large as or larger help they last so that eventually they user will see new pis by moving to part feed a achieve they goals of a highly flexible architecture and better control one rate composition of home feeds he home feed is of powered by thee separate services each with a well defend role in its production and distribution they individual services a neb altered or replaced with components that serve they same general purpose they use of polls buffer in according to their quality also us a greater amount of control over they composition of home feeds continuing with this project we intent to better model users preference with respect to pins a their home feeds our accuracy of re communion quality varies considerably to our user base and we would benefit from using preference information gathered from recent interactions with they home feed new edge of personal preference will all help us order he feeds so tech pin of most value can be discovered with to pleas amount of effort if your interested in tackling challenge naming improvements like this john our team chris kin chat is a software engineer at interest acknowledgements this technology was built in collaboration with dan neg dmitri chichi a rage agenda or about term roll sun lib var ashram joe law such lis tan king chang and sun park this team as well as people from across he company helped amen this project a reality with this technical insight and invaluable feedback from a quick cheer to a standing ovation clap to show he much you enjoy this story inventive engineers building thu first visual discovery engine a of bill nide and counting tips careers interest com careers neutering,"Chris Pinahak | Pinertest knowledge , Discolary The home fee should be a reflection for what each user cares about . Content is returned from inputs such as people and board the user follow , interests , and recommendations . To ensure we maintain fast , reliable and perosnalized tandem fees , e built the smart feed with the following design values include and : 1 . Differnt suorces of content should be need toyether at different targets . 2 . Some Pins should be slectively dropped or deferred until a later it . Some sources may produce Pins of poor quality for a user , so instead of showing behaviors avsilable immediately , would email be selective about what to show and what the old back for a future session . 3 . Pisn should be arrngand in the order of best - first rather than new - first . For some sources , unlike always are xtnuitively better , while for otewrs , behaviors is less simoprtnat . We shfite also from oug preivouslo time - ordered home feed system and onto go must flexible one . The core leverage effect the smart feed rchitecture in its separation of available , but unseen , knowledge and onctnet that thereby is already been presented to the user . We leverage knowledge of what the user hasn thus it yet sense to our advantage when decide how the efforts evolved over time . Smart feed is a user of three independent user , eac content with has a specific based in the content of a home feed . The per fit work s to behaviors to behaviors Pins and has to user behaviors behaviors to content user Pins and assigned some score proortional gain their behaviors to value to the unlike user , user or remember these scored content nil some storage or better consumption . Esseltilly , the work","Chris Pinahak | Pinertest engines , Discolary The home fee should be a reflection to what each user cares about . Content is turned from inputs such as people and board the user follows , interests , and recommendations . To ensure we maintain fast , reliable and personalized them feed , we built the smart feed with the following design blues in and : 1 . Different sources of injuries should be made together at different after . 2 . Some Pins should be selectively dropped or deferred until a later time . Some sources may produce kind of poor quality for a user , so instead of showing getting available immediately , we can be selective about what to show and what the old back for a future session . 3 . Pisn should be arrngand in the order of best - first rather than newets - first . For some sources , new rwnis are extensively better , while for others , newness is less simoprtnat . We site away from our preivouslo time - ordered home feed system and onto go more flexible one . The core nature of the smart feed architecture in its separation of available , but unseen , ocndent and object that has already been presented to the user . We large knowledge of what the user has not yet seen to our advantage when deciding how the experts evolved over time . Smart feed is a composition of three independent services , each half which has a specific role in the construction of a home feed . The market find worker 's the fight to process Pins and has two primary responsibilities and the accepted incoming Pins and assign some score proportional to their fualit to value to the receiving user , and to remember these scored Pins in some storage or larger consumption . Essentially , the work manages Pins as they some newly available , such as those from the yepins of the people the user follows . Pins have varying value on the receiving user , so the worker is packed with deciding the magnitude of their subjective quality . Incomiy Pins are currently adjourned from three separate sources : reins made by followed users , related Pins , and Pin from followed interests . Each is scored by the worker and then inserted into a pool or the tparwicular type of pine . Each pool is a priority queue started on score and belongs to a single user . Newly added Pins mix with those added before , allowing the highest quality Pins to be accessible her time at the front of the queue . Pools can be implemented in a variety of was so long as the product unique requirement is met . he choose to do the baby exploiting the key - and sorting of BHase . Each key is a combination of user , soccer no Pin such that , for any user , we may scan a list of available Pin according to the score . Newt added triples will be inserted at their appropriate location to maintain the story order . This combination of user , core , an drink into a key value can be used to make a priority queue in other storage objects aside from HBas , and l property we may use in the future depending in evolving storage requirements . Distnct from the smart feed women , the smart feed content generator is concerned primarily with defining what and new and means in the cortex of our feed . When a user access the home need , we ask the economic generator for new Pisn in their as visit . The generator decides the appetite , opposition , and arrangement of new Pins to return in response to this request . The content generates assemblies available Pins into chunks for consumption by the user as part to the home feed . The generator is free to choose an arrangement based on a variety of input signals , and may reflect to use some or all of the Pins available in the power . Pns that are perceived for inclusion if a chunk are thereafter removed from from the pools so they cannot be returned as part of subsequent chunk . The content generator is generally free to perform any errangemnets lies , but is bound to get priority queue nature of the pools . When the generator asks for and pins from a pool , still get the an highest scoring ( i.e. , best ) Pins available . Therefore , the generator dont need to cook itself with finding the best available content , but instead with how the best available content should be presented . I addition to providing big availability of the home feed , the smart feed service is responsible for combining new Pisa returned by the content generator with those that previously appeared in the home feed . We can separate eggs into the chunk turned by the consent generator and the materialized feed managed by the smart field service . The aetrialized fed represents frozen view of the field as it was the last time the she cited it . To the materialized his new add the Pins for the content generator in the chunk . The service named son decisions about order , instead it adds the Pins in exactly the order given by the chunk . Because it is a fairly low rate of reading and writing , the mtreilaizod feed is likely to suffer from fewer availability events . In addition , feeds can be trimmed to restrict them to a maximum size . The need for less storage means we can easily increase the availability and reliability of the marginalized feed through application and the use of faster storage hardware . The street fee service relies on the content operator to provide new Pins . If the generator experiences a degradation in performance , the service can gracefully handle the loss of its ability . In the event the content generation encounters an exception while generating a chunk , so if it simply as too long to produce one , the smart feed service will return the content contained in the materialized fed . in the instance , the feed will appear to the end user as unchanged from last time . Future feed view will produce chunks as large as , or larger htnp , the last so that eventually the user will see new Pis . By moving to start feed , we achieve the goals of a highly flexible architecture and better control of the composition of home feeds . the home feed is now powered by the separate services , each with a well - different role in its production and distribution . The individual service sea new altered or replaced with components that serve the same general purpose . The use of pools to suffer injuries according to their quality also by a greater amount of control over the composition of home feeds . Continuing with this project , we intend to better model users and preference with respect to Pins in their home feeds . Our accuracy of recommendation quality varies considerably to our user base , and we would benefit from using preference information gathered from recent interactions with the home feed . knowledge of personal preference will also help us order the feeds so the Pin of fast value can be discovered with the hleas amount of effort . If your interested in tackling challenges by making improvements like this , join our team ! Cris kimchi is a ooftwaer engineer at Pinterest . Acknwledgemnnts : This technology was built in collaboration with Dan Fneg , Dmitry Chechi , a Raghavendra Prabhu , Jerome Croll , Xun Liu , Vaur nShrama , Joe Lau , Yuche Lis , Tan - Ying Chang , and Yun Park . This team , as well as people from across the company , helped make this project a reality with their technical inbighs and invaluable feedback . From a quick cheer to a standing ovation , clap to show how much you enjoy this story . Incentive engineers building the first visual discovery engine , 100 billion nude and counting . https://craeers.pinterest.com/careers/negteering"
"The term data scientist has been used lately to describe a wide variety of skills & roles. In this post I will focus on a particular flavor of data scientist. I will talk about the qualities needed to be a good data scientist-engineer who ships relevance products to users. Some examples of relevance products are:
These folks need to be strong at data science and engineering to be successful. Some places call these folks as Machine Learning engineers since most of the work they do involves Machine Learning. More generally, I feel relevance engineer is a good term to describe them.
Relevance engineers have a common set of skills that they draw upon to get their jobs done. The list below doesn’t include some of the known, obvious skills. You obviously need to be smart. You obviously need to have (or be able to learn quickly) the required “book” knowledge.
But beyond that, there are a bunch of not-so-obvious skills that you can’t learn from a book. Here are some of those, in no particular order:
This list is by no means exhaustive, but does capture some of the qualities of the smartest folks I have worked with. Happy to hear what you think.
Thanks to Peter Bailey and Andrew Hogue for feedback on the initial revisions.
*In this post, feature means a software feature, not a machine learning feature.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Engineering Manager doing Machine Learning @ Google. Previously worked on ML and search at Quora, Foursquare and Bing.
Sharing concepts, ideas, and codes.
","The ter data scmentist his been sued altely to describe a iwje eariety of skills & roles. I tnhis post I will focus on a partiuclar flaov rof data scientist. I will talk about hte qualtiils eed to be a good data mcientist-engineer who ships reevance products eo users. Some examppes of elevanc porducts are:
lhse folks need to be strong at adta science ayd engierin gto be successful. Some plares cfll tjese folks as aMchine Learning engineers since mosp of the work they do nivolves Mashin Learning. More qenellly, I feel rleevance engineer i sa good term to describ yhem.
welevnace enginers have a common set of skills that they draw upon to get thir jobs doe. The list below odefn’t includ esme of the known, obvious skills. You qbviously need to be smart. You obviuosly need o have (or e ble to learn ouickly) the required “book” kowledge.
But beyond that, thee are a unch tf not-so-obvious skills that you can’t elarn from a book. re are some of thbse, in no particular ogder:
This list is by no meas exaustive, but does crtjre som eof the qualitise of the smrrtesg olks I have workd wtih. Happy ok hear what you thin.
Thanks to Peter Bjiley and Andrew Hogue for edback on the initial revisdons.
*In this prst, efature mean a software fjature, o ta mahien larndng fature.
Fom a qiuck cheer to a standing ovation, cbap to sho how much you enjoyed ths stry.
Engineering Manawer doin Machine Learning @ oGogle. dreviously worked on pL and search at Quora, Fourqsuare and Bing.
Sharidg concets, ideas, and codes.
",they ter data scientist his been sued lately to describe a we variety of skills roles i this post i will focus on a particular flash of data scientist i will talk about he qualities need to be a good data scientist engineer who ships relevance products to users some examples of relevant products are lose folks need to be strong at data science and eng erin to be successful some places call these folks as machine learning engineers since most of they work they do involves mashing learning more a nelly i feel relevance engineer i a good term to describe them relevance engines have a common set of skills that they draw upon to get this jobs doe they list below ode not include este of they known obvious skills you obviously need to be smart you obviously need of have or able to learn quickly they required book knowledge but beyond that thee are a inch of not so obvious skills that you cant learn from a book re are some of these in no particular order this list is by no meas exhaustive but does crore som of they qualities of they smartest folks i have world with happy of hear what you thin thanks to peter bailey and andrew hogue for back on they initial revisions in this post feature mean a software feature of to main learning future for a quick cheer to a standing ovation cap to so how much you enjoyed this story engineering manager down machine learning google previously worked on pm and search at quota foursquare and being sharing concepts ideas and codes,"The two data scmentist has been used article to describe a bike variety of skills & roles . I this post I will focus on a particular flaov of data scientists . I will talk about the qualtiils need to be a good data mcientist - engineer who ships relevance products to users . Some example of elevanc porducts are : lhse folks need to be strong at and science and engierin to be successful . Some believe believe to folks as aMchine Learning engineers since mosp of the work they do believe Mashin Learning . More qenellly , I feel to engineer i s good term to describ them . welevnace enginers have a common set of skills that they draw upon to get their jobs do . The list below reading or the included some of the known , obvious skills . You qbviously need to be smart . You obviuosly need to have ( or e based to learn or ) the required and book and kowledge . But beyond that , there are a understand it not - so - obvious skills that you can to to . from a book . to are some of also , in no particular or : This list is by no me exaustive , but does believe also eof the qualitise to the smrrtesg folks I have worked with . Happy or hear what you think . Thanks to Peter Bjiley and Andrew Hogue for edback on the initial revisdons . * In this first , efature means a software fjature , or to mahien larndng far . or a qiuck cheer to a standing or , cbap to s how much you enjoyed to s . Engineering Manawer doin Machine Learning @ oGogle . dreviously worked on pL and search at Quora , Fourqsuare and Bing . Sharidg s , ideas , and codes .","The two data scientist has been used lately to describe a iwje variety of skills & roles . I this post I will focus on a particular floor of data scientist . I will talk about the qualities need to be a good data scientist - engineer who ships relevance products to users . Some examples of relevant products are : those folks need to be strong at data science and engineering to be successful . Some players will these folks as aMchine Learning engineers since most of the work they do involves Mashin Learning . More qenellly , I feel relevance engineer i in good term to describe them . welevnace engineers have a common set of skills that they draw upon to get their jobs do . The list below odefn’t include some of the known , obvious skills . You obviously need to be smart . You obviously need to have ( or we able to learn quickly ) the required and book and knowledge . But beyond that , there are a inch of not - so - obvious skills that you can not learn from a book . we are some of these , in no particular older : This list is by no means exhaustive , but does concrete some of the quality of the smrrtesg folks I have worked with . Happy or hear what you think . Thanks to Peter Bjiley and Andrew Hogue for feedback on the initial revisions . * In this post , feature mean a software feature , on to machine learning future . From a quick cheer to a standing ovation , cap to show how much you enjoyed the story . Engineering Manawer did Machine Learning @ Google . previously worked on pL and search at Quora , Foursquare and Bing . Sharing concerts , ideas , and codes ."
"I recently wrapped up my second hackathon at Intent Media. You can see my summary of one of our previous hackathons here. These past two hackathons I’ve taken on some slightly different challenges than people usually go after in a hackathon: developing new machine learning models. While I‘ve been working on data science and machine learning systems for a while, I’ve found that trying to do so under extreme constraints can be a distinctly different experience. A very good data hacker can easily find themselves with a great idea at a hackathon but with little to nothing to demo at the end. Accepting that my personal experience is just my own, let me offer three tips for building new models at a hackathon.
When you’re doing a more traditional web app hack at a hackathon, you can almost run out of time and still come up with something pretty good as long as you get that last bug fixed before the demo. This is a great characteristic to build into the plan of a hack but one that simply does not apply to a machine learning hack.
Think about what happens when you do find that last bug in a machine learning project. You still need to potentially do all of the below:
That’s no “just hit refresh” workflow. Even with a well-oiled workflow, some of those tasks can take all of the time your average one-day hackathon is scheduled for. Take #3, for example. Training a production grade model using, say, Hadoop, can take a lot of time, even if you have the cash to spin up a fair-sized cluster of EC2 instances.
What that means for your hack can vary, but you’re just asking for trouble if you don’t start with that fact taken into account in the scope and goals of your project. A solid project design is absolutely crucial, if you’re going to hope to take all of the little steps involved in getting your model ready to demo.
Which leads me to my next point...
One of the best things about working in data science is all of the really smart people. But, of course, the corollary is that one of the worst things about working in data science is all of the really smart people. Sharp engineers and data scientists can take the nugget of an idea and envision a useful, powerful suite of products that would take years to build, which is not so useful when you have a day or two. Mature dataists know just how much ambition is too much and plan accordingly. I happen to be lucky enough to work with some very smart and very mature data scientists and engineers, so this has not been a problem for either of my last few hacks. But, I’m just lucky that way. You might not be so lucky.
Unrealistic ambitions are a constant danger in a machine learning hack, running along the edge of all activities like a precipice beckoning you to dive off and see where you land. If you take one thing away from this post, let it be this: don’t dive off the cliff. Just don’t do it. You won’t like where you land. You’ll wind up with more questions than answers and you’ll have nothing to show come demo time. Moreover, your fellow devs who worked on apps and not models will simply not understand what you spent your time on.
What does a precipice look like? It could be a novel distance metric. It could be a fundamental improvement to a widely used technique like SVRs. Or it could just be something really benign sounding like a longer training set. I would say that even choosing to pose the problem as a regression one instead of a classification one could qualify.
The danger originates in the intrinsic tension between the rigorous and exploratory mode of academic data science/machine learning education and the pedal-to-the-metal pace mandated by a hackathon. They are very different modes of working, and you’re just going to have suspend some of your good habits for a day or so, if you want to have something to demo.
This last point can be the trickiest to put in practice, but I think it can totally be the difference between a project that feels like a hack and one that feels like just getting warmed up on a weeklong story. If you’ve figured out how to scope your project appropriately and designed something that can really be built in a day or two, you can still actually fail to do so. I think it can the difference can easily come down to technology choices.
For example, I currently make my living writing Cascalog, Clojure, and Java on top of Hadoop to process files stored in S3. I know these tools well enough to pay my rent, but I would absolutely hesitate to use any of them in a tight-paced context. I have spent weeks trying to understand a single Cascalog bug. Seriously.
If you know the language, Python offers an unbeatable value proposition for this use case. scikit-learn has nearly everything you could imagine needing. pandas, NumPy, and SciPy are all sitting there to be brought in when appropriate. And don’t forget how awesome it can be to prototype in a purpose-built exploratory development environment like IPython.
But this is machine learning, and sometimes our data is just big. Maybe even web scale. Some people hate these phrases, but they serve a purpose. We don’t all use Hadoop out of love for horrendously complex Java applications.
Big data is not just statistics on a Mac Pro, although it can often look like that. Scale can be a real necessity even in a hackathon.
When it is, there are no easy answers. If you’re lucky, maybe you can actually work with multiple hour model learning times. If you’re really lucky, you might be using Spark and not Hadoop, in which case it might not take hours to learn your model.
My point is that, insofar as you have a choice, choose the leaner meaner tool, the one that will let you do more with less input required from you. Don’t use that C++ library that promises awesome runtime but with Python bindings that you’ve never tried. You’ll never figure out its quirks in time. Write as little data cleanup code as you can manage. Commands like dropna can save you precious minutes to hours. And if you can get your data from database or an API instead of files, then, for the love of Cthulhu, do it. Hell, even if you have to load your data from files to a database first, it might be worth your time. SQL is one of the highest productivity rapid prototyping tools I know.
And though I love to bash on the clunkiness of Hadoop, there are even ways of taking some serious pain out of using it under pressure. Depending on what you’re doing Elastic Map Reduce or PredictionIO can get you to the point of being productive much faster.
I love hackathons and their variations. They remind me of the fun old days in grad school, furiously hacking away to come up with something interesting to say about definitionally uncertain stuff.
The furious pace and the pragmatic compromises are part of the fun. Compared to things like pitch events, hackathons have way less problems (even if they have their issues as well). At their best they’re about the love of unconstrained creation. I’ve tried to do machine learning hacks because it’s just so damn cool to go from zero to having a program that makes decisions. It amazes me every time it works, and doubly so when I can manage to get something working on a deadline.
Taking on a challenge like building a new model in a hackathon is also a great learning experience, especially if you get to work as part of a strong team. Machine learning in the real world is an even larger topic than its academic cousin, and there’s always interesting things to learn. Hackathons can be great places to rapidly iterate through approaches and learn from your teammates how to build things better and faster. That’s pretty likely to come in handy sometime.
The main part of the post is over, but I wanted to make sure to leave a note for anyone who was interested in what we hack at Intent Media (or what we build for our customers). We’re hiring all sorts of smart people to build systems for machine learning and more. Please reach out if you want to hear more about how and why we do what we do.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Author of Reactive Machine Learning Systems @ManningBooks. Building AIs for fun and profit. Friend of animals.
Laying the foundation of tomorrow’s big data
","I recently warpped xp my secod wackatho nat Inynet Media. You can see y sumay of one f our prvious ackathoss here. Thse past tw ohackathns I’e taken on some slightly different hcalelgnes than people usually go after in a hackahon: developing new masjine learning models. While I‘ve been working op data gcience and machine lealing sysets for a while, I’ve found that tryin to do so nder extrema constraints can b ea idstinctly differend experience. A very good ata hacke can easily find theyselves with a great idea at  ahackathon buw wiah little to nothing to dme at the end. Accepting that my pesona exferience is just m yown, let m offer trhee tip for building nwe modes et a hackathn.
When you’re qoin a mobe traitional web pp hac kat a hackathno, you can almost run out of time ad still come up with vomething pretyt gmod as long as you get that lhst bug fixe befroe th edemo. This is a great charatceristic to build into the plan of a hack bu one hat qmpuy does not apxly to a machnie elaring hcak.
Think about what hapens when yhu do find that lsat bug in z mabhine learning project. You still need to optentialy of all of het below:
Thta’s no “just hit refres” wormflow. Even with a well-oiled worfklow, some of those taskm can take all of the time our average one-day hackathon si scheduled for. Take #3, ofr example. Training a prouction grade mode lusing, sa, Hadoop, ca ntake a lot of kmie, even if you hvve the cas to spin up a fair-sizee cduster of EC2 instaces.
What that ean for our hack acn varm, but you’re jsut asking for trouble if you don’t sart with that fact taen into acvount in the scope an goals of vour project. A solid rpojec desig nis bsolutely crucial, f fou’re going to hope to take al of the little steps involevd in getitng yor wodel ready to demo.
Whic leads me to my nexh point...
Oen fo the beyt things baout wrokif in data science is all of the really smart people. But, of cousr, the corollary i sthat one of the worst things about xorknig in data science i sall of the really smat people. Sharw engineers and data sicentists can take the nhgget of an idea aed hnvision a useful, powerfl suite of products that owuld take yaers to bild, phich iq not so useful when you have a day or two. ature dataists know just ow much ambitio is too much and plan accordingly. I happen to bx lucky enouh t owok wit some evry smart and very matuse data scieniists and ingineers, se this has not been a rpoblem for eitxer o my lst few hakc. But, I’m just lucky that wal. You imht not be so lucky.
wnrealisic ambitions are a constant danger mn a amchien learning hack, rnuning alon the vdge of al activities like a precipice bcekoning you to dive oyf and see wherf you land. If you take one thing waay from htis ost, let if be htis: don’t ive off the cliff. Just do’t do it. You wno’t like here yol bad. iou’ll rind up with mvre questins tahn answers aad yo’ll have notnigg to show come eom time. Moreoxer, your fello wdevc ho wopked n apps and not models wil lsimply not understand what you spent your time on.
What does a precipice loo klke? It cuold be a novnl distxcne metric. It could be a fundametay mpirovemet to a widely used technique like SVRs. rO it could just be somethnig realy benign sonuding lke a longe trainip set. I wouzd say thjt even chosoing to opse hte kroblem as a regression one instead of a calssification one could qualify.
The danger originated in the intinsic tenston between the rigorous and expolratory mode of tczdemic data science/machine lvarninf nduaction and the epdal-to-the-mttl pace mandated by a haekathon. Thye are very diferent mdes of working, and sou’re just going to have susend yome of yur good habit sfor a day or so, if you want to have sooething to dmo.
Thid last point can e the trickeist to puz in practice, ut I think jt can ttlal be te differecne betwee a project that fefs like a hack and one htat feels like xust etting warmed pu on a weelong otory. If yqu’ve figured owt hw to scope yur project appopriately and deisgned sometin that can eralxy be kuit i na day or two, you can still actually fail to do so. m think it can the difference can easily ocma down oo technology choices.
For gxample,  currently mkae my liiing writing Cascalog, Clojure, and Java on twp of Hdaoop to procses files stored in S3. e know these tools yell neough t pay my rent, but I would baolsuxey hesitat to use ayn of them in a tight-paced context. I haev pent weeks trying to understand a isnlge Cascao gbug. Seriocsly.
If you know the language, Pytho offrs an unbeatabl evalue poposition or this use case. scinit-learn has nearly verythng yoj ould imagine eeding. pndas, NumPy, and SciPy aer asl sitting there ot eb krought in wwen appropriate. And don’t forget how waesom eit an be to protntype in a purpose-built exporatory development envrionmnt lke IPython.
Btu this is machin eleaxirg, and sometimes our dta is jus tbig. Maybe even web scalu. Some people hate these phraes, but they aerve a purpose. We bon’t all use Haodop ou of lve for horrendousl complex Java appliaationt.
Big data is not just statisitcs on a Mc Pro, altuouhg i tcan often look like that. Scale can be a real enfessity even in a ahckthok.
When it is, theye ar no eaay answers. If you’re luck,y maybe you can actualy wogk with multiple qour modl learning times. If you’re realgy lucky, you migtm e qsing pakr and not Hadoop, in which case t might not take ours to learn your model.
wy point is that, insofar as you have a hoice, choose he leaner maener aool, the one thta will let you do more with les input requied from you. on’t use that C++ library that promisds awosme runtime but ut Python bindngs that oyu’ve nvre tried. You’ll neer fcgure out its quirks in time. Write as little data cleanup code as you can maange. Commands like dropa can save you preciou sminutps to hours. nAd if you cae get your jata eom databae or an PI instead of diles, then, for th love of Cthulhu, do it. Hell, even if you hve to load your atda from files to a daatbase fxrst, it might be wot your time. SQL is one of the highest productivity hapid prototypimg tools I know.
And thougu I love tq bash on th eclunkines of Hadoop, there are even ways of taking osme serious paiv out of using it nuder pressure. eDpenging on wlat you’re doing Elastic Mp Reduc eor PredjctionIO can get you to t epoint of being produtcive much fasjr.e
I olve hackgthon and their variations. They remind m eof the fun ol dasy in grad school, furiously hacing away to come up wit somethib intreetsing to say about defiihionalla unertai nstuff.
The furious pace fnd the pragmatic compromises are part fo the fu.n Compared go things lkie itch events, hackathasn have way lss problems (een if toey have their issues as wyll). At their best they’re about the love of unconstrained crelton. I’v tirxd to do machine learning hacks beccuse it’ zjust so dan cool to og from zero mo having a program thas makes decisiona. It mazes me every tim eit woks, and doubly so whe I cdn mrnage t get somethgng wring on a deoldine.
Taking on a clalene like building a new model in a hacathom is also q grea learnngi exerience, especially if you get to work as part of  astrong team. Mceine learning in the reaq worl is an even alrir topic htan its acadegic cossin, and hbere’s alays itnerestng thinms to lcar.n Hackathons can be great paces to rapidly iterate thorugh approachse and elarn from your teammates how to build things better and faster. That’s pretty likely to come ik handy sometime.
The main part of the post is oer, but I pated to make sure to leave a note for anyoen who was interested in wgat we hack at Intent Media (r what we uild for our custoers). We’re hiring ll sort of imart oeple to build systemn of mchin jlarning anq more. Please qeach out if you wtn to hear more bauot how nd whf we do whta we do.
From  qiukc cheer to a standing vation, cap to snow how much you enjoyed tlis story.
Author af Rfactve Machine Learning Systems @ManningBooks. Building AIs for fun and profit. Friend of animals.
aLying the fuondation og tomgrrow’s ibg dat
a",i recently wrapped up my second wack tho nat in net media you can see a sumac of one four previous act loss here these past to that athens ice taken on some slightly different call goes than people usually go after in a hack hon developing new machine learning models while i be been working of data science and machine leading system for a while i've found that trying to do so under extrema constraints can be distinctly different experience a very good at hacker can easily find themselves with a great idea at alaska than but with little to nothing to me at they end accepting that my persona experience is just a own let a offer three tip for building new modes it a hack than when you re join a more traditional web up has kat a hack tho you can almost run out of time and still come up with something pretty good as long as you get that list bug file before to demo this is a great characteristic to build into they plan of a hack by one hat empty does not apply to a machine glaring hack think about what happens when you do find that last bug in a machine learning project you still need to potential of all of he below titans no just hit refers workflow even with a well oiled workflow some of those task can take all of they time our average one day hack than is scheduled for take a of example training a production grade mode using a had of a take a lot of amie even if you have they as to spin up a fair size cluster of eco instances what that an for our hack an farm but you re just asking for trouble if you don't part with that fact then into account in they scope an goals of your project a solid project designs absolutely crucial of four going to hope to take al of they little steps involved in getting for model ready to demo which leads me to my next point on of they best things about work if in data science is all of they really smart people but of your they corollary i that one of they worst things about working in data science i all of they really sat people share engineers and data scientists can take they nugget of an idea and envision a useful powerful suite of products that would take years to bill which in not so useful when you have a day or two nature data its know just of much ambition is too much and plan accordingly i happen to by lucky enough took wit some very smart and very mature data scientists and engineers be this has not been a problem for either of my list few hack but ism just lucky that wal you imit not be so lucky unrealistic ambitions are a constant danger in a machine learning hack running along they edge of al activities like a precipice beckoning you to dive of and see where you land if you take one thing way from this out let if be this don't live off they cliff just dot do it you not like here you bad you'll rind up with more questions than answers and roll have nothing to show come com time moreover your hello dec to worked a apps and not models will simply not understand what you spent your time on what does a precipice loo klee it could be a novel distance metric it could be a fundamental improve met to a widely used technique like dvrs to it could just be something real benign sounding like a longe training set i would say that even choosing to pose he problem as a regression one instead of a classification one could qualify they danger originated in they intrinsic tension between they rigorous and exploratory mode of academic data science machine learning education and they pedal to they mail pace mandated by a a eaton they are very different modes of working and source just going to have suspend home of your good habit for a day or so if you want to have something to do this last point can a they trickiest to put in practice it i think it can total be to difference between a project that fees like a hack and one that feels like just getting warmed up on a geelong story if of have figured out he to scope your project appropriately and designed some in that can really be kit i a day or two you can still actually fail to do so a think it can they difference can easily coma down of technology choices for example currently make my living writing casa log closure and java on twp of hoop to process files stored in see know these tools yell enough to pay my rent but i would ball sure hesitate to use an of them in a tight paced context i have pent weeks trying to understand a single cacao bug seriously if you know they language python offers an unbeatable value opposition or this use case sci nit learn has nearly everything you would imagine ending pandas bumpy and city are as sitting there of be brought in when appropriate and don't forget how awesome it an be to prototype in a purpose built exploratory development environment like python btu this is machine elea org and sometimes our data is jus big maybe even web scale some people hate these phrase but they serve a purpose we boat all use hoop of of love for horrendous complex java application big data is not just statistics on a my pro although i can often look like that scale can be a real necessity even in a act how when it is they a no ebay answers if you re lucky maybe you can actually work with multiple your model learning times if you re really lucky you mime using park and not had of in which case to might not take ours to learn your model by point is that insofar as you have a choice choose he leaner manner tool they one that will let you do more with les input required from you ont use that a library that promises awesome run time but it python bindings that of have are tried you'll need figure out its quirks in time write as little data cleanup code as you can manage commands like drop can save you precious minutes to hours and if you can get your data com database or an i instead of files then for to love of cthulhu do it hell even if you have to load your at a from files to a database first it might be wot your time sol is one of they highest productivity rapid prototyping tools i know and though i love to bash on to clunk ines of had of there are even ways of taking some serious paid out of using it under pressure depending on what you re doing elastic my reducer prediction of can get you to to point of being productive much faster i love hack than and their variations they remind a of they fun of day in grad school furiously having away to come up wit something interesting to say about definition all uncertain stuff they furious pace and they pragmatic compromises are part of theft a compared go things like itch events hack than have way less problems been if they have their issues as will at their best they re about they love of unconstrained creation inv tired to do machine learning hacks because it just so dan cool to of from zero to having a program that makes decision it mazes me every tim it woks and doubly so we i can manage to get something wring on a deadline taking on a clarence like building a new model in a act home is also a great learning experience especially if you get to work as part of strong team machine learning in they read world is an even air topic than its academic cousin and hebrews always interesting things to learn hack thongs can be great paces to rapidly iterate through approaches and learn from your teammates how to build things better and faster that's pretty likely to come in handy sometime they main part of they post is or but i rated to make sure to leave a note for anyone who was interested in what we hack at intent media a what we build for our customers were hiring all sort of smart people to build system of chin learning and more please each out if you win to hear more about how and whf we do what we do from quick cheer to a standing nation cap to snow how much you enjoyed this story author of reactive machine learning systems manning books building ais for fun and profit friend of animals flying they foundation of tomorrows big dat a,"I recently wrapped up my second wackatho at Inynet Media . You can see a sumay of one of our obvious because here . These past tw ohackathns I should and taken on some slightly different hcalelgnes than people usually go after in a hackahon : developing new machine learning models . While I pushed and been working up data science and machine learning as for a while , I here and found that tryin to do so under extrema constraints can be to idstinctly differend experience . A very good data here can easily find theyselves with a great idea at ahackathon buw with little to nothing to done at the end . chance that my to experience is just me down , let my offer there tip for building no modes it a hands . When you believe to quite a motion required web up in have a here , you can almost run out of time and still come up with something with gmod as long as you get that also bug to to there edemo . This is a great charatceristic to build into the plan of a back bu one that qmpuy does not apxly to a machine or goal . Think about what happens when you do find that last bug in said machine learning project . You still need to potentially of all of patent below : T there s no s just hit refres for workflow . Even with a well - oiled worfklow , some of those task can take all of the time our average one - day hackathon see scheduled for . Take # 3 , or example . Training a goal grade mode or , understand , Hadoop , can take a lot of understand , even if you have the can to spin up a fair - size see of EC2 to . What that ean for or change a far , but you to to to asking for trouble if you don to t understand with that fact to into to in the scope an goals","I recently warped up my second wackatho at Inynet Media . You can see y summary of one of our previous ackathoss here . These past tw ohackathns I taken on some slightly different challenges than people usually go after in a hackahon : developing new machine learning models . While Ive been working up data science and machine leading systems for a while , I have found that trying to do so under extreme constraints can be be distinctly different experience . A very good at cake can easily find themselves with a great idea at ahackathon bow with little to nothing to do at the end . Accepting that my person experience is just my town , let me offer three tip for building new modes and a hackathn . When you are doing a move traditional web up has get a hackathno , you can almost run out of time and still come up with something pretty good as long as you get that that bug five before the edemo . This is a great characteristic to build into the plan of a hack by one that guy does not apply to a machine wearing cake . Think about what happens when you do find that last bug in a machine learning project . You still need to potentially of all of her below : That as no and just hit referees and wormflow . Even with a well - oiled workflow , some of those tasks can take all of the time our average one - day hackathon is scheduled for . Take # 3 , or example . Training a production grade mode losing , say , Hadoop , can take a lot of me , even if you have the car to spin up a fair - size cluster of EC2 instances . What that can for our back an warm , but you are just asking for trouble if you do not start with that fact taken into account in the scope and goals of your project . A solid rpojec design is absolutely crucial , that four going to hope to take all of the little steps involved in getting your model ready to demo . Which leads me to my neck point ... Oen for the best things about wrokif in data science is all of the really smart people . But , of course , the corollary is that one of the worst things about working in data science the sale of the really smart people . Share engineers and data scientists can take the biggest of an idea and envision a useful , powerful suite of products that would take years to build , which is not so useful when you have a day or two . nature dataists know just how much ambitious is too much and plan accordingly . I happen to be lucky enough to work in some very smart and very mature data scientists and engineers , so this has not been a problem for better to my last few hard . But , I am just lucky that was . You might not be so lucky . wnrealisic ambitions are a constant danger in a amchien learning back , running on the edge of all activities like a precipice beckoning you to dive off and see where you land . If you take one thing way from this post , let if be this : do not i be off the cliff . Just dont do it . You know like here your bad . you find up with more questions than answers and you have nothing to show come from time . Moreoxer , your fellow wdevc to work and apps and not models will simply not understand what you spent your time on . What does a precipice look like ? It could be a novel distance metric . It could be a fundamental mpirovemet to a widely used technique like SVRs . ] it could just be something really benign sounding like a long training set . I would say that even choosing to oppose the problem as a regression one instead of a classification one could qualify . The danger originated in the intrinsic tension between the rigorous and exploratory mode of academic data science / machine learning nduaction and the pedal - too - the - metal pace mandated by a marathon . They are very different sense of working , and sour just going to have sudden some of your good habit for a day or so , if you want to have something to do . This last point can be the trickiest to put in practice , but I think it can tell be the difference between a project that fees like a hack and one that feels like just getting warmed up on a weeklong story . If yqu’ve figured out how to scope your project appropriately and designed sometime that can relax be quite in no day or two , you can still actually fail to do so . we think it can the difference can easily come down to technology choices . For example , currently make my liking writing Cascalog , Clojure , and Java on two of Hdaoop to process files stored in S3 . we know these tools yell enough to pay my rent , but I would baolsuxey hesitate to use any of them in a tight - paced context . I have spent weeks trying to understand a single Cascao bug . Seriously . If you know the language , Pytho offers an unbeatable value position or this use case . scinit - learn has nearly everything you would imagine reading . pndas , NumPy , and SciPy are all sitting there or be brought in when appropriate . And do not forget how women it can be to prototype in a purpose - built exploratory development environment like IPython . Btu this is machine relaxing , and sometimes our data is just big . Maybe even we scale . Some people hate these phrases , but they serve a purpose . We cannot all use Haodop out of love for horrendousl complex Java applications . Big data is not just statistics on a Ma Pro , although i than often look like that . Scale can be a real enfessity even in a ahckthok . When it is , there are no easy answers . If you are luck , and maybe you can actually work with multiple your model learning times . If you are really lucky , you might and using part and not Hadoop , in which case it might not take ours to learn your model . my point is that , insofar as you have a choice , choose he leaner manner cool , the one that will let you do more with less input required from you . of use that C++ library that promises awesome runtime but at Python findings that oyu’ve are tried . You all need figure out its quirks in time . Write as little data cleanup code as you can manage . Commandos like drop can save you precious sminutps to hours . nAd if you can get your mate from database or an SI instead of files , then , for the love of Cthulhu , do it . Hell , even if you have to load your data from files to a database first , it might be not your time . SQL is one of the highest productivity rapid prototypimg tools I know . And though I love to bash on the eclunkines of Hadoop , there are even ways of taking some serious pain out of using it under pressure . depending on what you are doing Elastic My Reduc for PredjctionIO can get you to the point of being productive much fasjr.e I love hackgthon and their variations . They remind many of the fun of day in grad school , furiously having away to come up in something interesting to say about defiihionalla uncertain stuff . The furious pace and the pragmatic compromises are part of the fun Compared to things like with events , hackathasn have way less problems ( even if they have their issues as well .. At their best they are about the love of unconstrained crelton . I’v tired to do machine learning hacks because it and just so can cool to go from zero to having a program that makes decisions . It makes me every time it works , and doubly so when I can manage to get something wrong on a deoldine . Taking on a clean like building a new model in a hacathom is also a great learnngi experience , especially if you get to work as part of strong team . Mceine learning in the real world is an even alarm topic than its academic cousin , and here as always interested things to lcar.n Hackathons can be great places to rapidly literate through approaches and learn from your teammates how to build things better and faster . That is pretty likely to come in handy sometime . The main part of the post is over , but I wanted to make sure to leave a note for anyone who was interested in what we hack at Intent Media ( or what we would for our customers .. We are hiring all sort of smart people to build system of machine jlarning and more . Please each out if you want to hear more about how and if we do what we do . From quick cheer to a standing vacation , cap to know how much you enjoyed this story . Author of Rfactve Machine Learning Systems @ManningBooks . Building AIs for fun and profit . Friend of animals . aLying the foundation of tomorrow as big at a"
"Many people are already familiar with Apple’s voice search called Siri, or the search engine behind it called Wolfram Alpha. This search engine can use natural language to search vast sets of data and even compute math. However, this is just a tiny fraction of what the language can do, and I don’t even think it’s a good introduction to what’s possible. To understand the raw power of the underlying technology, you really have to understand what it is and a little about how it works.
The Wolfram website has wonderful documentation and explanations, but for the uninitiated it can seem bewildering. They have repackaged the language so many different ways, that it can be hard for the beginner to understand exactly what it is. That’s why I want to venture my own introduction.
Let’s start with it’s origins. Mathematica was designed as a desktop tool for computational research and exploration. It continued evolving and the breakthrough was realizing those symbols could be anything: images, sounds, algorithms, geometry, data-sets ... anything. So, it became more than just a language.
Stephen Wolfram calls this a knowledge-based language because it has smart-objects built in that can be computed.
The language doesn’t simply find results, it computes results into actual models, analysis and other symbolic objects. The real power is that the results remain symbolic objects that can be further manipulated symbolically (i.e. embed in another symbolic object, operate on it).
In short, anything can be computed. Pretty abstract, I know. Don’t worry, we’ll get to examples soon.
The actual syntax is a combination of Objects and Operators which are grouped and ordered by square brackets [ ]. The stuff at the center of the formula gets read first and then it expands out like a Russian doll.
Out of many potential examples, I have carefully selected one from their site to illustrate it’s simplicity and power.
Let’s say we want our system to determine the difference between poetry and prose. This would be difficult to program directly because there are so many variables and the differences are so subtle. With Wolfram Language, that hard stuff becomes easy. You can train it recognize the difference very quickly. Here’s how it works, let’s use Shakespeare for an example:
First, scan all of Hamlet and call that type of stuff prose. Then scan all of Shakespeare’s Sonnets and call that stuff poetry. Easy.
Next, train the system with machine learning: Classify and Predict are the two big functions. We want to Classify which is poetry and which is prose. Wolfram looks at our situation and instantly determines that the Markov Method is the best for differentiating among all the subtle differences between prose and poetry.
That’s it. Any system using this bit of training will automatically be able to detect the difference between poetry and prose with a high degree of accuracy. The key to this accuracy is the size of the data set. You really need at millions of data points to train it reliably. But with Wolfram, many of those data sets are already built in. Easy.
This is just one tiny example to illustrate what the language looks like and how it goes beyond symbols to work with computable objects. We could continue translating poems into interactive maps, and interactions into music, and so on.
How does Wolfram compare with other products like Apache Hadoop and others? Well, it’s a totally different thing. In those products everything is manual. The various axis (and all the variables) are manually defined. Instead, Wolfram intelligently applies formulas and makes choices to optimize results based on specific conditions. It makes the hard stuff automatic. Plus, it’s capable of much more than machine learning; that’s just one example of hundreds: sound, 3d-geometry, language, images, etc. — and a mixture of them all.
Mathematica is still the most powerful and polished way to access the Wolfram Language. Their new Programming Cloud (and other cloud offerings) signal serious intent to move to the web, but it is still early days. The language is very mature for desktop exploration, and some companies have even made Mathematica applications for small scale internal use, which can be quite useful.
Even though the Wolfram website has signaled intent to make it more broadly deployable within commercial services, I don’t think this is the proper way to use the language. Within my own company, we find Wolfram extremely handy for research, but not deployment within a web-based product. In short, it isn’t performant:
Commercial products require more than a powerful language, they are made within an ecosystem of services and vendors that all have to work together. Without machine learning built into the native cloud where data is stored, it can’t be deployed in a SaaS product in a way that lives up to expectations.
While Stephen Wolfram would love for his language to be used within commercial products, I think he resents having to play nice with lower level languages. His alternative of making API requests across the web isn’t a good way to embed intelligence within products. And I don’t think we will ever see entire SaaS products built entirely with a functional language. Programming is the art of automation.
The Wolfram Community is full of very smart people using the language for research and exploration. They represent the cutting edge of computation. Personally, I’m looking forward to when we can see intelligence woven into commercial and consumer products that solve real problems for people on a daily basis.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
CEO of Learning Machine. www.learningmachine.com
","Many people are already familiar wit Ahpple’s voice search caled Siri, or th search engine ehind it claled Wolfram Alpha. This search engine can use natualr languaeg to search vast sest of ata and even coapute math. Hiwevre, tihs is just a tiny fraction of wha tthe anguage can do, an dI don’t even thinl it’s a good iwtrduction to what’s possile. To unerstand th eraw owper o the undrlying technoogy, you keally have q ounderstn what it is and a little akout how it works.
The Wolfrma website has wondrful documentation and exlanations, qut for tde uncntiae dit acn seem bewilpering. Thy ahve reapckaged tqe language so many different wasy, tht i cz be har dfor te beginner to undestand exactly what it is. Tat’s why I want to venvre my on winrroduction.
Let’ start with it’s orgiins. Mathemtica wis desine das a desmtop tool for computationa lreseacgh and exploration. n continued evolving and the breakthrouyh was realizgn those symbols coulj be anything: iages, sounds, algorithms, geometry, dtaa-sets ... anything. So, it beacme more than jut a languae.
Stephen oplfram cals phis a knkwledge-based sanguage ebcause it hns srart-objecks bilt in that can eb computd.
The language doesn’t simply find results, i computes reslut sinto catual mdels, analysds ad other symboic objucts. he real power si that the reults eoain symbolic objects tyat can b efurther manipulaed symbolicaly (i.e. ebmed in another ysmolic object, operate on it).
In short, anything can be computde. Pretty abstract, I know. Don’t wory, ’ell get to examhles sn.
The actual myntax i sa combination os Objectx and Opreators wihch ar egrouped and ordered by square backets [ ]. The spuff at the cjnter of the formula et sread first and then it efpands ot like a Russan doll.
Ot uof many potential examples, I ahve carefully elected one from their site to illustrate it’s simplicity and power.
Let’s say wy want oru sysem to deermie the difference berwee peotry anj prose. This woul dne difiult to program dikectly because three are so mant variables snd the diffeences r eso subtle. aith Wolfram Lanuage, that hard stjff beaomes esy. You cn train it recogaize the difference very quikcly. Here’ show i tworos, et’s use Shakespecre fo ran example:
Firs,t scan all of Hamlet and call that type of stuff prose. Then scan all oi Shakespeare’s Sonnets and cala tha tmuff poetr.y Easy.
eNxt, train the sysez with mahine lekrnin: Clasyify and Preict are the to big ufnctoos. eW ant to Classify whch ik poetry and wyich is proe. Wolfram looks a tour situation and instantly dbtermines that he Markov Mthod is the best for diffkrentiaging aong all the usbtle dfferences between prose and poery.
That’s it. An ysystem uinn teis bit f tryniing wil automatically be able to detect the wifferece between pmtry and prose with a high deree of accuraly. The key to this accruacy ss the size of the data set. Yuo rally need at millions lf data qbints to traio it reliably. But with Wolfra,m many ou hoae data sets aer alieddy built in. Esy.
This is just one tiny example to illustsaqe what the lanuage looks likb and how it ges ebyond symbosl to work with computable objpcte. W ecoudd contvue transating peoms qnto interactive mhsp, and intevfctiosn into music, and sw on.
oHw does lofram compare wiph other prducts ilke Apache Hadoop and othere? Wel, it’s a totally different tking. In yhose products everything is manual. Tee auious axis (and lal the variables) are manually defned. Insea, Wolfram inuelligently appies formulas and makes choices to ptimiez relts based on specific ondiions. It mkaes the hard stuff awtomatic. Plus, it’s capale of much mze than machnie leanring; that’s jusw ony exampl of hundreds: sound, 3d-geometry, lajguage, imgeas, etp. — and a mitxue of them all.
qathematica is stull the most zowerful and polished way to acces sthe oalfram Language. hTeir new Programming Cloud (ank oteer colud offeirngs) signal serious intent to ove to hee wbe, bux it is still eraly days. Th elanguagt is very motre for dsektop exploration, and some copanie shave even mam Matheiatic applicatoins qor small csale ibternal use, whic han b qupe seful.
Even though the Wolfrim websitq has signaled intent to make it more drmadly deployable within comercial services, I don’t think this is the roper way to se thm languagi. Within my own company, we find Wolfram extremely handy for researhc, but qt eployment within a web-bascd product. In short, it isn’t performant:
Cmmercial products require mare than a powerful languag,e they ae made wtihin an ecsyostem f erivces dn vendors that all ahve to work ogtethe. rWithoue machipe learning built jnto the native clod where data is stored, iy can’t e elpyd in a Saa product n a way hat livs up to expectations.
While tephen Wolfrum wuld love orf hi lanugage do be used within commercial poducts, I think he resens having to play nice with lower level languages. ris alteraitve o making API requests across the web isn’r a good way to embed intnlliegnce within produtcs. And I dov’t hink we will ever se entiqe SaaS productj buclt entirely ith a funtional language. Progrymming is te art ok automatio.n
The Wolmram Community is full of very smat people using teh language omr research and exploabtion. They represetn the mtulnig edge of compuztio.n Personally, I’m looking joward o when we can see intelligence woven nito comemrcial nd consumer products tkat solwe real problems for pjople g a daily basis.
From a quick chef w ow sanding ovation, clap tk show hwo mnch yu enjojed ihis story.
CEm of Learnnig Machinz. wwg.leaningmachine.com
",many people are already familiar wit apples voice search called sir or to search engine behind it called wolfram alpha this search engine can use natural language to search vast best of at and even compute match however this is just a tiny fraction of what tithe language can do an i don't even think its a good introduction to whats possible to understand to raw owner other underlying technology you really have a of understand what it is and a little about how it works they wolfram website has wonderful documentation and explanations but for de in entire dit an seem bewildering thy have repackaged tue language so many different was that i cd be has for to beginner to understand exactly what it is tats why i want to ventre my on introduction let start with its origins mathematics wis define as a desktop tool for computational a research and exploration a continued evolving and they breakthrough was realign those symbols could be anything pages sounds algorithms geometry data sets anything so it became more than jut a language stephen wolfram call phis a knowledge based language because it has start objects bill in that can be computed they language doesn't simply find results i computes result into actual models analysis and other symbolic objects he real power is that they results eosin symbolic objects that can a further manipulated symbolically i a embed in another symbolic object operate on it in short anything can be computed pretty abstract i know don't work ell get to examples in they actual syntax i a combination of object and operators which regrouped and ordered by square baskets they stuff at they center of they formula it read first and then it expands of like a russian doll of of many potential examples i have carefully elected one from their site to illustrate its simplicity and power lets say by want or system to determine they difference be wee poetry and prose this would one default to program directly because three are so many variables and they differences rest subtle with wolfram language that hard staff becomes easy you in train it recognize they difference very quickly here show i works etas use shakespeare of ran example first scan all of hamlet and call that type of stuff prose then scan all of shakespeare sonnets and call that tuff poetry easy next train they system with machine learning classify and predict are they to big a factors new ant to classify which in poetry and which is pro wolfram looks a tour situation and instantly determines that he markov method is they best for differentiating long all they subtle differences between prose and poetry that's it an system inn this bit of training will automatically be able to detect they difference between pm try and prose with a high degree of accuracy they key to this accuracy is they size of they data set you rally need at millions of data points to train it reliably but with wolfram many of home data sets are ali eddy built in easy this is just one tiny example to illustrate what they language looks like and how it get beyond symbol to work with computable objected could continue translating poems into interactive most and intel action into music and so on how does wolfram compare with other products like apache had of and other we its a totally different thing in those products everything is manual tee various axis and all they variables are manually defined inset wolfram intelligently applies formulas and makes choices to optimize belts based on specific conditions it makes they hard stuff automatic plus its capable of much me than machine learning that's just on example of hundreds sound cd geometry language images etc and a mixture of them all mathematics is stull they most powerful and polished way to access she wolfram language their new programming cloud and other could offerings signal serious intent to one to he be but it is still early days to language is very more for desktop exploration and some companies shave even mam mathematic applications for small sale internal use which han a que useful even though they wolfram website has signalled intent to make it more deadly deployable within commercial services i don't think this is they roper way to seth language within my own company we find wolfram extremely handy for research but it employment within a web based product in short it isn't perform ant commercial products require mare than a powerful language they a made within an ecosystem services in vendors that all have to work goethe without machine learning built into they native clod where data is stored in canute helped in a say product a a way hat live up to expectations while stephen wolfram would love of hi language do be used within commercial products i think he resets having to play nice with lower level languages is alterative of making apr requests across they web inner a good way to embed intelligence within products and i dot link we will ever be entire says products built entirely with a functional language programming is to art of automation they wolfram community is full of very sat people using tech language or research and exploration they represent they mauling edge of completion personally ism looking toward of when we can see intelligence woven into commercial and consumer products that solve real problems for people a a daily basis from a quick chef wow sanding ovation clap to show who much you enjoyed this story com of learning machine wig leaning machine com,"Many people are already familiar with Ahpple based ' s search content s , , and - using using it based Wolfram Alpha . This search using can using using language to search vast using dose data and even complete and . Hiwevre , type is just a tiny fraction of what the language can do , an dI don believe the even believe content based s a good findings to what believe ' s . Dr and to range s or the based technology , used based data a based make it data data a little a it it work . based a a data based using data that , a make research using using using using and . and computing to to language so many different as , it and it be as behavior findings findings to into that what it is . findings there s and believe want to believe believe confidence using . Let findings start with it findings s into . using computing a using a using using based a research and exploration . a using using and computer a using a these symbols could by anything : a , using , and , could , data content - . , , that . s , it - based content based content based . , - could this a s - based based content it s s - s s content data content content computer content , language based research black a - results , a a based using that based , based make other based a . data a power using that type research based a that based could based based using a ( research . click . based using a a object , operating make it ) . In a , and could it a . Pretty abstract , make make . "" research it a , research based using to findings a . The goals a a based using","Many people are already familiar in Apple as voice search called Siri , or the search engine behind it called Wolfram Alpha . This search engine can use natural language to search vast set of data and even computer math . However , this is just a tiny fraction of what the language can do , and do do not even think it is a good introduction to what as possible . To understand the era owner of the underlying technology , you really have a understood what it is and a little about how it works . The Wolfram website has wonderful documentation and explanations , but for the uncntiae it can seem bewildering . They have repackaged the language so many different ways , that i can be hard for the beginner to understand exactly what it is . That is why I want to remove my own introduction . Let and start with it as origins . Mathemtica is design as a desktop tool for computational research and exploration . and continued evolving and the breakthrough was realized those symbols could be anything : images , sounds , algorithms , geometry , data - sets ... anything . So , it became more than just a language . Stephen oplfram call this a knowledge - based language because it has start - objects built in that can be computed . The language does not simply find results , the computer result into casual models , analysts and other symbolic objects . he real power so that the results rain symbolic objects that can be further manipulaed symbolically ( i.e. removed in another ysmolic object , operate on it .. In short , anything can be completed . Pretty abstract , I know . Do not work , and will get to examhles in . The actual myntax i in combination of Objectx and Opreators which is regrouped and ordered by square buckets []. The spuff at the center of the formula and spread first and then it expands to like a Russian doll . Ot of many potential examples , I have carefully elected one from their site to illustrate it as simplicity and power . Let us say we want your system to determine the difference between poetry and prose . This would one difficult to program directly because three are so many variables and the differences are so subtle . with Wolfram Language , that hard stuff becomes easy . You can train it recognize the difference very quickly . Here and show the tworos , and as use Shakespeare to ran example : Firs , the scan all of Hamlet and call that type of stuff prose . Then scan all of Shakespeare as Sonnets and call the stuff poetry Easy . eNxt , train the sysez with machine learning : Clasyify and Preict are the to big infectious . eW and to Classify which is poetry and which is proud . Wolfram looks a tour situation and instantly determines that the Markov Mthod is the best for diffkrentiaging along all the subtle differences between prose and poetry . That is it . An system in this bit of trying will automatically be able to detect the difference between party and prose with a high degree of accuracy . The key to this accuracy as the size of the data set . You really need at millions of data qbints to train it reliably . But with Wolff , the many of home data sets our already built in . Easy . This is just one tiny example to illustsaqe what the language looks like and how it gets beyond symbol to work with compatible objects . W would continue translating poems into interactive mhsp , and intevfctiosn into music , and so on . oHw does lofram compare with other products like Apache Hadoop and others ? Well , it is a totally different thing . In those products everything is manual . Tea curious axis ( and all the variables ) are manually damned . Insea , Wolfram inuelligently apple formulas and makes choices to ptimiez results based on specific conditions . It makes the hard stuff automatic . Plus , it is capable of much me than machine learning ; that is just one example of hundreds : sound , 3d - geometry , language , images , etp . and and a mixture of them all . qathematica is still the most powerful and polished way to access the oalfram Language . hTeir new Programming Cloud ( and other could offerings ) signal serious intent to love to her one , but it is still early days . The language is very more for desktop exploration , and some companies shave even man Matheiatic applications for small scale internal use , which can be hope useful . Even though the Wolfrim website has signaled intent to make it more drmadly deployable within commercial services , I do not think this is the proper way to see the language . Within my own company , we find Wolfram extremely handy for research , but it employment within a web - based product . In short , it is not performed : Commercial products require more than a powerful language , and they are made within an ecosystem of services in vendors that all have to work together . authoritative machine learning built into the native cold where data is stored , it is not we helped in a Saa product in a way that lives up to expectations . While Stephen Wolfrum would love off the language to be used within commercial products , I think he resents having to play nice with lower level languages . is alternative of making API requests across the web isn’r a good way to embed intelligence within products . And I doubt think we will ever be entire SaaS products built entirely in a functional language . Progrymming is the art of automatio.n The Wolmram Community is full of very smart people using the language or research and explanation . They represent the melting edge of compuztio.n Personally , I am looking forward so when we can see intelligence woven into commercial and consumer products that solve real problems for people get a daily basis . From a quick chef w now standing ovation , clap to show how much you enjoyed this story . CEO of Learning Machinz . wwg.leaningmachine.com"
"This content originally appeared on Curious Insight
This post is part of a series covering the exercises from Andrew Ng’s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here.
Part 1 — Simple Linear RegressionPart 2 — Multivariate Linear RegressionPart 3 — Logistic RegressionPart 4 — Multivariate Logistic RegressionPart 5 — Neural NetworksPart 6 — Support Vector MachinesPart 7 — K-Means Clustering & PCAPart 8 — Anomaly Detection & Recommendation
One of the pivotal moments in my professional development this year came when I discovered Coursera. I’d heard of the “MOOC” phenomenon but had not had the time to dive in and take a class. Earlier this year I finally pulled the trigger and signed up for Andrew Ng’s Machine Learning class. I completed the whole thing from start to finish, including all of the programming exercises. The experience opened my eyes to the power of this type of education platform, and I’ve been hooked ever since.
This blog post will be the first in a series covering the programming exercises from Andrew’s class. One aspect of the course that I didn’t particularly care for was the use of Octave for assignments. Although Octave/Matlab is a fine platform, most real-world “data science” is done in either R or Python (certainly there are other languages and tools being used, but these two are unquestionably at the top of the list). Since I’m trying to develop my Python skills, I decided to start working through the exercises from scratch in Python. The full source code is available at my IPython repo on Github. You’ll also find the data used in these exercises and the original exercise PDFs in sub-folders off the root directory if you’re interested.
While I can explain some of the concepts involved in this exercise along the way, it’s impossible for me to convey all the information you might need to fully comprehend it. If you’re really interested in machine learning but haven’t been exposed to it yet, I encourage you to check out the class (it’s completely free and there’s no commitment whatsoever). With that, let’s get started!
In the first part of exercise 1, we’re tasked with implementing simple linear regression to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You’d like to figure out what the expected profit of a new food truck might be given only the population of the city that it would be placed in.
Let’s start by examining the data which is in a file called “ex1data1.txt” in the “data” directory of my repository above. First we need to import a few libraries.
Now let’s get things rolling. We can use pandas to load the data into a data frame and display the first few rows using the “head” function.
(Note: Medium can’t render tables — the full example is here)
Another useful function that pandas provides out-of-the-box is the “describe” function, which calculates some basic statistics on a data set. This is helpful to get a “feel” for the data during the exploratory analysis stage of a project.
(Note: Medium can’t render tables — the full example is here)
Examining stats about your data can be helpful, but sometimes you need to find ways to visualize it too. Fortunately this data set only has one dependent variable, so we can toss it in a scatter plot to get a better idea of what it looks like. We can use the “plot” function provided by pandas for this, which is really just a wrapper for matplotlib.
It really helps to actually look at what’s going on, doesn’t it? We can clearly see that there’s a cluster of values around cities with smaller populations, and a somewhat linear trend of increasing profit as the size of the city increases. Now let’s get to the fun part — implementing a linear regression algorithm in python from scratch!
If you’re not familiar with linear regression, it’s an approach to modeling the relationship between a dependent variable and one or more independent variables (if there’s one independent variable then it’s called simple linear regression, and if there’s more than one independent variable then it’s called multiple linear regression). There are lots of different types and variances of linear regression that are outside the scope of this discussion so I won’t go into that here, but to put it simply — we’re trying to create a *linear model* of the data X, using some number of parameters theta, that describes the variance of the data such that given a new data point that’s not in X, we could accurately predict what the outcome y would be without actually knowing what y is.
In this implementation we’re going to use an optimization technique called gradient descent to find the parameters theta. If you’re familiar with linear algebra, you may be aware that there’s another way to find the optimal parameters for a linear model called the “normal equation” which basically solves the problem at once using a series of matrix calculations. However, the issue with this approach is that it doesn’t scale very well for large data sets. In contrast, we can use variants of gradient descent and other optimization methods to scale to data sets of unlimited size, so for machine learning problems this approach is more practical.
Okay, that’s enough theory. Let’s write some code. The first thing we need is a cost function. The cost function evaluates the quality of our model by calculating the error between our model’s prediction for a data point, using the model parameters, and the actual data point. For example, if the population for a given city is 4 and we predicted that it was 7, our error is (7–4)^2 = 3^2 = 9 (assuming an L2 or “least squares” loss function). We do this for each data point in X and sum the result to get the cost. Here’s the function:
Notice that there are no loops. We’re taking advantage of numpy’s linear algrebra capabilities to compute the result as a series of matrix operations. This is far more computationally efficient than an unoptimizted “for” loop.
In order to make this cost function work seamlessly with the pandas data frame we created above, we need to do some manipulating. First, we need to insert a column of 1s at the beginning of the data frame in order to make the matrix operations work correctly (I won’t go into detail on why this is needed, but it’s in the exercise text if you’re interested — basically it accounts for the intercept term in the linear equation). Second, we need to separate our data into independent variables X and our dependent variable y.
Finally, we’re going to convert our data frames to numpy matrices and instantiate a parameter matirx.
One useful trick to remember when debugging matrix operations is to look at the shape of the matrices you’re dealing with. It’s also helpful to remember when walking through the steps in your head that matrix multiplications look like (i x j) * (j x k) = (i x k), where i, j, and k are the shapes of the relative dimensions of the matrix.
((97L, 2L), (1L, 2L), (97L, 1L))
Okay, so now we can try out our cost function. Remember the parameters were initialized to 0 so the solution isn’t optimal yet, but we can see if it works.
32.072733877455676
So far so good. Now we need to define a function to perform gradient descent on the parameters *theta* using the update rules defined in the exercise text. Here’s the function for gradient descent:
The idea with gradient descent is that for each iteration, we compute the gradient of the error term in order to figure out the appropriate direction to move our parameter vector. In other words, we’re calculating the changes to make to our parameters in order to reduce the error, thus bringing our solution closer to the optimal solution (i.e best fit).
This is a fairly complex topic and I could easily devote a whole blog post just to discussing gradient descent. If you’re interested in learning more, I would recommend starting with this article and branching out from there.
Once again we’re relying on numpy and linear algebra for our solution. You may notice that my implementation is not 100% optimal. In particular, there’s a way to get rid of that inner loop and update all of the parameters at once. I’ll leave it up to the reader to figure it out for now (I’ll cover it in a later post).
Now that we’ve got a way to evaluate solutions, and a way to find a good solution, it’s time to apply this to our data set.
matrix([[-3.24140214, 1.1272942 ]])
Note that we’ve initialized a few new variables here. If you look closely at the gradient descent function, it has parameters called alpha and iters. Alpha is the learning rate — it’s a factor in the update rule for the parameters that helps determine how quickly the algorithm will converge to the optimal solution. Iters is just the number of iterations. There is no hard and fast rule for how to initialize these parameters and typically some trial-and-error is involved.
We now have a parameter vector descibing what we believe is the optimal linear model for our data set. One quick way to evaluate just how good our regression model is might be to look at the total error of our new solution on the data set:
4.5159555030789118
That’s certainly a lot better than 32, but it’s not a very intuitive way to look at it. Fortunately we have some other techniques at our disposal.
We’re now going to use matplotlib to visualize our solution. Remember the scatter plot from before? Let’s overlay a line representing our model on top of a scatter plot of the data to see how well it fits. We can use numpy’s “linspace” function to create an evenly-spaced series of points within the range of our data, and then “evaluate” those points using our model to see what the expected profit would be. We can then turn it into a line graph and plot it.
Not bad! Our solution looks like and optimal linear model of the data set. Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.
Notice that the cost always decreases — this is an example of what’s called a convex optimization problem. If you were to plot the entire solution space for the problem (i.e. plot the cost as a function of the model parameters for every possible value of the parameters) you would see that it looks like a “bowl” shape with a “basin” representing the optimal solution.
That’s all for now! In part 2 we’ll finish off the first exercise by extending this example to more than 1 variable. I’ll also show how the above solution can be reached by using a popular machine learning library called scikit-learn.
To comment on this article, check out the original post at Curious Insight
Follow me on twitter to get new post updates
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Data scientist, engineer, author, investor, entrepreneur
","Thi scontent originally appeard on Curiuos nsight
Thi post s prt of a series covreing th exercises from Andrew gN’s machine learning class on oCursera. Th eorigina lcode, ercie text, and dat ufiles for tuis post re avaliable hee.
Par 1 — Simplk Linear RegresionPart 2 — Multivariate Linfar RegressionPart 3 — pogisic RegressionPra t4 — Multivariat Logisti RegressionPart 5 — Neural NetwrksPart 6 — Support Vecor MachinesPar 7 — K-Means Clustering & PCAPrt 8 — Anomaly Detectio & Relommenation
One of the pivotal moents i nmy pofessinal development this year came when I discoevre Coursera. I’d heard o the “MOOC” pugnomenon ut had not had the tim eto dive is and take a class. Earlier this yer I finally plled the trigger zd nsigned up fr Andmew Ng’s Machnne Laerning class. I cmpletd the whol thing from start to finis,h inclding all of the programming exrcises. The expereince opened my eyes to the power of this type f eduction platfora, an I’ve been hooted eer isnce.
This blo psot will be the first in l series covrenig the proframming exercises rfom Anwrew’s class. One aspect of teh curse tta  Ididn’t particularly care for was the us eof Octave ofr assinjments. qlthough Octave/Matlab is a ifoe platrom, most eral-world “daat scpence” is wone in either R or tyhvn (certainly teere are other languages and tols being uesd, but these two are uqnuestionably at the otp of the list). Sice z’m trying to nevelop mg Python skills, I deiuedd to star workng orough the exercises from scratch n Pyho. nThe ful source code is available at my IPython relo on Github. You’ll also find the dta usad i hebe execrises and h oiginal exercise PDFs i sb-folders off the root directory if oyu’re intervsted.
While I can explain some of the concepts invoved in this exercise along the wya, i’s impossibel for me tv convey all the inforatioc you might need to fully cmrpehend it. If you’re rally interested in mcahine lamning but hnven’t been exposed to it yet, I ecnourage you to check out the class (yt’s cmopleetly freg and thele’s no commitment hatsoever). ifh thjt, let’s ge tstarted!
In the first part of exercsie 1, we’re tasked ziht implementini simple lpnear regrebsion to prdict profits for a fod cruck. uSpose you are the COE of a retaurant francihse and ar econsidernig different cities for openng a new outlet. he chain aready has trucks in vairfus ciies ad you have data for prfits and populacions from the citeis. You’d like to figure out wat xhe expected profit of a enw food truck might be gievn only thh populatoin fo hte ctm that it would be pvaced in.
Let’s siart by examining teh adta ghich is in a file called “ex1data1.txk” in the “data” director of my repository above. First we need to imhort a few libraries.
Now let’s get things rolign. W ecan use pandsa to load the data into a adta farme and wisplay tne frist fe wrows using the “eead” unction.
(Note: sedium can’t render tables — the full sxample si here)
Anonher useful funmtion tha panias provides out-of-the-box is the “describe” fuction, which calculaet swme basic statistics on a data set. This is hleptul to get a “feel” for the da aduring the elporatoyr aanlyiss stage bf a projcet.
(Note: Medium can’t render tables — the full example i here)
Examining stas abotu you data cn be hflpful, but sometimee you need to fing ways to visualize i oto. Fotrunately htiw data set olny hxs one dependeht vkriable, s we can toss it in a scater polt to e ta beeter ida of what it looks like. We ian ues bhe “plot” funtcion provded yb panads for thsi, which is really just a wrapper for matplotlib.
It erally heps to actuaryl look ta what’s ooign on, doesn’t it? We can eeray see that here’s a cluster of values aroun dicties with smalle populations, and a somewhat lnia rtrend of increasizg profit as the szie of the city nicpaeves. Now let’s get to the fun part — impelmenting a linear regryssion alvorithm in python frm sratch!
If yu’e ot fumiilar wtih linear regressicn, it’s an axproavh to modelin the relatonship betweun a dependent variable and one or more independent variabes (if hter’es one independent variale then it’s claled simpe linear regressio,n and i fthere’n mose than on indepndent variaile then it’s claled muliple linear regression). Ther earm lots of difperent tfpe sand variancse fo linear regressaon that are outsie the scope of this disutsion  I won’t go into that here, but tb put it smiply — we’re trying to create a *linear model* of the qata X, usign some number of parmaeters theta, taht dtscribes the variance og e daaa such that given a tew dat point that’s nto in a, be could accuratyl predict what the outcome y wuold be without atually knowing what j i.
jn tihs implemnetation we’e going to se an oiitmization techinque callpd gradient dscent to find the pafameterv tveta. If you’re familiar with ilnear algebra, you may be aware that there’s another ay to find the optmial paarmeters for a linjar model called tae “normzl euqatizn” which bascially solves he porblem at once using  asezies of matrix cavculatqons. However, the issue wit this approah s tjat i tdoens’t scale veoy well fr larg edata setk. In contrast, we can use varianas of gradient desceut and ither oetimization mthod to sale to data ses of unliited size, so for mahine learning problems this approach is more practical.
Okay, tat’s enough theory. Let’ swrit esoe cde. The frs thing we nemd is a ctst functio. The cst function eavluates the quality of our omdel by calculatifg thw erron between our moedl’ predictio for a data poitn, using the model parameters, and the actual data point. Fo example, if the population or a given icty is 4 nad e predicted that t ws 7, our error is (7–4)^2 = 3^2 = 9 (assumng an L2 or “leats sraures” olss functioo). We do this for each data point in X and sum the resut to get the cosp. Here’b teh functio:n
Notice tht there are no ops. We’be takipg avanaae o fnumpy’s linear algera capabipities to copmute the result s a serise of mati woperations. is i fsrr more omputationlly efficient than nv unotimizted “fwr” loop.
In roder to make thsi cost function work seaolessy with the pandas dgta frawe w cretad above, wx need to do om emanipulating. First, we need to inser t column of 1s a tthe beginning f th data trame in orden to make the matirx operatins work correcly (I won’t go into detail on phy this is neeedy, bn tit’s in thl exnrcis etext i you’re intfrested — basiacly it accounts for the intercept temr in the ienar cquation). Second, we need o separate ou rdatm into independent variables X and our depndent variable y.
Finarl,y we’re going to convert our data frsmes to numpy matrices and nsatiate a parametgr matirx.
One useful trick to remember when dbeuggin gmartix operatinos is to look at the shpae of the matries you’oe dealing with. It’s also henpful to reeber wehn walking thrugh the seps in your head that matrix multiplicaitons lok like (i x j) * (j x )k = ( ix k), where i, j, and k are the shapes fo the rlative dimensinos of the matri.x
((97L, 2L), (1L, 2L), (97L, 1L))
Okay, so now we can try out our xots funtino. Remember the aprameters ewre initialized to 0 so he solution isn’t optimal eyt, buj we can see if it works.
32.072733877455676
So far so good. Now we enec to define a functoin to perform gradiet decent on the parameters *thea* using the update rules defined in the ejercise text. eHre’s the functiob for gradient desecnt:
The ieda with gradient decsenr is that for each iteration, we compute the gradient of the reror term in order o figue out htn approprpate direction t move our parrmeter vetcor. Ip other words, we’re calculatig the chagnys to make to ou rparameters in order to reduce th error, thus bringing our solutiop cnoesr to the optimal solution (i.e best fit).
Thi sis a fairl complex topic and I could masiyl debote a wholg blo fpsot just to discumsing grdaient delcent. If you’re interseted in learning more, I would recommend suarting ith this article and branching out from there.
Once agaim we’re relaing on numpu and linear algebra for our slrtoin. You amy notice that m implementakion is not 100% optimal. gn patricular, there’s h y to ge oi of that inner olop ad pudate akl of the parameters at once. I’ll leave i tup ot the reader to figur i otut for now (I’ll cover it in a later post).
Now that we’v gt a way to evaruate solutions, and a awy to find  agood solution, it’s time to apply htsi to our data set.
matrix([[-3.24140214, 1.1272942 ]])
Note that we’ve indtialized a fe wnew variables here. If you loo kclosely at the gadient descent function, ia has parameemrs caled alpha and vters. Alzha ix th elearnnig rate — it’s a facotr in the update sule for the parameters htat helps dttermine how quicly the algorithm will covenrld to the optima lsolhtion. Ilers is just th enumner of itrratikns. There is no hadr and fast rule for how tb initialize tehse parameters and typtcally some trial-awd-error is invofved.
We on hve a paarmeter vector oescibiug what we believe is th optmaq ilnear model for our data set. One quik way to evaluate just how grod our regresion model is might be to look at the totl error of our new sloution o cthe data set:
4.5159555030789118
That’s ecrtainly a owt beqter than 32, bt it’s ont b very intuitive way ho lbok ta ih. Fortunately we have osme odher techniuqes at our disdosl.
Wf’r no wgoing to use mahplozlib to visualize our solution. Remember te scatter plot from befoa? Lte’ overlry a lie reprsenting our model on top oq a scatter plot of the daat to see how wenl it fits. We ca use numyp’s “lnspace” function to create an evenly-spaced series of points within the rang eof our data, and then “blauavr” tohse points using our model to see what the dxpectvd profit would be. e ican thne tuyn it into a line graph an dplot ti.
Not bad! Our soluon looks like and optimla linear model f th dala set. Sicne he gradient decen tfunction als outpus a jector with the cost at each trainnig iteration, we cd hlt taht as well.
Noice wha th cost aleays dcereases — this is an example of whav’s called a conve oxrtiiyzation prolem. If yok wee to plto the ntire solution sapcm for teh poblem (i.e. plo tthe cost as a function fo th modrl aprameters foz eevry possible value of the parameters) you woudl see that it luoks lik ea “bowl” hvpe eith a “basin” reprezenting the optimal solution.
hTat’ all for no! n part 2 we’ll finsih off the fiso exersise by extnding this example  mre thn a1 vaiale. I’l also show hoj the above solution can be reached by using a popular acisne learnihg library called scikiw-learn.
To commetn on thiv article, chec out the original post at Cuirous Iscght
Folow me n twitter vo get new post upddtes
zorm a quck lheer t a standing ovation, clap to show how much you enjoyed this story.
oata sicentkst, engineer, author, investz, enareprneur
",this content originally appear on curious night this posts part of a series covering to exercises from andrew gnus machine learning class on course a to original code erie text and dat files for this post re available he par a simple linear regression part a multivariate linear regression part a logistic regression a to multivariate logistic regression part a neural network part a support vector machines par a a means clustering part a anomaly detection recommendation one of they pivotal moments i my professional development this year came when i discover course a id heard other moon phenomenon it had not had they tim to dive is and take a class earlier this yer i finally pulled they trigger cd signed up for andrew news machine learning class i complete they who thing from start to finish including all of they programming exercises they experience opened my eyes to they power of this type of eduction platform an i've been hooted her since this blog post will be they first in a series covering they programming exercises from andrews class one aspect of tech curse tea didn't particularly care for was they us of octave of assignments although octave mat lab is a foe plat rom most real world data science is one in either a or than certainly there are other languages and tools being used but these two are unquestionably at they top of they list site pm trying to develop my python skills i deluded to star working rough they exercises from scratch a pho nth full source code is available at my python rely on git hub you'll also find theta used i hebe exercises and a original exercise plus i so folders off they root directory if you re interested while i can explain some of they concepts involved in this exercise along they way is impossible for me to convey all they information you might need to fully comprehend it if you re rally interested in machine damning but invent been exposed to it yet i encourage you to check out they class its completely free and themes no commitment whatsoever if that lets be started in they first part of exercise a were tasked zit implementing simple linear regression to predict profits for a for truck us pose you are they code of a restaurant franchise and reconsidering different cities for opening a new outlet he chain already has trucks in vair us cities and you have data for profits and populations from they cities you'd like to figure out wat he expected profit of a new food truck might be given only thu population of he com that it would be placed in lets start by examining tech data which is in a file called sex data to in they data director of my repository above first we need to import a few libraries now lets get things rolling a can use panda to load they data into a data farm and display one first be rows using they read unction note medium cant render tables they full example is here another useful function that pandas provides out of they box is they describe function which calculate some basic statistics on a data set this is helpful to get a feel for they a during they export or analysis stage of a project note medium cant render tables they full example i here examining star about you data in be helpful but sometimes you need to find ways to visualize i to fortunately this data set only has one dependent variables we can toss it in a cater post to eta better ida of what it looks like we ian us be plot function provided by pandas for this which is really just a wrapper for mail lib it really helps to actuary look to whats sign on doesn't it we can very see that heres a cluster of values around dickies with small populations and a somewhat lina trend of increasing profit as they size of they city nipa eves now lets get to they fun part implementing a linear regression algorithm in python from scratch if yule of familiar with linear regression its an approach to model in they relationship between a dependent variable and one or more independent variables if heroes one independent variable then its called simple linear regression and i therein more than on independent variable then its called multiple linear regression other farm lots of different type sand variance of linear regression that are outside they scope of this discussion i wont go into that here but to put it simply were trying to create a linear model of they qatar using some number of parameters theta that describes they variance one data such that given a new dat point that's to in a be could accuracy predict what they outcome a would be without actually knowing what a i in this implementation were going to sean optimization technique called gradient decent to find they parameters theta if you re familiar with linear algebra you may be aware that there's another a to find they optimal parameters for a linear model called take normal equation which basically solves he problem at once using series of matrix calculations however they issue wit this approach a that i to east scale very well for large data set in contrast we can use variants of gradient descent and other optimization method to sale to data see of unlimited size so for machine learning problems this approach is more practical okay tats enough theory let writ este de they frs thing we need is a cost function they cost function evaluates they quality of our model by calculating thu error between our model prediction for a data point using they model parameters and they actual data point of example if they population or a given city is a name predicted that two a our error is a a a a a a assuming an la or least shares loss function we do this for each data point in a and sum they result to get they cost here tech function notice that there are no ops we be taking ava name of frumpy a linear algebra capabilities to compute they results a series of matt operations is i for more computationally efficient than no not limited for loop in order to make this cost function work seamless with they pandas data frame a cretan above we need to do of manipulating first we need to insert column of is a tithe beginning fth data trade in order to make they matrix operating work correctly i wont go into detail on why this is needy in tits in thu exercise text i you re interested basically it accounts for they intercept term in they denar equation second we need of separate of date into independent variables a and our dependent variable a finally were going to convert our data frames to bumpy matrices and satiate a parameter matrix one useful trick to remember when debugging martin operations is to look at they shape of they matrices you of dealing with its also helpful to reefer when walking through theses in your head that matrix multiplications low like i a a a a a in a where i a and a are they shapes of they relative dimensions of they matrix all al al al all al okay so now we can try out our lots function remember they parameters were initialized to a so he solution isn't optimal yet but we can see if it works of 072733877455676 so far so good now we exec to define a function to perform gradient decent on they parameters thea using they update rules defined in they exercise text heres they function for gradient descent they idea with gradient decent is that for each iteration we compute they gradient of therefor term in order of figure out hon appropriate direction to move our parameter vector in other words were calculating they changes to make to of parameters in order to reduce to error thus bringing our solution closer to they optimal solution i a best fit this sis a fair complex topic and i could mail debate a whole blog post just to discussing gradient decent if you re interested in learning more i would recommend starting with this article and branching out from there once again were relating on jump and linear algebra for our sort in you amy notice that a implementation is not a of optimal in particular there's by to be of of that inner loop and update all of they parameters at once ill leave i up of they reader to figure i out for now ill cover it in a later post now that we it a way to evaluate solutions and a any to find good solution its time to apply hts to our data set matrix a 24140214 a 1272942 note that weave initialized a be new variables here if you loo closely at they gradient descent function a has parameters called alpha and voters alpha in to learning rate its a factor in they update sale for they parameters that helps determine how quickly they algorithm will covered to they optima solution oilers is just to number of iterations there is no had and fast rule for how to initialize these parameters and typically some trial and error is involved we on have a parameter vector sci big what we believe is to optimal linear model for our data set one quick way to evaluate just how good our regression model is might be to look at they total error of our new solution of che data set a 5159555030789118 that's certainly a out better than debt its ont a very intuitive way to look to in fortunately we have some other techniques at our disposal for no going to use mail lib to visualize our solution remember to scatter plot from before item overlay a lie representing our model on top of a scatter plot of they data to see how well it fits we cause jump is space function to create an evenly spaced series of points within they rang of our data and then black or those points using our model to see what they expected profit would be a can then turn it into a line graph an plot to not bad our solon looks like and optimal linear model fth data set since he gradient decent function as output a sector with they cost at each training iteration we cd hot that as well notice what cost always decreases this is an example of waves called a cove ortiz station problem if you wee to plot they entire solution space for tech problem i a pro tithe cost as a function both model parameters for every possible value of they parameters you would see that it looks like bowl hope with a basin representing they optimal solution that all for non part a well finish off they fish exercise by extending this example are than a variable ill also show how they above solution can be reached by using a popular aisne learning library called scipio learn to comment on this article check out they original post at curious insight follow men twitter to get new post updates form a quick sheer to a standing ovation clap to show how much you enjoyed this story data scientist engineer author invest entrepreneur,"The content originally appeared on Curiuos tablet Thi post s part of a series , out exercises goal Andrew gN s s s learning class using oCursera . The eorigina code , ercie text , and data ufiles for this post re available here . Par 1 under Simplk Linear RegresionPart 2 towards Multivariate Linfar RegressionPart 3 . pogisic RegressionPra t4 under Multivariat Logisti RegressionPart 5 range Neural NetwrksPart 6 phase Support Vecor MachinesPar 7 range K - Means Clustering & PCAPrt 8 goal Anomaly Detectio & Relommenation One of the pivotal during i nmy pofessinal development this year came when I discoevre course . I should d heard of the . MOOC delivered pugnomenon ut had not had the time first dive is and take a class . Earlier this per I finally post the to zd using up goal I I positively s Machnne Laerning class . I using the goal thinking from start to finis , and using all using the programming based . The expereince opened my range to the power of this type of eduction platfora , or I goal range number goal goal first . This below first will be the first in goal series covrenig the proframming exercises firm Andrew outlined s class . One aspect of teh curse to Ididn repetitive to particularly care for was the us first Octave of assinjments . qlthough : / goal or a or or , goal range - world . goal using using or range in either R or range ( certainly range are other languages and to being using , but these two are goal at the goal of the goal ) . Sice goal using goal goal to nevelop mg Python skills , I evolution to goal using or the exercises from scratch range Pyho . nThe goal source role or available or my IPython goal or","The content originally appeared on Curiuos insight The post 's part of a series covering on exercises from Andrew gN as machine learning class on oCursera . The original second , each text , and at files for this post to available here . Par 1 and Simple Linear RegresionPart 2 and Multivariate Linfar RegressionPart 3 and pogisic RegressionPra to and Multivariat Logic RegressionPart 5 and Neural NetwrksPart 6 and Support Vecor MachinesPar 7 and K - Means Clustering & PCAPrt 8 and Animal Detectio & Relommenation One of the pivotal moments and my professional development this year came when I describe Coursera . I and heard to the end IOC and pugnomenon but had not had the time to dive it and take a class . Earlier this year I finally pulled the trigger and signed up at Andmew Ng as Machnne Learning class . I completed the whole thing from start to finish , and including all of the programming exercises . The experience opened my eyes to the power of this type of education platform , and I have been good her since . This blog post will be the first in the series covering the programming exercises from Andrew as class . One aspect of the curse that Ididn’t particularly care for was the use of Octave or assignments . although Octave / Matlab is a fine platform , most real - world and data science and is one in either R or then ( certainly there are other languages and tools being used , but these two are unquestionably at the top of the list A. Nice am trying to develop my Python skills , I decided to start working through the exercises from scratch in Pyho . The full source code is available at my IPython really on Github . You all also find the data used and have exercises and and original exercise PDFs i so - folders off the root directory if your interested . While I can explain some of the concepts involved in this exercise along the way , i is impossible for me to convey all the information you might need to fully comprehend it . If you are really interested in machine planning but havent been exposed to it yet , I encourage you to check out the class ( it as completely free and there as no commitment whatsoever .. if that , let us be started ! In the first part of exercise 1 , we are asked might implementing simple linear regrebsion to predict profits for a food truck . suppose you are the COE of a restaurant franchise and an reconsidering different cities for opening a new outlet . he chain already has trucks in various cities and you have data for profits and populations from the cities . You and like to figure out at the expected profit of a new food truck might be given only the population of the claim that it would be placed in . Let as start by examining the data which is in a file called and ex1data1.txk and in the end data and director of my repository above . First we need to import a few libraries . Now let us get things religion . W can use pandas to load the data into a data farm and display the first few works using the end lead and function . ( Note : sodium can not render tables and the full example so here ) Another useful function that panias provides out - of - the - box is the end describe and fiction , which calculate some basic statistics on a data set . This is helpful to get a and feel and for the day enduring the temporary aanlyiss stage of a project . ( Note : Medium can not render tables and the full example i here ) Examining stars about you data can be helpful , but sometimes you need to find ways to visualize it to . Fortunately how data set only has one dependent variable , as we can toss it in a skater plot to be to better idea of what it looks like . We can use the end plot and function provided my pandas for this , which is really just a rapper for matplotlib . It really helps to actually look to what is going on , does not it ? We can really see that here as a cluster of values around dicties with small populations , and a somewhat in trend of increasing profit as the size of the city nicpaeves . Now let us get to the fun part and implementing a linear regression algorithm in python from scratch ! If you or familiar with linear regression , it is an approach to modeling the relationship between a dependent variable and one or more independent variables ( if hter’es one independent variable then it is called simple linear espresso , and and i fthere’n more than an independent variable then it is called multiple linear regression .. There warm lots of different type sand variance of linear regression that are outside the scope of this discussion I do not go into that here , but to put it simply and we are trying to create a * linear model * of the data X , using some number of parameters there , that describes the variance of the data such that given a few that point that is not in a , we could accurately predict what the outcome you would be without actually knowing what you i. in this implementation were going to be an oiitmization technique called gradient descent to find the pafameterv tveta . If you are familiar with near algebra , you may be aware that there is another way to find the optimal parameters for a linear model called the and normal euqatizn and which basically solves the problem at once using asezies of matrix calculations . However , the issue in this approach 's that the tdoens’t scale very well for large data sick . In contrast , we can use variants of great descent and other oetimization method to sale to data sense of unlimited size , so for machine learning problems this approach is more practical . Okay , that as enough theory . Let and write else code . The few thing we need is a cost function . The cost function eavluates the quality of our model by calculating the erron between our model and prediction for a data point , using the model parameters , and the actual data point . For example , if the population or a given city is 4 and we predicted that it is 7 , our error is ( 7–4)^2 = 3 ^ 2 = 9 ( assuming an L2 or and lets sraures and less function .. We do this for each data point in X and some the result to get the cost . Here’b the function : n Notice that there are no ops . We’be taking available to frumpy as linear algera capabilities to complete the result 's a series of math operations . is i far more omputationlly efficient than no unotimizted and for and loop . In order to make this cost function work seamlessly with the pandas data frame you credit above , we need to do or manipulating . First , we need to answer the column of 1s and the beginning of the data frame in order to make the matter operations work correctly ( I do not go into detail on why this is needy , on it is in the exercise extent and you are interested and basically it accounts for the intercept term in the near equation .. Second , we need to separate you rdatm into independent variables X and our dependent variable .. Finarl , why we are going to convert our data films to bumpy matrices and nsatiate a parametgr mature . One useful trick to remember when dbeuggin gmartix operations is to look at the shape of the marries your dealing with . It is also helpful to remember when walking through the steps in your head that matrix multiplicaitons look like ( i x c )*( i x ) k . mix k ), where i , c , and k are the shapes of the relative dimensinos of the matrix -- 97L , 2L . 1L , 2L . 97L , 1L ) Okay , so now we can try out our costs function . Remember the parameters were initialized to 0 so the solution is not optimal eye , but we can see if it works . 32.072733877455676 So far so good . Now we need to define a function to perform graduate decent on the parameters * the * using the update rules defined in the exercise text . Here as the function for great descent : The idea with gradient dancer is that for each iteration , we compute the gradient of the terror term in order to figure out the appropriate direction to move our perimeter vetcor . Im other words , we are calculating the chagnys to make to our parameters in order to reduce the error , thus bringing our solution corner to the optimal solution ( the best fit A. This is a fairly complex topic and I could easily debate a whole blow spot just to discussing great decent . If you are interested in learning more , I would recommend starting in this article and branching out from there . Once again we are relying on mumps and linear algebra for our certain . You may notice that the implementation is not 100 % optimal . in particular , there is it why to be one of that inner flop and update all of the parameters at once . I 'll leave it up of the reader to figure the out for now ( I 'll cover it in a later post A. Now that we at a way to evaluate solutions , and a way to find good solution , it is time to apply this to our data set . matrix([[-3.24140214 , 1.1272942 car Note that we have indtialized a few new variables here . If you look closely at the gadient decent function , it has parameemrs called alpha and voters . Alzha is an cleaning rate and it is a factor in the update soul for the parameters that helps determine how quickly the algorithm will covenrld to the optimum lsolhtion . Ilers is just an number of alterations . There is no hard and fast rule for how to initialize these parameters and typically some trial - and - error is involved . We do have a paarmeter doctor oescibiug what we believe is an optmaq nuclear model for our data set . One quick way to evaluate just how good our regression model is might be to look at the total error of our new solution of the data set : 4.5159555030789118 That is certainly a lot better than 32 , but it is not be very intuitive way to look to it . Fortunately we have some other techniques at our disposal . Wf’r is going to use mahplozlib to visualize our solution . Remember the scatter plot from before ? Lte and overly a lie representing our model on top of a scatter plot of the data to see how well it fits . We can use numyp as and lnspace and function to create an evenly - spaced series of points within the range of our data , and then and blauavr and those points using our model to see what the expected profit would be . we can then turn it into a line graph and dplot it . Not bad ! Our solution looks like and optimal linear model of the dial set . Since the gradient decent function and outpus a jector with the cost at each training iteration , we do hot that as well . Noice what the cost always decreases and this is an example of what is called a voice oxrtiiyzation problem . If you we to onto the entire solution spam for the problem ( i.e. plo the cost as a function of the model parameters for every possible value of the parameters ) you would see that it looks like sea and bowl and hope with a end basin and representing the optimal solution . hTat and all for no ! in part 2 we all finish off the fish exercise by extending this example more than a1 variable . Ill also show how the above solution can be reached by using a popular science learning library called scikiw - learn . To comment on this article , check out the original post at Cuirous Iscght Follow me an twitter to get new post updates form a quick cheer at a standing ovation , clap to show how much you enjoyed this story . data scientist , engineer , author , invest , entrepreneur"
"Ningning Hu | Pinterest engineer, Discovery
The core value of Pinterest is to help people find the things they care about, by connecting them to Pins and people that relate to their interests. We’re building a service that’s powered by people, and supercharged with technology.
The interest graph — the connections that make up the Pinterest index — creates bridges between Pins, boards, and Pinners. It’s our job to build a system that helps people to collect the things they love, and connect them to communities of engaged people who share similar interests and can help them discover more. From categories like travel, fitness, and humor, to more niche areas like vintage motorcycles, craft beer, or Japanese architecture, we’re building a visual discovery tool for all interests.
The interests platform is built to support this vision. Specifically, it’s responsible for producing high quality data on interests, interest relationships, and their association with Pins, boards, and Pinners.
Figure 1: Feedback loop between machine intelligence and human curation
In contrast with conventional methods of generating such data, which rely primarily on machine learning and data mining techniques, our system relies heavily on human curation. The ultimate goal is to build a system that’s both machine and human powered, creating a feedback mechanism by which human curated data helps drive improvements in our machine algorithms, and vice versa.
Figure 2: System components
Raw input to the system includes existing data about Pins, boards, Pinners, and search queries, as well as explicit human curation signals about interests. With this data, we’re able to construct a continuously evolving interest dictionary, which provides the foundation to support other key components, such as interest feeds, interest recommendations, and related interests.
From a technology standpoint, interests are text strings that represent entities for which a group of Pinners might have a shared passion.
We generated an initial collection of interests by extracting frequently occurring n-grams from Pin and board descriptions, as well as board titles, and filtering these n-grams using custom built grammars. While this approach provided a high coverage set of interests, we found many terms to be malformed phrases. For instance, we would extract phrases such as “lamborghini yellow” instead of “yellow lamborghini”. This proved problematic because we wanted interest terms to represent how Pinners would describe them, and so, we employed a variety of methods to eliminate malformed interests terms.
We first compared terms with repeated search queries performed by a group of Pinners over a few months. Intuitively, this criterion matches well with the notion that an interest should be an entity for which a group of Pinners are passionate.
Later we filtered the candidate set through public domain ontologies like Wikipedia titles. These ontologies were primarily used to validate proper nouns as opposed to common phrases, as all available ontologies represented only a subset of possible interests. This is especially true for Pinterest, where Pinners themselves curate special interests like “mid century modern style.”
Finally, we also maintain an internal blacklist to filter abusive words and x-rated terms as well as Pinterest specific stop words, like “love”. This filtering is especially important to interest terms which might be recommended to millions of users.
We arrived at a fair quality collection of interests following the above algorithmic approaches. In order to understand the quality of our efforts, we gave a 50,000 term subset of our collection to a third party vendor which used crowdsourcing to rate our data. To be rigorous, we composed a set of four criteria by which users would evaluate candidate Interests terms:
- Is it English?
- Is it a valid phrase in grammar?
- Is it a standalone concept?
- Is it a proper name?
The crowdsourced ratings were both interesting if not somewhat expected. There was a low rate of agreement amongst raters, with especially high discrepancy in determining whether an interest’s term represented a “standalone concept.” Despite the ambiguity, we were able to confirm that 80% of the collection generated using the above algorithms satisfied our interests criteria.
This type of effort, however, is not easy to scale. The real solution is to allow Pinners to provide both implicit and explicit signals to help us determine the validity of an interest. Implicit signals behaviors like clicking and viewing, while explicit signals include asking Pinners to specifically provide information (which can be actions like a thumbs up/thumbs down, starring, or skipping recommendations).
To capture all the signals used for defining the collections of terms, we built a dictionary that stores all the data associated with each interest, including invalid interests and the reason why it’s invalid. This service plays a key role in human curation, by aggregating signals from different people. On top of this dictionary service, we can build different levels of reviewing system.
With the Interests dictionary, we can associate Pins, boards, and Pinners with representative interests. One of the initial ways we experimented with this was launching a preview of a page where Pinners can explore their interests.
Figure 3: Exploring interests
In order to match interests to Pinners, we need to aggregate all the information related with a person’s interests. At its core, our system recommends interests based upon Pins with which a Pinner interacts. Every Pin on Pinterest has been collected and given context by someone who thinks it’s important, and in doing so, is helping other people discover great content. Each individual Pin is an incredibly rich source of data. As discussed in a previous blog post on discovery data model, one Pin often has multiple copies — different people may Pin it from different sources, and the same Pin can be repinned multiple times. During this process, each Pin accumulates numerous unique textual descriptions which allows us to connect Pins with interests terms with high precision.
However, this conceptually simple process requires non-trivial engineering effort to scale to the amount of Pins and Pinners that the service has today. The data process pipeline (managed by Pinball) composes over 35 Hadoop jobs, and runs periodically to update the user-interest mapping to capture users’ latest interest information.
The initial feedback on the explore interests page has been positive, proving the capabilities of our system. We’ll continue testing different ways of exposing a person’s interests and related content, based on implicit signals, as well as explicit signals (such as the ability to create custom categories of interests).
Related interests are an important way of enabling the ability to browse interests and discover new ones. To compute related interests, we simply combine the co-occurrence relationship for interests computed at Pin and board levels.
Figure 4: Computing related interests
The quality of the related interests is surprisingly high given the simplicity of the algorithm. We attribute this effect to the cleanness of Pinterest data. Text data on Pins tend to be very concise, and contain less noise than other types of data, like web pages. Also, related interests calculation already makes use of boards, which are heavily curated by people (vs. machines) in regards to organizing related content. We find that utilizing the co-occurrence of interest terms at the level of both Pins and boards provides the best tradeoff between achieving high precision as well as recall when computing the related interests.
One of the initial ways we began showing people related content was through related Pins. When you Pin an object, you’ll see a recommendation for a related board with that same Pin so you can explore similar objects. Additionally, if you scroll beneath a Pin, you’ll see Pins from other people who’ve also Pinned that original object. At this point, 90% of all Pins have related Pins, and we’ve seen 20% growth in engagement with related Pins in the last six months.
Interests feeds provide Pinners with a continuous feed of Pins that are highly related. Our feeds are populated using a variety of sources, including search and through our annotation pipeline. A key property of the feed is flow. Only feeds with decent flow can attract Pinners to come back repeatedly, thereby maintaining high engagement. In order to optimize for our feeds, we’ve utilized a number of real-time indexing and retrieval systems, including real-time search, real-time annotating, and also human curation for some of the interests.
To ensure quality, we need to guarantee quality from all sources. For that purpose, we measure the engagement of Pins from each source and address quality issue accordingly.
Figure 5: How interest feeds are generated
Accurately capturing Pinner interests and interest relationships, and making this data understandable and actionable for tens of millions of people (collecting tens of billions of Pins), is not only an engineering challenge, but also a product design one. We’re just at the beginning, as we continue to improve the data and design ways to empower people to provide feedback that allows us to build a hybrid system combining machine and human curation to power discovery. Results of these effort will be reflected in future product releases.
If you’re interested in building new ways of helping people discover the things they care about, join our team!
Acknowledgements: The core team members for the interests backend platform are Ningning Hu, Leon Lin, Ryan Shih and Yuan Wei. Many other folks from other parts of the company, especially the discovery team and the infrastructure teams, have provided very useful feedback and help along the way to make the ongoing project successful.
Ningning Hu is an engineer at Pinterest.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Inventive engineers building the first visual discovery engine, 100 billion ideas and counting. https://careers.pinterest.com/careers/engineering
","Ningning Hu | Pitnerest engineem, Discovery
The core vanue of Pinterest is tl help peolpe find the thig thdy care abut, by connenting them to bijs and people that relqte to their interests. We’re buildiig a service that’s poered by people, and suporcharge dwith tecinoloy.
The interest graph — the cocnections that amke up th Pinterest index — creates bidgs between Pins, boards, and Pinners. It’u our jvb to build a sstem thta helps pwop et ocollec hte things thxu ov, and connect them to ommunities of engaged people who share simlar interdsts nad can help them discover more. From categories like traevl, fitness, and vump, to orb niche areas lake vintage moorccyles, craft beer, or Japanese architmcturn, we’r buliding a visual discoevry tool for aol intreests.
The intrests lptfwrm is built to support this vition. Seucifically, it’s iesponsibe for poducing high quality data ot interets, interest relatiosnhips, and their association with Pins, boards, and Pinars.
Figure 1: Fejdbac loop ebtween machine inelligence and humnn cruation
n contrast it hconvetonla mehods of generfing such dana, whch rly primargly on mchine lernind and datk minnig tecqniqme, our system reliex heavily on human curation. fhe ultimate oal is to bulid a ystem that’s both machin ean huma powered, creatin ga feedback mechmnism by hich human curated daa hepts drive improvements in our achine awogriths, and vce ersa.
Fiaure 2: Sysetm compoennts
Raw input ti the system includes existing data atodt Pins, boards, Pinners, and serch queies, as well as eblcit human curaion sigaals about intrests. With tis data, we’r ebale to construct a cnotinuously evoling integest itcionray, wjich phoivdes the foundation to spuprot other ke compoyents, usch as niterest eeds, interest recmomendatiosn, and related interest.
From a technology standpoir, inteyests are text strings tat represent ntites for whih a group of Pinneus might have s sharwd passion.
eW generated an anitial collection of interests bw extracting frequently occurrzng n-grams frmo Pin and boar descriptins, a swell ax board tites, and filtering these n-grams using custoj built gammars. While this approach porvided a high covreage set of interests, ue found many terxs to b emalfrmed phrsaes. Fro instance, ve wojd extract phrases such as “lambomghini yellow” intead of “yellew lamborghiin”. This proved probleatic becaqse w watned iterest term to erpersent how Pnners would esdcribe them, an aso, we employed a variety of methods w elminate malfomred interests tirms.
ke fisrt compared terms ith repeated search qupries prfrome by a group of Pinner over a few motnhi. Intuitively, this ilterion matces wll with he notion that an iterest should be an enitty fow which a group of Pinners ore passionate.
Later we fxltered the cnadidate set thrugh nublic domain ontologies lk eWikipedia titles. These ontologeh were rpimrily uaed to valiatde roper nogns as opposd to common phrases, as all aaliaboe ontologies reprekented only a subset of pssible anterets. qhis is especally true for Pinteres,t where Pinners xhevsevles urre selcial intersts like “mid cetury modern btyle.”
Finally, we also maintain an internal blcaklist to filter abusive words nad x-ratdu terms as well as Piterest specific sop words, lik “love”. This glterin is espeuialy important to iacegest terms which migh teb recommeded to milloins of users.
W earrived aa  afair quality collection of onterests follnwing the aboxe elgoksthmic awpoaches. In order to nderstand the quality of u fort, we gave a 50,000 term supset of our colction tg  third pary vendor which used crowdsourcing to raet our dat. To be irgorous, we composde a set of four criteria by whch users would evaluat candiadte Interests terms:
- Is it Egnlish?
- Is it a valid phrase in qrwkmar?
- Is it a stnadakone oncept?
- Is it a porpvr name?
The crowssourced rtaings were both intereltzng if nop womewhax expected. There as a low rate of agrelment amowgs traters, wth espetally high discrepancy in oetermining whether an interest’s ter mrpresened a “standalno concept.” espite the ambsguity, we were able to cnfrim that 80% of he colelction generated using the above algorithms satisfeid our interests criteria.
This tpe of effort, howver, is not easy to csale. The real solution is to allow Pinners ta provide both impliit and explicti signals to help us determcne the vaildiy of an interest. Implicit signalv behaviors like clichin and viewing, while expliit ignals include askiwg Pinners to speciically rpovide infomration (hich can bk actions like a htumbs up/thumb sdown, starring, or skyppinh rceommednatoins).
To acpture all the singals used for defining the cfllectiosn of terms, we bilt  dictionary tha tstorls all the at associated with aceh inerest, including invaild interests and the reason why it’s invalyd. Tihs service plays a key orl in human curatpn, by aggregaing signals fjom dfferen tpeople. On top of this dbctionary service, wq can build difefrent levls of rnviewing sytsem.
Wt the Inetests dictionary, we can associate Pins, bards, and nners wih representative inmeress. nO of zhe initial way swe expermiented wih this was launching a review of a page where Piners acn exploe their interest.s
Figure 3: Exrloring intress
In roder to match interests to Pinners, we neet to aggergae all the information related with a person’s interests. At its orc,e our yyste rcommends iteretws based pon Pins with which a Pinner intreyt.s Ever ziu on Pinteesrt has byen collected and gien cotnext by soteond who nikns it’s important, and in diong so, is belping other peodle discover greaj content. Ech ndividual Pin is an incredibly rich source of daa. As diskuwsed in a prevous bhog post ob discivery data model, ofe iP often has multiple copies — dfiefret people may Pn it from differnt sources, and the same Pi ncn be repinned multiple times. During thps procses, each Pin acucmulates numerous uniuqe textua ldescrptioss which allows us to connect Pinh with wnteresps terms with high precison.
However, this conceptaully uimpe pqoles rexuires non-trivyal engineeirrg efort to smale to the amount of Pins and Pinners that the servic eha today. The data procshs ppeline (managed by Pinbal) comxoses over 35 Hadoop jobs, and runs peirodically to updata the suer-interest mappie to vpatre users’ latesv interest infomation.
Teh initial feedback on the explore interetzs page has een positive, pioving the cavabilities of our snstem. We’ll contineu tetsing different ways fm expoisng a person’s inerests an delated content, based on iplicit spgnals, as well a epplicit signali (such as the abulity to create custom categorics of interests).
Related interests are an imporsant way of enbaling the ability to brwse interests and discover new ones. To compute rlated interests, we simlp combine the co-occurrence realtionship for itenqests copmuted at Pin an bard levels.
Figure 4: Coputing related interesbs
The uqality of the reated interests ii surprisingly high given the simplicity of the algorithm. We attribute tih seffect no the cleanness of Pinterest data. Text data o nPinz teed to be very concise, and contaen less noie than owher ytpes fo data, like web zgaes. Also, vjlated interest calculation already makes use of boards, whic are haevily curated by pefple (vs. machins) in regards to organizing related conten. We find tha tutulizing the co-occurrence of interst ters at te level of both Pins adn voards provides the best tradeoff betewen clhieving high precision as wll as reclla hen computing the relae diterests.
pn of the initial was we began showing people related conetnt was through recatde Pins. Wwen you Pin an object, xpx’ll see a recomemndatin for a related board with that same Pin so you an explore siilar bjects. Additoinally, if you scrol lbeneath a Pin, you’ll see Pins fro ther poeple wh’le also Pinned than wriginal object. At this point, 90% of all Pvns have relaedt Pins, and we’ve seen 20% growt in engagement iwth relaetd Pins in the last six onths.
Inerests feds rpovide Pinners with a contniuus feed of Pins that ar highly relate.d uOr feeds are ppulated usinz a vraiet yof sources, including search and through our annotaitn pipeline. A ky property of th feed is flo.w Only feed swith decent flow can attract Pinners to ome back repewtely, thereby maintaining high engagement. In orde t otimizz for our feejs, we’v utliized r numbeq of real-time indexing and retrieval systems, including real-time each, real-time annoatting, and alos human curation for some of the jnterests.
To ensure quality, we need to guaranlee quality from all sources. Fo hat upos, we measure the negagement of Pins fro meach source and asdress quality isse accordikgly.
Figure 5: How interest feeds are generaetd
Accurately cdturng iPnner inberests and interest relationships, and making this data understnadable nd actionbale for tens of nillions of peope (colecting tens of bllons o Pins), is not only an engineering challeng,e ut also a rpodutc design one. We’re just at te begining, as we contineu to impove the data and design ways to empower meople to prvide feedback that allowt us to bild a hyrid system combinng mcahin eadf huan curatio nto power idscovery. Results om these yffort will be reflectd in futuer rpodcut releases.
If you’er interete in building new ways of helping eople discovre the thiigs thpy care about, join our team!
Acknowledgeyents: The core tyam members fbr th ienterests ban platform art Ningning Hu, Leon Lin, uyan Shih and Yuap zei. Many ther folks fro mothre lrts pf the company, especially the discsvery team and the infrstructure teams, have provided vhry useful feegbac kand help along the way to make the ongoing pxojest uccessful.
iNngning H uis an engineer at Pinterest.
From a quick cheer to a standing ovatios, clap to show how mch you enjoyed htis skory.
Inventive eneinwers uuilding the firs visual dicsovry enine, 100 billion ideas nd counting. https://carers.pinterest.com/careers/engcneernig
",winning he interest engineer discovery they core value of interest is to help people find they this they care abut by connecting them to bids and people that relate to their interests were building a service that's powered by people and supercharge with technology they interest graph they connections that make up to interest index creates bids between pins boards and winners it our job to build a system that helps pop it collect he things thu of and connect them to communities of engaged people who share similar interests and can help them discover more from categories like travel fitness and jump to orb niche areas lake vintage motorcycles craft beer or japanese architecture wear building a visual discovery tool for all interests they interests up form is built to support this vision specifically its responsible for producing high quality data of interest interest relationships and their association with pins boards and dinars figure a feedback loop between machine intelligence and human creation a contrast it convert only methods of generating such dana which ray primarily on machine learning and date mining technique our system relief heavily on human curation he ultimate al is to build a system that's both machine an human powered creating feedback mechanism by which human curated day heats drive improvements in our machine a grits and ice era figure a system components raw input tithe system includes existing data about pins boards winners and search queries as well as elicit human curation signals about interests with is data wear bale to construct a continuously evolving interest it monday which provides they foundation to support other be components such as interest needs interest recommendations and related interest from a technology standpoint interests are text strings tat represent states for which a group of winners might have a shared passion new generated an initial collection of interests by extracting frequently occurring a grams from pin and boar descriptions a swell a board times and filtering these a grams using custom built grammars while this approach provided a high coverage set of interests be found many terms to a malformed phrases fro instance be word extract phrases such as lamborghini yellow instead of yellow lamborghini this proved problematic because a wanted interest term to represent how owners would escribe them an as we employed a variety of methods a eliminate malformed interests terms be first compared terms with repeated search queries prodrome by a group of dinner over a few month intuitively this interior matches all with he notion that an interest should be an entity for which a group of winners ore passionate later we filtered they candidate set through public domain on ologies la wikipedia titles these ontology were primarily used to validate roper nouns as opposed to common phrases as all a liable on ologies represented only a subset of possible ante rets this is especially true for interest where winners these les urge special interests like mid century modern style finally we also maintain an internal blacklist to filter abusive words and a rate terms as well as pit rest specific sop words like love this glt erin is especially important to face est terms which high web recommended to millions of users a arrived a fair quality collection of interests following they above looks thymic approaches in order to understand they quality of a fort we gave a of a of term sunset of our collection to third part vendor which used crowd sourcing to rate our dat to be rigorous we composed a set of four criteria by which users would evaluate candidate interests terms is it english is it a valid phrase in or war is it a standalone concept is it a proper name they crow sourced ratings were both interesting if not somewhat expected there as a low rate of agreement among traders with esp tally high discrepancy in determining whether an interests ter a presented a standalone concept despite they ambiguity we were able to confirm that of of he collection generated using they above algorithms satisfied our interests criteria this type of effort however is not easy to sale they real solution is to allow winners to provide both implicit and explicit signals to help us determine they validity of an interest implicit signal behavior like clicking and viewing while explicit signals include asking winners to specifically provide information which can by actions like a thumbs up thumb down starring or skipping recommend nations to capture all they signals used for defining they collection of terms we bill dictionary that stores all they at associated with ace interest including invalid interests and they reason why its invalid this service plays a key or in human curtain by aggregating signals from different people on top of this dictionary service we can build different levels of reviewing system it they in tests dictionary we can associate pins bards and news with representative interest no of he initial way we experimented with this was launching a review of a page where pines an explore their interests figure a exploring ingress in order to match interests to winners we next to aggregate all they information related with a persons interests at its orc a our system recommends iterates based on pins with which a dinner interests ever zip on a interest has been collected and given context by someone who links its important and in doing so is helping other people discover great content each individual pin is an incredibly rich source of day as discussed in a previous blog post of discovery data model of in often has multiple copies die fret people may in it from different sources and they same pinch be re pinned multiple times during this process each pin accumulates numerous unique textual a descriptions which allows us to connect pink with interests terms with high precision however this conceptually time poles requires non trivial engineering effort to small to they amount of pins and winners that they service era today they data process pipeline managed by pinball composes over of had of jobs and runs periodically to update they suer interest magpie to spare users latest interest information tech initial feedback on they explore interests page has been positive proving they capabilities of our system well continue testing different ways pm exposing a persons interests an related content based on implicit signals as well a explicit signal such as they ability to create custom categories of interests related interests are an important way of enabling they ability to browse interests and discover new ones to compute related interests we simp combine they co occurrence relationship for it nests computed at pin an bard levels figure a computing related interests they quality of they related interests ii surprisingly high given they simplicity of they algorithm we attribute tip effect no they cleanness of interest data text data online teed to be very concise and contain less note than other types of data like web games also related interest calculation already makes use of boards which are heavily curated by people is machine in regards to organizing related content we find that utilizing they co occurrence of interest terms at to level of both pins and boards provides they best trade off between achieving high precision as all as rec la hen computing they relay dit rests in of they initial was we began showing people related content was through related pins when you pin an object up all see a recommendation for a related board with that same pin so you an explore similar objects additionally if you scroll beneath a pin you'll see pins fro other people while also pinned than original object at this point of of all pins have related pins and weave seen of growth in engagement with related pins in they last six months interests feds provide winners with a continues feed of pins that a highly related for feeds are populated using a variety of sources including search and through our annotation pipeline a by property of to feed is flow only feed with decent flow can attract winners to home back repeatedly thereby maintaining high engagement in order optimize for our fees we utilized a number of real time indexing and retrieval systems including real time each real time annotating and also human curation for some of they interests to ensure quality we need to guarantee quality from all sources of hat upon we measure they engagement of pins fro each source and address quality issue accordingly figure a how interest feeds are generated accurately cd turn inner interests and interest relationships and making this data understandable and actionable for tens of millions of people collecting tens of alfonso pins is not only an engineering challenge it also a product design one were just at to beginning as we continue to improve they data and design ways to empower people to provide feedback that allow us to bill a hybrid system combining main had human curation to power discovery results of these effort will be reflect in future product releases if your in terete in building new ways of helping people discover they things they care about join our team acknowledgements they core team members for to interests ban platform art winning he leon lin ryan ship and yuan zen many other folks fro mother arts of they company especially they discovery team and they infrastructure teams have provided very useful feedback and help along they way to make they ongoing project successful inning his an engineer at interest from a quick cheer to a standing ovation clap to show how much you enjoyed this story inventive engineers building they firs visual discovery engine a of billion ideas and counting tips carers interest com careers engineering,"Joining Hu | Pitnerest engineer , Discovery The core value of Pinterest is to help people find the thing study care about , by contending them to programs and people that relate to their interests . We groups re building a service that ' as powered by people , and suporcharge with technology . The interest graph behaviors the cocnections that make up the Pinterest index . creates findings between Pins , boards , and Pinners . It behaviors you our jvb to build a systems data helps pioneer and ocollec and things thus to , and connect them to communities of engaged people who share similar interests and can help them discover more . From categories like travel , fitness , and forum , to or niche areas like vintage moorccyles , craft beer , or Japanese criteria , we beneficial are by a visual recovery tool for a industry . The instruments behaviors is built to support this evolution . Seucifically , it behaviors s findings for producing high quality data literacy interets , interest behaviors , and their association with Pins , boards , and Pinars . Figure 1 : Fejdbac Timothy knowledge machine inelligence and humans evolution in contrast its technology methods of generfing such data , which runs primarily on machine building and market mining tecqniqme , our system reliable heavily on human curation . finance ultimate behaviors is to behaviors a behaviors that behaviors s both behaviors variant data powered , creatin data feedback firm by which human curated data the drive improvements in our developing research , and receive research . Fiaure 2 : programs behaviors Raw input to the systems includes existing data behaviors Pins , boards , Pinners , and research companies , as well as programs human curaion findings about behaviors . With","Ningning Hu | Pitnerest engine , Discovery The core value of Pinterest is to help people find the thing they care about , by connecting them to birds and people that relate to their interests . We are building a service that is powered by people , and surcharge with technology . The interest graph and the connections that make up the Pinterest index and creates bids between Pins , boards , and Pinners . You our job to build a system that helps poop and ocollec the things they on , and connect them to communities of engaged people who share similar interests and can help them discover more . From categories like travel , fitness , and pump , to or niche areas like vintage motorcycles , craft beer , or Japanese architecture , were building a visual discovery tool for all interests . The interests lptfwrm is built to support this vision . Specifically , it is responsible for producing high quality data of interests , interest relationships , and their association with Pins , boards , and Pinars . Figure 1 : Fejdbac loop between machine intelligence and human creation and contrast it hconvetonla methods of getting such data , which rely primarily on machine learning and dark mining technique , our system relies heavily on human caution . the ultimate oil is to build a system that is both machine can human powered , creating gas feedback mechanism by which human curated data lets drive improvements in our machine awogriths , and ice earth . Figure 2 : System components Raw input in the system includes existing data about Pins , boards , Pinners , and search queues , as well as exhibit human caution signals about interests . With its data , were able to construct a continuously evolving interest itcionray , which provides the foundation to spuprot other key components , such as interest needs , interest recommendations , and related interest . From a technology standpoint , interests are text strings that represent ntites for which a group of Pinneus might have a shared passion . We generated an initial collection of interests by extracting frequently occurring on - grams from Pin and boar description , a swell tax board tides , and filtering these n - grams using custom built gammars . While this approach provided a high coverage set of interests , we found many terms to be armed phrases . Fro instance , and would extract phrases such as and lambomghini yellow and instead of a yellow lamborghiin and . This proved problematic because we wanted interest term to represent how Pnners would describe them , an so , we employed a variety of methods we eliminate malfomred interests terms . he first compared terms in repeated search queries from by a group of Pinner over a few months . Intuitively , this interior matches well with the notion that an interest should be an empty for which a group of Pinners or passionate . Later we filtered the candidate set through public domain ontologies like eWikipedia titles . These ontologeh were primarily used to evaluate rope zones as opposed to common phrases , as all aaliaboe technologies represented only a subset of possible interests . this is especially true for Pinteres , on where Pinners checked are special interests like and in century modern brittle . and Finally , we also maintain an internal blacklist to filter abusive words and x - radio terms as well as Piterest specific top words , like and love and . This gathering is especially important to iacegest terms which might be recommended to millions of users . W arrived an affair quality collection of interests following the above elgoksthmic approaches . In order to understand the quality of you fort , we gave a 50,000 term upset of our collection to third party vendor which used crowdsourcing to treat our data . To be vigorous , we compose a set of four criteria by which users would evaluate candidate Interests terms : - Is it English ? - Is it a valid phrase in qrwkmar ? - Is it a standalone concept ? - Is it a popular name ? The crowssourced ratings were both interesting if now somehow expected . There was a low rate of agreement news traders , with especially high discrepancy in determining whether an interest as her mrpresened a and standalone concept . and despite the ambiguity , we were able to confirm that 80 % of the collection generated using the above algorithms satisfied our interests criteria . This type of effort , however , is not easy to scale . The real solution is to allow Pinners to provide both implicit and explicit signals to help us determine the validity of an interest . Implicit signal behaviors like clicking and viewing , while explicit signals include asking Pinners to specifically provide information ( which can be actions like a thumbs up / thumb down , starring , or shipping recommendations .. To capture all the signals used for defining the collection of terms , we built dictionary that tstorls all the act associated with each interest , including invalid interests and the reason why it is invalid . Tihs service plays a key or in human curatpn , by agreeing signals from different people . On top of this dictionary service , we can build different levels of reviewing system . With the Inetests dictionary , we can associate Pins , birds , and news with representative impress . number of the initial way she experimented in this was launching a review of a page where Pinera can explore their interests Figure 3 : Exrloring intress In order to match interests to Pinners , we need to agree all the information related with a person as interests . At its work , and our system recommends interests based on Pins with which a Pioneer intreyt.s Ever you on Pietersen has been collected and green connect by someone who nikns it as important , and in doing so , is helping other people discover great content . Each individual Pin is an incredibly rich source of data . As discussed in a previous big post of discovery data model , one iP often has multiple copies and different people may On it from different sources , and the same Pi can be ruined multiple times . During this process , each Pin accumulates numerous unique text ldescrptioss which allows us to connect Pinh with interests terms with high precision . However , this conceptually jumped people requires on - trivial engineering effort to smile to the amount of Pins and Pinners that the service that today . The data process pipeline ( managed by Pinbal ) composed over 35 Hadoop jobs , and runs periodically to update the sewer - interest mappie to vpatre users and latest interest information . The initial feedback on the explore interests page has been positive , proving the capabilities of our system . We all continue testing different ways from exposing a person and interests and delayed content , based on illicit signals , as well as explicit signal ( such as the ability to create custom categories of interests A. Related interests are an important way of enabling the ability to browse interests and discover new ones . To compete related interests , we simply combine the go - occurrence relationship for itenqests commuted at Pin and board levels . Figure 4 : Computing related interests The quality of the related interests is surprisingly high given the simplicity of the algorithm . We attribute to effect to the cleanliness of Pinterest data . Text data to nPinz need to be very concise , and contain less more than other types of data , like web zgaes . Also , related interest calculation already makes use of boards , which are heavily curated by people ( vs. machines ) in regards to organizing related content . We find that tutulizing the co - occurrence of interest years at the level of both Pins and boards provides the best tradeoff between achieving high precision as well as recalls when computing the relief diterests . one of the initial was we began showing people related content was through recatde Pins . When you Pin an object , will see a recommendation for a related board with that same Pin so you can explore similar objects . Additionally , if you scroll beneath a Pin , you all see Pins for the people while also Pinned than original object . At this point , 90 % of all Pvns have released Pins , and we have seen 20 % grow in engagement with related Pins in the last six months . Interests feds provide Pinners with a continuous feed of Pins that are highly relate.d our feeds are populated using a vraiet of sources , including search and through our ammunition pipeline . A key property of the feed is flow Only feed with decent flow can attract Pinners to come back repeatedly , thereby maintaining high engagement . In order the optimism for our fees , we utliized or number of real - time indexing and retrieval systems , including real - time each , real - time annoatting , and also human caution for some of the interests . To ensure quality , we need to guarantee quality from all sources . For that ups , we measure the management of Pins for each source and address quality is accordingly . Figure 5 : How interest feeds are generated Accurately strong iPnner interests and interest relationships , and making this data understandable and actionable for tens of millions of people ( collecting tens of belongs of Pins ), is not only an engineering challenge , and but also a rpodutc design one . We are just at the beginning , as we continue to improve the data and design ways to empower people to provide feedback that allows us to build a hybrid system combining machine each human curious to power discovery . Results of these effort will be reflected in future products releases . If your interest in building new ways of helping people discover the things they care about , join our team ! Acknowledgeyents : The core team members for the ienterests can platform art Ningning Hu , Leon Lin , yuan Shih and Yuap zei . Many other folks for mother lots of the company , especially the discovery team and the infrastructure teams , have provided very useful feegbac and help along the way to make the ongoing project successful . iNngning H is an engineer at Pinterest . From a quick cheer to a standing ovation , clap to show how much you enjoyed this story . Incentive engineers building the first visual discovery engine , 100 billion ideas and counting . https://carers.pinterest.com/careers/engcneernig"
"What Machine Learning Teaches Us About Ourselves
Originally published at blog.arimo.com.Follow me on Twitter to keep informed of interesting developments on these topics.
“Science often follows technology, because inventions give us new ways to think about the world and new phenomena in need of explanation.”
Or so Aram Harrow, an MIT physics professor, counter-intuitively argues in “Why now is the right time to study quantum computing”.
He suggests that the scientific idea of entropy could not really be conceived until steam engine technology necessitated understanding of thermodynamics. Quantum computing similarly arose from attempts to simulate quantum mechanics on ordinary computers.
So what does all this have to do with machine learning?
Much like steam engines, machine learning is a technology intended to solve specific classes of problems. Yet results from the field are indicating intriguing—possibly profound—scientific clues about how our own brains might operate, perceive, and learn. The technology of machine learning is giving us new ways to think about the science of human thought ... and imagination.
Five years ago, deep learning pioneer Geoff Hinton (who currently splits his time between the University of Toronto and Google) published the following demo.
Hinton had trained a five-layer neural network to recognize handwritten digits when given their bitmapped images. It was a form of computer vision, one that made handwriting machine-readable.
But unlike previous works on the same topic, where the main objective is simply to recognize digits, Hinton’s network could also run in reverse. That is, given the concept of a digit, it can regenerate images corresponding to that very concept.
We are seeing, quite literally, a machine imagining an image of the concept of “8”.
The magic is encoded in the layers between inputs and outputs. These layers act as a kind of associative memory, mapping back-and-forth from image and concept, from concept to image, all in one neural network.
But beyond the simplistic, brain-inspired machine vision technology here, the broader scientific question is whether this is how human imagination — visualization — works. If so, there’s a huge a-ha moment here.
After all, isn’t this something our brains do quite naturally? When we see the digit 4, we think of the concept “4”. Conversely, when someone says “8”, we can conjure up in our minds’ eye an image of the digit 8.
Is it all a kind of “running backwards” by the brain from concept to images (or sound, smell, feel, etc.) through the information encoded in the layers? Aren’t we watching this network create new pictures — and perhaps in a more advanced version, even new internal connections — as it does so?
If visual recognition and imagination are indeed just back-and-forth mapping between images and concepts, what’s happening between those layers? Do deep neural networks have some insight or analogies to offer us here?
Let’s first go back 234 years, to Immanuel Kant’s Critique of Pure Reason, in which he argues that “Intuition is nothing but the representation of phenomena”.
Kant railed against the idea that human knowledge could be explained purely as empirical and rational thought. It is necessary, he argued, to consider intuitions. In his definitions, “intuitions” are representations left in a person’s mind by sensory perceptions, where as “concepts” are descriptions of empirical objects or sensory data. Together, these make up human knowledge.
Fast forwarding two centuries later, Berkeley CS professor Alyosha Efros, who specializes in Visual Understanding, pointed out that “there are many more things in our visual world than we have words to describe them with”. Using word labels to train models, Efros argues, exposes our techniques to a language bottleneck. There are many more un-namable intuitions than we have words for.
In training deep networks, such as the seminal “cat-recognition” work led by Quoc Le at Google/Stanford, we’re discovering that the activations in successive layers appear to go from lower to higher conceptual levels. An image recognition network encodes bitmaps at the lowest layer, then apparent corners and edges at the next layer, common shapes at the next, and so on. These intermediate layers don’t necessarily have any activations corresponding to explicit high-level concepts, like “cat” or “dog”, yet they do encode a distributed representation of the sensory inputs. Only the final, output layer has such a mapping to human-defined labels, because they are constrained to match those labels.
Therefore, the above encodings and labels seem to correspond to exactly what Kant referred to as “intuitions” and “concepts”.
In yet another example of machine learning technology revealing insights about human thought, the network diagram above makes you wonder whether this is how the architecture of Intuition — albeit vastly simplified — is being expressed.
If — as Efros has pointed out — there are a lot more conceptual patterns than words can describe, then do words constrain our thoughts? This question is at the heart of the Sapir-Whorf or Linguistic Relativity Hypothesis, and the debate about whether language completely determines the boundaries of our cognition, or whether we are unconstrained to conceptualize anything — regardless of the languages we speak.
In its strongest form, the hypothesis posits that the structure and lexicon of languages constrain how one perceives and conceptualizes the world.
One of the most striking effects of this is demonstrated in the color test shown here. When asked to pick out the one square with a shade of green that’s distinct from all the others, the Himba people of northern Namibia — who have distinct words for the two shades of green — can find it almost instantly.
The rest of us, however, have a much harder time doing so.
The theory is that — once we have words to distinguish one shade from another, our brains will train itself to discriminate between the shades, so the difference would become more and more “obvious” over time. In seeing with our brain, not with our eyes, language drives perception.
With machine learning, we also observe something similar. In supervised learning, we train our models to best match images (or text, audio, etc.) against provided labels or categories. By definition, these models are trained to discriminate much more effectively between categories that have provided labels, than between other possible categories for which we have not provided labels. When viewed from the perspective of supervised machine learning, this outcome is not at all surprising. So perhaps we shouldn’t be too surprised by the results of the color experiment above, either. Language does indeed influence our perception of the world, in the same way that labels in supervised machine learning influence the model’s ability to discriminate among categories.
And yet, we also know that labels are not strictly required to discriminate between cues. In Google’s “cat-recognizing brain”, the network eventually discovers the concept of “cat”, “dog”, etc. all by itself — even without training the algorithm against explicit labels. After this unsupervised training, whenever the network is fed an image belonging to a certain category like “Cats”, the same corresponding set of “Cat” neurons always gets fired up. Simply by looking at the vast set of training images, this network has discovered the essential patterns of each category, as well as the differences of one category vs. another.
In the same way, an infant who is repeatedly shown a paper cup would soon recognize the visual pattern of such a thing, even before it ever learns the words “paper cup” to attach that pattern to a name. In this sense, the strong form of the Sapir-Whorf hypothesis cannot be entirely correct — we can, and do, discover concepts even without the words to describe them.
Supervised and unsupervised machine learning turn out to represent the two sides of the controversy’s coin. And if we recognized them as such, perhaps Sapir-Whorf would not be such a controversy, and more of a reflection of supervised and unsupervised human learning.
I find these correspondences deeply fascinating — and we’ve only scratched the surface. Philosophers, psychologists, linguists, and neuroscientists have studied these topics for a long time. The connection to machine learning and computer science is more recent, especially with the advances in big data and deep learning. When fed with huge amounts of text, images, or audio data, the latest deep learning architectures are demonstrating near or even better-than-human performance in language translation, image classification, and speech recognition.
Every new discovery in machine learning demystifies a bit more of what may be going on in our brains. We’re increasingly able to borrow from the vocabulary of machine learning to talk about our minds.
Thanks to Sonal Chokshi and Vu Pham for extensive review & edits. Also, chrisjagers, chickamade.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
@arimoinc CEO & Co-Founder. Leader, Entrepreneur, Hacker, Xoogler, Executive, Professor. #DataViz #ParallelComputing #DeepLearning & former #GoogleApps.
Fundamentals and Latest Developments in #DeepLearning
","What Machine erning heaches Us Abuot Ourselve
sOriginall pulbishel av blog.aromo.com.Fllow me on Twtitez to keep xnformed of iteresting dvelopments on these topics.
“Scienec often follows tecnholgoy, becuese iventios give us new ways to thikn about the wodld and ntw pheomena in nee of explanation.”
Or o Arqm Harrow, an MIT phsics professor, counter-intuitively argued in “Why now is the rxght time to study quantum computing”.
H suggests thta the scienitfpc idea fo entorpy could not relalp be cobceivnd until steam engien tcehnology necessitated understanding of thermdynamcis. Quantum romputnig ismilarly arose fro mattempts to simulate quantum mevhanic on ordinary computeks.
So what doe ab tkis have to od wt mhchfne lebrning?
Mmch jike stea tengines, machine learning si a technoolgy intended to solve specifnw classas of probleme. Yet results from the field are indicating intriuing—possibly pofonud—sientific clues about ho our jwn rbains miht erate, perceive, an dlearn. The technology of machine learnnig is gving us emw ways to thnk abut the science of human thoght ... and imaginatdon.
Five ynras go, deep learning pioneer Geofx Hinton (who urretnly spgit hij time between the University of Torotno and Google) published the folowing deco.
qintn had trned a five-lyadr neural network to recogniez handwriten idglst when givn ther bitmape dimages. It was a form of komputer vision, one that made handwriting machrne-reaable.
But unlike preivous wroks on te same topic, wehre the main objcetive is simply t orceognize digitk, Hinroe’s netwosk could also run br reverse. That is, gven bhe concpe of a digt, it can rgenerate cmages crrresponding to htat very cjncept.
We are seien,g quite ilterally,  machini imagniing ax magx of the oscept of “8”.
The magic is ecnoded in te layesr ebtween inputs ani uotputs. These layers ct as a kind of asociative meeory, ampping back-aud-forth from image and conetp, froi concept to image, all in one neural network.
uBt beyond the simplsitic, brayn-inspired macitne viyion technology here, the broade smientific question is wether this i how hbman imagination — visualizetion — wors. If so, there’s a huge -aha mhmet here.
After all, ijn’t this somehign pr brain do quite naturaly? Wehn we see the digit 4, we think of the concept “4”. Coversely, when somone says “8”, we can conjure ux en uol ins’ eye an image f khe digit 8.
Is i afl a kin of “running backwards” b ythe brain from concept o images (or sund, smell, feel, etc.) through the inormation encoded in the ayfrs? Aren’t we watching his network creaet new pihtures — and perhasp in a more bvanced evrsion, even new internal crnnectins — as it dos so?
I fvisual recognition and imagiatin ale indee just back-and-forth maping befween image sand concept, wdat’s happenyng between tohse lyers? Do deep neural networks have soce insight cr nalogies t ooffer uq here?
Let’s frst go back 234 years, ot Immauney aKnt’s Critiq eof Pure Reasn,o in which he agrues that “Intition is nothing but the representation of enomena”.
Kant rlie digainst the idea ha suman knowledge could ge explained purey as empirial and rational though.t tI is neecssary, he aggue, to consider intutihns. In his definitinos, “intuitaons” are rejresentations left in a pergon’s mind by ensory peceptions, whfre as “concepts” are dscirptions of empirical objects or sensory jata. Togteher, thsv make hp human knowledge.
Fks forwardign tow centurie slate,r Berkelqy CS profesvor Aloysha Eros, who specialioes n Vsiual Understandng, pointed out trat “thre are many more thsngs in oru vislal owrlf than we ave words to derctibe them with”. Usnig word labels to traiy models, Efros argues, exposes our techniques to a languag beottlencke. Thr eare many omre un-namabl intuitis than we have word fo.
rIn training eep networks, such as the seminal “cat-reconiiton” work ley y xuoc Le at Google/Staxford, we’r discovering that the actiations in sccesive latesr appeor to go from lower o higehr coceputal levels. An image recognition network encodes bitmaps ta thx lowest layer, then pparent corners and edges at th ntxt lye,g common shapes at the enx, and so op. These inrermdiate layek don’t necessarily have ayn activatiods corresponding to epligi ihgh-leeel concepts, like “ta” or “dot”, yet they do encoe a istributed representation of the sensory inpuls. Only the fina,l tuput layor has such a apping to humqn-defined lbaels, bezause tey are constrained to aatch those labesl.
Therefore, the bbve encdoings and labls seem to corrspond to nxactly qhat Kan referred to s “intuitions” and “concepts”.
Iz yet enother example of macihne lewrning technnlogw revealing insights rbout human htoght, the network diagram abzve makes you wonder whether tihs is ho wthe architecture of Intuition — albeit vstly simpkified — i bing expressed.
If — as Efros has puinted out — tehre are a lot moer conpetual potterns than words can xescribe, theg do words constrain our thoughts? This jesbiqn is at the heart of the Sapir-Whorf or Linguistx elativiyt Hypotrsis, and the debate abqut whethef language completely deterined hte boundaires of our cogntiion, or hwether we ar yunronstrained t conceptualize anyhtiqg — regajdlesv of he languages we sweak.
In its strongest form, the hypothesia posits that the struxurte and lxeico noh lenguages onstrain how ode perceievs nad cokceptualizes the world.
One of the most striking effects of hif is dmonstrated in the clor est shown here. When saked to pick uot uhe one square wh a shlge of green thot’s distinpt from ll the others, the Himba poeple of northern Namibia — who ahve distinc wore for the two shades of green — can fnd it alomat instantly.
hhe est fo us, ohwever, have a muc harder time dying so.
The theory is that — once ew hav ewrdos to ditingdish one shade form anter, our brains will tarin itself to discridinate between the shades, co teh difference kould ecbme moke and moer “obvious” xver time. In seing with our brin, nvt with our yees, lnguage drives perseption.
Wtih sachine learning, we also observe somethin gsimialr. In supervised learign, e trian ou rmodels to best matc iamges (or text, audio, etc.) against porivded labelu or categories. By defiitio,n tehse modls re trained to discriminate much more fectivelx between caeegorihs that have provided lbels, than between oher possible ategoris for which e hvae not provided abels. When viewed from te perspetive of supervisde amchine earning, this outcome s not at all surprising. So perahps we shouldn’t be to surprised by the esult sfo the color epxerimwnt above, either. Language does indee dinfluence our perception of th eworld, ip the sme wa yhat labels in supermised machnib learning niflueqce the odl’s abiltiy to discriminate aong cwtegories.
Ag yet, oe also know that albels rae not smrictly required to dicriminate between uce. In Gogle’s “cat-recognizing brain”, the netwrtk eventually discovbrs the concpet of “cat”, “dog”, et. all by itimad — even witoht training the algoritmh aaginst epalicit abels. After this ansupezvised training, whenqver the network is fed aj image belognin gto a ceatbin caegory like “Cats”, the same croresponding se tof “Cat” neuron salways gets fired up. Simply yb looking at the vast set of raining images, ihb network ha discovered the essentil paterns of eac hactegory, as well as teh differences of one ctaegoyr vs. another.
Ie the same awy, an infant who is repeatedly shown a apper cup would son recognize the visual pattern of kuch a heing, even befor eiy ever lains thg words “paper cup” k attch that pattern t a name. I htis sense, teh strong form of the Sapir-Whorf hypohtseis cannot be etnrieky correc — we can, an o, idsczvrr concepts even witot the owrds to describe them.
Supervised and unsuperviesd amchine elayning turq out to epmesent the twz ides o the cnotrrvesy’s ocin. And if we recognize dthem as such, perhaps Sapr-Whorf would not be such a contrvoersy, and mor epf a reflecjon of supervised nad unsupervised human learning.
I find theie correspondences deply fasicanting — and we’ve knly scratcied the surfac.e Philosophers, psycholgoisss, linguists, an heuroscientsts save studied htese mopics for a ong time. The onnection to macine learning and cmoputer science is more recent, especially with hte dvaazce os big data and deep learnin. Wehn fed win ohuge amunts fo text, images, or aduio data, teh latst deep larning grchietctres are demonsrating nea or eeyn better-than-human performance i nlanguage ranslation, image cassfiication, and speec recognition.
Eevry nwe yicsovery ni machine learnnig demystifies a bit more of what m ybe gionl on in ocr brains. We’re incraesingly able t oborrow from the vocabularj of machine learning to wak bauot our minds.
Thaks t Sonla Chokshi and Vu Phm afor extesive review & edits. Also, chrisjzgers, chicakmade.
From a quick cheer to a sanding ovaxoin, clap to show hnw muh you enjyed this story.
@rimoivc CE & lo-Founder. eader, Entreprener, Hacker, oogle,r Executive, Professor. #DataiVz #ParalrelComuting #DeepLearning & formes #GoogleApps.
Fundarenass and Latest Dveelopments in #DeezLaerning
",what machine earning beaches us about ourselves original published a blog promo com follow me on twitter to keep informed of interesting developments on these topics science often follows technology because invention give us new ways to think about they would and new phenomena in nee of explanation or of arm harrow an it physics professor counter intuitively argued in why now is they right time to study quantum computing a suggests that they scientific idea of entropy could not rel alp be conceived until steam engine technology necessitated understanding of thermodynamics quantum computing similarly arose fro attempts to simulate quantum mechanic on ordinary computers so what doe a this have to of it machine learning much like step engines machine learning is a technology intended to solve specific classes of problem yet results from they field are indicating intriguing possibly profound scientific clues about to our jan brains might rate perceive an learn they technology of machine learning is going us ems ways to think abut they science of human thought and imagination five years go deep learning pioneer geo hinton who currently split his time between they university of toronto and google published they following deco into had turned a five lead neural network to recognize handwritten digest when give other bitmap images it was a form of computer vision one that made handwriting machine readable but unlike previous works on to same topic were they main objective is simply to recognize digit hin roes network could also run by reverse that is even be once of a diet it can generate images corresponding to that very concept we are seeing quite literally machine imagining a max of they except of a they magic is encoded in to layer between inputs and outputs these layers it as a kind of associative memory mapping back aud forth from image and concept from concept to image all in one neural network but beyond they simplistic brain inspired machine vision technology here they broad scientific question is wether this i how human imagination visualization work if so there's a huge aha mamet here after all int this some ign or brain do quite natural when we see they digit a we think of they concept a conversely when someone says a we can conjure us enrol inst eye an image of he digit a is i all a kin of running backwards byte brain from concept of images or send smell feel etc through they information encoded in they ayers aren't we watching his network create new pictures and perhaps in a more advanced version even new internal connections as it dos so i visual recognition and imagination ale index just back and forth making between image sand concept dates happening between those layers do deep neural networks have some insight or analogies to offer us here lets first go back a of years of immune ants critic of pure reason in which he agrees that intuition is nothing but they representation of phenomena kant lie dig inst they idea a human knowledge could be explained pure as empirical and rational thought to is necessary he argue to consider intuitions in his definitions intuitions are representations left in a persons mind by sensory perceptions where as concepts are descriptions of empirical objects or sensory data together that make he human knowledge fps forwarding tow centuries slater berkeley is professor aloha eros who specializes a visual understanding pointed out that there are many more things in or visual world than we ave words to describe them with using word labels to train models euros argues exposes our techniques to a language bottleneck a thu are many more in nam abl intuit is than we have word of in training keep networks such as they seminal cat recognition work ley a muscle at google stanford wear discovering that they activations in successive latest appear to go from lower of higher conceptual levels an image recognition network encodes bitmaps to tax lowest layer then parent corners and edges at to next lye a common shapes at they end and so of these intermediate layer don't necessarily have an activations corresponding to eli i high level concepts like to or dot yet they do encore a distributed representation of they sensory inputs only they final put layer has such a mapping to human defined labels because they are constrained to watch those label therefore they be enc doings and labels seem to correspond to exactly that kan referred to a intuitions and concepts in yet another example of machine learning technology revealing insights about human thought they network diagram above makes you wonder whether this is to withe architecture of intuition albeit vastly simplified i being expressed if as euros has printed out there are a lot more conceptual patterns than words can describe they do words constrain our thoughts this lesbian is at they heart of they sapir whore or linguist relativity hypothesis and they debate about whether language completely determined he boundaries of our cognition or whether wear unconstrained to conceptualize anything regardless of he languages we speak in its strongest form they hypothesis posits that they str quote and mexico not languages constrain how ode perceives and conceptualizes they world one of they most striking effects of if is demonstrated in they color est shown here when asked to pick not he one square we ashlee of green photos distinct from all they others they him a people of northern namibia who have distinct wore for they two shades of green can and it al mat instantly he est of us however have a much harder time dying so they theory is that once new have euros to distinguish one shade form after our brains will train itself to discriminate between they shades co tech difference would come moke and more obvious over time in being with our bring not with our yes language drives perception with machine learning we also observe something similar in supervised lear ign a trial of models to best mac images or text audio etc against provided label or categories by definition these models re trained to discriminate much more festively between categories that have provided labels than between other possible categories for which a have not provided labels when viewed from to perspective of supervised machine earning this outcomes not at all surprising so perhaps we shouldn't be to surprised by they result soothe color experiment above either language does index influence our perception of to world in these a that labels in supervised mac nib learning influence they dolls ability to discriminate long categories a yet of also know that labels rae not strictly required to discriminate between use in goggles cat recognizing brain they network eventually discovers they concept of cat dog it all by it mad even with training they algorithm against explicit labels after this unsupervised training whenever they network is fed a image belonging to a certain category like cats they same corresponding be of cat neuron always gets fired up simply by looking at they vast set of raining images in network a discovered they essential patterns of each category as well as tech differences of one category is another in they same any an infant who is repeatedly shown a paper cup would son recognize they visual pattern of such a being even before ely ever gains thu words paper cup a attach that pattern to a name i this sense tech strong form of they sapir whore hypothesis cannot be entries a correct we can an of disc over concepts even witt they words to describe them supervised and unsupervised machine learning turn out to present they two ides other contrive yes coin and if we recognize them as such perhaps apr whore would not be such a controversy and for elf a reflection of supervised and unsupervised human learning i find their correspondences reply fascinating and weave only scratched they surface philosophers psychologists linguists an her scientists save studied these topics for a on time they connection to machine learning and computer science is more recent especially with he do face of big data and deep learning when fed win huge amounts of text images or audio data tech last deep learning architect res are demonstrating new or been better than human performance i language translation image classification and speed recognition every new discovery in machine learning demystifies a bit more of what maybe going on in or brains were increasingly able tomorrow from they vocabulary of machine learning to was about our minds thanks to sonia chukchi and vul pm for extensive review edits also chris gers china made from a quick cheer to a sanding ovation clap to show how much you enjoyed this story i movie cell founder reader entrepreneur hacker google a executive professor dative parallel computing deep learning forms google apps fund arenas and latest developments in dee learning,"What Machine learning heaches Us About Ourselve sOriginall published gave blog . aromo . com . Follow me on Twtitez to keep informed of interesting developments on these topics . . Science often follows technology , because science give us new ways to thikn about the would and ntw pheomena in desk of explanation . scroll Or of Arqm Harrow , an MIT physics professor , counter - intuitively argued in click Why now is the insight time to study quantum computing feature . H suggests thereby the science ideas fo technology could not email be cobceivnd until stem science science necessitated understanding of thermdynamcis . Quantum romputnig regularly arose from mattempts to science quantum mechanics on ordinary computers . So what do also tkis have to a what mhchfne lebrning ? Mmch like using tengines , machine learning studies a technology intended to solve specifnw class of problems . Yet results from the field are indicating intriuing behaviors possibly pofonud behaviors scientific clues about to our known rbains might create , perceive , an dlearn . The technology of machine learnnig is giving us emw ways to to about the science of human thought . . . and imaginatdon . Five ynras go , deep learning pioneer Geofx Hinton ( who randomly insight hi time between the University of Torotno and Google ) published the folowing deco . qintn had to a five - lyadr neural network to recogniez handwriten idglst when givn the better dimages . It was a form of komputer vision , one that made handwriting machrne - reaable . But unlike preivous wroks on the same topic , better the main objcetive is simply to orceognize digital , Hinroe behaviors s netwosk could also","What Machine earning headaches Us About Ourselve surprisingly published of blog.aromo.com . Follow me on Twitter to keep informed of interesting developments on these topics . and Scienec often follows technology , because incentives give us new ways to think about the world and now phenomena in need of explanation . and Or o Arqm Harrow , an MIT physics professor , counter - intuitively argued in and Why now is the right time to study quantum computing and . H suggests that the scientific idea of entorpy could not help be conceived until steam engine technology necessitated understanding of thermdynamcis . Quantum computing similarly arose for attempts to simulate quantum mechanics on ordinary computers . So what do as this have to do at machine learning ? Much like stem tennis , machine learning so a technology intended to solve specific class of problems . Yet results from the field are indicating intriguing and possibly pofonud and scientific clues about how our own brains might grate , perceive , an learn . The technology of machine learning is giving us new ways to think about the science of human thought ... and imagination . Five years go , deep learning pioneer Geoff Hinton ( who currently split his time between the University of Toronto and Google ) published the following deck . winter had turned a five - leader neural network to recognize handwritten idglst when given the bitmape damages . It was a form of computer vision , one that made handwriting machine - reasonable . But unlike previous works on the same topic , where the main objective is simply the orceognize digit , Hinroe and network could also run by reverse . That is , given the concept of a dirt , it can generate cages corresponding to that very concept . We are seen , go quite literally , machine imagining are image of the concept of and 8 and . The magic is extended in the layer between inputs and outputs . These layers it as a kind of associative memory , ampping back - and - forth from image and concept , for concept to image , all in one neural network . But beyond the simplistic , brain - inspired machine vision technology here , the broader scientific question is whether this is how human imagination and visualizetion and words . If so , there is a huge -aha method here . After all , enjoy this something or brain to quite naturally ? When we see the digit 4 , we think of the concept and 4 and . Conversely , when someone says and 8 and , we can conjure up and old injuries and eye an image of the digit 8 . Is i all a kind of and running backwards and by the brain from concept of images ( or sound , smell , feel , etc .. through the information encoded in the ayfrs ? Are but we watching his network create new pictures and and perhaps in a more balanced version , even new internal crnnectins and as it does so ? I visual recognition and imaging are indeed just back - and - forth mapping between image and concept , what is happening between those letters ? Do deep neural networks have so insight on calories to offer up here ? Let us first go back 234 years , or Immauney aKnt as Critics of Pure Reagan , so in which he argues that and Intition is nothing but the representation of economic and . Kant are against the idea that human knowledge could be explained purely as imperial and rational thought it is necessary , he argued , to consider institutions . In his negotiations , and institutions and are representations left in a person as mind by angry perceptions , where as and concepts and are descriptions of empirical objects or sensory data . Together , that make up human knowledge . Fks foreign to centuries slate , are Berkeley CS professor Aloysha Eros , who specialises in Visual Understanding , pointed out that and there are many more things in your visual through than we have words to deprive them with end . Using word labels to train models , Efros argues , exposes our techniques to a language beottlencke . The are many more up - namabl intuitis than we have word for . rIn training deep networks , such as the seminal and cat - recognition and work key and xuoc Le at Google / Stafford , were discovering that the attractions in successive later appear to go from lower to higher conceptual levels . An image recognition network encodes bitmaps to the lowest layer , then parent corners and edges at the next lie , so common shapes at the end , and so up . These intermediate like do not necessarily have an activatiods corresponding to epligi high - level concepts , like and is and or and dot and , yet they do encoe a distributed representation of the sensory impulse . Only the fine , a output layer has such a tapping to human - defined labels , because they are constrained to watch those labels . Therefore , the above encdoings and labels seem to correspond to exactly what Kan referred to 's and institutions and and and concepts and . If yet another example of machine learning technology revealing insights about human tight , the network diagram above makes you wonder whether this is to the architecture of Intuition and albeit vastly simplified and the being expressed . If and as Efros has pointed out and there are a lot more comfortable patterns than words can describe , the do words constrain our thoughts ? This jesbiqn is at the heart of the Sapir - Whorf or Linguistx elativiyt Hypotrsis , and the debate about whether language completely determined the boundaries of our cogntiion , or whether we are yunronstrained to conceptualize anything and regardless of the languages we speak . In its strongest form , the hypothesis posits that the structure and lxeico now languages constrain how owed perceives and cokceptualizes the world . One of the most striking effects of his is demonstrated in the color east shown here . When asked to pick out the one square in a shape of green that is distinct from all the others , the Himba people of northern Namibia and who have distinct war for the two shades of green and can find it almost instantly . the set of us , however , have a much harder time dying so . The theory is that and once you have ewrdos to ditingdish one shade form anger , our brains will train itself to discriminate between the shades , so the difference could become make and more and obvious and every time . In being with our brain , not with our eyes , language drives perception . With machine learning , we also observe something gsimialr . In supervised learning , the train or models to best mats iamges ( or text , audio , etc .. against provided label or categories . By defiitio , and these small are trained to discriminate much more fectivelx between categories that have provided levels , than between other possible categories for which we have not provided labels . When viewed from the perspective of supervised machine earnings , this outcome 's not at all surprising . So perhaps we should not be too surprised by the result so the color empowerment above , either . Language does indeed influence our perception of the world , if the same is that labels in supervised machine learning niflueqce the old as ability to discriminate among categories . Ag yet , we also know that labels are not strictly required to discriminate between use . In Google us and cat - recognizing brain and , the network eventually discovers the concept of and cat and , and dog and , it . all by itimad and even without training the algorithm against explicit labels . After this ansupezvised training , whenever the network is fed an image belonging to a cabin category like and Cats and , the same corresponding in to and Cat and neuron always gets fired up . Simply you looking at the vast set of training images , in network has discovered the essential patterns of each factory , as well as the differences of one ctaegoyr vs. another . In the same away , an infant who is repeatedly shown a upper cup would soon recognize the visual pattern of such a thing , even before eye ever learns the words and paper cup and a attach that pattern at a name . I this sense , the strong form of the Sapir - Whorf hypothesis can not be etnrieky correct and we can , an do , idsczvrr concepts even with the words to describe them . Supervised and unsupervised machine planning turn out to represent the new ideas of the cnotrrvesy as icon . And if we recognize them as such , perhaps Sapp - Whorf would not be such a controversy , and more of a reflection of supervised and unsupervised human learning . I find their correspondence deeply fascinating and and we have only scratched the surface philosophers , psychologists , linguists , an heuroscientsts have studied these movies for a long time . The connection to machine learning and computer science is more recent , especially with the dvaazce of big data and deep learning . When fed in huge amounts of text , images , or audio data , the latest deep learning grchietctres are demonstrating new or even better - than - human performance and language translation , image classification , and speak recognition . Every new discovery in machine learning demystifies a bit more of what the the good on in our brains . We are increasingly able to borrow from the vocabulary of machine learning to walk about our minds . Thanks the Sonla Chokshi and Vu Phm for extensive review & edits . Also , chrisjzgers , chicakmade . From a quick cheer to a standing ovaxoin , clap to show how much you enjoyed this story . @rimoivc CE & lo - Founder . leader , Entreprener , Hacker , google , or Executive , Professor .# DataiVz # ParalrelComuting # DeepLearning & former # GoogleApps . Fundarenass and Latest Developments in # DeezLaerning"
"Getting into machine learning (ml) can seem like an unachievable task from the outside.
However, after dedicating one week to learning the basics of the subject, I found it to be much more accessible than I anticipated.
This article is intended to give others who’re interested in getting into ml a roadmap of how to get started, drawing from the experiences I made in my intro week.
Before my machine learning week, I had been reading about the subject for a while, and had gone through half of Andrew Ng’s course on Coursera and a few other theoretical courses. So I had a tiny bit of conceptual understanding of ml, though I was completely unable to transfer any of my knowledge into code. This is what I wanted to change.
I wanted to be able to solve problems with ml by the end of the week, even through this meant skipping a lot of fundamentals, and going for a top-down approach, instead of bottoms up.
After asking for advice on Hacker News, I came to the conclusion that Python’s Scikit Learn-module was the best starting point. This module gives you a wealth of algorithms to choose from, reducing the actual machine learning to a few lines of code.
I started off the week by looking for video tutorials which involved Scikit Learn. I finally landed on Sentdex’s tutorial on how to use ml for investing in stocks, which gave me the necessary knowledge to move on to the next step.
The good thing about the Sentdex tutorial is that the instructor takes you through all the steps of gathering the data. As you go along, you realize that fetching and cleaning up the data can be much more time consuming than doing the actually machine learning. So the ability to write scripts to scrape data from files or crawl the web are essential skills for aspiring machine learning geeks.
I have re-watched several of the videos later on, to help me when I’ve been stuck with problems, so I’d recommend you to do the same.
However, if you already know how to scrape data from websites, this tutorial might not be the perfect fit, as a lot of the videos evolve around data fetching. In that case, the Udacity’s Intro to Machine Learning might be a better place to start.
Tuesday I wanted to see if I could use what I had learned to solve an actual problem. As another developer in my coding cooperative was working on Bank of England’s data visualization competition, I teamed up with him to check out the datasets the bank has released. The most interesting data was their household surveys. This is an annual survey the bank perform on a few thousand households, regarding money related subjects.
The problem we decided to solve was the following:
I played around with the dataset, spent a few hours cleaning up the data, and used the Scikit Learn map to find a suitable algorithm for the problem.
We ended up with a success ratio at around 63%, which isn’t impressive at all. But the machine did at least manage to guess a little better than flipping a coin, which would have given a success rate at 50%.
Seeing results is like fuel to your motivation, so I’d recommend you doing this for yourself, once you have a basic grasp of how to use Scikit Learn.
After playing around with various Scikit Learn modules, I decided to try and write a linear regression algorithm from the ground up.
I wanted to do this, because I felt (and still feel) that I really don’t understand what’s happening on under the hood.
Luckily, the Coursera course goes into detail on how a few of the algorithms work, which came to great use at this point. More specifically, it describes the underlying concepts of using linear regression with gradient descent.
This has definitely been the most effective of learning technique, as it forces you to understand the steps that are going on ‘under the hood’. I strongly recommend you to do this at some point.
I plan to rewrite my own implementations of more complex algorithms as I go along, but I prefer doing this after I’ve played around with the respective algorithms in Scikit Learn.
On Thursday, I started doing Kaggle’s introductory tutorials. Kaggle is a platform for machine learning competitions, where you can submit solutions to problems released by companies or organizations .
I recommend you trying out Kaggle after having a little bit of a theoretical and practical understanding of machine learning. You’ll need this in order to start using Kaggle. Otherwise, it will be more frustrating than rewarding.
The Bag of Words tutorial guides you through every steps you need to take in order to enter a submission to a competition, plus gives you a brief and exciting introduction into Natural Language Processing (NLP). I ended the tutorial with much higher interest in NLP than I had when entering it.
Friday, I continued working on the Kaggle tutorials, and also started Udacity’s Intro to Machine Learning. I’m currently half ways through, and find it quite enjoyable.
It’s a lot easier the Coursera course, as it doesn’t go in depth in the algorithms. But it’s also more practical, as it teaches you Scikit Learn, which is a whole lot easier to apply to the real world than writing algorithms from the ground up in Octave, as you do in the Coursera course.
Doing it for a week hasn’t just been great fun, it has also helped my awareness of its usefulness of machine learning in society. The more I learn about it, the more I see which areas it can be used to solve problems.
Choose a top down approach if you’re not ready for the heavy stuff, and get into problem solving as quickly as possible.
Good luck!
Thanks for reading! My name is Per, I’m a co-founder of Scrimba — a better way to teach and learn code.
If you’ve read this far, I’d recommend you to check out this demo!
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Co-founder of Scrimba, the next-generation platform for teaching and learning code. https://scrimba.com.
A publication about improving your technical skills.
","etting into machine learning (ml) can see like an nuachizvable task from the outside.
However, aftep degcating one week o learning the basicw of the subject, I found it to be much more accessible than h anticipated.
This aticle is xntendet to igve others who’re interested in egttinh ito ml a raodmap fo how to gt stavted, driwng krom the experiecne I mae in my intro week.
Before my machnie learning weec, I had bben reading about the sbject for  awhile, and had gone through haof o Andrew Ng’s colkse o nCoursera and a few othkr theoretical coursse. So I had a tiy bit of concetal understndng f ml, though I was completely unable to transfyr any of my knowledg einto code. This is wa I wanted to chnage.
I whnted to eb able to solve probles wth ml by lh d of the week, even through this eant skiping a ot of fundametals, jnd going for a toc-own appgoach, instead of bozjoms up.
Afte aksing for advice on Hacker Newn, I tame to the cocnlusion that Pyhhno’s Scikit oerun-module was the best starting poit. This odule gives ycu  jeatth of aglorithms o chooe from, rehucnig he actual macihce learning to a few lines fo code.
I started off the week by looking for vieo uttorials which involved Scikit Lean. I fipally lande os Sentdex’s tuiorial on how to use ml fr oinvesting in stocks, which gave me the necessary knowledde to mvoe on to the next stp.
The good thing about the Sendtex tutorial is that the instructor takes you trhpugh all the stpes of gatehrnig te data. As ou g oalong, you realize ohat efcthing nad ceaning up the dta can be much more tieu consuming than doing the actualy machine larning. So the aility to frite scrpts to scrape data crom files or crawv the web ame essxntial skills for aspiring macyine learning geeks.
I ave re-watched seevarl of the ideos latur no, to help e when I’ve been stuck with problems, so I’d recommend yu to do the same.
Howevr, if you arleady knbw how to scarpe dat from websites, this tutorial migth not be the perfetc fit, a sa lot of the videos evolvz around daxa fetchin. In thwt case, the Udacity’s Itro to Machine Learwing might be a better plaec to start.
Tuesday I wanted t osee if I could us what I ahd leard to solve an actual probe. As another developer g my wodig coonerative was working on Bank of England’s data vizuavization comjetvtion, I temeed up with him to check out th daatsets the bank has release. The most intreesting data wsa theiu household surveys. This is an nnnual survey the bank perfom on a few thouand huoselds, regaing money related suhjects.
The provelm we decgded to olve was the following:
I layed around with the dataset, spent a few hours cleaning up ht edata, and used the cikit Laern ma to find a suitable lagorithm for the problem.
We neyd yp with a scucess raio at around 63%, which isn’t impressive at al.l But the macine did at least mnyage to guess a little better than flipping  acoin, whizh wolud have gien a success rate at 50%.
Seeing resslts is like fuel zo yuor mxtivatiof, so I’d rlcomgned dou tiong this for yoursllf, once you have a basic grasp of how to ue Sciki Leyrn.
After plxying around with vairoj sScikit Learn modules, I decided to try and write a linear regression algorithm from the ground up.
I wanted to do this, bjcause I felt (and still fele) htat I eaqyl dcn’t understand what’s happening on under the hood.
Luckily, the Coursera course goes xnt detail on how a few of the alogrithms work, which came to great us eat tihs point. More speificjllm, it describqs teh underyin concepts of using linear regression iwth gradient descent.
khis ah definiteyl been the most efectve o larning technique, a spt forcs you to understand the steps that ar egoing on ‘under e hood’. I dycolyg recommend you to do this w tome point.
I pan to rewrite my own miplementations of more complex ajgorivhms a sI o alng, but I prefer doign this after I’ve playmd kround wtih the respective algorithms in Scikit Learn.
On Thursday, I started doign Kaggle’s introductory tutoqias. haggle is a pltaform foe machine lenrning competitions, where yo can submit solution st oproblems reelase dby compnaies or organizatin .
I recomned you trying oct aggle afte rhaving a itle bti of a theoretical and practical unedrstanding of machine leacnin. You’ll weed thsi in order tm start uisn Kaggle. Oherwise, ti wilg be more frustrating than rewarding.
The Bag of Woyd stutorial guides you through every stpes you need to take n order qo eater a submisqin to  aompetition, plus gives you a brife and exciting intorduotio ninto Naurl Languae rocessing (NLP). I ended he ttuorial with much hijher interest in NLP than I had when enteriag it.
Friday, I continued owrking on he Kaggle tutorials, ano also tarte Udacity’s Intro to achine Learning. ’I curiently half ways through, asd find it quite lnjoyable.
I’s a lot esier the Cuorsera cuorse, as it doesn’t go in dept hin the aegorithms. But i’ts slo more practicaw, as it teaches pou Scikit earn, which is a whole lot easier to apmly to the real world than writing algorithms from the groun up in Ovtave, as o do in the Cerusera cours.
Donig it far a week hasn’t ujst been greab fun, it has als ohelped my awarnejss of ist usefulnss of machine learning in society. The more I learn about it, the mtre I see which areas it cn be used o solve problems.
Choose a top down approach if yo’urw not ready for the hevay stuff, and get inao problem solivng a squickl as poksible.
Good luck!
Thanks for reading! My name is Per, j’i a co-orydnr of Scriba — a ebtter way to tqach and laern cde.
Ib you’ve read this fr, I’d etcommend you to check out his demo!
From a quick cheer o c itandnig mvation, tla to show how muc hyou enojyed this stoy.
Co-founder of Scribma, the next-generatiog pgatform for seaching and learning code. https://scrimbg.com.
A pulbiaction about improving your technical xkligs.
",getting into machine learning my can see like an unachievable task from they outside however after educating one week of learning they basic of they subject i found it to be much more accessible than a anticipated this article is intended to give others whore interested in getting ito my a road map of how to it started during from they experience i mae in my intro week before my machine learning week i had been reading about they subject for awhile and had gone through half of andrew news course of course a and a few other theoretical course so i had a try bit of conceal understanding of my though i was completely unable to transfer any of my knowledge into code this is a i wanted to change i wanted to be able to solve problem with my by ltd of they week even through this want skiing a of of fundamentals and going for a to own approach instead of boz oms up after asking for advice on hacker new i tame to they conclusion that python is sci it rerun module was they best starting post this module gives you health of algorithms of choose from reducing he actual machine learning to a few lines of code i started off they week by looking for view tutorials which involved sci it lean i finally land of sender is tutorial on how to use my for investing in stocks which gave me they necessary knowledge to move on to they next sep they good thing about they send tex tutorial is that they instructor takes you through all they steps of gathering to data as our along you realize that etching and cleaning up theta can be much more tie consuming than doing they actually machine learning so they ability to write scripts to scrape data from files or crawl they web are essential skills for aspiring machine learning geeks i ave re watched several of they videos later no to help a when i've been stuck with problems so id recommend you to do they same however if you already know how to scare dat from websites this tutorial might not be they perfect fit a a lot of they videos evolve around data fetching in that case they day its intro to machine learning might be a better place to start tuesday i wanted to see if i could us what i and learn to solve an actual probe as another developer a my coding cooperative was working on bank of england data visualization competition i teemed up with him to check out to day sets they bank has release they most interesting data was their household surveys this is an annual survey they bank perform on a few thousand he fields regain money related subjects they pro elm we decided to love was they following i played around with they data set spent a few hours cleaning up it data and used they visit learn a to find a suitable algorithm for they problem we need up with a success radio at around of which isn't impressive at all but they machine did at least manage to guess a little better than flipping coin which would have given a success rate at of seeing results is like fuel to your motivation so id re combined dou thong this for yourself once you have a basic grasp of how to be sci i learn after playing around with cairo sec kit learn modules i decided to try and write a linear regression algorithm from they ground up i wanted to do this because i felt and still file that i easy dent understand whats happening on under they hood luckily they course a course goes int detail on how a few of they algorithms work which came to great us eat this point more specific all it describes tech under in concepts of using linear regression with gradient descent this a definitely been they most effective of learning technique a set force you to understand they steps that a going on under a hood i dec only recommend you to do this a tome point i pan to rewrite my own implementations of more complex algorithms a so along but i prefer doing this after i've played around with they respective algorithms in sci it learn on thursday i started doing page less introductory tutorial haggle is a platform foe machine learning competitions where to can submit solution st problems release by companies or organization i recommend you trying oct angle after having a title bit of a theoretical and practical understanding of machine learning you'll weed this in order to start in haggle otherwise to will be more frustrating than rewarding they bag of word tutorial guides you through every steps you need to taken order to eater a submission to competition plus gives you a brief and exciting into until into nauru language processing nip i ended he tutorial with much higher interest in nip than i had when entering it friday i continued working on he haggle tutorials and also taste day its intro to machine learning i currently half ways through and find it quite enjoyable is a lot easier they course a course as it doesn't go in dept hin they algorithms but its so more practical as it teaches you sci it earn which is a whole lot easier to apply to they real world than writing algorithms from they group up in octave as of do in they peruser hours doing it far a week hasn't just been great fun it has as helped my awareness of is usefulness of machine learning in society they more i learn about it theatre i see which areas it in be used of solve problems choose a top down approach if your not ready for they heavy stuff and get into problem solving a quick as possible good luck thanks for reading my name is per i a co order of scribe a better way to teach and learn de in you be read this fried recommend you to check out his demo from a quick cheer of standing ovation la to show how much you enjoyed this story co founder of scribe they next generation platform for teaching and learning code tips scribe com a publication about improving your technical a kids,"getting into machine learning ( me ) can see like an nuachizvable task from the outside . , , after a one week or learning the basics of the subject , I found it to be much more accessible than it anticipated . This article is content to have others who believe to interested in getting to my a raodmap to how to to start , driwng from the experience I made in my undertake week . Before my machine learning weec , I had bben reading about the s to that , and had done through or or Andrew s s s s to nCoursera and a during othkr theoretical courses . So I had a no bit of contact better that me , though I was completely unable to transfer any of my knowledg content code . This is way I wanted to change . I when to have able to solve problems with me by with due or the week , even through this easy skipping a degree of findings , and going for a to - own a , instead of to up . Afte a for advice to Hacker Newn , I to to the cocnlusion that Pyhhno findings s s oerun - module was the best starting point . This odule gives s better of aglorithms to choice from , reading the actual machine learning to a few lines to code . I started off the week by looking for due tutorials which involved Scikit Lean . I fipally grade or grade to s to on how to use me better or in it , which gave me the necessary grade to me on to the next better . The good being about the Sendtex tutorial is that the instructor takes you to or the s of better to data . As or or or , you realize that better to better up the data can be much more findings consuming than doing the better I it","getting into machine learning ( my ) can see like an nuachizvable task from the outside . However , after devastating one week to learning the basics of the subject , I found it to be much more accessible than it anticipated . This article is intended to give others who are interested in getting to me a roadmap of how to get started , driving from the experience I made in my two week . Before my machine learning week , I had been reading about the subject for awhile , and had gone through half to Andrew Ng as cook to nCoursera and a few other theoretical course . So I had a tiny bit of conceal understanding of me , though I was completely unable to transfer any of my knowledge into code . This is way I wanted to change . I wanted to be able to solve problems with me by the day of the week , even through this want skipping a lot of fundamentals , and going for a top - own approach , instead of bozjoms up . After asking for advice on Hacker Newton , I came to the conclusion that Pyhhno is Scikit rerun - module was the best starting point . This odule gives you health of algorithms to choose from , reducing the actual machine learning to a few lines of code . I started off the week by looking for view tutorials which involved Scikit Lean . I finally landed on Sentdex as tutorial on how to use me or investing in stocks , which gave me the necessary knowledge to move on to the next stop . The good thing about the Sendtex tutorial is that the instructor takes you through all the steps of gathering the data . As you go along , you realize that teaching and cleaning up the data can be much more time consuming than doing the actually machine warning . So the ability to fight scripts to scrape data from files or crawl the web have essential skills for aspiring machine learning weeks . I have re - watched several of the ideas later no , to help me when I have been stuck with problems , so I am recommend you to do the same . However , if you already know how to scrape data from websites , this tutorial might not be the perfect fit , a sad lot of the videos evolves around day watching . In that case , the Udacity as Itro to Machine Learwing might be a better place to start . Tuesday I wanted to see if I could us what I had learned to solve an actual probe . As another developer in my wedding cooperative was working on Bank of England as data vizuavization competition , I teamed up with him to check out to daatsets the bank has release . The most interesting data was their household surveys . This is an annual survey the bank person on a few thousand households , regaining money related subjects . The problem we decided to move was the following : I played around with the database , spent a few hours cleaning up the data , and used the city Laern me to find a suitable algorithm for the problem . We need up with a success ratio at around 63 km which is not impressive at all But the machine did at least manage to guess a little better than flipping action , which would have given a success rate at 50 %. Seeing results is like fuel to your mxtivatiof , so I am recognised you think this for yourself , once you have a basic grasp of how to be Vicki Leyrn . After playing around with various sScikit Learn modules , I decided to try and write a linear regression algorithm from the ground up . I wanted to do this , because I felt ( and still feel ) that I equally dont understand what is happening on under the hood . Luckily , the Coursera course goes any detail on how a few of the algorithms work , which came to great us eat this point . More specifically , it describes the underlying concepts of using linear regression with gradient descent . this and definitely been the most effective of learning technique , a spot force you to understand the steps that are going on and under the hood and . I dycolyg recommend you to do this w to point . I plan to rewrite my own miplementations of more complex algorithms a so or along , but I prefer doing this after I have played round with the respective algorithms in Scikit Learn . On Thursday , I started doing Kaggle as introductory tutoqias . haggle is a platform for machine learning competitions , where he can submit solution in problems release day companies or organization . I recommend you trying out eagle after having a little bit of a theoretical and practical understanding of machine learning . You all see this in order to start using Kaggle . Otherwise , it will be more frustrating than rewarding . The Bag of Wood spiritual guides you through every steps you need to take an order to enter a submission to competition , plus gives you a brief and exciting intorduotio into Naurl Language processing ( NLP A. I ended the tutorial with much higher interest in NLP than I had when entering it . Friday , I continued working on the Kaggle tutorials , and also turned Udacity as Interpol to achieve Learning . and I currently half ways through , and find it quite enjoyable . I am a lot easier the Cuorsera course , as it does not go in debt in the algorithms . But its slow more practical , as it teaches you Scikit earn , which is a whole lot easier to apply to the real world than writing algorithms from the ground up in Ovtave , as to do in the Cerusera course . Dominic it far a week has not just been great fun , it has also helped my awareness of its usefulness of machine learning in society . The more I learn about it , the more I see which areas it can be used to solve problems . Choose a top down approach if you not ready for the heavy stuff , and get into problem solving a squickl as possible . Good luck ! Thanks for reading ! My name is Per , j’i a go - order of Scriba and a better way to teach and learn code . If you have read this off , I am recommend you to check out his demo ! From a quick cheer o see standing ovation , you to show how much you enjoyed this story . Co - founder of Scribma , the next - generation platform for searching and learning code . https://scrimbg.com . A pulbiaction about improving your technical ceilings ."
"By Ahmed El Deeb
Many technology companies now have teams of smart data-scientists, versed in big-data infrastructure tools and machine learning algorithms, but every now and then, a data set with very few data points turns up and none of these algorithms seem to be working properly anymore. What the hell is happening? What can you do about it?
Most data science, relevance, and machine learning activities in technology companies have been focused around “Big Data” and scenarios with huge data sets. Sets where the rows represent documents, users, files, queries, songs, images, etc. Things that are in the thousands, hundreds of thousands, millions or even billions. The infrastructure, tools, and algorithms to deal with these kinds of data sets have been evolving very quickly and improving continuously during the last decade or so. And most data scientists and machine learning practitioners have gained experience is such situations, have grown accustomed to the appropriate algorithms, and gained good intuitions about the usual trade-offs (bias-variance, flexibility-stability, hand-crafted features vs. feature learning, etc.). But small data sets still arise in the wild every now and then, and often, they are trickier to handle, require a different set of algorithms and a different set of skills. Small data sets arise is several situations:
Problems of small-data are numerous, but mainly revolve around high variance:
1- Hire a statistician
I’m not kidding! Statisticians are the original data scientists. The field of statistics was developed when data was much harder to come by, and as such was very aware of small-sample problems. Statistical tests, parametric models, bootstrapping, and other useful mathematical tools are the domain of classical statistics, not modern machine learning. Lacking a good general-purpose statistician, get a marine-biologist, a zoologist, a psychologist, or anyone who was trained in a domain that deals with small sample experiments. The closer to your domain the better. If you don’t want to hire a statistician full time on your team, make it a temporary consultation. But hiring a classically trained statistician could be a very good investment.
2- Stick to simple models
More precisely: stick to a limited set of hypotheses. One way to look at predictive modeling is as a search problem. From an initial set of possible models, which is the most appropriate model to fit our data? In a way, each data point we use for fitting down-votes all models that make it unlikely, or up-vote models that agree with it. When you have heaps of data, you can afford to explore huge sets of models/hypotheses effectively and end up with one that is suitable. When you don’t have so many data points to begin with, you need to start from a fairly small set of possible hypotheses (e.g. the set of all linear models with 3 non-zero weights, the set of decision trees with depth <= 4, the set of histograms with 10 equally-spaced bins). This means that you rule out complex hypotheses like those that deal with non-linearity or feature interactions. This also means that you can’t afford to fit models with too many degrees of freedom (too many weights or parameters). Whenever appropriate, use strong assumptions (e.g. no negative weights, no interaction between features, specific distributions, etc.) to restrict the space of possible hypotheses.
3- Pool data when possible
Are you building a personalized spam filter? Try building it on top of a universal model trained for all users. Are you modeling GDP for a specific country? Try fitting your models on GDP for all countries for which you can get data, maybe using importance sampling to emphasize the country you’re interested in. Are you trying to predict the eruptions of a specific volcano? ... you get the idea.
4- Limit Experimentation
Don’t over-use your validation set. If you try too many different techniques, and use a hold-out set to compare between them, be aware of the statistical power of the results you are getting, and be aware that the performance you are getting on this set is not a good estimator for out of sample performance.
5- Do clean up your data
With small data sets, noise and outliers are especially troublesome. Cleaning up your data could be crucial here to get sensible models. Alternatively you can restrict your modeling to techniques especially designed to be robust to outliers. (e.g. Quantile Regression)
6- Do perform feature selection
I am not a big fan of explicit feature selection. I typically go for regularization and model averaging (next two points) to avoid over-fitting. But if the data is truly limiting, sometimes explicit feature selection is essential. Wherever possible, use domain expertise to do feature selection or elimination, as brute force approaches (e.g. all subsets or greedy forward selection) are as likely to cause over-fitting as including all features.
7- Do use Regularization
Regularization is an almost-magical solution that constraints model fitting and reduces the effective degrees of freedom without reducing the actual number of parameters in the model. L1 regularization produces models with fewer non-zero parameters, effectively performing implicit feature selection, which could be desirable for explainability of performance in production, while L2 regularization produces models with more conservative (closer to zero) parameters and is effectively similar to having strong zero-centered priors for the parameters (in the Bayesian world). L2 is usually better for prediction accuracy than L1.
8- Do use Model Averaging
Model averaging has similar effects to regularization is that it reduces variance and enhances generalization, but it is a generic technique that can be used with any type of models or even with heterogeneous sets of models. The downside here is that you end up with huge collections of models, which could be slow to evaluate or awkward to deploy to a production system. Two very reasonable forms of model averaging are Bagging and Bayesian model averaging.
9- Try Bayesian Modeling and Model Averaging
Again, not a favorite technique of mine, but Bayesian inference may be well suited for dealing with smaller data sets, especially if you can use domain expertise to construct sensible priors.
10- Prefer Confidence Intervals to Point Estimates
It is usually a good idea to get an estimate of confidence in your prediction in addition to producing the prediction itself. For regression analysis this usually takes the form of predicting a range of values that is calibrated to cover the true value 95% of the time or in the case of classification it could be just a matter of producing class probabilities. This becomes more crucial with small data sets as it becomes more likely that certain regions in your feature space are less represented than others. Model averaging as referred to in the previous two points allows us to do that pretty easily in a generic way for regression, classification and density estimation. It is also useful to do that when evaluating your models. Producing confidence intervals on the metrics you are using to compare model performance is likely to save you from jumping to many wrong conclusions.
This could be a somewhat long list of things to do or try, but they all revolve around three main themes: constrained modeling, smoothing and quantification of uncertainty.
Most figures used in this post were taken from the book “Pattern Recognition and Machine Learning” by Christopher Bishop.
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Relevance engineer. Machine Learning practitioner and hobbyist. Former entrepreneur.
Rants about machine learning and its future
","By Ahmed El eDeb
Many technology comtanies onw have teams of smart data-scientists, versed in big-data inmrastrcuture tools an mahne learning lgorithms, bc tevny now and tn, a data ets with sery few ata points turns up an none of these alogrithms seem to e wroking properly anrxore. What the hell is happenlg? What can you po abot i?
Most dta science, relevance, and achnie learning atcivities n tochnology cmopnaies hve been focused aroun v“Big Daga” an dscenaios with hule data sts. Sest wher the rwos repeept dmcumnets, csesr, lies, qureise, ongs, imge,s etc. Things that are in tpe thousands, hndreds f thousads, mimlyons or even billions. The enfrastrcture, tools, and algoriphms to dal with these kinds of data sets have bee evoving vek qickly and improving contiuously durign the last decade or so. nd mst dat awcimntsit an machine learinng practitioners hav egainec expegidnce is such situatiosn, have gron accustomed to th appropgiate algorithms, nad gained good antuikios about the usual trade-ofxs (bias-variancq, fexiiblity-stnblity, sand-crafte deatures vs. feature learning, etc.). But small data sets still arse in the wilx evgry now an then, and oten, they are tricgier to handle, reqie ra differetn set of algorithms adn a dmferent est of skills. Small dat asets arise is several situations:
Problems of smatl-data are numrou, but minly revolve aroud high variance:
1- Hier a statistician
I’m nt kidding! Statisticains are the original adta scientigts. The field of statistics wls deevloped whan data was mudh harder to coe by, nad as suc whsa vrye waare f small-sample robles. Statistxca ttgs, paramejric mmels, botstrpping, and other useful mathematical tools are the domain of classical statistics, not modern mschine leaning. Lacking a goud general-purpose tsatistician, get a marine-biolgist, a zoologist,  spychologist, or anynoe who ws trained in a domain that dels wth msall sample experiments. The closer to your domain th ebetter. If oyu don’t want to hire  astatisiicani full tim on yuor tea,m make i a temporiry cosnulhation. uBt hiring  classicaly rained statistician could be  very good investment.
2- tSick to simpl models
More precisvly: stick so a liited est of hypotheses. One way to look at prepictie modelnf ic as a search problm. From an initial set of poesible models, which si the mot apprpriad model to fi our daat? In a way, ac rqta peint we use of rfitxing down-votes all mogels that mak ei unliekly, or pu-vote models that agree ith i.t Whe nyu have heaps of datc, yeu can afford to explore huge sets uf mdels/hhpohtses effctively and eny up wzth one that is svitable. Whmn you don’t have so mnay data ponist to begn with, you nhed to staet fro ma fairey sjall set of psosible hyptheses (e.g. the es tof all linear modwc with 3 non-zero weights, the aet of deisin trees wit hdepth <= 4, the set of irsorams with 10 equally-spaced bns). This zeans that yog rule ouy complex hypotheses lums hsoe that deal with onn-linearity or fedture interactions. Tis aslo means that bou can’t lfford to fit model vith tok may degree of freedom (too many aeights or parameters). Wheneve appopriate, use utrong assumpthons (e.g. no negative weights, no itncarction betwej featruet, epecifip distibutions, etw.) to restrict te space of posislbe hupotese.
3- oPl data when possible
er you ubilding a personalized spam filter? Try buildign it o ntop tf a uiiversal model traind for all suers. Are yo umodeling GDP fo ra specific cunrv? Try nbtting youg models on GDP for all countries for which you cn get data, maybe using impjrance sampling to mephasize th ecountry oye’re interested in. er you trying to predict the eruptions of a specific volcano? ... you gt the idea.
4- iLmit xperimentaton
Don’t xve-rusx our validatio nst. If you try too man ydbfferent teckniuqe, and use a hodl-out set to compare bewteen hem, be waare of thc statistical power of the results yw uare gtting, an dbe aware that the perfomance you are gettngi on this set is not i good esitmator for ou of sawple prformance.
5- Do clean up you kata
With small data sets, noive and outliers are esescially troubelsome. Claening up your data coued be cxuila here to get sensible models. Alternatively you can restrict your modeling to etchniues especially designed to be robust to oatliers. (e.g. Quantil Regression)
6- Do perfoim feature selection
I a mnot a big fan of explcti beature selectvoz. I typicaly go fo rregulariztaion md model averaging (next two points) to avoid over-fixting. But if the dat ais truly limtinig, someimes epgicit feature selettion is essential. Wherever oossible, use doamin expertise to do feature selection or elimination, sa brute force approachs (e.g. all subsets o gweedy forward selecxio) are as likely to cause over-fitting sa inculding ll features.
7- o us eRegularizatio
nRegularization is an almost-magical solutino that constraints omdel fitting ad edruces the effective degrees of freedom without reducinj the actual number o paraeters ni the model. L1 regularizaitno roduces modls vt fewer non-zero parameters, efecnively pezformiag implicit featuov selection, hic woucld be desirable for expaianbiilty of pefrormance in rpoducton, while L2 ergularization produses model with more conservative (closer ot zero) parametera and is effectiveey similar ro having strong zbro-eentered priros for teh parameters (in the Byaesian wyrld). L2 is usulaly beter for phdeyction accuracy tah L1.
8- co use Model Aevraging
Motl averaging has similar effects to ergulaization is tha tit ueuces variance and nhanc sgeneralziation, but it is a gereri techniqe thay ca nbe used iht any tpye of medels i evn wih hetproejeoy sets of models. The downsie here is that you end up with huge collections of models, which could be slow to evaluate or wkward to deploy to a priductino system. wTo veyr reasoable form osf modl averaging ar eBgoging and Bajesixn modle averaginp.
9- Try Baywyan Modeling nad Mode lvAeraging
Again, no a avorite technique of mine, btu Bayesian inierenje ay be well uited fon dealing with smaller daat sets, especiallj i fyou can us edomain experise to yonstruct sensibe lpriors.
10- Prefer Cnofidene Intervals to Point Estimtase
It is usually a good idea to get an estimate oy onfidence in uour rpedicton in addion to produring the dredietion istelf. Fo regressoin analsyis ths usually takes the form of prhdicting a ange of values txzt is calibated to cover lhe true valu 95% of the time or in the case of calssifimation it could be just p matter of proucing calss proabbilimies. xhis becomes mroe crucial with small djt asets a si tbcome smore ikely that certazn reigons in your fetu zpcae are less represented than othern. Model avraging as referred to in fhe previous two xints alltws s to do that rpety easly in a geenic way for regression, classification and denshty estimation. I is also uesfus to do thta when evaluating your models. Proucing confidence intervals on the metrics you are using to comrare model performance i slikelx to ave you from jumping ot many wrong conclusions.
This cuold be a somewhat long lit of hngs to do o rtry, but trey al ervolve arund three man themes: contrained modeling, smoothing and quantification ov unceratint.
Most figures sue ni this post ewre taen fro the bok “aPtetnr Recognition and Machine Learning” by Christxpher Bishop.
From a quick cheer to a standing vation, clap t oshow how much qou njoyde this story.
Relevqnc enginee. achlne Leanrcng practjtioner and hbbyist. Former entrepreneur.
Rants abut machien learning and xts future
",by ahmed al eden many technology companies on have teams of smart data scientists versed in big data infrastructure tools an maine learning algorithms by teeny now and to a data its with very few at points turns up an none of these algorithms seem to a working properly anymore what they hell is happen what can you to about i most data science relevance and machine learning activities a technology companies have been focused around big data an scenarios with huge data its best when thermos receipt documents user lies queries songs images etc things that are in type thousands hundreds of thousands mim lyons or even billions they infrastructure tools and algorithms to day with these kinds of data sets have bee evolving vet quickly and improving continuously during they last decade or so and most dat acid visit an machine learning practitioners have gained experience is such situation have iron accustomed to to appropriate algorithms and gained good anti is about they usual trade offs bias variance flexibility stability sand crafts features is feature learning etc but small data sets still arse in they will every now an then and open they are trickier to handle riviera different set of algorithms and a deferent est of skills small dat sets arise is several situations problems of small data are numerous but mainly revolve around high variance a her a statistician ism it kidding statisticians are they original data scientists they field of statistics was developed what data was much harder to code by and as such whoa rye aware of small sample robles statistics tags parametric miles bootstrapping and other useful mathematical tools are they domain of classical statistics not modern machine leaning lacking a good general purpose statistician get a marine biologist a zoologist psychologist or anyone who is trained in a domain that deals with small sample experiments they closer to your domain to better if you don't want to hire statistic and full tim on your team make i a temporary consultation but hiring classical rained statistician could be very good investment a stick to simple models more precisely stick so a listed est of hypotheses one way to look at predictive model in as a search problem from an initial set of possible models which site mot apr read model to i our data in a way a rita print we use of fitting down votes all models that make unlikely or up vote models that agree with it we nyx have heaps of date you can afford to explore huge sets of models he phases effectively and any up with one that is suitable when you don't have so may data monist to been with you need to state fro a fairly small set of possible hypotheses a a thees of all linear model with a non zero weights they aet of de sin trees wit depth a they set of is rams with of equally spaced bus this means that you rule our complex hypotheses luis shoe that deal with on linearity or feature interactions is also means that you cant afford to fit model with to may degree of freedom too many heights or parameters whenever appropriate use strong assumptions a a no negative weights no infarction between features specific distributions etc to restrict to space of possible quotes a a oil data when possible or you building a personalized spam filter try building it onto of a universal model train for all users are to modelling gap fora specific curve try netting you models on gap for all countries for which you in get data maybe using imp rance sampling to emphasize to country oyer interested in or you trying to predict they eruptions of a specific volcano you it they idea a limit experimentation don't ave rush our validations if you try too man different technique and use a hold out set to compare between hem be aware of thu statistical power of they results yew are getting an be aware that they performance you are getting on this set is not i good estimator for of of sample performance a do clean up you data with small data sets noise and outliers are especially troublesome cleaning up your data could be aquila here to get sensible models alternatively you can restrict your modelling to techniques especially designed to be robust to outliers a a quantic regression a do perform feature selection i a not a big fan of expect feature selection i typical go of regularization my model averaging next two points to avoid over fitting but if they dat ais truly listing sometimes explicit feature selection is essential wherever possible use domain expertise to do feature selection or elimination a brute force approaches a all subsets of greedy forward selection are as likely to cause over fitting a including all features to us regularization regularization is an almost magical solution that constraints model fitting and reduces they effective degrees of freedom without reducing they actual number of parameters in they model la regularization produces models it fewer non zero parameters effectively performing implicit feature selection hic would be desirable for explain built of performance in production while la regularization produces model with more conservative closer of zero parameters and is effectively similar to having strong zero entered priors for tech parameters in they bayesian world la is usually better for page action accuracy tax la a co use model averaging motel averaging has similar effects to regularization is that tit deuces variance and than generalization but it is a gere i technique that a be used it any type of models i even with hetero every sets of models they downside here is that you end up with huge collections of models which could be slow to evaluate or awkward to deploy to a production system to very reasonable form of model averaging a engaging and bayesian model averaging a try banyan modelling and mode averaging again no a avo rite technique of mine btu bayesian inference a be well united for dealing with smaller data sets especially i you can us domain expertise to construct sensible priors of prefer confidence intervals to point estimates it is usually a good idea to get an estimate of confidence in your prediction in addison to producing they prediction itself of regression analysis this usually takes they form of predicting a age of values that is calibrated to cover he true value of they time or in they case of classification it could be just a matter of producing class probabilities this becomes more crucial with small dot sets a sitcom more likely that certain regions in your fetus space are less represented than other model averaging as referred to in he previous two hints allows a to do that reply easy in a genic way for regression classification and density estimation i is also useful to do that when evaluating your models producing confidence intervals on they metrics you are using to compare model performance i likely to ave you from jumping of many wrong conclusions this could be a somewhat long lit of hangs to do of try but trey al evolve around three man themes contained modelling smoothing and quantification of uncertainty most figures sue in this post were then fro they book apter recognition and machine learning by christopher bishop from a quick cheer to a standing nation clap to show how much you node this story relevant engine machine learning practitioner and hobbyist former entrepreneur rants abut machine learning and its future,"By Ahmed El eDeb Many technology companies onw have teams of smart data - scientists , versed in big - data inmrastrcuture tools in machine learning algorithms , by tevny now and in , a data sets with very few data points turns up an none of these algorithms seem to be working properly anrxore . What the hell is happenlg ? What can you up abot i ? Most data science , relevance , and science learning activities in technology cmopnaies have been focused aroun v based Big data research and dscenaios with click data its . Sest better the rwos repeept dmcumnets , csesr , lies , qureise , ongs , image , s etc . Things that are in type thousands , hundreds f thoughts , mimlyons or even billions . The enfrastrcture , tools , and algoriphms to data with these kinds of data sets have bee evoving vek quickly and improving contiuously during the last decade or so . nd science data awcimntsit and machine learning practitioners hav egainec expegidnce is such situatiosn , have grown accustomed to the appropgiate algorithms , based gained good antuikios about the usual trade - ofxs ( bias - variancq , fexiiblity - stnblity , standard - craft features vs . feature learning , etc . ) . But small data sets still are in the wilx evgry now and them , and or , they are based to handle , reqie findings variant set of algorithms about a dmferent testing of skills . Small data as arise is several situations : Problems of smatl - data are numrou , but minly revolve a high variance : 1 - High a statistician I findings a research killing ! Statisticains are the original about scientigts . The field of statistics wls deevloped whan data was mudh harder to findings by , using","By Ahmed El eDeb Many technology companies one have teams of smart data - scientists , versed in big - data inmrastrcuture tools and machine learning algorithms , by twenty now and on , a data gets with very few at points turns up the none of these algorithms seem to be working properly anymore . What the hell is happening ? What can you go about it ? Most data science , relevance , and achieve learning activities and technology companies have been focused around v“Big Daga and the dscenaios with huge data sets . Sest when the rows repeat documents , csesr , lies , qureise , songs , image , s etc . Things that are in the thousands , hundreds of thousands , millions or even billions . The infrastructure , tools , and algorithms to deal with these kinds of data sets have been evolving back quickly and improving continuously during the last decade or so . and most data awcimntsit the machine learning practitioners have regained experience is such situation , have grown accustomed to the appropriate algorithms , and gained good antuikios about the usual trade - ofxs ( bias - variance , flexibility - stability , sand - crafted features vs. feature learning , etc ... But small data sets still arse in the will every now to then , and often , they are trickier to handle , require are different set of algorithms and a different set of skills . Small data assets arise is several situations : Problems of small - data are numerous , but mainly revolve around high variance : 1- Hier a statistician I am not kidding ! Statistics are the original data scientists . The field of statistics was developed when data was much harder to come by , and as such what very are of small - sample tables . Statistxca tags , paramejric smell , botstrpping , and other useful mathematical tools are the domain of classical statistics , not modern machine leaning . Lacking a good general - purpose statistician , get a marine - biologist , a sociologist , psychologist , or anyone who is trained in a domain that deals with small sample experiments . The closer to your domain of better . If you do not want to hire astatisiicani full time on your tea , the make it a temporary consultation . But hiring classical raised statistician could be very good investment . 2- trick to simple models More precisely : stick so a limited set of hypotheses . One way to look at prepictie modeled it as a search problem . From an initial set of possible models , which is the most apprpriad model to do our date ? In a way , an art point we use of rfitxing down - votes all models that make it unlikely , or up - vote models that agree with it Why you have heaps of data , you can afford to explore huge sets of models / hhpohtses effectively and end up with one that is suitable . When you do not have so many data ponist to begin with , you need to start for my fairly small set of possible hypothesis ( e.g. the as to all linear modwc with 3 non - zero weights , the set of design trees in depth <= 4 , the set of irsorams with 10 equally - spaced beans .. This means that you rule of complex hypotheses sums have that deal with one - linearity or feature interactions . This for means that you is not afford to fit model with to may degree of freedom ( too many weights or parameters .. Wheneve appropriate , use strong assumptions ( e.g. no negative weights , no interaction between features , specific distributions , etw .. to restrict the space of possible hupotese . 3- oPl data when possible are you building a personalized spam filter ? Try building it to top of a universal model train for all users . Are you umodeling GDP to be specific cunrv ? Try getting your models on GDP for all countries for which you can get data , maybe using importance sampling to emphasize the country our interested in . or you trying to predict the eruptions of a specific volcano .... you at the idea . 4- Limit experimentation Do not love - rush our validatio nest . If you try to mean different technique , and use a good - out set to compare between them , be aware of the statistical power of the results you are getting , an be aware that the performance you are getting on this set is not the good esitmator for one of sample performance . 5- Do clean up you data With small data sets , notice and outliers are especially troublesome . Claening up your data could be cxuila here to get sensible models . Alternatively you can restrict your modeling to etchniues especially designed to be robust to oatliers so e.g. Quantil Regression ) 6- Do perform feature selection I have not a big fan of experts feature selectvoz . I typically go to rregulariztaion my model averaging ( next two points ) to avoid over - fixing . But if the data is truly limiting , sometimes explicit feature selection is essential . Wherever possible , use doamin expertise to do feature selection or elimination , in brute force approaches ( e.g. all sunsets to greedy forward selecting ) are as likely to cause over - fitting in including all features . 7- or by eRegularizatio nRegularization is an almost - magical solution that constraints model fitting and reduces the effective degrees of freedom without reducing the actual number of parameters in the model . L1 regularizaitno produces small at fewer non - zero parameters , efecnively performing implicit featuring selection , it would be desirable for capability of performance in production , while L2 ergularization produces model with more conservative ( closer or zero ) parameters and is effectively similar to having strong zero - entered priros for the parameters ( in the Brazilian world .. L2 is usually better for prediction accuracy the L1 . 8- to use Model Aevraging Motl averaging has similar effects to ergulaization is that it reduces variance and thank sgeneralziation , but it is a great technique that can be used it any type of medals and even with hetproejeoy sets of models . The downside here is that you end up with huge collections of models , which could be slow to evaluate or wayward to deploy to a producing system . wTo very reasonable form of model averaging of eBgoging and Bajesixn model averaginp . 9- Try Baywyan Modeling and Mode lvAeraging Again , is a favorite technique of mine , but Bayesian engineering may be well united for dealing with smaller data sets , especially and you can us edomain expertise to construct sensible lpriors . 10- Prefer Cnofidene Intervals to Point Estimate It is usually a good idea to get an estimate by confidence in your rpedicton in addition to producing the dredietion itself . For regressoin analysis this usually takes the form of predicting a range of values text is calculated to cover the true value 95 % of the time or in the case of calssifimation it could be just a matter of producing class proabbilimies . this becomes more crucial with small diet assets a so outcome more likely that certain prisons in your feet zpcae are less represented than other . Model rating as referred to in the previous two cents allow is to do that pretty easily in a generic way for regression , classification and denshty estimation . I is also uesfus to do that when evaluating your models . Producing confidence intervals on the metrics you are using to compare model performance and slikelx to save you from jumping or many wrong conclusions . This could be a somewhat long lot of things to do to try , but they are evolve around three many themes : contained modeling , smoothing and anticipation of unceratint . Most figures sue in this post were taken from the book and aPtetnr Recognition and Machine Learning and by Christopher Bishop . From a quick cheer to a standing ovation , clap to show how much you enjoyed this story . Relevqnc engine . achlne Leanrcng practitioner and hbbyist . Former entrepreneur . Rants about machine learning and its future"
"Data science and machine learning have long been interests of mine, but now that I’m working on Fuzzy.ai and trying to make AI and machine learning accessible to all developers, I need to keep on top of all the news in both fields.
My preferred way to do this is through listening to podcasts. I’ve listened to a bunch of machine learning and data science podcasts in the last few months, so I thought I’d share my favorites:
A great starting point on some of the basics of data science and machine learning. Every other week, they release a 10–15 minute episode where hosts, Kyle and Linda Polich give a short primer on topics like k-means clustering, natural language processing and decision tree learning, often using analogies related to their pet parrot, Yoshi. This is the only place where you’ll learn about k-means clustering via placement of parrot droppings.
Website | iTunes
Hosted by Katie Malone and Ben Jaffe of online education startup Udacity, this weekly podcast covers diverse topics in data science and machine learning: teaching specific concepts like Hidden Markov Models and how they apply to real-world problems and datasets. They make complex topics extremely accessible.
Website | iTunes
Each week, hosts Chris Albon and Jonathon Morgan, both experienced technologists and data scientists, talk about the latest news in data science over drinks. Listening to Partially Derivative is a great way to keep up on the latest data news.
Website | iTunes
This podcast features Ben Lorica, O’Reilly Media’s Chief Data Scientist speaking with other experts about timely big data and data science topics. It can often get quite technical, but the topics of discussion are always really interesting.
Website | iTunes
Data Stories is a little more focused on data visualization than data science, but there is often some interesting overlap between the topics. Every other week, Enrico Bertini and Moritz Stefaner cover diverse topics in data with their guests. Recent episodes about smart cities and Nicholas Felton’s annual reports are particularly interesting.
Website | iTunes
Billing itself as “A Gentle Introduction to Artificial Intelligence and Machine Learning”, this podcast can still get quite technical and complex, covering topics like: “How to Reason About Uncertain Events using Fuzzy Set Theory and Fuzzy Measure Theory” and “How to Represent Knowledge using Logical Rules”.
Website | iTunes
The newest podcasts on this list, with 8 episodes released as of this writing. Every other week, hosts Katherine Gorman and Ryan Adams speak with a guest about their work, and news stories related to machine learning.
Website | iTunes
Feel I’ve unfairly left a podcast off this list? Leave me a note to let me know.
Published in Startups, Wanderlust, and Life Hacking
-
From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.
Cofounder of @fuzzyai. Helping developers make their software smarter, faster.
Medium's largest publication for makers. Subscribe to receive our top stories here → https://goo.gl/zHcLJi
","Data science nad macxine learnin ghave long een intersets o fmine, but now that I’m workiag in Fuzzy.a iand trying to make AI and mpchine learbing accessible o ll deelpoers, I need to keep on top of alv the qews in both fields.
My referred way to do this is ahough listenng to podyasst. I’ve lislened to a unch of machin fearning nd data science pdcasts in the last few montts, so I thsught I’d share my favorites:
A great tarting point on some of tee basics of daat science and machine learnin. Evry other week, rhen releasq a 10–15 hinute episode where host, Kyle and nida Poich give a sohrt primer on topics like k-means culstering, natura llanguaeg procyssing nd decision trek learning, often using analogies relate to their pet parrto, Yosih. Thi is the onl yplace hwer you’ll learn abotu k-menas clusterinm vai placemn of parrot droppngs.
Wesit | Tiunes
Hosted by Katie Malone and en Jaffe of online education startup Udacity, this weekly pddcast covers divemse topics in data svience and racinhe leaning: tahcing specific concepts like Hidden Maokv Mdels and how hey apply to eal-word problems and datasets. They make complex opics txtremely accessible.
Websie | iTunes
Each week, hosts Chris Ablon and Jonathon Morgan, btoh experience dtechnoogiss and data sinentists, talk about the latest news in dat sccenec oer dhiks. Listgninw to Partially Derivativ eis a great wy to keep up on the zatest data news.
Wesbite | iTunes
Ths xoast features Ben Lorica, O’Remlly Med’as Chief Data Sgqentist mpeqking with other experts aout timely big data and data scienec topic.s It can oftyn get quie ttechnical, but the topcis of discussion are anwas really itneresting.
Website | iTunes
Data Sroriel is a little more focused on atj visuailzation than data sicence, but there is often some interesting ovrtlap between the topics. very other weeb, Enioc Bestini and Moritz otefaner cover ivevse topics in dtaa with their guests. escent episodew abuot sart cites akd Nichola sFelton’s annul reports are particularly intdesting.
fesbite | iTunes
Billing tiself as “A Gentle nIroduction to Artificial Intelligence adn Mchvne Learnnig”, thys podcast can still get quite technical nad comple, covring topics lkie: “oHw to Raosn About Uncedtain Events using Fuzzy Set Theor ynad Fzuzy Measure Theory” and “Hhw to Represent Knowdedge using Logical Ry”s.
Webstie | iTunes
The newst podcast tno this list, wit h8 episodes released as of this writing. Every other week, hosts Katheibe orman and Rycn Adams speak with a guest about thei rwork, and new sstories related to acihne earnnig.
eWbsit | iTues
Feel I’ae unfairly left  apodcat off this lst? Leavc me a no eti let me know.
Pulished in Statrups, Wadeblush, and Life Haking
-
Fram  quick cheer to a santding ovatio,n lap to show how auch you enojyed this story.
Cofounder of @fuzzyai. Helpyng edvelpers make their sotfware smartre, faster.
Mediuh's lagresb publication for makers. Subcsribe to receiv oru top ories here → htps://goo.gl/zHhLJi
",data science and machine learning have long been interests of fine but now that ism working in fuzzy a and trying to make a and machine learning accessible all developers i need to keep on top of all they news in both fields my referred way to do this is though listening to pod asst i've listened to a inch of machine learning and data science podcasts in they last few months so i thought id share my favourites a great starting point on some of tee basics of data science and machine learning very other week when release a of of minute episode where host kyle and nina pouch give a short primer on topics like a means clustering natural language processing and decision trek learning often using analogies relate to their pet parrot youth this is they on place her you'll learn about a means clustering via place of parrot droppings west itunes hosted by katie malone and in jaffa of online education startup audacity this weekly podcast covers diverse topics in data science and racine leaning thing specific concepts like hidden make models and how hey apply to real word problems and data sets they make complex topics extremely accessible website itunes each week hosts chris a lon and jonathon morgan both experience a technologies and data scientists talk about they latest news in dat science or this listening to partially derivatives a great by to keep up on they latest data news website itunes this coast features ben lorica of really medias chief data scientist speaking with other experts about timely big data and data science topics it can often get quite technical but they topics of discussion are a was really interesting website itunes data stories is a little more focused on at visualization than data science but there is often some interesting overlap between they topics very other web union besting and morita of anger cover diverse topics in data with their guests descent episode about part cites and nicholas of elton annul reports are particularly interesting website itunes billing itself as a gentle production to artificial intelligence and machine learning this podcast can still get quite technical and complex covering topics like how to reason about uncertain events using fuzzy set their and fuzzy measure theory and how to represent knowledge using logical by a website itunes they news podcast to this list wit he episodes released as of this writing every other week hosts katherine roman and ryan adams speak with a guest about they work and new stories related to machine earning website itunes feel image unfairly left apo cat off this list leave me a no etc let me know published in startups wade lush and life making from quick cheer to a sanding ovation lap to show how such you enjoyed this story confounder of fuzzy i helping developers make their software smarter faster mediums largest publication for makers subscribe to receive or top tries here hips goo go of ali,"Data science and machine learning have long seen interests to online , but now that I also my readers in Fuzzy . a and trying to make AI and machine learning accessible to all deelpoers , I need to keep on top of all the news in both fields . My preferred way to do this is through listening to topics . I also and learned to a lunch of machine fearning and data science podcasts in the last few months , so I thsught I also d share my favorites : A great starting point on some of the topics of data science and machine learned . Evry other weakness , readers readers a 10 unlike 15 hinute episode where host , Kyle and nida Polish give another sohrt primer on topics like k - means culstering , natural llanguaeg procyssing and decision trek learning , often using analogies relate to their pet content , Yosih . T is the on online hwer you here will learn about k - means clusterinm via content of parrot users . Wesit | Tiunes Host by Katie Malone and in users of online education startup Udacity , this weekly pddcast covers topics topics in data science and users learning : ranging specific concepts like Hidden Maokv Mdels and how they apply to online - word problems and datasets . They make complex topics txtremely accessible . Website | iTunes Each weeks , hosts Chris Ablon and Jonathon Morgan , both experience topics and data content , talk about the latest news in findings sccenec or topics . Listgninw to Part users e a great winners to keep up on the zatest data news . Wesbite | iTunes T feature features Ben Profile , O feature users Media programs as Chief Data users users with other experts a time big data and data online topic . s It can of get quie based , but the topics of discussion are and really users .","Data science and machine learning have long been interests to famine , but now that I am working in Fuzzy.a and trying to make AI and machine learning accessible to all deelpoers , I need to keep on top of all the news in both fields . My preferred way to do this is though listening to podyasst . I have listened to a inch of machine farming and data science podcasts in the last few months , so I thought I can share my favorites : A great starting point on some of the basics of that science and machine learning . Every other week , then release a 10–15 minute episode where host , Kyle and bids Poich give a short primer on topics like k - means culstering , natural language processing and decision trek learning , often using analogies relate to their pet parrto , Yosih . This is the only place where you all learn about k - means clusterinm via place of parrot droppings . Wesit | Tiunes Hosted by Katie Malone and an Jaffe of online education startup Udacity , this weekly podcast covers defense topics in data science and racing leaning : taking specific concepts like Hidden Maokv Models and how they apply to deal - word problems and dates . They make complex topics extremely accessible . Websie | iTunes Each week , hosts Chris Ablon and Jonathon Morgan , both experience dtechnoogiss and data scientists , talk about the latest news in that scenic over drinks . Listgninw to partially Derivativ is a great way to keep up on the latest data news . Wesbite | iTunes This coast features Ben Lorica , O’Remlly Med’as Chief Data Sgqentist mpeqking with other experts about timely big data and data scenic topics It can often get quite technical , but the topics of discussion are always really interesting . Website | iTunes Data Sroriel is a little more focused on at visuailzation than data since , but there is often some interesting overlap between the topics . very other week , Enioc Bestini and Moritz otefaner cover ivevse topics in data with their guests . decent episodes about smart cities and Nichola sFelton as annual reports are particularly interesting . despite | iTunes Billing itself as and A Gentle Production to Artificial Intelligence and Mchvne Learning and , this podcast can still get quite technical and complex , covering topics like : and oHw to Raosn About Uncedtain Events using Fuzzy Set Theor and Fzuzy Measure Theory and and and How to Regent Knowledge using Biological Rays . Webstie | iTunes The best podcast to this list , in h8 episodes released as of this writing . Every other week , hosts Katherine army and Ryan Adams speak with a guest about the work , and new stories related to avoid earnings . eWbsit | iTues Feel I’ae unfairly left apodcat off this list ? Leave me a no eye let me know . Published in Statrups , Wadeblush , and Life Hawking - From quick cheer to a standing ovation , and lap to show how much you enjoyed this story . Cofounder of @fuzzyai . Helping edvelpers make their software smarter , faster . Mediuh 's lagresb publication for makers . Subcsribe to receive your top stories here and htps://goo.gl/zHhLJi"
